<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2308.05168] A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision</title><meta property="og:description" content="Existing model evaluation tools mainly focus on evaluating classification models, leaving a gap in evaluating more complex models, such as object detection.
In this paper, we develop an open-source visual analysis tool‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2308.05168">

<!--Generated on Wed Feb 28 12:57:28 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\onlineid</span>
<p id="p1.2" class="ltx_p">1405
<span id="p1.2.1" class="ltx_ERROR undefined">\vgtccategory</span>Research












<span id="p1.2.2" class="ltx_ERROR undefined">\authorfooter</span>
C.¬†Chen, Y.¬†Guo, F.¬†Tian, W.¬†Yang, Z.¬†Wang, and S.¬†Liu are with the School of Software, BNRist, Tsinghua University. C.¬†Chen and Y.¬†Guo are joint first authors. S.¬†Liu is the corresponding author.
E-mail: {{ccj17, gyj22, tianfy21, yangwk21, wzw20}@mails., shixia@}tsinghua.edu.cn.
S.¬†Liu, and H.¬†Su are with the Department of Computer Science and Technology, Tsinghua University.
E-mail: slongliu86@gmail.com, suhangss@tsinghua.edu.cn.
J.¬†Wu is with Cardiff University.
E-mail: wuj11@cardiff.ac.uk.
H.¬†Pfister is with Harvard University.
E-mail: pfister@seas.harvard.edu.</p>
</div>
<h1 class="ltx_title ltx_title_document">A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Changjian Chen
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Yukai Guo
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Fengyuan Tian
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Shilong Liu
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Weikai Yang
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> 
<br class="ltx_break">Zhaowei Wang
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Jing Wu
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Hang Su
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Hanspeter Pfister
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
and Shixia Liu
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="1.1" class="ltx_p">Existing model evaluation tools mainly focus on evaluating classification models, leaving a gap in evaluating more complex models, such as object detection.
In this paper, we develop an open-source visual analysis tool, Uni-Evaluator, to support a unified model evaluation for classification, object detection, and instance segmentation in computer vision.
The key idea behind our method is to formulate both discrete and continuous predictions in different tasks as unified probability distributions.
Based on these distributions, we develop 1) a matrix-based visualization to provide an overview of model performance; 2) a table visualization to identify the problematic data subsets where the model performs poorly; 3) a grid visualization to display the samples of interest.
These visualizations work together to facilitate the model evaluation from a global overview to individual samples.
Two case studies demonstrate the effectiveness of Uni-Evaluator in evaluating model performance and making informed improvements.
</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>Model evaluation, computer vision, classification, object detection, instance segmentation
</div>
<div id="p2" class="ltx_para">
<span id="p2.1" class="ltx_ERROR undefined">\teaser</span><img src="/html/2308.05168/assets/x1.png" id="p2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="237" alt="[Uncaptioned image]">
<p id="p2.2" class="ltx_p ltx_align_center"><span id="p2.2.1" class="ltx_text ltx_caption">Uni-Evaluator:
(a) the filtering panel;
(b) the matrix-based visualization provides an overview of model performance;
(c) the table visualization helps identify problematic data subsets;
(d) the grid visualization displays the samples of interest.</span>


 

</p>
</div>
<div id="p3" class="ltx_para">
<p id="p3.1" class="ltx_p">Introduction</p>
</div>
<div id="p4" class="ltx_para">
<p id="p4.1" class="ltx_p">Model evaluation assesses performance of machine learning models and helps identify the causes of poor performance for further improvement¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>.
It is a critical step in the development of machine learning models.
For example, machine learning practitioners usually evaluate models generated with different parameters and select the best one for deployment.
Currently, the most widely-used way for evaluating models is to use performance metrics, such as accuracy and mAP (mean Average Precision).
However, using these metrics alone can sometimes be misleading.
For example, a classification model that always predicts the majority class in an imbalanced dataset can achieve high accuracy but has no predictive power at all¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>.
To better understand model performance, evaluation at finer granularity is required.
Confusion matrices¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> are thus developed, which provide evaluations at the class level by presenting confusion between classes.
However, they only focus on the classification task where the accuracy of classifying samples is of concern.
In addition to classification, there are other tasks that are popularly used in real-world applications¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>.
Their model evaluations concern more than the accuracy of sample-level classification.
For example, object detection concerns not only the accuracy of classifying objects in images, but also the precision of locating the objects, which is measured by mAP.
For these tasks, confusion matrices are not directly applicable.
In addition, like object detection, there are scenarios where multiple tasks are considered simultaneously¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>.
Although we can develop different fine-grained
model evaluation methods for different tasks, it will increase the learning cost and cognitive load of users when switching among them¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>.
Thus, a unified model evaluation method for different tasks in computer vision is desirable.</p>
</div>
<div id="p5" class="ltx_para">
<p id="p5.1" class="ltx_p">From a review of previous studies and a survey with 151 computer vision experts,
we identified two key challenges in developing a unified model evaluation method.
First, predictions in different tasks can be discrete (<em id="p5.1.1" class="ltx_emph ltx_font_italic">e.g</em>.‚Äâ, class labels) or continuous (<em id="p5.1.2" class="ltx_emph ltx_font_italic">e.g</em>.‚Äâ, positions of objects).
To analyze these predictions in a single tool, we need to model them in a unified way.
While previous research has explored how to model different discrete predictions¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>,
it is largely unexplored how to jointly model discrete and continuous predictions.
Second, users not only evaluate overall performance on the entire dataset, but are also interested in analyzing performance at different levels of detail, such as subsets and individual samples.
For example, users are interested in problematic subsets that are sliced along different attributes, such as class labels or object sizes.
Examples of such subsets include the samples of a class with low accuracy in a classification task and the small objects that the model failed to detect in an object detection task.
Finding these problematic subsets in a large dataset and examining their predictions are difficult and time-consuming.
Therefore, a multi-level exploration environment for efficient model evaluation is necessary.</p>
</div>
<div id="p6" class="ltx_para">
<p id="p6.1" class="ltx_p">To tackle these challenges, we develop Uni-Evaluator, a unified interactive model evaluation method for computer vision tasks.
We focus on three main tasks in computer vision: classification, object detection, and instance segmentation¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
To support a unified analysis of model performance in different tasks, we propose a unified formulation that extends the discrete probability formulation proposed by G√∂rtler¬†<em id="p6.1.1" class="ltx_emph ltx_font_italic">et al</em>.‚Äâ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.
The key idea is to formulate both discrete and continuous predictions as unified probability distributions.
Based on these distributions, we first develop a matrix-based visualization with three evaluation modes.
It provides an overview of model performance.
Next, a table visualization is developed based on LineUp¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.
It is supported by a frequent pattern mining-based search method to facilitate the identification of problematic data subsets.
In addition, a grid visualization is used to display the samples of interest.
With the three coordinated visualizations, users can comprehensively evaluate model performance from a global overview to individual samples and make informed improvements.
Uni-Evaluator relies only on the input and output of models to perform the evaluation, so it is model-agnostic.
Two case studies are conducted on object detection and instance segmentation, respectively, to validate the contributions of Uni-Evaluator.
A Python package is released at <a target="_blank" href="http://uni-evaluator.thuvis.org/" title="" class="ltx_ref ltx_href">http://uni-evaluator.thuvis.org/</a>.</p>
</div>
<div id="p7" class="ltx_para">
<p id="p7.1" class="ltx_p">The contributions of our work include:</p>
</div>
<div id="p8" class="ltx_para">
<ul id="S0.I1" class="ltx_itemize">
<li id="S0.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S0.I1.i1.p1" class="ltx_para ltx_noindent">
<p id="S0.I1.i1.p1.1" class="ltx_p"><span id="S0.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Coordinated visualizations</span> to explain model performance at different levels, which include a matrix-based visualization with three evaluation modes, a table visualization supported by frequent pattern mining-based search, and a grid visualization.</p>
</div>
</li>
<li id="S0.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S0.I1.i2.p1" class="ltx_para ltx_noindent">
<p id="S0.I1.i2.p1.1" class="ltx_p"><span id="S0.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">A unified probability distribution method</span> that jointly models discrete and continuous predictions for evaluating different models in one tool.</p>
</div>
</li>
<li id="S0.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S0.I1.i3.p1" class="ltx_para ltx_noindent">
<p id="S0.I1.i3.p1.1" class="ltx_p"><span id="S0.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">An open-source visual analysis tool</span> that integrates the probability distribution with the coordinated visualizations to support a unified model evaluation for computer vision tasks.</p>
</div>
</li>
</ul>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Related Work</h2>

<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Model Evaluation in Computer Vision</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">In the field of computer vision, the most widely-used method for evaluating models is to use performance metrics such as accuracy, mAP, and mIoU.
Some recent methods further classify model errors into multiple types for understanding model performance¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
For example, Bolya¬†<em id="S1.SS1.p1.1.1" class="ltx_emph ltx_font_italic">et al</em>.‚Äâ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> proposed to categorize object detection errors into six disentangled ones to provide a detailed performance summary from different perspectives.
Although these methods are useful in quantitatively summarizing model performance,
they fail to identify the causes of poor performance.
Compared with these methods, our work combines a matrix-based visualization, a table visualization, and a grid visualization to
evaluate model performance from high-level metrics to individual samples.
Such a comprehensive evaluation enables users to diagnose poor performance of a computer vision model and make informed improvements to the model and/or the associated data.</p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Model Evaluation in Visualization</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p">In the field of visualization, model evaluation methods are classified into two categories: model-specific and model-agnostic methods.
Model-specific methods analyze the inner components of a specific type of models, such as the convolutional layers of deep neural networks¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib65" title="" class="ltx_ref">65</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>.
Model-agnostic methods explain model performance by considering the inputs and outputs of models regardless of their model types.
Our work falls into the latter category,
where the methods are divided into three groups: class level, instance level, and their combination.</p>
</div>
<div id="S1.SS2.p2" class="ltx_para">
<p id="S1.SS2.p2.1" class="ltx_p">Class-level evaluations focus on analyzing the confusion between classes.
The most widely-used method is the confusion matrix¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>.
This matrix shows which classes are confused with each other and how heavy the confusion is.
Later efforts focus on generalizing the confusion matrix to other scenarios¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.
For example, Boxer¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> compares performance of different classifiers on the selected subsets by combining a confusion matrix with several chart visualizations.
ConfusionFlow¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> enhances
the confusion matrix with stacked heatmaps and line charts to support the analysis of class confusion over time.</p>
</div>
<div id="S1.SS2.p3" class="ltx_para">
<p id="S1.SS2.p3.1" class="ltx_p">Instance-level evaluations seek to analyze the predictions of individual samples and their similarity relationships.
Scatterplots and grid visualizations are the two most widely-used instance-level methods.
In a scatterplot, each sample is represented by a dot with the color encoding its predicted or ground-truth class label.
The positions of the dots are determined by their sample attributes (<em id="S1.SS2.p3.1.1" class="ltx_emph ltx_font_italic">e.g</em>.‚Äâ, prediction confidence¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>),
or projection (<em id="S1.SS2.p3.1.2" class="ltx_emph ltx_font_italic">e.g</em>.‚Äâ, t-SNE¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>) of their sample attributes.
Grid visualizations place the content of samples (<em id="S1.SS2.p3.1.3" class="ltx_emph ltx_font_italic">e.g</em>.‚Äâ, images) in grids, which overcomes the overlapping and space-wasting issues of scatterplots.
Along this line, Chen¬†<em id="S1.SS2.p3.1.4" class="ltx_emph ltx_font_italic">et al</em>.‚Äâ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> presented a hierarchical grid visualization to support the exploration of a large number of samples.
To maintain mental maps during exploration, DendroMap¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
enhances the grid visualization with an interactive zoomable treemap.</p>
</div>
<div id="S1.SS2.p4" class="ltx_para">
<p id="S1.SS2.p4.1" class="ltx_p">In real-world settings, both class-level and instance-level evaluations are required to understand model performance.
Accordingly, these two evaluation methods are combined to maximize the values of both.
The most intuitive combination is to use separate but coordinated views¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.
For example, ConfusionWheel¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> utilizes a chord diagram augmented with histograms to display confusion between classes and the prediction confidence distribution of each class.
Users can inspect a subset of selected samples in scatterplots and examine their feature distribution in bar charts.
Such a strategy is also utilized by TensorBoard¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and Weights&amp;Biases¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.
Although these methods enable the simultaneous examination of the class-level metrics and instance-level predictions, users have to switch views between the two levels.
To tackle this issue, Squares¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> combines the two-level information using stacked bar charts,
which utilize bars to display confusion between classes and small squares in each bar to represent the associated samples.</p>
</div>
<div id="S1.SS2.p5" class="ltx_para">
<p id="S1.SS2.p5.1" class="ltx_p">Despite their effectiveness,
these methods only consider single-output labels and fail to support more complex data structures, such as multi-output labels.
To solve this problem, a pioneering work, Neo¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, employs probability distributions to represent confusion matrices.
These distributions generalize the traditional confusion matrix to support hierarchical and multi-output labels.
However, Neo primarily focuses on classification tasks and lacks support for object detection and instance segmentation tasks.
To provide a unified model evaluation method for different computer vision tasks, we extend the discrete formulation in Neo to jointly model discrete and continuous predictions presented in different tasks.
Moreover, we develop three coordinated visualizations to facilitate the identification of factors contributing to poor performance.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Survey-Based Task Analysis and System Design</h2>

<figure id="S2.F1" class="ltx_figure"><img src="/html/2308.05168/assets/x2.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="221" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.6.3.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S2.F1.4.2" class="ltx_text" style="font-size:90%;">Survey responses:
(a) the participants mainly focused on model training and evaluation;
(b) 91 participants (<math id="S2.F1.3.1.m1.1" class="ltx_Math" alttext="60.26\%" display="inline"><semantics id="S2.F1.3.1.m1.1b"><mrow id="S2.F1.3.1.m1.1.1" xref="S2.F1.3.1.m1.1.1.cmml"><mn id="S2.F1.3.1.m1.1.1.2" xref="S2.F1.3.1.m1.1.1.2.cmml">60.26</mn><mo id="S2.F1.3.1.m1.1.1.1" xref="S2.F1.3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.F1.3.1.m1.1c"><apply id="S2.F1.3.1.m1.1.1.cmml" xref="S2.F1.3.1.m1.1.1"><csymbol cd="latexml" id="S2.F1.3.1.m1.1.1.1.cmml" xref="S2.F1.3.1.m1.1.1.1">percent</csymbol><cn type="float" id="S2.F1.3.1.m1.1.1.2.cmml" xref="S2.F1.3.1.m1.1.1.2">60.26</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.3.1.m1.1d">60.26\%</annotation></semantics></math>) had worked with more than one task;
(c) 90 participants (<math id="S2.F1.4.2.m2.1" class="ltx_Math" alttext="59.60\%" display="inline"><semantics id="S2.F1.4.2.m2.1b"><mrow id="S2.F1.4.2.m2.1.1" xref="S2.F1.4.2.m2.1.1.cmml"><mn id="S2.F1.4.2.m2.1.1.2" xref="S2.F1.4.2.m2.1.1.2.cmml">59.60</mn><mo id="S2.F1.4.2.m2.1.1.1" xref="S2.F1.4.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.F1.4.2.m2.1c"><apply id="S2.F1.4.2.m2.1.1.cmml" xref="S2.F1.4.2.m2.1.1"><csymbol cd="latexml" id="S2.F1.4.2.m2.1.1.1.cmml" xref="S2.F1.4.2.m2.1.1.1">percent</csymbol><cn type="float" id="S2.F1.4.2.m2.1.1.2.cmml" xref="S2.F1.4.2.m2.1.1.2">59.60</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.4.2.m2.1d">59.60\%</annotation></semantics></math>) had worked with multi-task applications;
(d) most participants thought unified evaluation and multi-level exploration are (very) important.
</span></figcaption>
</figure>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2308.05168/assets/x3.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="126" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.3.2" class="ltx_text" style="font-size:90%;">System overview: (a) given data samples and model predictions with both discrete and continuous variables, (b) the unified formulation module models them by a probability distribution; (c) the visualization module explains model performance at different levels.</span></figcaption>
</figure>
<div id="S2.p1" class="ltx_para">
<p id="S2.p1.2" class="ltx_p">To better understand the current practice of model evaluation and the challenges computer vision experts face, we conducted an expert survey.
We first designed a survey based on the literature review.
Then we discussed it with four computer vision experts (<math id="S2.p1.1.m1.1" class="ltx_Math" alttext="E_{1}" display="inline"><semantics id="S2.p1.1.m1.1a"><msub id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml"><mi id="S2.p1.1.m1.1.1.2" xref="S2.p1.1.m1.1.1.2.cmml">E</mi><mn id="S2.p1.1.m1.1.1.3" xref="S2.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><apply id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p1.1.m1.1.1.1.cmml" xref="S2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.p1.1.m1.1.1.2.cmml" xref="S2.p1.1.m1.1.1.2">ùê∏</ci><cn type="integer" id="S2.p1.1.m1.1.1.3.cmml" xref="S2.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">E_{1}</annotation></semantics></math>‚Äì<math id="S2.p1.2.m2.1" class="ltx_Math" alttext="E_{4}" display="inline"><semantics id="S2.p1.2.m2.1a"><msub id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml"><mi id="S2.p1.2.m2.1.1.2" xref="S2.p1.2.m2.1.1.2.cmml">E</mi><mn id="S2.p1.2.m2.1.1.3" xref="S2.p1.2.m2.1.1.3.cmml">4</mn></msub><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><apply id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p1.2.m2.1.1.1.cmml" xref="S2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.p1.2.m2.1.1.2.cmml" xref="S2.p1.2.m2.1.1.2">ùê∏</ci><cn type="integer" id="S2.p1.2.m2.1.1.3.cmml" xref="S2.p1.2.m2.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">E_{4}</annotation></semantics></math>) for refinement.
The survey covered three parts: 1) the basic information of the participants (Q1‚ÄìQ3); 2) current practices in evaluating computer vision models (Q4‚ÄìQ8); 3) key features needed for a unified model evaluation (Q9‚ÄìQ12).
The exact questions can be found in supplemental material.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.8" class="ltx_p">We distributed the refined survey to the computer vision experts from three top universities and three major technology companies.
A total of 151 surveys were returned.
Among the participants, 28 (<math id="S2.p2.1.m1.1" class="ltx_Math" alttext="18.54\%" display="inline"><semantics id="S2.p2.1.m1.1a"><mrow id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml"><mn id="S2.p2.1.m1.1.1.2" xref="S2.p2.1.m1.1.1.2.cmml">18.54</mn><mo id="S2.p2.1.m1.1.1.1" xref="S2.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><apply id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1"><csymbol cd="latexml" id="S2.p2.1.m1.1.1.1.cmml" xref="S2.p2.1.m1.1.1.1">percent</csymbol><cn type="float" id="S2.p2.1.m1.1.1.2.cmml" xref="S2.p2.1.m1.1.1.2">18.54</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">18.54\%</annotation></semantics></math>) had less than 1 year of experience in computer vision, 50 (<math id="S2.p2.2.m2.1" class="ltx_Math" alttext="33.11\%" display="inline"><semantics id="S2.p2.2.m2.1a"><mrow id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml"><mn id="S2.p2.2.m2.1.1.2" xref="S2.p2.2.m2.1.1.2.cmml">33.11</mn><mo id="S2.p2.2.m2.1.1.1" xref="S2.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b"><apply id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1"><csymbol cd="latexml" id="S2.p2.2.m2.1.1.1.cmml" xref="S2.p2.2.m2.1.1.1">percent</csymbol><cn type="float" id="S2.p2.2.m2.1.1.2.cmml" xref="S2.p2.2.m2.1.1.2">33.11</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">33.11\%</annotation></semantics></math>) had 1-3 years of experience, 50 (<math id="S2.p2.3.m3.1" class="ltx_Math" alttext="33.11\%" display="inline"><semantics id="S2.p2.3.m3.1a"><mrow id="S2.p2.3.m3.1.1" xref="S2.p2.3.m3.1.1.cmml"><mn id="S2.p2.3.m3.1.1.2" xref="S2.p2.3.m3.1.1.2.cmml">33.11</mn><mo id="S2.p2.3.m3.1.1.1" xref="S2.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.3.m3.1b"><apply id="S2.p2.3.m3.1.1.cmml" xref="S2.p2.3.m3.1.1"><csymbol cd="latexml" id="S2.p2.3.m3.1.1.1.cmml" xref="S2.p2.3.m3.1.1.1">percent</csymbol><cn type="float" id="S2.p2.3.m3.1.1.2.cmml" xref="S2.p2.3.m3.1.1.2">33.11</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.3.m3.1c">33.11\%</annotation></semantics></math>) had 3-5 years of experience, 23 (<math id="S2.p2.4.m4.1" class="ltx_Math" alttext="15.23\%" display="inline"><semantics id="S2.p2.4.m4.1a"><mrow id="S2.p2.4.m4.1.1" xref="S2.p2.4.m4.1.1.cmml"><mn id="S2.p2.4.m4.1.1.2" xref="S2.p2.4.m4.1.1.2.cmml">15.23</mn><mo id="S2.p2.4.m4.1.1.1" xref="S2.p2.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.4.m4.1b"><apply id="S2.p2.4.m4.1.1.cmml" xref="S2.p2.4.m4.1.1"><csymbol cd="latexml" id="S2.p2.4.m4.1.1.1.cmml" xref="S2.p2.4.m4.1.1.1">percent</csymbol><cn type="float" id="S2.p2.4.m4.1.1.2.cmml" xref="S2.p2.4.m4.1.1.2">15.23</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.4.m4.1c">15.23\%</annotation></semantics></math>) had more than 5 years of experience.
In addition, the participants spent more attention on ‚Äúmodel training‚Äù (<math id="S2.p2.5.m5.1" class="ltx_Math" alttext="83.44\%" display="inline"><semantics id="S2.p2.5.m5.1a"><mrow id="S2.p2.5.m5.1.1" xref="S2.p2.5.m5.1.1.cmml"><mn id="S2.p2.5.m5.1.1.2" xref="S2.p2.5.m5.1.1.2.cmml">83.44</mn><mo id="S2.p2.5.m5.1.1.1" xref="S2.p2.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.5.m5.1b"><apply id="S2.p2.5.m5.1.1.cmml" xref="S2.p2.5.m5.1.1"><csymbol cd="latexml" id="S2.p2.5.m5.1.1.1.cmml" xref="S2.p2.5.m5.1.1.1">percent</csymbol><cn type="float" id="S2.p2.5.m5.1.1.2.cmml" xref="S2.p2.5.m5.1.1.2">83.44</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.5.m5.1c">83.44\%</annotation></semantics></math>) and ‚Äúmodel evaluation‚Äù (<math id="S2.p2.6.m6.1" class="ltx_Math" alttext="61.59\%" display="inline"><semantics id="S2.p2.6.m6.1a"><mrow id="S2.p2.6.m6.1.1" xref="S2.p2.6.m6.1.1.cmml"><mn id="S2.p2.6.m6.1.1.2" xref="S2.p2.6.m6.1.1.2.cmml">61.59</mn><mo id="S2.p2.6.m6.1.1.1" xref="S2.p2.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.6.m6.1b"><apply id="S2.p2.6.m6.1.1.cmml" xref="S2.p2.6.m6.1.1"><csymbol cd="latexml" id="S2.p2.6.m6.1.1.1.cmml" xref="S2.p2.6.m6.1.1.1">percent</csymbol><cn type="float" id="S2.p2.6.m6.1.1.2.cmml" xref="S2.p2.6.m6.1.1.2">61.59</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.6.m6.1c">61.59\%</annotation></semantics></math>) but less attention on ‚Äúdata collection and processing‚Äù (<math id="S2.p2.7.m7.1" class="ltx_Math" alttext="19.21\%" display="inline"><semantics id="S2.p2.7.m7.1a"><mrow id="S2.p2.7.m7.1.1" xref="S2.p2.7.m7.1.1.cmml"><mn id="S2.p2.7.m7.1.1.2" xref="S2.p2.7.m7.1.1.2.cmml">19.21</mn><mo id="S2.p2.7.m7.1.1.1" xref="S2.p2.7.m7.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.7.m7.1b"><apply id="S2.p2.7.m7.1.1.cmml" xref="S2.p2.7.m7.1.1"><csymbol cd="latexml" id="S2.p2.7.m7.1.1.1.cmml" xref="S2.p2.7.m7.1.1.1">percent</csymbol><cn type="float" id="S2.p2.7.m7.1.1.2.cmml" xref="S2.p2.7.m7.1.1.2">19.21</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.7.m7.1c">19.21\%</annotation></semantics></math>) and ‚Äúdata labeling‚Äù (<math id="S2.p2.8.m8.1" class="ltx_Math" alttext="12.58\%" display="inline"><semantics id="S2.p2.8.m8.1a"><mrow id="S2.p2.8.m8.1.1" xref="S2.p2.8.m8.1.1.cmml"><mn id="S2.p2.8.m8.1.1.2" xref="S2.p2.8.m8.1.1.2.cmml">12.58</mn><mo id="S2.p2.8.m8.1.1.1" xref="S2.p2.8.m8.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.8.m8.1b"><apply id="S2.p2.8.m8.1.1.cmml" xref="S2.p2.8.m8.1.1"><csymbol cd="latexml" id="S2.p2.8.m8.1.1.1.cmml" xref="S2.p2.8.m8.1.1.1">percent</csymbol><cn type="float" id="S2.p2.8.m8.1.1.2.cmml" xref="S2.p2.8.m8.1.1.2">12.58</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.8.m8.1c">12.58\%</annotation></semantics></math>) (Fig.¬†<a href="#S2.F1" title="Figure 1 ‚Ä£ 2 Survey-Based Task Analysis and System Design ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(a)).
This indicates that they have extensive experience to speak about the intricacies of model evaluation.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Why Unified Evaluation</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The survey results indicated that most of the participants (<math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="91.39\%" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mrow id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml"><mn id="S2.SS1.p1.1.m1.1.1.2" xref="S2.SS1.p1.1.m1.1.1.2.cmml">91.39</mn><mo id="S2.SS1.p1.1.m1.1.1.1" xref="S2.SS1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><apply id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1"><csymbol cd="latexml" id="S2.SS1.p1.1.m1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S2.SS1.p1.1.m1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.1.1.2">91.39</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">91.39\%</annotation></semantics></math>) were interested in a unified model evaluation tool (Fig.¬†<a href="#S2.F1" title="Figure 1 ‚Ä£ 2 Survey-Based Task Analysis and System Design ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(d)).
From their responses, we identified three main reasons.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.p2.1" class="ltx_p"><span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_bold">Reducing the learning curve</span>.
One major advantage of a unified model evaluation tool is that it can reduce the time for users to learn different tools for different tasks.
According to the survey results, 91 participants (<math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="60.26\%" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><mrow id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml"><mn id="S2.SS1.p2.1.m1.1.1.2" xref="S2.SS1.p2.1.m1.1.1.2.cmml">60.26</mn><mo id="S2.SS1.p2.1.m1.1.1.1" xref="S2.SS1.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><apply id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1"><csymbol cd="latexml" id="S2.SS1.p2.1.m1.1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1.1">percent</csymbol><cn type="float" id="S2.SS1.p2.1.m1.1.1.2.cmml" xref="S2.SS1.p2.1.m1.1.1.2">60.26</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">60.26\%</annotation></semantics></math>) had worked on more than one task (Fig.¬†<a href="#S2.F1" title="Figure 1 ‚Ä£ 2 Survey-Based Task Analysis and System Design ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(b)).
These participants pointed out that they had to use different tools to evaluate different tasks.
This significantly increased the time they spent on learning the different visual encodings and functionalities of these tools.
Moreover, different tools require different formats of data input.
Users have to maintain different data pre-processing codes, which is also a burden for them.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para ltx_noindent">
<p id="S2.SS1.p3.1" class="ltx_p"><span id="S2.SS1.p3.1.1" class="ltx_text ltx_font_bold">Improving the analysis efficiency</span>.
As shown in Fig.¬†<a href="#S2.F1" title="Figure 1 ‚Ä£ 2 Survey-Based Task Analysis and System Design ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(c), 90 participants (<math id="S2.SS1.p3.1.m1.1" class="ltx_Math" alttext="59.60\%" display="inline"><semantics id="S2.SS1.p3.1.m1.1a"><mrow id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml"><mn id="S2.SS1.p3.1.m1.1.1.2" xref="S2.SS1.p3.1.m1.1.1.2.cmml">59.60</mn><mo id="S2.SS1.p3.1.m1.1.1.1" xref="S2.SS1.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.1b"><apply id="S2.SS1.p3.1.m1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1"><csymbol cd="latexml" id="S2.SS1.p3.1.m1.1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1.1">percent</csymbol><cn type="float" id="S2.SS1.p3.1.m1.1.1.2.cmml" xref="S2.SS1.p3.1.m1.1.1.2">59.60</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.1c">59.60\%</annotation></semantics></math>) had experience in multi-task applications.
However, when evaluating application models, users still need to use multiple tools for multiple tasks.
Their analysis is often interrupted as they have to switch between different tools.
For example, a participant reported that while developing models for classifying medical images, he often carried out the segmentation of lesions.
When performance is poor, he uses a confusion matrix to identify the confused classes.
Then, to identify the main causes of the confusion,
he filters the associated samples and visualizes their segmentation results in a different tool. This significantly reduces the analysis efficiency.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para ltx_noindent">
<p id="S2.SS1.p4.1" class="ltx_p"><span id="S2.SS1.p4.1.1" class="ltx_text ltx_font_bold">Debugging a multi-task model</span>.
In addition to reducing the learning curve and improving the analysis efficiency,
a unified model evaluation will also help debug a multi-task model effectively.
In a multi-task application, the tasks mutually influence each other.
Understanding such mutual influence helps users debug the model¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>, <a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>, such as identifying which task is the weak point of the model.
Currently, different tasks are analyzed with different tools, which hinders the effective analysis of the mutual influence between tasks.
Therefore, a unified evaluation for multiple tasks is desirable.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Design Goals</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.2" class="ltx_p">To distill the detailed design goals and tasks for developing such a unified tool, we further conducted interviews with four experts (<math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="E_{1}" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><msub id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml"><mi id="S2.SS2.p1.1.m1.1.1.2" xref="S2.SS2.p1.1.m1.1.1.2.cmml">E</mi><mn id="S2.SS2.p1.1.m1.1.1.3" xref="S2.SS2.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><apply id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.1.m1.1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.p1.1.m1.1.1.2.cmml" xref="S2.SS2.p1.1.m1.1.1.2">ùê∏</ci><cn type="integer" id="S2.SS2.p1.1.m1.1.1.3.cmml" xref="S2.SS2.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">E_{1}</annotation></semantics></math>‚Äì<math id="S2.SS2.p1.2.m2.1" class="ltx_Math" alttext="E_{4}" display="inline"><semantics id="S2.SS2.p1.2.m2.1a"><msub id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml"><mi id="S2.SS2.p1.2.m2.1.1.2" xref="S2.SS2.p1.2.m2.1.1.2.cmml">E</mi><mn id="S2.SS2.p1.2.m2.1.1.3" xref="S2.SS2.p1.2.m2.1.1.3.cmml">4</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><apply id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.2.m2.1.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.p1.2.m2.1.1.2.cmml" xref="S2.SS2.p1.2.m2.1.1.2">ùê∏</ci><cn type="integer" id="S2.SS2.p1.2.m2.1.1.3.cmml" xref="S2.SS2.p1.2.m2.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">E_{4}</annotation></semantics></math>) for more detailed feedback and insights.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.p2.1" class="ltx_p"><span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_bold">G1 - Evaluating different models with a unified tool</span>.
According to the survey results, 138 participants (91.39%) wanted a unified tool for evaluating different models, as switching between multiple tools is inconvenient.
The main obstacle to building a unified tool is that the predictions in different tasks can be discrete or continuous.
Although Neo¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> has explored the modeling of discrete predictions, it is still under exploration how to model both discrete and continuous predictions.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para ltx_noindent">
<p id="S2.SS2.p3.1" class="ltx_p"><span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_bold">G2 - Analyzing model performance at different levels</span>.
The participants also expressed their need to analyze model performance at different levels.
As shown in Fig.¬†<a href="#S2.F1" title="Figure 1 ‚Ä£ 2 Survey-Based Task Analysis and System Design ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(d), most participants were interested in analyzing model performance at dataset level, subset level, and sample level.
In current practice, they often utilize performance metrics or confusion matrices to get an overview of model performance.
However, these methods may hinder the identification of problematic subsets where a model performs poorly.
Analyzing model performance on such subsets helps improve the robustness of the model.
In addition, when diagnosing poor performance on these subsets,
Therefore, a multi-level exploration environment is needed.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para ltx_noindent">
<p id="S2.SS2.p4.1" class="ltx_p"><span id="S2.SS2.p4.1.1" class="ltx_text ltx_font_bold">G3 - Finding problematic data subsets</span>.
As mentioned above, the participants needed to analyze model performance on problematic subsets.
However, identifying such problematic subsets is non-trivial, especially when the dataset is large.
Currently, they usually find such subsets by manually creating different rules to slice the data and then examining these subsets one by one.
This process is tedious and time-consuming.
Therefore, the participants wanted a more efficient way to identify the problematic subsets.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Task Analysis</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Based on the design goals, we derived several tasks as guidelines for designing the unified model evaluation tool.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para ltx_noindent">
<p id="S2.SS3.p2.1" class="ltx_p"><span id="S2.SS3.p2.1.1" class="ltx_text ltx_font_bold">T1 - Modeling both discrete and continuous predictions in a unified manner (G1)</span>.
This includes a unified formulation for both discrete and continuous predictions to enable the analysis of model performance across different tasks.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para ltx_noindent">
<p id="S2.SS3.p3.1" class="ltx_p"><span id="S2.SS3.p3.1.1" class="ltx_text ltx_font_bold">T2 - Visually explaining overall performance on the entire dataset (G2)</span>.
This includes the confusion between classes and imprecise sizes and positions of predicted objects.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para ltx_noindent">
<p id="S2.SS3.p4.1" class="ltx_p"><span id="S2.SS3.p4.1.1" class="ltx_text ltx_font_bold">T3 - Interactively identifying problematic data subsets (G2, G3)</span>.
This includes a subset search method to extract candidate subsets and a table visualization that allows users to identify problematic data subsets based on one or more attributes.</p>
</div>
<div id="S2.SS3.p5" class="ltx_para ltx_noindent">
<p id="S2.SS3.p5.1" class="ltx_p"><span id="S2.SS3.p5.1.1" class="ltx_text ltx_font_bold">T4 - Displaying the samples of interest for efficient exploration (G2)</span>.
This includes a grid visualization that places similar samples together and visually presents them in a compact way to facilitate exploration.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Design of Uni-Evaluator</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">Motivated by the identified tasks, we develop Uni-Evaluator to support a unified model evaluation for different computer vision tasks.
As shown in Fig.¬†<a href="#S2.F2" title="Figure 2 ‚Ä£ 2 Survey-Based Task Analysis and System Design ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, Uni-Evaluator consists of two main modules: <span id="S2.SS4.p1.1.1" class="ltx_text ltx_font_bold">unified formulation</span> (Sec.¬†<a href="#S3" title="3 Unified Probability Distribution ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) and <span id="S2.SS4.p1.1.2" class="ltx_text ltx_font_bold">visualization</span> (Sec.¬†<a href="#S4" title="4 Uni-Evaluator Visualization ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p id="S2.SS4.p2.1" class="ltx_p">Given data samples and model predictions with both discrete and continuous variables (Fig.¬†<a href="#S2.F2" title="Figure 2 ‚Ä£ 2 Survey-Based Task Analysis and System Design ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(a)),
the unified formulation module models them by a probability distribution (<span id="S2.SS4.p2.1.1" class="ltx_text ltx_font_bold">T1</span>, Fig.¬†<a href="#S2.F2" title="Figure 2 ‚Ä£ 2 Survey-Based Task Analysis and System Design ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(b)).
Based on the distribution, the visualization module provides three coordinated visualizations to explain model performance at different levels (Fig.¬†<a href="#S2.F2" title="Figure 2 ‚Ä£ 2 Survey-Based Task Analysis and System Design ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(c)).
The matrix-based visualization provides an overview of model performance (<span id="S2.SS4.p2.1.2" class="ltx_text ltx_font_bold">T2</span>).
Candidate data subsets are extracted and presented in the table visualization.
Users can identify the problematic subsets (<span id="S2.SS4.p2.1.3" class="ltx_text ltx_font_bold">T3</span>) and analyze their predictions in the matrix-based visualization.
In both visualizations, users can select the samples of interest and explore them in the grid visualization (<span id="S2.SS4.p2.1.4" class="ltx_text ltx_font_bold">T4</span>).
With Uni-Evaluator,
users can diagnose the causes of poor performance and make informed improvements.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2308.05168/assets/x4.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="225" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S2.F3.3.2" class="ltx_text" style="font-size:90%;">In object matching,
(a) treating each prediction independently results in incorrect matches, while (b) considering all predictions simultaneously enables the inference of the optimal matching.</span></figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Unified Probability Distribution</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Neo¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> utilizes probability distributions to model discrete variables in classification tasks by matching predicted classes with their ground truth.
We extend it to also model continuous predictions and ground truth.
However, in this probabilistic framing, it is non-trivial to match each continuous prediction with the corresponding ground truth due to multiple possible ground-truth annotations.
To tackle this issue, we develop an object matching algorithm to pair each prediction with the suitable ground truth.
Based on the matching results, we model discrete and continuous variables using a unified probability distribution.</p>
</div>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.11" class="ltx_p"><span id="S3.p2.11.1" class="ltx_text ltx_font_bold">Object matching</span>.
A straightforward way to match a prediction with its ground truth is to find the one with the largest overlap.
However, this strategy considers each prediction separately and may not capture the optimal matches between predictions and their associated ground truth.
For example, in Fig.¬†<a href="#S2.F3" title="Figure 3 ‚Ä£ 2.4 Design of Uni-Evaluator ‚Ä£ 2 Survey-Based Task Analysis and System Design ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(a), the detected object P<sub id="S3.p2.11.2" class="ltx_sub">1</sub> has a larger overlap with the ground truth G<sub id="S3.p2.11.3" class="ltx_sub">2</sub>.
By only considering P<sub id="S3.p2.11.4" class="ltx_sub">1</sub>,
it will be matched with G<sub id="S3.p2.11.5" class="ltx_sub">2</sub>.
However,
if another detected object, P<sub id="S3.p2.11.6" class="ltx_sub">2</sub>, is considered simultaneously (Fig.¬†<a href="#S2.F3" title="Figure 3 ‚Ä£ 2.4 Design of Uni-Evaluator ‚Ä£ 2 Survey-Based Task Analysis and System Design ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(b)),
the matching between P<sub id="S3.p2.11.7" class="ltx_sub">1</sub> and G<sub id="S3.p2.11.8" class="ltx_sub">2</sub> is incorrect.
The optimal way is to match P<sub id="S3.p2.11.9" class="ltx_sub">1</sub> with G<sub id="S3.p2.11.10" class="ltx_sub">1</sub> and P<sub id="S3.p2.11.11" class="ltx_sub">2</sub> with G<sub id="S3.p2.11.12" class="ltx_sub">2</sub>.
Therefore, it needs to consider all the predictions for optimal matching.
Mathematically, the matching between the predictions and ground truth can be modeled as a one-to-many assignment problem¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>,
which maximizes the total assignment award.</p>
<table id="S3.E1" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S3.E1X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1X.2.1.1.m1.1" class="ltx_Math" alttext="\displaystyle\underset{z_{ij}}{\mathrm{maximize}}" display="inline"><semantics id="S3.E1X.2.1.1.m1.1a"><munder accentunder="true" id="S3.E1X.2.1.1.m1.1.1" xref="S3.E1X.2.1.1.m1.1.1.cmml"><mi id="S3.E1X.2.1.1.m1.1.1.2" xref="S3.E1X.2.1.1.m1.1.1.2.cmml">maximize</mi><msub id="S3.E1X.2.1.1.m1.1.1.1" xref="S3.E1X.2.1.1.m1.1.1.1.cmml"><mi id="S3.E1X.2.1.1.m1.1.1.1.2" xref="S3.E1X.2.1.1.m1.1.1.1.2.cmml">z</mi><mrow id="S3.E1X.2.1.1.m1.1.1.1.3" xref="S3.E1X.2.1.1.m1.1.1.1.3.cmml"><mi id="S3.E1X.2.1.1.m1.1.1.1.3.2" xref="S3.E1X.2.1.1.m1.1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1X.2.1.1.m1.1.1.1.3.1" xref="S3.E1X.2.1.1.m1.1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.E1X.2.1.1.m1.1.1.1.3.3" xref="S3.E1X.2.1.1.m1.1.1.1.3.3.cmml">j</mi></mrow></msub></munder><annotation-xml encoding="MathML-Content" id="S3.E1X.2.1.1.m1.1b"><apply id="S3.E1X.2.1.1.m1.1.1.cmml" xref="S3.E1X.2.1.1.m1.1.1"><apply id="S3.E1X.2.1.1.m1.1.1.1.cmml" xref="S3.E1X.2.1.1.m1.1.1.1"><csymbol cd="ambiguous" id="S3.E1X.2.1.1.m1.1.1.1.1.cmml" xref="S3.E1X.2.1.1.m1.1.1.1">subscript</csymbol><ci id="S3.E1X.2.1.1.m1.1.1.1.2.cmml" xref="S3.E1X.2.1.1.m1.1.1.1.2">ùëß</ci><apply id="S3.E1X.2.1.1.m1.1.1.1.3.cmml" xref="S3.E1X.2.1.1.m1.1.1.1.3"><times id="S3.E1X.2.1.1.m1.1.1.1.3.1.cmml" xref="S3.E1X.2.1.1.m1.1.1.1.3.1"></times><ci id="S3.E1X.2.1.1.m1.1.1.1.3.2.cmml" xref="S3.E1X.2.1.1.m1.1.1.1.3.2">ùëñ</ci><ci id="S3.E1X.2.1.1.m1.1.1.1.3.3.cmml" xref="S3.E1X.2.1.1.m1.1.1.1.3.3">ùëó</ci></apply></apply><ci id="S3.E1X.2.1.1.m1.1.1.2.cmml" xref="S3.E1X.2.1.1.m1.1.1.2">maximize</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1X.2.1.1.m1.1c">\displaystyle\underset{z_{ij}}{\mathrm{maximize}}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E1X.3.2.2.m1.1" class="ltx_Math" alttext="\displaystyle\textstyle\sum_{j}^{M_{p}}\sum_{i}^{M_{g}}z_{ij}a_{ij}" display="inline"><semantics id="S3.E1X.3.2.2.m1.1a"><mrow id="S3.E1X.3.2.2.m1.1.1" xref="S3.E1X.3.2.2.m1.1.1.cmml"><msubsup id="S3.E1X.3.2.2.m1.1.1.1" xref="S3.E1X.3.2.2.m1.1.1.1.cmml"><mo id="S3.E1X.3.2.2.m1.1.1.1.2.2" xref="S3.E1X.3.2.2.m1.1.1.1.2.2.cmml">‚àë</mo><mi id="S3.E1X.3.2.2.m1.1.1.1.2.3" xref="S3.E1X.3.2.2.m1.1.1.1.2.3.cmml">j</mi><msub id="S3.E1X.3.2.2.m1.1.1.1.3" xref="S3.E1X.3.2.2.m1.1.1.1.3.cmml"><mi id="S3.E1X.3.2.2.m1.1.1.1.3.2" xref="S3.E1X.3.2.2.m1.1.1.1.3.2.cmml">M</mi><mi id="S3.E1X.3.2.2.m1.1.1.1.3.3" xref="S3.E1X.3.2.2.m1.1.1.1.3.3.cmml">p</mi></msub></msubsup><mrow id="S3.E1X.3.2.2.m1.1.1.2" xref="S3.E1X.3.2.2.m1.1.1.2.cmml"><msubsup id="S3.E1X.3.2.2.m1.1.1.2.1" xref="S3.E1X.3.2.2.m1.1.1.2.1.cmml"><mo lspace="0.167em" id="S3.E1X.3.2.2.m1.1.1.2.1.2.2" xref="S3.E1X.3.2.2.m1.1.1.2.1.2.2.cmml">‚àë</mo><mi id="S3.E1X.3.2.2.m1.1.1.2.1.2.3" xref="S3.E1X.3.2.2.m1.1.1.2.1.2.3.cmml">i</mi><msub id="S3.E1X.3.2.2.m1.1.1.2.1.3" xref="S3.E1X.3.2.2.m1.1.1.2.1.3.cmml"><mi id="S3.E1X.3.2.2.m1.1.1.2.1.3.2" xref="S3.E1X.3.2.2.m1.1.1.2.1.3.2.cmml">M</mi><mi id="S3.E1X.3.2.2.m1.1.1.2.1.3.3" xref="S3.E1X.3.2.2.m1.1.1.2.1.3.3.cmml">g</mi></msub></msubsup><mrow id="S3.E1X.3.2.2.m1.1.1.2.2" xref="S3.E1X.3.2.2.m1.1.1.2.2.cmml"><msub id="S3.E1X.3.2.2.m1.1.1.2.2.2" xref="S3.E1X.3.2.2.m1.1.1.2.2.2.cmml"><mi id="S3.E1X.3.2.2.m1.1.1.2.2.2.2" xref="S3.E1X.3.2.2.m1.1.1.2.2.2.2.cmml">z</mi><mrow id="S3.E1X.3.2.2.m1.1.1.2.2.2.3" xref="S3.E1X.3.2.2.m1.1.1.2.2.2.3.cmml"><mi id="S3.E1X.3.2.2.m1.1.1.2.2.2.3.2" xref="S3.E1X.3.2.2.m1.1.1.2.2.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1X.3.2.2.m1.1.1.2.2.2.3.1" xref="S3.E1X.3.2.2.m1.1.1.2.2.2.3.1.cmml">‚Äã</mo><mi id="S3.E1X.3.2.2.m1.1.1.2.2.2.3.3" xref="S3.E1X.3.2.2.m1.1.1.2.2.2.3.3.cmml">j</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E1X.3.2.2.m1.1.1.2.2.1" xref="S3.E1X.3.2.2.m1.1.1.2.2.1.cmml">‚Äã</mo><msub id="S3.E1X.3.2.2.m1.1.1.2.2.3" xref="S3.E1X.3.2.2.m1.1.1.2.2.3.cmml"><mi id="S3.E1X.3.2.2.m1.1.1.2.2.3.2" xref="S3.E1X.3.2.2.m1.1.1.2.2.3.2.cmml">a</mi><mrow id="S3.E1X.3.2.2.m1.1.1.2.2.3.3" xref="S3.E1X.3.2.2.m1.1.1.2.2.3.3.cmml"><mi id="S3.E1X.3.2.2.m1.1.1.2.2.3.3.2" xref="S3.E1X.3.2.2.m1.1.1.2.2.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1X.3.2.2.m1.1.1.2.2.3.3.1" xref="S3.E1X.3.2.2.m1.1.1.2.2.3.3.1.cmml">‚Äã</mo><mi id="S3.E1X.3.2.2.m1.1.1.2.2.3.3.3" xref="S3.E1X.3.2.2.m1.1.1.2.2.3.3.3.cmml">j</mi></mrow></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1X.3.2.2.m1.1b"><apply id="S3.E1X.3.2.2.m1.1.1.cmml" xref="S3.E1X.3.2.2.m1.1.1"><apply id="S3.E1X.3.2.2.m1.1.1.1.cmml" xref="S3.E1X.3.2.2.m1.1.1.1"><csymbol cd="ambiguous" id="S3.E1X.3.2.2.m1.1.1.1.1.cmml" xref="S3.E1X.3.2.2.m1.1.1.1">superscript</csymbol><apply id="S3.E1X.3.2.2.m1.1.1.1.2.cmml" xref="S3.E1X.3.2.2.m1.1.1.1"><csymbol cd="ambiguous" id="S3.E1X.3.2.2.m1.1.1.1.2.1.cmml" xref="S3.E1X.3.2.2.m1.1.1.1">subscript</csymbol><sum id="S3.E1X.3.2.2.m1.1.1.1.2.2.cmml" xref="S3.E1X.3.2.2.m1.1.1.1.2.2"></sum><ci id="S3.E1X.3.2.2.m1.1.1.1.2.3.cmml" xref="S3.E1X.3.2.2.m1.1.1.1.2.3">ùëó</ci></apply><apply id="S3.E1X.3.2.2.m1.1.1.1.3.cmml" xref="S3.E1X.3.2.2.m1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1X.3.2.2.m1.1.1.1.3.1.cmml" xref="S3.E1X.3.2.2.m1.1.1.1.3">subscript</csymbol><ci id="S3.E1X.3.2.2.m1.1.1.1.3.2.cmml" xref="S3.E1X.3.2.2.m1.1.1.1.3.2">ùëÄ</ci><ci id="S3.E1X.3.2.2.m1.1.1.1.3.3.cmml" xref="S3.E1X.3.2.2.m1.1.1.1.3.3">ùëù</ci></apply></apply><apply id="S3.E1X.3.2.2.m1.1.1.2.cmml" xref="S3.E1X.3.2.2.m1.1.1.2"><apply id="S3.E1X.3.2.2.m1.1.1.2.1.cmml" xref="S3.E1X.3.2.2.m1.1.1.2.1"><csymbol cd="ambiguous" id="S3.E1X.3.2.2.m1.1.1.2.1.1.cmml" xref="S3.E1X.3.2.2.m1.1.1.2.1">superscript</csymbol><apply id="S3.E1X.3.2.2.m1.1.1.2.1.2.cmml" xref="S3.E1X.3.2.2.m1.1.1.2.1"><csymbol cd="ambiguous" id="S3.E1X.3.2.2.m1.1.1.2.1.2.1.cmml" xref="S3.E1X.3.2.2.m1.1.1.2.1">subscript</csymbol><sum id="S3.E1X.3.2.2.m1.1.1.2.1.2.2.cmml" xref="S3.E1X.3.2.2.m1.1.1.2.1.2.2"></sum><ci id="S3.E1X.3.2.2.m1.1.1.2.1.2.3.cmml" xref="S3.E1X.3.2.2.m1.1.1.2.1.2.3">ùëñ</ci></apply><apply id="S3.E1X.3.2.2.m1.1.1.2.1.3.cmml" xref="S3.E1X.3.2.2.m1.1.1.2.1.3"><csymbol cd="ambiguous" id="S3.E1X.3.2.2.m1.1.1.2.1.3.1.cmml" xref="S3.E1X.3.2.2.m1.1.1.2.1.3">subscript</csymbol><ci id="S3.E1X.3.2.2.m1.1.1.2.1.3.2.cmml" xref="S3.E1X.3.2.2.m1.1.1.2.1.3.2">ùëÄ</ci><ci id="S3.E1X.3.2.2.m1.1.1.2.1.3.3.cmml" xref="S3.E1X.3.2.2.m1.1.1.2.1.3.3">ùëî</ci></apply></apply><apply id="S3.E1X.3.2.2.m1.1.1.2.2.cmml" xref="S3.E1X.3.2.2.m1.1.1.2.2"><times id="S3.E1X.3.2.2.m1.1.1.2.2.1.cmml" xref="S3.E1X.3.2.2.m1.1.1.2.2.1"></times><apply id="S3.E1X.3.2.2.m1.1.1.2.2.2.cmml" xref="S3.E1X.3.2.2.m1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E1X.3.2.2.m1.1.1.2.2.2.1.cmml" xref="S3.E1X.3.2.2.m1.1.1.2.2.2">subscript</csymbol><ci id="S3.E1X.3.2.2.m1.1.1.2.2.2.2.cmml" xref="S3.E1X.3.2.2.m1.1.1.2.2.2.2">ùëß</ci><apply id="S3.E1X.3.2.2.m1.1.1.2.2.2.3.cmml" xref="S3.E1X.3.2.2.m1.1.1.2.2.2.3"><times id="S3.E1X.3.2.2.m1.1.1.2.2.2.3.1.cmml" xref="S3.E1X.3.2.2.m1.1.1.2.2.2.3.1"></times><ci id="S3.E1X.3.2.2.m1.1.1.2.2.2.3.2.cmml" xref="S3.E1X.3.2.2.m1.1.1.2.2.2.3.2">ùëñ</ci><ci id="S3.E1X.3.2.2.m1.1.1.2.2.2.3.3.cmml" xref="S3.E1X.3.2.2.m1.1.1.2.2.2.3.3">ùëó</ci></apply></apply><apply id="S3.E1X.3.2.2.m1.1.1.2.2.3.cmml" xref="S3.E1X.3.2.2.m1.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.E1X.3.2.2.m1.1.1.2.2.3.1.cmml" xref="S3.E1X.3.2.2.m1.1.1.2.2.3">subscript</csymbol><ci id="S3.E1X.3.2.2.m1.1.1.2.2.3.2.cmml" xref="S3.E1X.3.2.2.m1.1.1.2.2.3.2">ùëé</ci><apply id="S3.E1X.3.2.2.m1.1.1.2.2.3.3.cmml" xref="S3.E1X.3.2.2.m1.1.1.2.2.3.3"><times id="S3.E1X.3.2.2.m1.1.1.2.2.3.3.1.cmml" xref="S3.E1X.3.2.2.m1.1.1.2.2.3.3.1"></times><ci id="S3.E1X.3.2.2.m1.1.1.2.2.3.3.2.cmml" xref="S3.E1X.3.2.2.m1.1.1.2.2.3.3.2">ùëñ</ci><ci id="S3.E1X.3.2.2.m1.1.1.2.2.3.3.3.cmml" xref="S3.E1X.3.2.2.m1.1.1.2.2.3.3.3">ùëó</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1X.3.2.2.m1.1c">\displaystyle\textstyle\sum_{j}^{M_{p}}\sum_{i}^{M_{g}}z_{ij}a_{ij}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="2" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(1)</span></td>
</tr>
<tr id="S3.E1Xa" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1Xa.2.1.1.m1.2" class="ltx_math_unparsed" alttext="\displaystyle\mathrm{s.t.}\quad z_{ij}\in\{0,1\},\quad z_{ij}\mathbb{I}" display="inline"><semantics id="S3.E1Xa.2.1.1.m1.2a"><mrow id="S3.E1Xa.2.1.1.m1.2b"><mi mathvariant="normal" id="S3.E1Xa.2.1.1.m1.1.1">s</mi><mo lspace="0em" rspace="0.167em" id="S3.E1Xa.2.1.1.m1.2.3">.</mo><mi mathvariant="normal" id="S3.E1Xa.2.1.1.m1.2.2">t</mi><mo lspace="0em" id="S3.E1Xa.2.1.1.m1.2.4">.</mo><mspace width="1.167em" id="S3.E1Xa.2.1.1.m1.2.5"></mspace><msub id="S3.E1Xa.2.1.1.m1.2.6"><mi id="S3.E1Xa.2.1.1.m1.2.6.2">z</mi><mrow id="S3.E1Xa.2.1.1.m1.2.6.3"><mi id="S3.E1Xa.2.1.1.m1.2.6.3.2">i</mi><mo lspace="0em" rspace="0em" id="S3.E1Xa.2.1.1.m1.2.6.3.1">‚Äã</mo><mi id="S3.E1Xa.2.1.1.m1.2.6.3.3">j</mi></mrow></msub><mo id="S3.E1Xa.2.1.1.m1.2.7">‚àà</mo><mrow id="S3.E1Xa.2.1.1.m1.2.8"><mo stretchy="false" id="S3.E1Xa.2.1.1.m1.2.8.1">{</mo><mn id="S3.E1Xa.2.1.1.m1.2.8.2">0</mn><mo id="S3.E1Xa.2.1.1.m1.2.8.3">,</mo><mn id="S3.E1Xa.2.1.1.m1.2.8.4">1</mn><mo stretchy="false" id="S3.E1Xa.2.1.1.m1.2.8.5">}</mo></mrow><mo rspace="1.167em" id="S3.E1Xa.2.1.1.m1.2.9">,</mo><msub id="S3.E1Xa.2.1.1.m1.2.10"><mi id="S3.E1Xa.2.1.1.m1.2.10.2">z</mi><mrow id="S3.E1Xa.2.1.1.m1.2.10.3"><mi id="S3.E1Xa.2.1.1.m1.2.10.3.2">i</mi><mo lspace="0em" rspace="0em" id="S3.E1Xa.2.1.1.m1.2.10.3.1">‚Äã</mo><mi id="S3.E1Xa.2.1.1.m1.2.10.3.3">j</mi></mrow></msub><mi id="S3.E1Xa.2.1.1.m1.2.11">ùïÄ</mi></mrow><annotation encoding="application/x-tex" id="S3.E1Xa.2.1.1.m1.2c">\displaystyle\mathrm{s.t.}\quad z_{ij}\in\{0,1\},\quad z_{ij}\mathbb{I}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E1Xa.3.2.2.m1.3" class="ltx_Math" alttext="\displaystyle(p_{ij}&lt;\alpha)=0,\quad\textstyle\sum_{i}z_{ij}\leq 1,\ \forall i,j." display="inline"><semantics id="S3.E1Xa.3.2.2.m1.3a"><mrow id="S3.E1Xa.3.2.2.m1.3.3.1"><mrow id="S3.E1Xa.3.2.2.m1.3.3.1.1.2" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.3.cmml"><mrow id="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.cmml"><mrow id="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.2" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.cmml"><msub id="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.2" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.2.2" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.2.2.cmml">p</mi><mrow id="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.2.3" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.2.3.2" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.2.3.1" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.2.3.1.cmml">‚Äã</mo><mi id="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.2.3.3" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.2.3.3.cmml">j</mi></mrow></msub><mo id="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.1" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.1.cmml">&lt;</mo><mi id="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.3" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.3.cmml">Œ±</mi></mrow><mo stretchy="false" id="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.3" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.2" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.2.cmml">=</mo><mn id="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.3" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.3.cmml">0</mn></mrow><mo rspace="1.000em" id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.3" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.3a.cmml">,</mo><mrow id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.cmml"><mrow id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.cmml"><msub id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.1" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.1.cmml"><mo id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.1.2" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.1.2.cmml">‚àë</mo><mi id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.1.3" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.1.3.cmml">i</mi></msub><msub id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.2" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.2.cmml"><mi id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.2.2" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.2.2.cmml">z</mi><mrow id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.2.3" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.2.3.cmml"><mi id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.2.3.2" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.2.3.1" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.2.3.1.cmml">‚Äã</mo><mi id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.2.3.3" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.2.3.3.cmml">j</mi></mrow></msub></mrow><mo id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.2" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.2.cmml">‚â§</mo><mrow id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.1.1" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.1.2.cmml"><mn id="S3.E1Xa.3.2.2.m1.1.1" xref="S3.E1Xa.3.2.2.m1.1.1.cmml">1</mn><mo rspace="0.667em" id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.1.1.2" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.1.2.cmml">,</mo><mrow id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.1.1.1" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.1.1.1.cmml"><mo rspace="0.167em" id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.1.1.1.1" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.1.1.1.1.cmml">‚àÄ</mo><mi id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.1.1.1.2" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.1.1.1.2.cmml">i</mi></mrow><mo id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.1.1.3" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.1.2.cmml">,</mo><mi id="S3.E1Xa.3.2.2.m1.2.2" xref="S3.E1Xa.3.2.2.m1.2.2.cmml">j</mi></mrow></mrow></mrow><mo lspace="0em" id="S3.E1Xa.3.2.2.m1.3.3.1.2">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1Xa.3.2.2.m1.3b"><apply id="S3.E1Xa.3.2.2.m1.3.3.1.1.3.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.E1Xa.3.2.2.m1.3.3.1.1.3a.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.3">formulae-sequence</csymbol><apply id="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1"><eq id="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.2.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.2"></eq><apply id="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1"><lt id="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.1"></lt><apply id="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.2.2">ùëù</ci><apply id="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.2.3"><times id="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.2.3.1"></times><ci id="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.2.3.2">ùëñ</ci><ci id="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.2.3.3">ùëó</ci></apply></apply><ci id="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.3">ùõº</ci></apply><cn type="integer" id="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.3.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.1.1.3">0</cn></apply><apply id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2"><leq id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.2.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.2"></leq><apply id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3"><apply id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.1.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.1"><csymbol cd="ambiguous" id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.1.1.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.1">subscript</csymbol><sum id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.1.2.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.1.2"></sum><ci id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.1.3.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.1.3">ùëñ</ci></apply><apply id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.2.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.2"><csymbol cd="ambiguous" id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.2.1.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.2">subscript</csymbol><ci id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.2.2.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.2.2">ùëß</ci><apply id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.2.3.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.2.3"><times id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.2.3.1.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.2.3.1"></times><ci id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.2.3.2.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.2.3.2">ùëñ</ci><ci id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.2.3.3.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.3.2.3.3">ùëó</ci></apply></apply></apply><list id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.1.2.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.1.1"><cn type="integer" id="S3.E1Xa.3.2.2.m1.1.1.cmml" xref="S3.E1Xa.3.2.2.m1.1.1">1</cn><apply id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.1.1.1.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.1.1.1"><csymbol cd="latexml" id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.1.1.1.1.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.1.1.1.1">for-all</csymbol><ci id="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.1.1.1.2.cmml" xref="S3.E1Xa.3.2.2.m1.3.3.1.1.2.2.1.1.1.2">ùëñ</ci></apply><ci id="S3.E1Xa.3.2.2.m1.2.2.cmml" xref="S3.E1Xa.3.2.2.m1.2.2">ùëó</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1Xa.3.2.2.m1.3c">\displaystyle(p_{ij}&lt;\alpha)=0,\quad\textstyle\sum_{i}z_{ij}\leq 1,\ \forall i,j.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p id="S3.p2.20" class="ltx_p"><math id="S3.p2.12.m1.1" class="ltx_Math" alttext="M_{p}" display="inline"><semantics id="S3.p2.12.m1.1a"><msub id="S3.p2.12.m1.1.1" xref="S3.p2.12.m1.1.1.cmml"><mi id="S3.p2.12.m1.1.1.2" xref="S3.p2.12.m1.1.1.2.cmml">M</mi><mi id="S3.p2.12.m1.1.1.3" xref="S3.p2.12.m1.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.12.m1.1b"><apply id="S3.p2.12.m1.1.1.cmml" xref="S3.p2.12.m1.1.1"><csymbol cd="ambiguous" id="S3.p2.12.m1.1.1.1.cmml" xref="S3.p2.12.m1.1.1">subscript</csymbol><ci id="S3.p2.12.m1.1.1.2.cmml" xref="S3.p2.12.m1.1.1.2">ùëÄ</ci><ci id="S3.p2.12.m1.1.1.3.cmml" xref="S3.p2.12.m1.1.1.3">ùëù</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.12.m1.1c">M_{p}</annotation></semantics></math> and <math id="S3.p2.13.m2.1" class="ltx_Math" alttext="M_{g}" display="inline"><semantics id="S3.p2.13.m2.1a"><msub id="S3.p2.13.m2.1.1" xref="S3.p2.13.m2.1.1.cmml"><mi id="S3.p2.13.m2.1.1.2" xref="S3.p2.13.m2.1.1.2.cmml">M</mi><mi id="S3.p2.13.m2.1.1.3" xref="S3.p2.13.m2.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.13.m2.1b"><apply id="S3.p2.13.m2.1.1.cmml" xref="S3.p2.13.m2.1.1"><csymbol cd="ambiguous" id="S3.p2.13.m2.1.1.1.cmml" xref="S3.p2.13.m2.1.1">subscript</csymbol><ci id="S3.p2.13.m2.1.1.2.cmml" xref="S3.p2.13.m2.1.1.2">ùëÄ</ci><ci id="S3.p2.13.m2.1.1.3.cmml" xref="S3.p2.13.m2.1.1.3">ùëî</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.13.m2.1c">M_{g}</annotation></semantics></math> are the numbers of predictions and ground truth, respectively.
<math id="S3.p2.14.m3.1" class="ltx_Math" alttext="z_{ij}" display="inline"><semantics id="S3.p2.14.m3.1a"><msub id="S3.p2.14.m3.1.1" xref="S3.p2.14.m3.1.1.cmml"><mi id="S3.p2.14.m3.1.1.2" xref="S3.p2.14.m3.1.1.2.cmml">z</mi><mrow id="S3.p2.14.m3.1.1.3" xref="S3.p2.14.m3.1.1.3.cmml"><mi id="S3.p2.14.m3.1.1.3.2" xref="S3.p2.14.m3.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.p2.14.m3.1.1.3.1" xref="S3.p2.14.m3.1.1.3.1.cmml">‚Äã</mo><mi id="S3.p2.14.m3.1.1.3.3" xref="S3.p2.14.m3.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.p2.14.m3.1b"><apply id="S3.p2.14.m3.1.1.cmml" xref="S3.p2.14.m3.1.1"><csymbol cd="ambiguous" id="S3.p2.14.m3.1.1.1.cmml" xref="S3.p2.14.m3.1.1">subscript</csymbol><ci id="S3.p2.14.m3.1.1.2.cmml" xref="S3.p2.14.m3.1.1.2">ùëß</ci><apply id="S3.p2.14.m3.1.1.3.cmml" xref="S3.p2.14.m3.1.1.3"><times id="S3.p2.14.m3.1.1.3.1.cmml" xref="S3.p2.14.m3.1.1.3.1"></times><ci id="S3.p2.14.m3.1.1.3.2.cmml" xref="S3.p2.14.m3.1.1.3.2">ùëñ</ci><ci id="S3.p2.14.m3.1.1.3.3.cmml" xref="S3.p2.14.m3.1.1.3.3">ùëó</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.14.m3.1c">z_{ij}</annotation></semantics></math> is a binary variable indicating whether the <math id="S3.p2.15.m4.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.p2.15.m4.1a"><mi id="S3.p2.15.m4.1.1" xref="S3.p2.15.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.p2.15.m4.1b"><ci id="S3.p2.15.m4.1.1.cmml" xref="S3.p2.15.m4.1.1">ùëñ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.15.m4.1c">i</annotation></semantics></math>-th ground truth is matched with the <math id="S3.p2.16.m5.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.p2.16.m5.1a"><mi id="S3.p2.16.m5.1.1" xref="S3.p2.16.m5.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.p2.16.m5.1b"><ci id="S3.p2.16.m5.1.1.cmml" xref="S3.p2.16.m5.1.1">ùëó</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.16.m5.1c">j</annotation></semantics></math>-th prediction.
<math id="S3.p2.17.m6.1" class="ltx_Math" alttext="a_{ij}" display="inline"><semantics id="S3.p2.17.m6.1a"><msub id="S3.p2.17.m6.1.1" xref="S3.p2.17.m6.1.1.cmml"><mi id="S3.p2.17.m6.1.1.2" xref="S3.p2.17.m6.1.1.2.cmml">a</mi><mrow id="S3.p2.17.m6.1.1.3" xref="S3.p2.17.m6.1.1.3.cmml"><mi id="S3.p2.17.m6.1.1.3.2" xref="S3.p2.17.m6.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.p2.17.m6.1.1.3.1" xref="S3.p2.17.m6.1.1.3.1.cmml">‚Äã</mo><mi id="S3.p2.17.m6.1.1.3.3" xref="S3.p2.17.m6.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.p2.17.m6.1b"><apply id="S3.p2.17.m6.1.1.cmml" xref="S3.p2.17.m6.1.1"><csymbol cd="ambiguous" id="S3.p2.17.m6.1.1.1.cmml" xref="S3.p2.17.m6.1.1">subscript</csymbol><ci id="S3.p2.17.m6.1.1.2.cmml" xref="S3.p2.17.m6.1.1.2">ùëé</ci><apply id="S3.p2.17.m6.1.1.3.cmml" xref="S3.p2.17.m6.1.1.3"><times id="S3.p2.17.m6.1.1.3.1.cmml" xref="S3.p2.17.m6.1.1.3.1"></times><ci id="S3.p2.17.m6.1.1.3.2.cmml" xref="S3.p2.17.m6.1.1.3.2">ùëñ</ci><ci id="S3.p2.17.m6.1.1.3.3.cmml" xref="S3.p2.17.m6.1.1.3.3">ùëó</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.17.m6.1c">a_{ij}</annotation></semantics></math> is the award for matching the <math id="S3.p2.18.m7.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.p2.18.m7.1a"><mi id="S3.p2.18.m7.1.1" xref="S3.p2.18.m7.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.p2.18.m7.1b"><ci id="S3.p2.18.m7.1.1.cmml" xref="S3.p2.18.m7.1.1">ùëñ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.18.m7.1c">i</annotation></semantics></math>-th ground truth with the <math id="S3.p2.19.m8.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.p2.19.m8.1a"><mi id="S3.p2.19.m8.1.1" xref="S3.p2.19.m8.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.p2.19.m8.1b"><ci id="S3.p2.19.m8.1.1.cmml" xref="S3.p2.19.m8.1.1">ùëó</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.19.m8.1c">j</annotation></semantics></math>-th prediction.
A larger value of <math id="S3.p2.20.m9.1" class="ltx_Math" alttext="a_{ij}" display="inline"><semantics id="S3.p2.20.m9.1a"><msub id="S3.p2.20.m9.1.1" xref="S3.p2.20.m9.1.1.cmml"><mi id="S3.p2.20.m9.1.1.2" xref="S3.p2.20.m9.1.1.2.cmml">a</mi><mrow id="S3.p2.20.m9.1.1.3" xref="S3.p2.20.m9.1.1.3.cmml"><mi id="S3.p2.20.m9.1.1.3.2" xref="S3.p2.20.m9.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.p2.20.m9.1.1.3.1" xref="S3.p2.20.m9.1.1.3.1.cmml">‚Äã</mo><mi id="S3.p2.20.m9.1.1.3.3" xref="S3.p2.20.m9.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.p2.20.m9.1b"><apply id="S3.p2.20.m9.1.1.cmml" xref="S3.p2.20.m9.1.1"><csymbol cd="ambiguous" id="S3.p2.20.m9.1.1.1.cmml" xref="S3.p2.20.m9.1.1">subscript</csymbol><ci id="S3.p2.20.m9.1.1.2.cmml" xref="S3.p2.20.m9.1.1.2">ùëé</ci><apply id="S3.p2.20.m9.1.1.3.cmml" xref="S3.p2.20.m9.1.1.3"><times id="S3.p2.20.m9.1.1.3.1.cmml" xref="S3.p2.20.m9.1.1.3.1"></times><ci id="S3.p2.20.m9.1.1.3.2.cmml" xref="S3.p2.20.m9.1.1.3.2">ùëñ</ci><ci id="S3.p2.20.m9.1.1.3.3.cmml" xref="S3.p2.20.m9.1.1.3.3">ùëó</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.20.m9.1c">a_{ij}</annotation></semantics></math> implies a stronger matching between the two.
It consists of three parts.</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S3.I1.i1.p1" class="ltx_para ltx_noindent">
<p id="S3.I1.i1.p1.1" class="ltx_p"><em id="S3.I1.i1.p1.1.1" class="ltx_emph ltx_font_italic">Label consistency score</em> (<math id="S3.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="c_{ij}" display="inline"><semantics id="S3.I1.i1.p1.1.m1.1a"><msub id="S3.I1.i1.p1.1.m1.1.1" xref="S3.I1.i1.p1.1.m1.1.1.cmml"><mi id="S3.I1.i1.p1.1.m1.1.1.2" xref="S3.I1.i1.p1.1.m1.1.1.2.cmml">c</mi><mrow id="S3.I1.i1.p1.1.m1.1.1.3" xref="S3.I1.i1.p1.1.m1.1.1.3.cmml"><mi id="S3.I1.i1.p1.1.m1.1.1.3.2" xref="S3.I1.i1.p1.1.m1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.I1.i1.p1.1.m1.1.1.3.1" xref="S3.I1.i1.p1.1.m1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.I1.i1.p1.1.m1.1.1.3.3" xref="S3.I1.i1.p1.1.m1.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.1.m1.1b"><apply id="S3.I1.i1.p1.1.m1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I1.i1.p1.1.m1.1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.I1.i1.p1.1.m1.1.1.2.cmml" xref="S3.I1.i1.p1.1.m1.1.1.2">ùëê</ci><apply id="S3.I1.i1.p1.1.m1.1.1.3.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3"><times id="S3.I1.i1.p1.1.m1.1.1.3.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3.1"></times><ci id="S3.I1.i1.p1.1.m1.1.1.3.2.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3.2">ùëñ</ci><ci id="S3.I1.i1.p1.1.m1.1.1.3.3.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3.3">ùëó</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.1.m1.1c">c_{ij}</annotation></semantics></math>) that ensures the
matched prediction and ground truth have the same label.
It is set to 1 when the prediction and ground truth have the same label, and 0 otherwise.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S3.I1.i2.p1" class="ltx_para ltx_noindent">
<p id="S3.I1.i2.p1.1" class="ltx_p"><em id="S3.I1.i2.p1.1.1" class="ltx_emph ltx_font_italic">Position consistency score</em> (<math id="S3.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="p_{ij}" display="inline"><semantics id="S3.I1.i2.p1.1.m1.1a"><msub id="S3.I1.i2.p1.1.m1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.cmml"><mi id="S3.I1.i2.p1.1.m1.1.1.2" xref="S3.I1.i2.p1.1.m1.1.1.2.cmml">p</mi><mrow id="S3.I1.i2.p1.1.m1.1.1.3" xref="S3.I1.i2.p1.1.m1.1.1.3.cmml"><mi id="S3.I1.i2.p1.1.m1.1.1.3.2" xref="S3.I1.i2.p1.1.m1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.I1.i2.p1.1.m1.1.1.3.1" xref="S3.I1.i2.p1.1.m1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.I1.i2.p1.1.m1.1.1.3.3" xref="S3.I1.i2.p1.1.m1.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.1b"><apply id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.1.m1.1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.I1.i2.p1.1.m1.1.1.2.cmml" xref="S3.I1.i2.p1.1.m1.1.1.2">ùëù</ci><apply id="S3.I1.i2.p1.1.m1.1.1.3.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3"><times id="S3.I1.i2.p1.1.m1.1.1.3.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3.1"></times><ci id="S3.I1.i2.p1.1.m1.1.1.3.2.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3.2">ùëñ</ci><ci id="S3.I1.i2.p1.1.m1.1.1.3.3.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3.3">ùëó</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.1c">p_{ij}</annotation></semantics></math>) that ensures the
matched prediction and ground truth have a large overlap.
We apply the widely-used IoU (Intersection over Union) to measure the overlap.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S3.I1.i3.p1" class="ltx_para ltx_noindent">
<p id="S3.I1.i3.p1.5" class="ltx_p"><em id="S3.I1.i3.p1.5.1" class="ltx_emph ltx_font_italic">Uniqueness score</em> of the <math id="S3.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.I1.i3.p1.1.m1.1a"><mi id="S3.I1.i3.p1.1.m1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.1.m1.1b"><ci id="S3.I1.i3.p1.1.m1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1">ùëñ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.1.m1.1c">i</annotation></semantics></math>-th ground truth (<math id="S3.I1.i3.p1.2.m2.1" class="ltx_Math" alttext="u_{i}" display="inline"><semantics id="S3.I1.i3.p1.2.m2.1a"><msub id="S3.I1.i3.p1.2.m2.1.1" xref="S3.I1.i3.p1.2.m2.1.1.cmml"><mi id="S3.I1.i3.p1.2.m2.1.1.2" xref="S3.I1.i3.p1.2.m2.1.1.2.cmml">u</mi><mi id="S3.I1.i3.p1.2.m2.1.1.3" xref="S3.I1.i3.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.2.m2.1b"><apply id="S3.I1.i3.p1.2.m2.1.1.cmml" xref="S3.I1.i3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.I1.i3.p1.2.m2.1.1.1.cmml" xref="S3.I1.i3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.I1.i3.p1.2.m2.1.1.2.cmml" xref="S3.I1.i3.p1.2.m2.1.1.2">ùë¢</ci><ci id="S3.I1.i3.p1.2.m2.1.1.3.cmml" xref="S3.I1.i3.p1.2.m2.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.2.m2.1c">u_{i}</annotation></semantics></math>) that ensures the ground truth is matched with one prediction.
We set <math id="S3.I1.i3.p1.3.m3.1" class="ltx_Math" alttext="u_{i}=e^{-\sum_{k}z_{ik}}" display="inline"><semantics id="S3.I1.i3.p1.3.m3.1a"><mrow id="S3.I1.i3.p1.3.m3.1.1" xref="S3.I1.i3.p1.3.m3.1.1.cmml"><msub id="S3.I1.i3.p1.3.m3.1.1.2" xref="S3.I1.i3.p1.3.m3.1.1.2.cmml"><mi id="S3.I1.i3.p1.3.m3.1.1.2.2" xref="S3.I1.i3.p1.3.m3.1.1.2.2.cmml">u</mi><mi id="S3.I1.i3.p1.3.m3.1.1.2.3" xref="S3.I1.i3.p1.3.m3.1.1.2.3.cmml">i</mi></msub><mo id="S3.I1.i3.p1.3.m3.1.1.1" xref="S3.I1.i3.p1.3.m3.1.1.1.cmml">=</mo><msup id="S3.I1.i3.p1.3.m3.1.1.3" xref="S3.I1.i3.p1.3.m3.1.1.3.cmml"><mi id="S3.I1.i3.p1.3.m3.1.1.3.2" xref="S3.I1.i3.p1.3.m3.1.1.3.2.cmml">e</mi><mrow id="S3.I1.i3.p1.3.m3.1.1.3.3" xref="S3.I1.i3.p1.3.m3.1.1.3.3.cmml"><mo id="S3.I1.i3.p1.3.m3.1.1.3.3a" xref="S3.I1.i3.p1.3.m3.1.1.3.3.cmml">‚àí</mo><mrow id="S3.I1.i3.p1.3.m3.1.1.3.3.2" xref="S3.I1.i3.p1.3.m3.1.1.3.3.2.cmml"><mstyle displaystyle="false" id="S3.I1.i3.p1.3.m3.1.1.3.3.2.1" xref="S3.I1.i3.p1.3.m3.1.1.3.3.2.1.cmml"><msub id="S3.I1.i3.p1.3.m3.1.1.3.3.2.1a" xref="S3.I1.i3.p1.3.m3.1.1.3.3.2.1.cmml"><mo id="S3.I1.i3.p1.3.m3.1.1.3.3.2.1.2" xref="S3.I1.i3.p1.3.m3.1.1.3.3.2.1.2.cmml">‚àë</mo><mi id="S3.I1.i3.p1.3.m3.1.1.3.3.2.1.3" xref="S3.I1.i3.p1.3.m3.1.1.3.3.2.1.3.cmml">k</mi></msub></mstyle><msub id="S3.I1.i3.p1.3.m3.1.1.3.3.2.2" xref="S3.I1.i3.p1.3.m3.1.1.3.3.2.2.cmml"><mi id="S3.I1.i3.p1.3.m3.1.1.3.3.2.2.2" xref="S3.I1.i3.p1.3.m3.1.1.3.3.2.2.2.cmml">z</mi><mrow id="S3.I1.i3.p1.3.m3.1.1.3.3.2.2.3" xref="S3.I1.i3.p1.3.m3.1.1.3.3.2.2.3.cmml"><mi id="S3.I1.i3.p1.3.m3.1.1.3.3.2.2.3.2" xref="S3.I1.i3.p1.3.m3.1.1.3.3.2.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.I1.i3.p1.3.m3.1.1.3.3.2.2.3.1" xref="S3.I1.i3.p1.3.m3.1.1.3.3.2.2.3.1.cmml">‚Äã</mo><mi id="S3.I1.i3.p1.3.m3.1.1.3.3.2.2.3.3" xref="S3.I1.i3.p1.3.m3.1.1.3.3.2.2.3.3.cmml">k</mi></mrow></msub></mrow></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.3.m3.1b"><apply id="S3.I1.i3.p1.3.m3.1.1.cmml" xref="S3.I1.i3.p1.3.m3.1.1"><eq id="S3.I1.i3.p1.3.m3.1.1.1.cmml" xref="S3.I1.i3.p1.3.m3.1.1.1"></eq><apply id="S3.I1.i3.p1.3.m3.1.1.2.cmml" xref="S3.I1.i3.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.I1.i3.p1.3.m3.1.1.2.1.cmml" xref="S3.I1.i3.p1.3.m3.1.1.2">subscript</csymbol><ci id="S3.I1.i3.p1.3.m3.1.1.2.2.cmml" xref="S3.I1.i3.p1.3.m3.1.1.2.2">ùë¢</ci><ci id="S3.I1.i3.p1.3.m3.1.1.2.3.cmml" xref="S3.I1.i3.p1.3.m3.1.1.2.3">ùëñ</ci></apply><apply id="S3.I1.i3.p1.3.m3.1.1.3.cmml" xref="S3.I1.i3.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.I1.i3.p1.3.m3.1.1.3.1.cmml" xref="S3.I1.i3.p1.3.m3.1.1.3">superscript</csymbol><ci id="S3.I1.i3.p1.3.m3.1.1.3.2.cmml" xref="S3.I1.i3.p1.3.m3.1.1.3.2">ùëí</ci><apply id="S3.I1.i3.p1.3.m3.1.1.3.3.cmml" xref="S3.I1.i3.p1.3.m3.1.1.3.3"><minus id="S3.I1.i3.p1.3.m3.1.1.3.3.1.cmml" xref="S3.I1.i3.p1.3.m3.1.1.3.3"></minus><apply id="S3.I1.i3.p1.3.m3.1.1.3.3.2.cmml" xref="S3.I1.i3.p1.3.m3.1.1.3.3.2"><apply id="S3.I1.i3.p1.3.m3.1.1.3.3.2.1.cmml" xref="S3.I1.i3.p1.3.m3.1.1.3.3.2.1"><csymbol cd="ambiguous" id="S3.I1.i3.p1.3.m3.1.1.3.3.2.1.1.cmml" xref="S3.I1.i3.p1.3.m3.1.1.3.3.2.1">subscript</csymbol><sum id="S3.I1.i3.p1.3.m3.1.1.3.3.2.1.2.cmml" xref="S3.I1.i3.p1.3.m3.1.1.3.3.2.1.2"></sum><ci id="S3.I1.i3.p1.3.m3.1.1.3.3.2.1.3.cmml" xref="S3.I1.i3.p1.3.m3.1.1.3.3.2.1.3">ùëò</ci></apply><apply id="S3.I1.i3.p1.3.m3.1.1.3.3.2.2.cmml" xref="S3.I1.i3.p1.3.m3.1.1.3.3.2.2"><csymbol cd="ambiguous" id="S3.I1.i3.p1.3.m3.1.1.3.3.2.2.1.cmml" xref="S3.I1.i3.p1.3.m3.1.1.3.3.2.2">subscript</csymbol><ci id="S3.I1.i3.p1.3.m3.1.1.3.3.2.2.2.cmml" xref="S3.I1.i3.p1.3.m3.1.1.3.3.2.2.2">ùëß</ci><apply id="S3.I1.i3.p1.3.m3.1.1.3.3.2.2.3.cmml" xref="S3.I1.i3.p1.3.m3.1.1.3.3.2.2.3"><times id="S3.I1.i3.p1.3.m3.1.1.3.3.2.2.3.1.cmml" xref="S3.I1.i3.p1.3.m3.1.1.3.3.2.2.3.1"></times><ci id="S3.I1.i3.p1.3.m3.1.1.3.3.2.2.3.2.cmml" xref="S3.I1.i3.p1.3.m3.1.1.3.3.2.2.3.2">ùëñ</ci><ci id="S3.I1.i3.p1.3.m3.1.1.3.3.2.2.3.3.cmml" xref="S3.I1.i3.p1.3.m3.1.1.3.3.2.2.3.3">ùëò</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.3.m3.1c">u_{i}=e^{-\sum_{k}z_{ik}}</annotation></semantics></math>.
The more predictions that are matched with the <math id="S3.I1.i3.p1.4.m4.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.I1.i3.p1.4.m4.1a"><mi id="S3.I1.i3.p1.4.m4.1.1" xref="S3.I1.i3.p1.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.4.m4.1b"><ci id="S3.I1.i3.p1.4.m4.1.1.cmml" xref="S3.I1.i3.p1.4.m4.1.1">ùëñ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.4.m4.1c">i</annotation></semantics></math>-th ground truth, the smaller <math id="S3.I1.i3.p1.5.m5.1" class="ltx_Math" alttext="u_{i}" display="inline"><semantics id="S3.I1.i3.p1.5.m5.1a"><msub id="S3.I1.i3.p1.5.m5.1.1" xref="S3.I1.i3.p1.5.m5.1.1.cmml"><mi id="S3.I1.i3.p1.5.m5.1.1.2" xref="S3.I1.i3.p1.5.m5.1.1.2.cmml">u</mi><mi id="S3.I1.i3.p1.5.m5.1.1.3" xref="S3.I1.i3.p1.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.5.m5.1b"><apply id="S3.I1.i3.p1.5.m5.1.1.cmml" xref="S3.I1.i3.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.I1.i3.p1.5.m5.1.1.1.cmml" xref="S3.I1.i3.p1.5.m5.1.1">subscript</csymbol><ci id="S3.I1.i3.p1.5.m5.1.1.2.cmml" xref="S3.I1.i3.p1.5.m5.1.1.2">ùë¢</ci><ci id="S3.I1.i3.p1.5.m5.1.1.3.cmml" xref="S3.I1.i3.p1.5.m5.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.5.m5.1c">u_{i}</annotation></semantics></math>.</p>
</div>
</li>
</ul>
<p id="S3.p2.39" class="ltx_p"><math id="S3.p2.21.m1.1" class="ltx_Math" alttext="a_{ij}" display="inline"><semantics id="S3.p2.21.m1.1a"><msub id="S3.p2.21.m1.1.1" xref="S3.p2.21.m1.1.1.cmml"><mi id="S3.p2.21.m1.1.1.2" xref="S3.p2.21.m1.1.1.2.cmml">a</mi><mrow id="S3.p2.21.m1.1.1.3" xref="S3.p2.21.m1.1.1.3.cmml"><mi id="S3.p2.21.m1.1.1.3.2" xref="S3.p2.21.m1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.p2.21.m1.1.1.3.1" xref="S3.p2.21.m1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.p2.21.m1.1.1.3.3" xref="S3.p2.21.m1.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.p2.21.m1.1b"><apply id="S3.p2.21.m1.1.1.cmml" xref="S3.p2.21.m1.1.1"><csymbol cd="ambiguous" id="S3.p2.21.m1.1.1.1.cmml" xref="S3.p2.21.m1.1.1">subscript</csymbol><ci id="S3.p2.21.m1.1.1.2.cmml" xref="S3.p2.21.m1.1.1.2">ùëé</ci><apply id="S3.p2.21.m1.1.1.3.cmml" xref="S3.p2.21.m1.1.1.3"><times id="S3.p2.21.m1.1.1.3.1.cmml" xref="S3.p2.21.m1.1.1.3.1"></times><ci id="S3.p2.21.m1.1.1.3.2.cmml" xref="S3.p2.21.m1.1.1.3.2">ùëñ</ci><ci id="S3.p2.21.m1.1.1.3.3.cmml" xref="S3.p2.21.m1.1.1.3.3">ùëó</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.21.m1.1c">a_{ij}</annotation></semantics></math> is defined as the weighted sum of the three scores: <math id="S3.p2.22.m2.1" class="ltx_Math" alttext="a_{ij}=\lambda_{1}c_{ij}+\\
\lambda_{2}p_{ij}+(1-\lambda_{1}-\lambda_{2})u_{i}" display="inline"><semantics id="S3.p2.22.m2.1a"><mrow id="S3.p2.22.m2.1.1" xref="S3.p2.22.m2.1.1.cmml"><msub id="S3.p2.22.m2.1.1.3" xref="S3.p2.22.m2.1.1.3.cmml"><mi id="S3.p2.22.m2.1.1.3.2" xref="S3.p2.22.m2.1.1.3.2.cmml">a</mi><mrow id="S3.p2.22.m2.1.1.3.3" xref="S3.p2.22.m2.1.1.3.3.cmml"><mi id="S3.p2.22.m2.1.1.3.3.2" xref="S3.p2.22.m2.1.1.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.p2.22.m2.1.1.3.3.1" xref="S3.p2.22.m2.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.p2.22.m2.1.1.3.3.3" xref="S3.p2.22.m2.1.1.3.3.3.cmml">j</mi></mrow></msub><mo id="S3.p2.22.m2.1.1.2" xref="S3.p2.22.m2.1.1.2.cmml">=</mo><mrow id="S3.p2.22.m2.1.1.1" xref="S3.p2.22.m2.1.1.1.cmml"><mrow id="S3.p2.22.m2.1.1.1.3" xref="S3.p2.22.m2.1.1.1.3.cmml"><msub id="S3.p2.22.m2.1.1.1.3.2" xref="S3.p2.22.m2.1.1.1.3.2.cmml"><mi id="S3.p2.22.m2.1.1.1.3.2.2" xref="S3.p2.22.m2.1.1.1.3.2.2.cmml">Œª</mi><mn id="S3.p2.22.m2.1.1.1.3.2.3" xref="S3.p2.22.m2.1.1.1.3.2.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S3.p2.22.m2.1.1.1.3.1" xref="S3.p2.22.m2.1.1.1.3.1.cmml">‚Äã</mo><msub id="S3.p2.22.m2.1.1.1.3.3" xref="S3.p2.22.m2.1.1.1.3.3.cmml"><mi id="S3.p2.22.m2.1.1.1.3.3.2" xref="S3.p2.22.m2.1.1.1.3.3.2.cmml">c</mi><mrow id="S3.p2.22.m2.1.1.1.3.3.3" xref="S3.p2.22.m2.1.1.1.3.3.3.cmml"><mi id="S3.p2.22.m2.1.1.1.3.3.3.2" xref="S3.p2.22.m2.1.1.1.3.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.p2.22.m2.1.1.1.3.3.3.1" xref="S3.p2.22.m2.1.1.1.3.3.3.1.cmml">‚Äã</mo><mi id="S3.p2.22.m2.1.1.1.3.3.3.3" xref="S3.p2.22.m2.1.1.1.3.3.3.3.cmml">j</mi></mrow></msub></mrow><mo id="S3.p2.22.m2.1.1.1.2" xref="S3.p2.22.m2.1.1.1.2.cmml">+</mo><mrow id="S3.p2.22.m2.1.1.1.4" xref="S3.p2.22.m2.1.1.1.4.cmml"><msub id="S3.p2.22.m2.1.1.1.4.2" xref="S3.p2.22.m2.1.1.1.4.2.cmml"><mi id="S3.p2.22.m2.1.1.1.4.2.2" xref="S3.p2.22.m2.1.1.1.4.2.2.cmml">Œª</mi><mn id="S3.p2.22.m2.1.1.1.4.2.3" xref="S3.p2.22.m2.1.1.1.4.2.3.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S3.p2.22.m2.1.1.1.4.1" xref="S3.p2.22.m2.1.1.1.4.1.cmml">‚Äã</mo><msub id="S3.p2.22.m2.1.1.1.4.3" xref="S3.p2.22.m2.1.1.1.4.3.cmml"><mi id="S3.p2.22.m2.1.1.1.4.3.2" xref="S3.p2.22.m2.1.1.1.4.3.2.cmml">p</mi><mrow id="S3.p2.22.m2.1.1.1.4.3.3" xref="S3.p2.22.m2.1.1.1.4.3.3.cmml"><mi id="S3.p2.22.m2.1.1.1.4.3.3.2" xref="S3.p2.22.m2.1.1.1.4.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.p2.22.m2.1.1.1.4.3.3.1" xref="S3.p2.22.m2.1.1.1.4.3.3.1.cmml">‚Äã</mo><mi id="S3.p2.22.m2.1.1.1.4.3.3.3" xref="S3.p2.22.m2.1.1.1.4.3.3.3.cmml">j</mi></mrow></msub></mrow><mo id="S3.p2.22.m2.1.1.1.2a" xref="S3.p2.22.m2.1.1.1.2.cmml">+</mo><mrow id="S3.p2.22.m2.1.1.1.1" xref="S3.p2.22.m2.1.1.1.1.cmml"><mrow id="S3.p2.22.m2.1.1.1.1.1.1" xref="S3.p2.22.m2.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.p2.22.m2.1.1.1.1.1.1.2" xref="S3.p2.22.m2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.p2.22.m2.1.1.1.1.1.1.1" xref="S3.p2.22.m2.1.1.1.1.1.1.1.cmml"><mn id="S3.p2.22.m2.1.1.1.1.1.1.1.2" xref="S3.p2.22.m2.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.p2.22.m2.1.1.1.1.1.1.1.1" xref="S3.p2.22.m2.1.1.1.1.1.1.1.1.cmml">‚àí</mo><msub id="S3.p2.22.m2.1.1.1.1.1.1.1.3" xref="S3.p2.22.m2.1.1.1.1.1.1.1.3.cmml"><mi id="S3.p2.22.m2.1.1.1.1.1.1.1.3.2" xref="S3.p2.22.m2.1.1.1.1.1.1.1.3.2.cmml">Œª</mi><mn id="S3.p2.22.m2.1.1.1.1.1.1.1.3.3" xref="S3.p2.22.m2.1.1.1.1.1.1.1.3.3.cmml">1</mn></msub><mo id="S3.p2.22.m2.1.1.1.1.1.1.1.1a" xref="S3.p2.22.m2.1.1.1.1.1.1.1.1.cmml">‚àí</mo><msub id="S3.p2.22.m2.1.1.1.1.1.1.1.4" xref="S3.p2.22.m2.1.1.1.1.1.1.1.4.cmml"><mi id="S3.p2.22.m2.1.1.1.1.1.1.1.4.2" xref="S3.p2.22.m2.1.1.1.1.1.1.1.4.2.cmml">Œª</mi><mn id="S3.p2.22.m2.1.1.1.1.1.1.1.4.3" xref="S3.p2.22.m2.1.1.1.1.1.1.1.4.3.cmml">2</mn></msub></mrow><mo stretchy="false" id="S3.p2.22.m2.1.1.1.1.1.1.3" xref="S3.p2.22.m2.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.p2.22.m2.1.1.1.1.2" xref="S3.p2.22.m2.1.1.1.1.2.cmml">‚Äã</mo><msub id="S3.p2.22.m2.1.1.1.1.3" xref="S3.p2.22.m2.1.1.1.1.3.cmml"><mi id="S3.p2.22.m2.1.1.1.1.3.2" xref="S3.p2.22.m2.1.1.1.1.3.2.cmml">u</mi><mi id="S3.p2.22.m2.1.1.1.1.3.3" xref="S3.p2.22.m2.1.1.1.1.3.3.cmml">i</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.22.m2.1b"><apply id="S3.p2.22.m2.1.1.cmml" xref="S3.p2.22.m2.1.1"><eq id="S3.p2.22.m2.1.1.2.cmml" xref="S3.p2.22.m2.1.1.2"></eq><apply id="S3.p2.22.m2.1.1.3.cmml" xref="S3.p2.22.m2.1.1.3"><csymbol cd="ambiguous" id="S3.p2.22.m2.1.1.3.1.cmml" xref="S3.p2.22.m2.1.1.3">subscript</csymbol><ci id="S3.p2.22.m2.1.1.3.2.cmml" xref="S3.p2.22.m2.1.1.3.2">ùëé</ci><apply id="S3.p2.22.m2.1.1.3.3.cmml" xref="S3.p2.22.m2.1.1.3.3"><times id="S3.p2.22.m2.1.1.3.3.1.cmml" xref="S3.p2.22.m2.1.1.3.3.1"></times><ci id="S3.p2.22.m2.1.1.3.3.2.cmml" xref="S3.p2.22.m2.1.1.3.3.2">ùëñ</ci><ci id="S3.p2.22.m2.1.1.3.3.3.cmml" xref="S3.p2.22.m2.1.1.3.3.3">ùëó</ci></apply></apply><apply id="S3.p2.22.m2.1.1.1.cmml" xref="S3.p2.22.m2.1.1.1"><plus id="S3.p2.22.m2.1.1.1.2.cmml" xref="S3.p2.22.m2.1.1.1.2"></plus><apply id="S3.p2.22.m2.1.1.1.3.cmml" xref="S3.p2.22.m2.1.1.1.3"><times id="S3.p2.22.m2.1.1.1.3.1.cmml" xref="S3.p2.22.m2.1.1.1.3.1"></times><apply id="S3.p2.22.m2.1.1.1.3.2.cmml" xref="S3.p2.22.m2.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.p2.22.m2.1.1.1.3.2.1.cmml" xref="S3.p2.22.m2.1.1.1.3.2">subscript</csymbol><ci id="S3.p2.22.m2.1.1.1.3.2.2.cmml" xref="S3.p2.22.m2.1.1.1.3.2.2">ùúÜ</ci><cn type="integer" id="S3.p2.22.m2.1.1.1.3.2.3.cmml" xref="S3.p2.22.m2.1.1.1.3.2.3">1</cn></apply><apply id="S3.p2.22.m2.1.1.1.3.3.cmml" xref="S3.p2.22.m2.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.p2.22.m2.1.1.1.3.3.1.cmml" xref="S3.p2.22.m2.1.1.1.3.3">subscript</csymbol><ci id="S3.p2.22.m2.1.1.1.3.3.2.cmml" xref="S3.p2.22.m2.1.1.1.3.3.2">ùëê</ci><apply id="S3.p2.22.m2.1.1.1.3.3.3.cmml" xref="S3.p2.22.m2.1.1.1.3.3.3"><times id="S3.p2.22.m2.1.1.1.3.3.3.1.cmml" xref="S3.p2.22.m2.1.1.1.3.3.3.1"></times><ci id="S3.p2.22.m2.1.1.1.3.3.3.2.cmml" xref="S3.p2.22.m2.1.1.1.3.3.3.2">ùëñ</ci><ci id="S3.p2.22.m2.1.1.1.3.3.3.3.cmml" xref="S3.p2.22.m2.1.1.1.3.3.3.3">ùëó</ci></apply></apply></apply><apply id="S3.p2.22.m2.1.1.1.4.cmml" xref="S3.p2.22.m2.1.1.1.4"><times id="S3.p2.22.m2.1.1.1.4.1.cmml" xref="S3.p2.22.m2.1.1.1.4.1"></times><apply id="S3.p2.22.m2.1.1.1.4.2.cmml" xref="S3.p2.22.m2.1.1.1.4.2"><csymbol cd="ambiguous" id="S3.p2.22.m2.1.1.1.4.2.1.cmml" xref="S3.p2.22.m2.1.1.1.4.2">subscript</csymbol><ci id="S3.p2.22.m2.1.1.1.4.2.2.cmml" xref="S3.p2.22.m2.1.1.1.4.2.2">ùúÜ</ci><cn type="integer" id="S3.p2.22.m2.1.1.1.4.2.3.cmml" xref="S3.p2.22.m2.1.1.1.4.2.3">2</cn></apply><apply id="S3.p2.22.m2.1.1.1.4.3.cmml" xref="S3.p2.22.m2.1.1.1.4.3"><csymbol cd="ambiguous" id="S3.p2.22.m2.1.1.1.4.3.1.cmml" xref="S3.p2.22.m2.1.1.1.4.3">subscript</csymbol><ci id="S3.p2.22.m2.1.1.1.4.3.2.cmml" xref="S3.p2.22.m2.1.1.1.4.3.2">ùëù</ci><apply id="S3.p2.22.m2.1.1.1.4.3.3.cmml" xref="S3.p2.22.m2.1.1.1.4.3.3"><times id="S3.p2.22.m2.1.1.1.4.3.3.1.cmml" xref="S3.p2.22.m2.1.1.1.4.3.3.1"></times><ci id="S3.p2.22.m2.1.1.1.4.3.3.2.cmml" xref="S3.p2.22.m2.1.1.1.4.3.3.2">ùëñ</ci><ci id="S3.p2.22.m2.1.1.1.4.3.3.3.cmml" xref="S3.p2.22.m2.1.1.1.4.3.3.3">ùëó</ci></apply></apply></apply><apply id="S3.p2.22.m2.1.1.1.1.cmml" xref="S3.p2.22.m2.1.1.1.1"><times id="S3.p2.22.m2.1.1.1.1.2.cmml" xref="S3.p2.22.m2.1.1.1.1.2"></times><apply id="S3.p2.22.m2.1.1.1.1.1.1.1.cmml" xref="S3.p2.22.m2.1.1.1.1.1.1"><minus id="S3.p2.22.m2.1.1.1.1.1.1.1.1.cmml" xref="S3.p2.22.m2.1.1.1.1.1.1.1.1"></minus><cn type="integer" id="S3.p2.22.m2.1.1.1.1.1.1.1.2.cmml" xref="S3.p2.22.m2.1.1.1.1.1.1.1.2">1</cn><apply id="S3.p2.22.m2.1.1.1.1.1.1.1.3.cmml" xref="S3.p2.22.m2.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.p2.22.m2.1.1.1.1.1.1.1.3.1.cmml" xref="S3.p2.22.m2.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.p2.22.m2.1.1.1.1.1.1.1.3.2.cmml" xref="S3.p2.22.m2.1.1.1.1.1.1.1.3.2">ùúÜ</ci><cn type="integer" id="S3.p2.22.m2.1.1.1.1.1.1.1.3.3.cmml" xref="S3.p2.22.m2.1.1.1.1.1.1.1.3.3">1</cn></apply><apply id="S3.p2.22.m2.1.1.1.1.1.1.1.4.cmml" xref="S3.p2.22.m2.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.p2.22.m2.1.1.1.1.1.1.1.4.1.cmml" xref="S3.p2.22.m2.1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S3.p2.22.m2.1.1.1.1.1.1.1.4.2.cmml" xref="S3.p2.22.m2.1.1.1.1.1.1.1.4.2">ùúÜ</ci><cn type="integer" id="S3.p2.22.m2.1.1.1.1.1.1.1.4.3.cmml" xref="S3.p2.22.m2.1.1.1.1.1.1.1.4.3">2</cn></apply></apply><apply id="S3.p2.22.m2.1.1.1.1.3.cmml" xref="S3.p2.22.m2.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.p2.22.m2.1.1.1.1.3.1.cmml" xref="S3.p2.22.m2.1.1.1.1.3">subscript</csymbol><ci id="S3.p2.22.m2.1.1.1.1.3.2.cmml" xref="S3.p2.22.m2.1.1.1.1.3.2">ùë¢</ci><ci id="S3.p2.22.m2.1.1.1.1.3.3.cmml" xref="S3.p2.22.m2.1.1.1.1.3.3">ùëñ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.22.m2.1c">a_{ij}=\lambda_{1}c_{ij}+\\
\lambda_{2}p_{ij}+(1-\lambda_{1}-\lambda_{2})u_{i}</annotation></semantics></math>.
<math id="S3.p2.23.m3.1" class="ltx_Math" alttext="\lambda_{1}" display="inline"><semantics id="S3.p2.23.m3.1a"><msub id="S3.p2.23.m3.1.1" xref="S3.p2.23.m3.1.1.cmml"><mi id="S3.p2.23.m3.1.1.2" xref="S3.p2.23.m3.1.1.2.cmml">Œª</mi><mn id="S3.p2.23.m3.1.1.3" xref="S3.p2.23.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.p2.23.m3.1b"><apply id="S3.p2.23.m3.1.1.cmml" xref="S3.p2.23.m3.1.1"><csymbol cd="ambiguous" id="S3.p2.23.m3.1.1.1.cmml" xref="S3.p2.23.m3.1.1">subscript</csymbol><ci id="S3.p2.23.m3.1.1.2.cmml" xref="S3.p2.23.m3.1.1.2">ùúÜ</ci><cn type="integer" id="S3.p2.23.m3.1.1.3.cmml" xref="S3.p2.23.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.23.m3.1c">\lambda_{1}</annotation></semantics></math> and <math id="S3.p2.24.m4.1" class="ltx_Math" alttext="\lambda_{2}" display="inline"><semantics id="S3.p2.24.m4.1a"><msub id="S3.p2.24.m4.1.1" xref="S3.p2.24.m4.1.1.cmml"><mi id="S3.p2.24.m4.1.1.2" xref="S3.p2.24.m4.1.1.2.cmml">Œª</mi><mn id="S3.p2.24.m4.1.1.3" xref="S3.p2.24.m4.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.p2.24.m4.1b"><apply id="S3.p2.24.m4.1.1.cmml" xref="S3.p2.24.m4.1.1"><csymbol cd="ambiguous" id="S3.p2.24.m4.1.1.1.cmml" xref="S3.p2.24.m4.1.1">subscript</csymbol><ci id="S3.p2.24.m4.1.1.2.cmml" xref="S3.p2.24.m4.1.1.2">ùúÜ</ci><cn type="integer" id="S3.p2.24.m4.1.1.3.cmml" xref="S3.p2.24.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.24.m4.1c">\lambda_{2}</annotation></semantics></math> are used to balance the three parts.
They are set as <math id="S3.p2.25.m5.1" class="ltx_Math" alttext="0.5" display="inline"><semantics id="S3.p2.25.m5.1a"><mn id="S3.p2.25.m5.1.1" xref="S3.p2.25.m5.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S3.p2.25.m5.1b"><cn type="float" id="S3.p2.25.m5.1.1.cmml" xref="S3.p2.25.m5.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.25.m5.1c">0.5</annotation></semantics></math> and <math id="S3.p2.26.m6.1" class="ltx_Math" alttext="0.25" display="inline"><semantics id="S3.p2.26.m6.1a"><mn id="S3.p2.26.m6.1.1" xref="S3.p2.26.m6.1.1.cmml">0.25</mn><annotation-xml encoding="MathML-Content" id="S3.p2.26.m6.1b"><cn type="float" id="S3.p2.26.m6.1.1.cmml" xref="S3.p2.26.m6.1.1">0.25</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.26.m6.1c">0.25</annotation></semantics></math>
to emphasize the importance of the label consistency score¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
The constraint <math id="S3.p2.27.m7.1" class="ltx_Math" alttext="z_{ij}\mathbb{I}(p_{ij}&lt;\alpha)=0" display="inline"><semantics id="S3.p2.27.m7.1a"><mrow id="S3.p2.27.m7.1.1" xref="S3.p2.27.m7.1.1.cmml"><mrow id="S3.p2.27.m7.1.1.1" xref="S3.p2.27.m7.1.1.1.cmml"><msub id="S3.p2.27.m7.1.1.1.3" xref="S3.p2.27.m7.1.1.1.3.cmml"><mi id="S3.p2.27.m7.1.1.1.3.2" xref="S3.p2.27.m7.1.1.1.3.2.cmml">z</mi><mrow id="S3.p2.27.m7.1.1.1.3.3" xref="S3.p2.27.m7.1.1.1.3.3.cmml"><mi id="S3.p2.27.m7.1.1.1.3.3.2" xref="S3.p2.27.m7.1.1.1.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.p2.27.m7.1.1.1.3.3.1" xref="S3.p2.27.m7.1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.p2.27.m7.1.1.1.3.3.3" xref="S3.p2.27.m7.1.1.1.3.3.3.cmml">j</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.p2.27.m7.1.1.1.2" xref="S3.p2.27.m7.1.1.1.2.cmml">‚Äã</mo><mi id="S3.p2.27.m7.1.1.1.4" xref="S3.p2.27.m7.1.1.1.4.cmml">ùïÄ</mi><mo lspace="0em" rspace="0em" id="S3.p2.27.m7.1.1.1.2a" xref="S3.p2.27.m7.1.1.1.2.cmml">‚Äã</mo><mrow id="S3.p2.27.m7.1.1.1.1.1" xref="S3.p2.27.m7.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.p2.27.m7.1.1.1.1.1.2" xref="S3.p2.27.m7.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.p2.27.m7.1.1.1.1.1.1" xref="S3.p2.27.m7.1.1.1.1.1.1.cmml"><msub id="S3.p2.27.m7.1.1.1.1.1.1.2" xref="S3.p2.27.m7.1.1.1.1.1.1.2.cmml"><mi id="S3.p2.27.m7.1.1.1.1.1.1.2.2" xref="S3.p2.27.m7.1.1.1.1.1.1.2.2.cmml">p</mi><mrow id="S3.p2.27.m7.1.1.1.1.1.1.2.3" xref="S3.p2.27.m7.1.1.1.1.1.1.2.3.cmml"><mi id="S3.p2.27.m7.1.1.1.1.1.1.2.3.2" xref="S3.p2.27.m7.1.1.1.1.1.1.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.p2.27.m7.1.1.1.1.1.1.2.3.1" xref="S3.p2.27.m7.1.1.1.1.1.1.2.3.1.cmml">‚Äã</mo><mi id="S3.p2.27.m7.1.1.1.1.1.1.2.3.3" xref="S3.p2.27.m7.1.1.1.1.1.1.2.3.3.cmml">j</mi></mrow></msub><mo id="S3.p2.27.m7.1.1.1.1.1.1.1" xref="S3.p2.27.m7.1.1.1.1.1.1.1.cmml">&lt;</mo><mi id="S3.p2.27.m7.1.1.1.1.1.1.3" xref="S3.p2.27.m7.1.1.1.1.1.1.3.cmml">Œ±</mi></mrow><mo stretchy="false" id="S3.p2.27.m7.1.1.1.1.1.3" xref="S3.p2.27.m7.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.p2.27.m7.1.1.2" xref="S3.p2.27.m7.1.1.2.cmml">=</mo><mn id="S3.p2.27.m7.1.1.3" xref="S3.p2.27.m7.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.27.m7.1b"><apply id="S3.p2.27.m7.1.1.cmml" xref="S3.p2.27.m7.1.1"><eq id="S3.p2.27.m7.1.1.2.cmml" xref="S3.p2.27.m7.1.1.2"></eq><apply id="S3.p2.27.m7.1.1.1.cmml" xref="S3.p2.27.m7.1.1.1"><times id="S3.p2.27.m7.1.1.1.2.cmml" xref="S3.p2.27.m7.1.1.1.2"></times><apply id="S3.p2.27.m7.1.1.1.3.cmml" xref="S3.p2.27.m7.1.1.1.3"><csymbol cd="ambiguous" id="S3.p2.27.m7.1.1.1.3.1.cmml" xref="S3.p2.27.m7.1.1.1.3">subscript</csymbol><ci id="S3.p2.27.m7.1.1.1.3.2.cmml" xref="S3.p2.27.m7.1.1.1.3.2">ùëß</ci><apply id="S3.p2.27.m7.1.1.1.3.3.cmml" xref="S3.p2.27.m7.1.1.1.3.3"><times id="S3.p2.27.m7.1.1.1.3.3.1.cmml" xref="S3.p2.27.m7.1.1.1.3.3.1"></times><ci id="S3.p2.27.m7.1.1.1.3.3.2.cmml" xref="S3.p2.27.m7.1.1.1.3.3.2">ùëñ</ci><ci id="S3.p2.27.m7.1.1.1.3.3.3.cmml" xref="S3.p2.27.m7.1.1.1.3.3.3">ùëó</ci></apply></apply><ci id="S3.p2.27.m7.1.1.1.4.cmml" xref="S3.p2.27.m7.1.1.1.4">ùïÄ</ci><apply id="S3.p2.27.m7.1.1.1.1.1.1.cmml" xref="S3.p2.27.m7.1.1.1.1.1"><lt id="S3.p2.27.m7.1.1.1.1.1.1.1.cmml" xref="S3.p2.27.m7.1.1.1.1.1.1.1"></lt><apply id="S3.p2.27.m7.1.1.1.1.1.1.2.cmml" xref="S3.p2.27.m7.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.p2.27.m7.1.1.1.1.1.1.2.1.cmml" xref="S3.p2.27.m7.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.p2.27.m7.1.1.1.1.1.1.2.2.cmml" xref="S3.p2.27.m7.1.1.1.1.1.1.2.2">ùëù</ci><apply id="S3.p2.27.m7.1.1.1.1.1.1.2.3.cmml" xref="S3.p2.27.m7.1.1.1.1.1.1.2.3"><times id="S3.p2.27.m7.1.1.1.1.1.1.2.3.1.cmml" xref="S3.p2.27.m7.1.1.1.1.1.1.2.3.1"></times><ci id="S3.p2.27.m7.1.1.1.1.1.1.2.3.2.cmml" xref="S3.p2.27.m7.1.1.1.1.1.1.2.3.2">ùëñ</ci><ci id="S3.p2.27.m7.1.1.1.1.1.1.2.3.3.cmml" xref="S3.p2.27.m7.1.1.1.1.1.1.2.3.3">ùëó</ci></apply></apply><ci id="S3.p2.27.m7.1.1.1.1.1.1.3.cmml" xref="S3.p2.27.m7.1.1.1.1.1.1.3">ùõº</ci></apply></apply><cn type="integer" id="S3.p2.27.m7.1.1.3.cmml" xref="S3.p2.27.m7.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.27.m7.1c">z_{ij}\mathbb{I}(p_{ij}&lt;\alpha)=0</annotation></semantics></math> requires that the matched prediction and ground truth should have a minimum overlap <math id="S3.p2.28.m8.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.p2.28.m8.1a"><mi id="S3.p2.28.m8.1.1" xref="S3.p2.28.m8.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="S3.p2.28.m8.1b"><ci id="S3.p2.28.m8.1.1.cmml" xref="S3.p2.28.m8.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.28.m8.1c">\alpha</annotation></semantics></math>.
<math id="S3.p2.29.m9.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.p2.29.m9.1a"><mi id="S3.p2.29.m9.1.1" xref="S3.p2.29.m9.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="S3.p2.29.m9.1b"><ci id="S3.p2.29.m9.1.1.cmml" xref="S3.p2.29.m9.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.29.m9.1c">\alpha</annotation></semantics></math> is set to <math id="S3.p2.30.m10.1" class="ltx_Math" alttext="0.1" display="inline"><semantics id="S3.p2.30.m10.1a"><mn id="S3.p2.30.m10.1.1" xref="S3.p2.30.m10.1.1.cmml">0.1</mn><annotation-xml encoding="MathML-Content" id="S3.p2.30.m10.1b"><cn type="float" id="S3.p2.30.m10.1.1.cmml" xref="S3.p2.30.m10.1.1">0.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.30.m10.1c">0.1</annotation></semantics></math> following the setting in¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
<math id="S3.p2.31.m11.1" class="ltx_Math" alttext="\mathbb{I}(\cdot)" display="inline"><semantics id="S3.p2.31.m11.1a"><mrow id="S3.p2.31.m11.1.2" xref="S3.p2.31.m11.1.2.cmml"><mi id="S3.p2.31.m11.1.2.2" xref="S3.p2.31.m11.1.2.2.cmml">ùïÄ</mi><mo lspace="0em" rspace="0em" id="S3.p2.31.m11.1.2.1" xref="S3.p2.31.m11.1.2.1.cmml">‚Äã</mo><mrow id="S3.p2.31.m11.1.2.3.2" xref="S3.p2.31.m11.1.2.cmml"><mo stretchy="false" id="S3.p2.31.m11.1.2.3.2.1" xref="S3.p2.31.m11.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.p2.31.m11.1.1" xref="S3.p2.31.m11.1.1.cmml">‚ãÖ</mo><mo stretchy="false" id="S3.p2.31.m11.1.2.3.2.2" xref="S3.p2.31.m11.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.31.m11.1b"><apply id="S3.p2.31.m11.1.2.cmml" xref="S3.p2.31.m11.1.2"><times id="S3.p2.31.m11.1.2.1.cmml" xref="S3.p2.31.m11.1.2.1"></times><ci id="S3.p2.31.m11.1.2.2.cmml" xref="S3.p2.31.m11.1.2.2">ùïÄ</ci><ci id="S3.p2.31.m11.1.1.cmml" xref="S3.p2.31.m11.1.1">‚ãÖ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.31.m11.1c">\mathbb{I}(\cdot)</annotation></semantics></math> is the indicator function.
Constraint <math id="S3.p2.32.m12.1" class="ltx_Math" alttext="\sum_{i}z_{ij}\leq 1" display="inline"><semantics id="S3.p2.32.m12.1a"><mrow id="S3.p2.32.m12.1.1" xref="S3.p2.32.m12.1.1.cmml"><mrow id="S3.p2.32.m12.1.1.2" xref="S3.p2.32.m12.1.1.2.cmml"><msub id="S3.p2.32.m12.1.1.2.1" xref="S3.p2.32.m12.1.1.2.1.cmml"><mo id="S3.p2.32.m12.1.1.2.1.2" xref="S3.p2.32.m12.1.1.2.1.2.cmml">‚àë</mo><mi id="S3.p2.32.m12.1.1.2.1.3" xref="S3.p2.32.m12.1.1.2.1.3.cmml">i</mi></msub><msub id="S3.p2.32.m12.1.1.2.2" xref="S3.p2.32.m12.1.1.2.2.cmml"><mi id="S3.p2.32.m12.1.1.2.2.2" xref="S3.p2.32.m12.1.1.2.2.2.cmml">z</mi><mrow id="S3.p2.32.m12.1.1.2.2.3" xref="S3.p2.32.m12.1.1.2.2.3.cmml"><mi id="S3.p2.32.m12.1.1.2.2.3.2" xref="S3.p2.32.m12.1.1.2.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.p2.32.m12.1.1.2.2.3.1" xref="S3.p2.32.m12.1.1.2.2.3.1.cmml">‚Äã</mo><mi id="S3.p2.32.m12.1.1.2.2.3.3" xref="S3.p2.32.m12.1.1.2.2.3.3.cmml">j</mi></mrow></msub></mrow><mo id="S3.p2.32.m12.1.1.1" xref="S3.p2.32.m12.1.1.1.cmml">‚â§</mo><mn id="S3.p2.32.m12.1.1.3" xref="S3.p2.32.m12.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.32.m12.1b"><apply id="S3.p2.32.m12.1.1.cmml" xref="S3.p2.32.m12.1.1"><leq id="S3.p2.32.m12.1.1.1.cmml" xref="S3.p2.32.m12.1.1.1"></leq><apply id="S3.p2.32.m12.1.1.2.cmml" xref="S3.p2.32.m12.1.1.2"><apply id="S3.p2.32.m12.1.1.2.1.cmml" xref="S3.p2.32.m12.1.1.2.1"><csymbol cd="ambiguous" id="S3.p2.32.m12.1.1.2.1.1.cmml" xref="S3.p2.32.m12.1.1.2.1">subscript</csymbol><sum id="S3.p2.32.m12.1.1.2.1.2.cmml" xref="S3.p2.32.m12.1.1.2.1.2"></sum><ci id="S3.p2.32.m12.1.1.2.1.3.cmml" xref="S3.p2.32.m12.1.1.2.1.3">ùëñ</ci></apply><apply id="S3.p2.32.m12.1.1.2.2.cmml" xref="S3.p2.32.m12.1.1.2.2"><csymbol cd="ambiguous" id="S3.p2.32.m12.1.1.2.2.1.cmml" xref="S3.p2.32.m12.1.1.2.2">subscript</csymbol><ci id="S3.p2.32.m12.1.1.2.2.2.cmml" xref="S3.p2.32.m12.1.1.2.2.2">ùëß</ci><apply id="S3.p2.32.m12.1.1.2.2.3.cmml" xref="S3.p2.32.m12.1.1.2.2.3"><times id="S3.p2.32.m12.1.1.2.2.3.1.cmml" xref="S3.p2.32.m12.1.1.2.2.3.1"></times><ci id="S3.p2.32.m12.1.1.2.2.3.2.cmml" xref="S3.p2.32.m12.1.1.2.2.3.2">ùëñ</ci><ci id="S3.p2.32.m12.1.1.2.2.3.3.cmml" xref="S3.p2.32.m12.1.1.2.2.3.3">ùëó</ci></apply></apply></apply><cn type="integer" id="S3.p2.32.m12.1.1.3.cmml" xref="S3.p2.32.m12.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.32.m12.1c">\sum_{i}z_{ij}\leq 1</annotation></semantics></math> ensures that each prediction is matched to a maximum of one ground truth.
We solve this assignment problem with a greedy strategy.
We match the predictions in an image one by one according to their confidence in descending order.
For each prediction, we match it with a ground truth to maximize Eq.¬†(<a href="#S3.E1" title="Equation 1 ‚Ä£ 3 Unified Probability Distribution ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).
Thus, the time complexity to match the predictions with the ground truth in an image is <math id="S3.p2.33.m13.1" class="ltx_Math" alttext="O(M_{p}M_{g})" display="inline"><semantics id="S3.p2.33.m13.1a"><mrow id="S3.p2.33.m13.1.1" xref="S3.p2.33.m13.1.1.cmml"><mi id="S3.p2.33.m13.1.1.3" xref="S3.p2.33.m13.1.1.3.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.p2.33.m13.1.1.2" xref="S3.p2.33.m13.1.1.2.cmml">‚Äã</mo><mrow id="S3.p2.33.m13.1.1.1.1" xref="S3.p2.33.m13.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.p2.33.m13.1.1.1.1.2" xref="S3.p2.33.m13.1.1.1.1.1.cmml">(</mo><mrow id="S3.p2.33.m13.1.1.1.1.1" xref="S3.p2.33.m13.1.1.1.1.1.cmml"><msub id="S3.p2.33.m13.1.1.1.1.1.2" xref="S3.p2.33.m13.1.1.1.1.1.2.cmml"><mi id="S3.p2.33.m13.1.1.1.1.1.2.2" xref="S3.p2.33.m13.1.1.1.1.1.2.2.cmml">M</mi><mi id="S3.p2.33.m13.1.1.1.1.1.2.3" xref="S3.p2.33.m13.1.1.1.1.1.2.3.cmml">p</mi></msub><mo lspace="0em" rspace="0em" id="S3.p2.33.m13.1.1.1.1.1.1" xref="S3.p2.33.m13.1.1.1.1.1.1.cmml">‚Äã</mo><msub id="S3.p2.33.m13.1.1.1.1.1.3" xref="S3.p2.33.m13.1.1.1.1.1.3.cmml"><mi id="S3.p2.33.m13.1.1.1.1.1.3.2" xref="S3.p2.33.m13.1.1.1.1.1.3.2.cmml">M</mi><mi id="S3.p2.33.m13.1.1.1.1.1.3.3" xref="S3.p2.33.m13.1.1.1.1.1.3.3.cmml">g</mi></msub></mrow><mo stretchy="false" id="S3.p2.33.m13.1.1.1.1.3" xref="S3.p2.33.m13.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.33.m13.1b"><apply id="S3.p2.33.m13.1.1.cmml" xref="S3.p2.33.m13.1.1"><times id="S3.p2.33.m13.1.1.2.cmml" xref="S3.p2.33.m13.1.1.2"></times><ci id="S3.p2.33.m13.1.1.3.cmml" xref="S3.p2.33.m13.1.1.3">ùëÇ</ci><apply id="S3.p2.33.m13.1.1.1.1.1.cmml" xref="S3.p2.33.m13.1.1.1.1"><times id="S3.p2.33.m13.1.1.1.1.1.1.cmml" xref="S3.p2.33.m13.1.1.1.1.1.1"></times><apply id="S3.p2.33.m13.1.1.1.1.1.2.cmml" xref="S3.p2.33.m13.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.p2.33.m13.1.1.1.1.1.2.1.cmml" xref="S3.p2.33.m13.1.1.1.1.1.2">subscript</csymbol><ci id="S3.p2.33.m13.1.1.1.1.1.2.2.cmml" xref="S3.p2.33.m13.1.1.1.1.1.2.2">ùëÄ</ci><ci id="S3.p2.33.m13.1.1.1.1.1.2.3.cmml" xref="S3.p2.33.m13.1.1.1.1.1.2.3">ùëù</ci></apply><apply id="S3.p2.33.m13.1.1.1.1.1.3.cmml" xref="S3.p2.33.m13.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.p2.33.m13.1.1.1.1.1.3.1.cmml" xref="S3.p2.33.m13.1.1.1.1.1.3">subscript</csymbol><ci id="S3.p2.33.m13.1.1.1.1.1.3.2.cmml" xref="S3.p2.33.m13.1.1.1.1.1.3.2">ùëÄ</ci><ci id="S3.p2.33.m13.1.1.1.1.1.3.3.cmml" xref="S3.p2.33.m13.1.1.1.1.1.3.3">ùëî</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.33.m13.1c">O(M_{p}M_{g})</annotation></semantics></math>, and for a dataset with <math id="S3.p2.34.m14.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.p2.34.m14.1a"><mi id="S3.p2.34.m14.1.1" xref="S3.p2.34.m14.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.p2.34.m14.1b"><ci id="S3.p2.34.m14.1.1.cmml" xref="S3.p2.34.m14.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.34.m14.1c">N</annotation></semantics></math> images, it is <math id="S3.p2.35.m15.1" class="ltx_Math" alttext="O(M_{p}M_{g}N)" display="inline"><semantics id="S3.p2.35.m15.1a"><mrow id="S3.p2.35.m15.1.1" xref="S3.p2.35.m15.1.1.cmml"><mi id="S3.p2.35.m15.1.1.3" xref="S3.p2.35.m15.1.1.3.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.p2.35.m15.1.1.2" xref="S3.p2.35.m15.1.1.2.cmml">‚Äã</mo><mrow id="S3.p2.35.m15.1.1.1.1" xref="S3.p2.35.m15.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.p2.35.m15.1.1.1.1.2" xref="S3.p2.35.m15.1.1.1.1.1.cmml">(</mo><mrow id="S3.p2.35.m15.1.1.1.1.1" xref="S3.p2.35.m15.1.1.1.1.1.cmml"><msub id="S3.p2.35.m15.1.1.1.1.1.2" xref="S3.p2.35.m15.1.1.1.1.1.2.cmml"><mi id="S3.p2.35.m15.1.1.1.1.1.2.2" xref="S3.p2.35.m15.1.1.1.1.1.2.2.cmml">M</mi><mi id="S3.p2.35.m15.1.1.1.1.1.2.3" xref="S3.p2.35.m15.1.1.1.1.1.2.3.cmml">p</mi></msub><mo lspace="0em" rspace="0em" id="S3.p2.35.m15.1.1.1.1.1.1" xref="S3.p2.35.m15.1.1.1.1.1.1.cmml">‚Äã</mo><msub id="S3.p2.35.m15.1.1.1.1.1.3" xref="S3.p2.35.m15.1.1.1.1.1.3.cmml"><mi id="S3.p2.35.m15.1.1.1.1.1.3.2" xref="S3.p2.35.m15.1.1.1.1.1.3.2.cmml">M</mi><mi id="S3.p2.35.m15.1.1.1.1.1.3.3" xref="S3.p2.35.m15.1.1.1.1.1.3.3.cmml">g</mi></msub><mo lspace="0em" rspace="0em" id="S3.p2.35.m15.1.1.1.1.1.1a" xref="S3.p2.35.m15.1.1.1.1.1.1.cmml">‚Äã</mo><mi id="S3.p2.35.m15.1.1.1.1.1.4" xref="S3.p2.35.m15.1.1.1.1.1.4.cmml">N</mi></mrow><mo stretchy="false" id="S3.p2.35.m15.1.1.1.1.3" xref="S3.p2.35.m15.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.35.m15.1b"><apply id="S3.p2.35.m15.1.1.cmml" xref="S3.p2.35.m15.1.1"><times id="S3.p2.35.m15.1.1.2.cmml" xref="S3.p2.35.m15.1.1.2"></times><ci id="S3.p2.35.m15.1.1.3.cmml" xref="S3.p2.35.m15.1.1.3">ùëÇ</ci><apply id="S3.p2.35.m15.1.1.1.1.1.cmml" xref="S3.p2.35.m15.1.1.1.1"><times id="S3.p2.35.m15.1.1.1.1.1.1.cmml" xref="S3.p2.35.m15.1.1.1.1.1.1"></times><apply id="S3.p2.35.m15.1.1.1.1.1.2.cmml" xref="S3.p2.35.m15.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.p2.35.m15.1.1.1.1.1.2.1.cmml" xref="S3.p2.35.m15.1.1.1.1.1.2">subscript</csymbol><ci id="S3.p2.35.m15.1.1.1.1.1.2.2.cmml" xref="S3.p2.35.m15.1.1.1.1.1.2.2">ùëÄ</ci><ci id="S3.p2.35.m15.1.1.1.1.1.2.3.cmml" xref="S3.p2.35.m15.1.1.1.1.1.2.3">ùëù</ci></apply><apply id="S3.p2.35.m15.1.1.1.1.1.3.cmml" xref="S3.p2.35.m15.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.p2.35.m15.1.1.1.1.1.3.1.cmml" xref="S3.p2.35.m15.1.1.1.1.1.3">subscript</csymbol><ci id="S3.p2.35.m15.1.1.1.1.1.3.2.cmml" xref="S3.p2.35.m15.1.1.1.1.1.3.2">ùëÄ</ci><ci id="S3.p2.35.m15.1.1.1.1.1.3.3.cmml" xref="S3.p2.35.m15.1.1.1.1.1.3.3">ùëî</ci></apply><ci id="S3.p2.35.m15.1.1.1.1.1.4.cmml" xref="S3.p2.35.m15.1.1.1.1.1.4">ùëÅ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.35.m15.1c">O(M_{p}M_{g}N)</annotation></semantics></math>.
As <math id="S3.p2.36.m16.1" class="ltx_Math" alttext="M_{p}" display="inline"><semantics id="S3.p2.36.m16.1a"><msub id="S3.p2.36.m16.1.1" xref="S3.p2.36.m16.1.1.cmml"><mi id="S3.p2.36.m16.1.1.2" xref="S3.p2.36.m16.1.1.2.cmml">M</mi><mi id="S3.p2.36.m16.1.1.3" xref="S3.p2.36.m16.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.36.m16.1b"><apply id="S3.p2.36.m16.1.1.cmml" xref="S3.p2.36.m16.1.1"><csymbol cd="ambiguous" id="S3.p2.36.m16.1.1.1.cmml" xref="S3.p2.36.m16.1.1">subscript</csymbol><ci id="S3.p2.36.m16.1.1.2.cmml" xref="S3.p2.36.m16.1.1.2">ùëÄ</ci><ci id="S3.p2.36.m16.1.1.3.cmml" xref="S3.p2.36.m16.1.1.3">ùëù</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.36.m16.1c">M_{p}</annotation></semantics></math> and <math id="S3.p2.37.m17.1" class="ltx_Math" alttext="M_{g}" display="inline"><semantics id="S3.p2.37.m17.1a"><msub id="S3.p2.37.m17.1.1" xref="S3.p2.37.m17.1.1.cmml"><mi id="S3.p2.37.m17.1.1.2" xref="S3.p2.37.m17.1.1.2.cmml">M</mi><mi id="S3.p2.37.m17.1.1.3" xref="S3.p2.37.m17.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.37.m17.1b"><apply id="S3.p2.37.m17.1.1.cmml" xref="S3.p2.37.m17.1.1"><csymbol cd="ambiguous" id="S3.p2.37.m17.1.1.1.cmml" xref="S3.p2.37.m17.1.1">subscript</csymbol><ci id="S3.p2.37.m17.1.1.2.cmml" xref="S3.p2.37.m17.1.1.2">ùëÄ</ci><ci id="S3.p2.37.m17.1.1.3.cmml" xref="S3.p2.37.m17.1.1.3">ùëî</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.37.m17.1c">M_{g}</annotation></semantics></math> are usually significantly smaller than <math id="S3.p2.38.m18.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.p2.38.m18.1a"><mi id="S3.p2.38.m18.1.1" xref="S3.p2.38.m18.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.p2.38.m18.1b"><ci id="S3.p2.38.m18.1.1.cmml" xref="S3.p2.38.m18.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.38.m18.1c">N</annotation></semantics></math> in a large dataset, the time complexity is almost <math id="S3.p2.39.m19.1" class="ltx_Math" alttext="O(N)" display="inline"><semantics id="S3.p2.39.m19.1a"><mrow id="S3.p2.39.m19.1.2" xref="S3.p2.39.m19.1.2.cmml"><mi id="S3.p2.39.m19.1.2.2" xref="S3.p2.39.m19.1.2.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.p2.39.m19.1.2.1" xref="S3.p2.39.m19.1.2.1.cmml">‚Äã</mo><mrow id="S3.p2.39.m19.1.2.3.2" xref="S3.p2.39.m19.1.2.cmml"><mo stretchy="false" id="S3.p2.39.m19.1.2.3.2.1" xref="S3.p2.39.m19.1.2.cmml">(</mo><mi id="S3.p2.39.m19.1.1" xref="S3.p2.39.m19.1.1.cmml">N</mi><mo stretchy="false" id="S3.p2.39.m19.1.2.3.2.2" xref="S3.p2.39.m19.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.39.m19.1b"><apply id="S3.p2.39.m19.1.2.cmml" xref="S3.p2.39.m19.1.2"><times id="S3.p2.39.m19.1.2.1.cmml" xref="S3.p2.39.m19.1.2.1"></times><ci id="S3.p2.39.m19.1.2.2.cmml" xref="S3.p2.39.m19.1.2.2">ùëÇ</ci><ci id="S3.p2.39.m19.1.1.cmml" xref="S3.p2.39.m19.1.1">ùëÅ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.39.m19.1c">O(N)</annotation></semantics></math>.
For example, it only takes four minutes to process the COCO dataset¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> with more than 100,000 images.
In addition, object matching is a one-time pre-processing step.
Thus, this method can effectively scale up to handle large datasets with millions of images.</p>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.5" class="ltx_p"><span id="S3.p3.5.1" class="ltx_text ltx_font_bold">Modeling both discrete and continuous variables</span>.
Based on the matching
between predictions and ground truth,
we adopt the joint probability distribution to model both discrete and continuous variables.
Let <math id="S3.p3.1.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.p3.1.m1.1a"><mi id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><ci id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1">ùê∂</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">C</annotation></semantics></math> and <math id="S3.p3.2.m2.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.p3.2.m2.1a"><mi id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><ci id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1">ùê∑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">D</annotation></semantics></math> denote continuous and discrete variables, and let <math id="S3.p3.3.m3.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.p3.3.m3.1a"><mi id="S3.p3.3.m3.1.1" xref="S3.p3.3.m3.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.p3.3.m3.1b"><ci id="S3.p3.3.m3.1.1.cmml" xref="S3.p3.3.m3.1.1">ùëã</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.m3.1c">X</annotation></semantics></math> and <math id="S3.p3.4.m4.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S3.p3.4.m4.1a"><mi id="S3.p3.4.m4.1.1" xref="S3.p3.4.m4.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S3.p3.4.m4.1b"><ci id="S3.p3.4.m4.1.1.cmml" xref="S3.p3.4.m4.1.1">ùëå</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.4.m4.1c">Y</annotation></semantics></math> denote ground truth and predictions.
Then the joint probability distribution is given by <math id="S3.p3.5.m5.4" class="ltx_Math" alttext="P(C_{X},D_{X},C_{Y},D_{Y})" display="inline"><semantics id="S3.p3.5.m5.4a"><mrow id="S3.p3.5.m5.4.4" xref="S3.p3.5.m5.4.4.cmml"><mi id="S3.p3.5.m5.4.4.6" xref="S3.p3.5.m5.4.4.6.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.p3.5.m5.4.4.5" xref="S3.p3.5.m5.4.4.5.cmml">‚Äã</mo><mrow id="S3.p3.5.m5.4.4.4.4" xref="S3.p3.5.m5.4.4.4.5.cmml"><mo stretchy="false" id="S3.p3.5.m5.4.4.4.4.5" xref="S3.p3.5.m5.4.4.4.5.cmml">(</mo><msub id="S3.p3.5.m5.1.1.1.1.1" xref="S3.p3.5.m5.1.1.1.1.1.cmml"><mi id="S3.p3.5.m5.1.1.1.1.1.2" xref="S3.p3.5.m5.1.1.1.1.1.2.cmml">C</mi><mi id="S3.p3.5.m5.1.1.1.1.1.3" xref="S3.p3.5.m5.1.1.1.1.1.3.cmml">X</mi></msub><mo id="S3.p3.5.m5.4.4.4.4.6" xref="S3.p3.5.m5.4.4.4.5.cmml">,</mo><msub id="S3.p3.5.m5.2.2.2.2.2" xref="S3.p3.5.m5.2.2.2.2.2.cmml"><mi id="S3.p3.5.m5.2.2.2.2.2.2" xref="S3.p3.5.m5.2.2.2.2.2.2.cmml">D</mi><mi id="S3.p3.5.m5.2.2.2.2.2.3" xref="S3.p3.5.m5.2.2.2.2.2.3.cmml">X</mi></msub><mo id="S3.p3.5.m5.4.4.4.4.7" xref="S3.p3.5.m5.4.4.4.5.cmml">,</mo><msub id="S3.p3.5.m5.3.3.3.3.3" xref="S3.p3.5.m5.3.3.3.3.3.cmml"><mi id="S3.p3.5.m5.3.3.3.3.3.2" xref="S3.p3.5.m5.3.3.3.3.3.2.cmml">C</mi><mi id="S3.p3.5.m5.3.3.3.3.3.3" xref="S3.p3.5.m5.3.3.3.3.3.3.cmml">Y</mi></msub><mo id="S3.p3.5.m5.4.4.4.4.8" xref="S3.p3.5.m5.4.4.4.5.cmml">,</mo><msub id="S3.p3.5.m5.4.4.4.4.4" xref="S3.p3.5.m5.4.4.4.4.4.cmml"><mi id="S3.p3.5.m5.4.4.4.4.4.2" xref="S3.p3.5.m5.4.4.4.4.4.2.cmml">D</mi><mi id="S3.p3.5.m5.4.4.4.4.4.3" xref="S3.p3.5.m5.4.4.4.4.4.3.cmml">Y</mi></msub><mo stretchy="false" id="S3.p3.5.m5.4.4.4.4.9" xref="S3.p3.5.m5.4.4.4.5.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.5.m5.4b"><apply id="S3.p3.5.m5.4.4.cmml" xref="S3.p3.5.m5.4.4"><times id="S3.p3.5.m5.4.4.5.cmml" xref="S3.p3.5.m5.4.4.5"></times><ci id="S3.p3.5.m5.4.4.6.cmml" xref="S3.p3.5.m5.4.4.6">ùëÉ</ci><vector id="S3.p3.5.m5.4.4.4.5.cmml" xref="S3.p3.5.m5.4.4.4.4"><apply id="S3.p3.5.m5.1.1.1.1.1.cmml" xref="S3.p3.5.m5.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p3.5.m5.1.1.1.1.1.1.cmml" xref="S3.p3.5.m5.1.1.1.1.1">subscript</csymbol><ci id="S3.p3.5.m5.1.1.1.1.1.2.cmml" xref="S3.p3.5.m5.1.1.1.1.1.2">ùê∂</ci><ci id="S3.p3.5.m5.1.1.1.1.1.3.cmml" xref="S3.p3.5.m5.1.1.1.1.1.3">ùëã</ci></apply><apply id="S3.p3.5.m5.2.2.2.2.2.cmml" xref="S3.p3.5.m5.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.p3.5.m5.2.2.2.2.2.1.cmml" xref="S3.p3.5.m5.2.2.2.2.2">subscript</csymbol><ci id="S3.p3.5.m5.2.2.2.2.2.2.cmml" xref="S3.p3.5.m5.2.2.2.2.2.2">ùê∑</ci><ci id="S3.p3.5.m5.2.2.2.2.2.3.cmml" xref="S3.p3.5.m5.2.2.2.2.2.3">ùëã</ci></apply><apply id="S3.p3.5.m5.3.3.3.3.3.cmml" xref="S3.p3.5.m5.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.p3.5.m5.3.3.3.3.3.1.cmml" xref="S3.p3.5.m5.3.3.3.3.3">subscript</csymbol><ci id="S3.p3.5.m5.3.3.3.3.3.2.cmml" xref="S3.p3.5.m5.3.3.3.3.3.2">ùê∂</ci><ci id="S3.p3.5.m5.3.3.3.3.3.3.cmml" xref="S3.p3.5.m5.3.3.3.3.3.3">ùëå</ci></apply><apply id="S3.p3.5.m5.4.4.4.4.4.cmml" xref="S3.p3.5.m5.4.4.4.4.4"><csymbol cd="ambiguous" id="S3.p3.5.m5.4.4.4.4.4.1.cmml" xref="S3.p3.5.m5.4.4.4.4.4">subscript</csymbol><ci id="S3.p3.5.m5.4.4.4.4.4.2.cmml" xref="S3.p3.5.m5.4.4.4.4.4.2">ùê∑</ci><ci id="S3.p3.5.m5.4.4.4.4.4.3.cmml" xref="S3.p3.5.m5.4.4.4.4.4.3">ùëå</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.5.m5.4c">P(C_{X},D_{X},C_{Y},D_{Y})</annotation></semantics></math>.
In Uni-Evaluator, discrete variables include predicted/ground-truth classes,
and continuous variables include the sizes and aspect ratios of predicted objects and ground truth, and prediction confidence.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.2" class="ltx_p">To determine the corresponding probability function,
we need to consider the probability of both discrete and continuous variables.
For discrete variables,
the probability mass function is used to determine the probability of taking on a specific value, which is determined by the frequency of the variable equal to that value.
For continuous variables,
the probability is calculated over intervals using the cumulative distribution function (CDF), which is determined by the frequency of the variable in that interval.
In our implementation, we utilize the empirical CDF because of its time efficiency¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.
This method discretizes a continuous variable and counts the frequency of the discretized variable less than a specific value.
With the two functions,
the joint probability is calculated using the conditional probability.
Here, we illustrate the calculation of the joint probability using an object detection example.
Suppose that we want to analyze the detected objects with the ground-truth label ‚Äúcat‚Äù and high prediction confidence (<math id="S3.p4.1.m1.1" class="ltx_Math" alttext="&gt;0.5" display="inline"><semantics id="S3.p4.1.m1.1a"><mrow id="S3.p4.1.m1.1.1" xref="S3.p4.1.m1.1.1.cmml"><mi id="S3.p4.1.m1.1.1.2" xref="S3.p4.1.m1.1.1.2.cmml"></mi><mo id="S3.p4.1.m1.1.1.1" xref="S3.p4.1.m1.1.1.1.cmml">&gt;</mo><mn id="S3.p4.1.m1.1.1.3" xref="S3.p4.1.m1.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.1.m1.1b"><apply id="S3.p4.1.m1.1.1.cmml" xref="S3.p4.1.m1.1.1"><gt id="S3.p4.1.m1.1.1.1.cmml" xref="S3.p4.1.m1.1.1.1"></gt><csymbol cd="latexml" id="S3.p4.1.m1.1.1.2.cmml" xref="S3.p4.1.m1.1.1.2">absent</csymbol><cn type="float" id="S3.p4.1.m1.1.1.3.cmml" xref="S3.p4.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.1.m1.1c">&gt;0.5</annotation></semantics></math>).
This is defined as
<math id="S3.p4.2.m2.1" class="ltx_Math" alttext="P(\mathrm{Label}_{X}=\mathrm{cat},\ \mathrm{Confidence}_{Y}&gt;0.5)" display="inline"><semantics id="S3.p4.2.m2.1a"><mrow id="S3.p4.2.m2.1.1" xref="S3.p4.2.m2.1.1.cmml"><mi id="S3.p4.2.m2.1.1.3" xref="S3.p4.2.m2.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.p4.2.m2.1.1.2" xref="S3.p4.2.m2.1.1.2.cmml">‚Äã</mo><mrow id="S3.p4.2.m2.1.1.1.1" xref="S3.p4.2.m2.1.1.cmml"><mo stretchy="false" id="S3.p4.2.m2.1.1.1.1.2" xref="S3.p4.2.m2.1.1.cmml">(</mo><mrow id="S3.p4.2.m2.1.1.1.1.1.2" xref="S3.p4.2.m2.1.1.1.1.1.3.cmml"><mrow id="S3.p4.2.m2.1.1.1.1.1.1.1" xref="S3.p4.2.m2.1.1.1.1.1.1.1.cmml"><msub id="S3.p4.2.m2.1.1.1.1.1.1.1.2" xref="S3.p4.2.m2.1.1.1.1.1.1.1.2.cmml"><mi id="S3.p4.2.m2.1.1.1.1.1.1.1.2.2" xref="S3.p4.2.m2.1.1.1.1.1.1.1.2.2.cmml">Label</mi><mi id="S3.p4.2.m2.1.1.1.1.1.1.1.2.3" xref="S3.p4.2.m2.1.1.1.1.1.1.1.2.3.cmml">X</mi></msub><mo id="S3.p4.2.m2.1.1.1.1.1.1.1.1" xref="S3.p4.2.m2.1.1.1.1.1.1.1.1.cmml">=</mo><mi id="S3.p4.2.m2.1.1.1.1.1.1.1.3" xref="S3.p4.2.m2.1.1.1.1.1.1.1.3.cmml">cat</mi></mrow><mo rspace="0.667em" id="S3.p4.2.m2.1.1.1.1.1.2.3" xref="S3.p4.2.m2.1.1.1.1.1.3a.cmml">,</mo><mrow id="S3.p4.2.m2.1.1.1.1.1.2.2" xref="S3.p4.2.m2.1.1.1.1.1.2.2.cmml"><msub id="S3.p4.2.m2.1.1.1.1.1.2.2.2" xref="S3.p4.2.m2.1.1.1.1.1.2.2.2.cmml"><mi id="S3.p4.2.m2.1.1.1.1.1.2.2.2.2" xref="S3.p4.2.m2.1.1.1.1.1.2.2.2.2.cmml">Confidence</mi><mi id="S3.p4.2.m2.1.1.1.1.1.2.2.2.3" xref="S3.p4.2.m2.1.1.1.1.1.2.2.2.3.cmml">Y</mi></msub><mo id="S3.p4.2.m2.1.1.1.1.1.2.2.1" xref="S3.p4.2.m2.1.1.1.1.1.2.2.1.cmml">&gt;</mo><mn id="S3.p4.2.m2.1.1.1.1.1.2.2.3" xref="S3.p4.2.m2.1.1.1.1.1.2.2.3.cmml">0.5</mn></mrow></mrow><mo stretchy="false" id="S3.p4.2.m2.1.1.1.1.3" xref="S3.p4.2.m2.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.2.m2.1b"><apply id="S3.p4.2.m2.1.1.cmml" xref="S3.p4.2.m2.1.1"><times id="S3.p4.2.m2.1.1.2.cmml" xref="S3.p4.2.m2.1.1.2"></times><ci id="S3.p4.2.m2.1.1.3.cmml" xref="S3.p4.2.m2.1.1.3">ùëÉ</ci><apply id="S3.p4.2.m2.1.1.1.1.1.3.cmml" xref="S3.p4.2.m2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.p4.2.m2.1.1.1.1.1.3a.cmml" xref="S3.p4.2.m2.1.1.1.1.1.2.3">formulae-sequence</csymbol><apply id="S3.p4.2.m2.1.1.1.1.1.1.1.cmml" xref="S3.p4.2.m2.1.1.1.1.1.1.1"><eq id="S3.p4.2.m2.1.1.1.1.1.1.1.1.cmml" xref="S3.p4.2.m2.1.1.1.1.1.1.1.1"></eq><apply id="S3.p4.2.m2.1.1.1.1.1.1.1.2.cmml" xref="S3.p4.2.m2.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.p4.2.m2.1.1.1.1.1.1.1.2.1.cmml" xref="S3.p4.2.m2.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.p4.2.m2.1.1.1.1.1.1.1.2.2.cmml" xref="S3.p4.2.m2.1.1.1.1.1.1.1.2.2">Label</ci><ci id="S3.p4.2.m2.1.1.1.1.1.1.1.2.3.cmml" xref="S3.p4.2.m2.1.1.1.1.1.1.1.2.3">ùëã</ci></apply><ci id="S3.p4.2.m2.1.1.1.1.1.1.1.3.cmml" xref="S3.p4.2.m2.1.1.1.1.1.1.1.3">cat</ci></apply><apply id="S3.p4.2.m2.1.1.1.1.1.2.2.cmml" xref="S3.p4.2.m2.1.1.1.1.1.2.2"><gt id="S3.p4.2.m2.1.1.1.1.1.2.2.1.cmml" xref="S3.p4.2.m2.1.1.1.1.1.2.2.1"></gt><apply id="S3.p4.2.m2.1.1.1.1.1.2.2.2.cmml" xref="S3.p4.2.m2.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.p4.2.m2.1.1.1.1.1.2.2.2.1.cmml" xref="S3.p4.2.m2.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.p4.2.m2.1.1.1.1.1.2.2.2.2.cmml" xref="S3.p4.2.m2.1.1.1.1.1.2.2.2.2">Confidence</ci><ci id="S3.p4.2.m2.1.1.1.1.1.2.2.2.3.cmml" xref="S3.p4.2.m2.1.1.1.1.1.2.2.2.3">ùëå</ci></apply><cn type="float" id="S3.p4.2.m2.1.1.1.1.1.2.2.3.cmml" xref="S3.p4.2.m2.1.1.1.1.1.2.2.3">0.5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.2.m2.1c">P(\mathrm{Label}_{X}=\mathrm{cat},\ \mathrm{Confidence}_{Y}&gt;0.5)</annotation></semantics></math>.
The probability can then be calculated using
the following conditional probability:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="P(\mathrm{Confidence}_{Y}&gt;0.5\mid\mathrm{Label}_{X}=\mathrm{cat})P(\mathrm{Label}_{X}=\mathrm{cat})," display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.4" xref="S3.E2.m1.1.1.1.1.4.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml">‚Äã</mo><mrow id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.2.2.cmml">Confidence</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.2.3.cmml">Y</mi></msub><mo id="S3.E2.m1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.3.cmml">&gt;</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.4" xref="S3.E2.m1.1.1.1.1.1.1.1.4.cmml"><mn id="S3.E2.m1.1.1.1.1.1.1.1.4.2" xref="S3.E2.m1.1.1.1.1.1.1.1.4.2.cmml">0.5</mn><mo id="S3.E2.m1.1.1.1.1.1.1.1.4.1" xref="S3.E2.m1.1.1.1.1.1.1.1.4.1.cmml">‚à£</mo><msub id="S3.E2.m1.1.1.1.1.1.1.1.4.3" xref="S3.E2.m1.1.1.1.1.1.1.1.4.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.4.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.4.3.2.cmml">Label</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.4.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.4.3.3.cmml">X</mi></msub></mrow><mo id="S3.E2.m1.1.1.1.1.1.1.1.5" xref="S3.E2.m1.1.1.1.1.1.1.1.5.cmml">=</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.6" xref="S3.E2.m1.1.1.1.1.1.1.1.6.cmml">cat</mi></mrow><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.3a" xref="S3.E2.m1.1.1.1.1.3.cmml">‚Äã</mo><mi id="S3.E2.m1.1.1.1.1.5" xref="S3.E2.m1.1.1.1.1.5.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.3b" xref="S3.E2.m1.1.1.1.1.3.cmml">‚Äã</mo><mrow id="S3.E2.m1.1.1.1.1.2.1" xref="S3.E2.m1.1.1.1.1.2.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.2.1.2" xref="S3.E2.m1.1.1.1.1.2.1.1.cmml">(</mo><mrow id="S3.E2.m1.1.1.1.1.2.1.1" xref="S3.E2.m1.1.1.1.1.2.1.1.cmml"><msub id="S3.E2.m1.1.1.1.1.2.1.1.2" xref="S3.E2.m1.1.1.1.1.2.1.1.2.cmml"><mi id="S3.E2.m1.1.1.1.1.2.1.1.2.2" xref="S3.E2.m1.1.1.1.1.2.1.1.2.2.cmml">Label</mi><mi id="S3.E2.m1.1.1.1.1.2.1.1.2.3" xref="S3.E2.m1.1.1.1.1.2.1.1.2.3.cmml">X</mi></msub><mo id="S3.E2.m1.1.1.1.1.2.1.1.1" xref="S3.E2.m1.1.1.1.1.2.1.1.1.cmml">=</mo><mi id="S3.E2.m1.1.1.1.1.2.1.1.3" xref="S3.E2.m1.1.1.1.1.2.1.1.3.cmml">cat</mi></mrow><mo stretchy="false" id="S3.E2.m1.1.1.1.1.2.1.3" xref="S3.E2.m1.1.1.1.1.2.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><times id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3"></times><ci id="S3.E2.m1.1.1.1.1.4.cmml" xref="S3.E2.m1.1.1.1.1.4">ùëÉ</ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1"><and id="S3.E2.m1.1.1.1.1.1.1.1a.cmml" xref="S3.E2.m1.1.1.1.1.1.1"></and><apply id="S3.E2.m1.1.1.1.1.1.1.1b.cmml" xref="S3.E2.m1.1.1.1.1.1.1"><gt id="S3.E2.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3"></gt><apply id="S3.E2.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2.2">Confidence</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2.3">ùëå</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.4.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.4"><csymbol cd="latexml" id="S3.E2.m1.1.1.1.1.1.1.1.4.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.4.1">conditional</csymbol><cn type="float" id="S3.E2.m1.1.1.1.1.1.1.1.4.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.4.2">0.5</cn><apply id="S3.E2.m1.1.1.1.1.1.1.1.4.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.4.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.4.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.4.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.4.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.4.3.2">Label</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.4.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.4.3.3">ùëã</ci></apply></apply></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1c.cmml" xref="S3.E2.m1.1.1.1.1.1.1"><eq id="S3.E2.m1.1.1.1.1.1.1.1.5.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.5"></eq><share href="#S3.E2.m1.1.1.1.1.1.1.1.4.cmml" id="S3.E2.m1.1.1.1.1.1.1.1d.cmml" xref="S3.E2.m1.1.1.1.1.1.1"></share><ci id="S3.E2.m1.1.1.1.1.1.1.1.6.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.6">cat</ci></apply></apply><ci id="S3.E2.m1.1.1.1.1.5.cmml" xref="S3.E2.m1.1.1.1.1.5">ùëÉ</ci><apply id="S3.E2.m1.1.1.1.1.2.1.1.cmml" xref="S3.E2.m1.1.1.1.1.2.1"><eq id="S3.E2.m1.1.1.1.1.2.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.2.1.1.1"></eq><apply id="S3.E2.m1.1.1.1.1.2.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2.1.1.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.2.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.1.1.2.2">Label</ci><ci id="S3.E2.m1.1.1.1.1.2.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.1.1.2.3">ùëã</ci></apply><ci id="S3.E2.m1.1.1.1.1.2.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.2.1.1.3">cat</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">P(\mathrm{Confidence}_{Y}&gt;0.5\mid\mathrm{Label}_{X}=\mathrm{cat})P(\mathrm{Label}_{X}=\mathrm{cat}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.p4.7" class="ltx_p">where <math id="S3.p4.3.m1.1" class="ltx_Math" alttext="P(\mathrm{Label}_{X}=\mathrm{cat})" display="inline"><semantics id="S3.p4.3.m1.1a"><mrow id="S3.p4.3.m1.1.1" xref="S3.p4.3.m1.1.1.cmml"><mi id="S3.p4.3.m1.1.1.3" xref="S3.p4.3.m1.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.p4.3.m1.1.1.2" xref="S3.p4.3.m1.1.1.2.cmml">‚Äã</mo><mrow id="S3.p4.3.m1.1.1.1.1" xref="S3.p4.3.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.p4.3.m1.1.1.1.1.2" xref="S3.p4.3.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.p4.3.m1.1.1.1.1.1" xref="S3.p4.3.m1.1.1.1.1.1.cmml"><msub id="S3.p4.3.m1.1.1.1.1.1.2" xref="S3.p4.3.m1.1.1.1.1.1.2.cmml"><mi id="S3.p4.3.m1.1.1.1.1.1.2.2" xref="S3.p4.3.m1.1.1.1.1.1.2.2.cmml">Label</mi><mi id="S3.p4.3.m1.1.1.1.1.1.2.3" xref="S3.p4.3.m1.1.1.1.1.1.2.3.cmml">X</mi></msub><mo id="S3.p4.3.m1.1.1.1.1.1.1" xref="S3.p4.3.m1.1.1.1.1.1.1.cmml">=</mo><mi id="S3.p4.3.m1.1.1.1.1.1.3" xref="S3.p4.3.m1.1.1.1.1.1.3.cmml">cat</mi></mrow><mo stretchy="false" id="S3.p4.3.m1.1.1.1.1.3" xref="S3.p4.3.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.3.m1.1b"><apply id="S3.p4.3.m1.1.1.cmml" xref="S3.p4.3.m1.1.1"><times id="S3.p4.3.m1.1.1.2.cmml" xref="S3.p4.3.m1.1.1.2"></times><ci id="S3.p4.3.m1.1.1.3.cmml" xref="S3.p4.3.m1.1.1.3">ùëÉ</ci><apply id="S3.p4.3.m1.1.1.1.1.1.cmml" xref="S3.p4.3.m1.1.1.1.1"><eq id="S3.p4.3.m1.1.1.1.1.1.1.cmml" xref="S3.p4.3.m1.1.1.1.1.1.1"></eq><apply id="S3.p4.3.m1.1.1.1.1.1.2.cmml" xref="S3.p4.3.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.p4.3.m1.1.1.1.1.1.2.1.cmml" xref="S3.p4.3.m1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.p4.3.m1.1.1.1.1.1.2.2.cmml" xref="S3.p4.3.m1.1.1.1.1.1.2.2">Label</ci><ci id="S3.p4.3.m1.1.1.1.1.1.2.3.cmml" xref="S3.p4.3.m1.1.1.1.1.1.2.3">ùëã</ci></apply><ci id="S3.p4.3.m1.1.1.1.1.1.3.cmml" xref="S3.p4.3.m1.1.1.1.1.1.3">cat</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.3.m1.1c">P(\mathrm{Label}_{X}=\mathrm{cat})</annotation></semantics></math> is determined by the probability mass function of discrete variable <math id="S3.p4.4.m2.1" class="ltx_Math" alttext="\mathrm{Label}_{X}" display="inline"><semantics id="S3.p4.4.m2.1a"><msub id="S3.p4.4.m2.1.1" xref="S3.p4.4.m2.1.1.cmml"><mi id="S3.p4.4.m2.1.1.2" xref="S3.p4.4.m2.1.1.2.cmml">Label</mi><mi id="S3.p4.4.m2.1.1.3" xref="S3.p4.4.m2.1.1.3.cmml">X</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p4.4.m2.1b"><apply id="S3.p4.4.m2.1.1.cmml" xref="S3.p4.4.m2.1.1"><csymbol cd="ambiguous" id="S3.p4.4.m2.1.1.1.cmml" xref="S3.p4.4.m2.1.1">subscript</csymbol><ci id="S3.p4.4.m2.1.1.2.cmml" xref="S3.p4.4.m2.1.1.2">Label</ci><ci id="S3.p4.4.m2.1.1.3.cmml" xref="S3.p4.4.m2.1.1.3">ùëã</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.4.m2.1c">\mathrm{Label}_{X}</annotation></semantics></math>, and
<math id="S3.p4.5.m3.1" class="ltx_Math" alttext="P(\mathrm{Confidence}_{Y}&gt;0.5\mid\mathrm{Label}_{X}=\mathrm{cat})" display="inline"><semantics id="S3.p4.5.m3.1a"><mrow id="S3.p4.5.m3.1.1" xref="S3.p4.5.m3.1.1.cmml"><mi id="S3.p4.5.m3.1.1.3" xref="S3.p4.5.m3.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.p4.5.m3.1.1.2" xref="S3.p4.5.m3.1.1.2.cmml">‚Äã</mo><mrow id="S3.p4.5.m3.1.1.1.1" xref="S3.p4.5.m3.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.p4.5.m3.1.1.1.1.2" xref="S3.p4.5.m3.1.1.1.1.1.cmml">(</mo><mrow id="S3.p4.5.m3.1.1.1.1.1" xref="S3.p4.5.m3.1.1.1.1.1.cmml"><msub id="S3.p4.5.m3.1.1.1.1.1.2" xref="S3.p4.5.m3.1.1.1.1.1.2.cmml"><mi id="S3.p4.5.m3.1.1.1.1.1.2.2" xref="S3.p4.5.m3.1.1.1.1.1.2.2.cmml">Confidence</mi><mi id="S3.p4.5.m3.1.1.1.1.1.2.3" xref="S3.p4.5.m3.1.1.1.1.1.2.3.cmml">Y</mi></msub><mo id="S3.p4.5.m3.1.1.1.1.1.3" xref="S3.p4.5.m3.1.1.1.1.1.3.cmml">&gt;</mo><mrow id="S3.p4.5.m3.1.1.1.1.1.4" xref="S3.p4.5.m3.1.1.1.1.1.4.cmml"><mn id="S3.p4.5.m3.1.1.1.1.1.4.2" xref="S3.p4.5.m3.1.1.1.1.1.4.2.cmml">0.5</mn><mo id="S3.p4.5.m3.1.1.1.1.1.4.1" xref="S3.p4.5.m3.1.1.1.1.1.4.1.cmml">‚à£</mo><msub id="S3.p4.5.m3.1.1.1.1.1.4.3" xref="S3.p4.5.m3.1.1.1.1.1.4.3.cmml"><mi id="S3.p4.5.m3.1.1.1.1.1.4.3.2" xref="S3.p4.5.m3.1.1.1.1.1.4.3.2.cmml">Label</mi><mi id="S3.p4.5.m3.1.1.1.1.1.4.3.3" xref="S3.p4.5.m3.1.1.1.1.1.4.3.3.cmml">X</mi></msub></mrow><mo id="S3.p4.5.m3.1.1.1.1.1.5" xref="S3.p4.5.m3.1.1.1.1.1.5.cmml">=</mo><mi id="S3.p4.5.m3.1.1.1.1.1.6" xref="S3.p4.5.m3.1.1.1.1.1.6.cmml">cat</mi></mrow><mo stretchy="false" id="S3.p4.5.m3.1.1.1.1.3" xref="S3.p4.5.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.5.m3.1b"><apply id="S3.p4.5.m3.1.1.cmml" xref="S3.p4.5.m3.1.1"><times id="S3.p4.5.m3.1.1.2.cmml" xref="S3.p4.5.m3.1.1.2"></times><ci id="S3.p4.5.m3.1.1.3.cmml" xref="S3.p4.5.m3.1.1.3">ùëÉ</ci><apply id="S3.p4.5.m3.1.1.1.1.1.cmml" xref="S3.p4.5.m3.1.1.1.1"><and id="S3.p4.5.m3.1.1.1.1.1a.cmml" xref="S3.p4.5.m3.1.1.1.1"></and><apply id="S3.p4.5.m3.1.1.1.1.1b.cmml" xref="S3.p4.5.m3.1.1.1.1"><gt id="S3.p4.5.m3.1.1.1.1.1.3.cmml" xref="S3.p4.5.m3.1.1.1.1.1.3"></gt><apply id="S3.p4.5.m3.1.1.1.1.1.2.cmml" xref="S3.p4.5.m3.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.p4.5.m3.1.1.1.1.1.2.1.cmml" xref="S3.p4.5.m3.1.1.1.1.1.2">subscript</csymbol><ci id="S3.p4.5.m3.1.1.1.1.1.2.2.cmml" xref="S3.p4.5.m3.1.1.1.1.1.2.2">Confidence</ci><ci id="S3.p4.5.m3.1.1.1.1.1.2.3.cmml" xref="S3.p4.5.m3.1.1.1.1.1.2.3">ùëå</ci></apply><apply id="S3.p4.5.m3.1.1.1.1.1.4.cmml" xref="S3.p4.5.m3.1.1.1.1.1.4"><csymbol cd="latexml" id="S3.p4.5.m3.1.1.1.1.1.4.1.cmml" xref="S3.p4.5.m3.1.1.1.1.1.4.1">conditional</csymbol><cn type="float" id="S3.p4.5.m3.1.1.1.1.1.4.2.cmml" xref="S3.p4.5.m3.1.1.1.1.1.4.2">0.5</cn><apply id="S3.p4.5.m3.1.1.1.1.1.4.3.cmml" xref="S3.p4.5.m3.1.1.1.1.1.4.3"><csymbol cd="ambiguous" id="S3.p4.5.m3.1.1.1.1.1.4.3.1.cmml" xref="S3.p4.5.m3.1.1.1.1.1.4.3">subscript</csymbol><ci id="S3.p4.5.m3.1.1.1.1.1.4.3.2.cmml" xref="S3.p4.5.m3.1.1.1.1.1.4.3.2">Label</ci><ci id="S3.p4.5.m3.1.1.1.1.1.4.3.3.cmml" xref="S3.p4.5.m3.1.1.1.1.1.4.3.3">ùëã</ci></apply></apply></apply><apply id="S3.p4.5.m3.1.1.1.1.1c.cmml" xref="S3.p4.5.m3.1.1.1.1"><eq id="S3.p4.5.m3.1.1.1.1.1.5.cmml" xref="S3.p4.5.m3.1.1.1.1.1.5"></eq><share href="#S3.p4.5.m3.1.1.1.1.1.4.cmml" id="S3.p4.5.m3.1.1.1.1.1d.cmml" xref="S3.p4.5.m3.1.1.1.1"></share><ci id="S3.p4.5.m3.1.1.1.1.1.6.cmml" xref="S3.p4.5.m3.1.1.1.1.1.6">cat</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.5.m3.1c">P(\mathrm{Confidence}_{Y}&gt;0.5\mid\mathrm{Label}_{X}=\mathrm{cat})</annotation></semantics></math> is determined
by the cumulative distribution function of continuous variable <math id="S3.p4.6.m4.1" class="ltx_Math" alttext="\mathrm{Confidence}_{Y}" display="inline"><semantics id="S3.p4.6.m4.1a"><msub id="S3.p4.6.m4.1.1" xref="S3.p4.6.m4.1.1.cmml"><mi id="S3.p4.6.m4.1.1.2" xref="S3.p4.6.m4.1.1.2.cmml">Confidence</mi><mi id="S3.p4.6.m4.1.1.3" xref="S3.p4.6.m4.1.1.3.cmml">Y</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p4.6.m4.1b"><apply id="S3.p4.6.m4.1.1.cmml" xref="S3.p4.6.m4.1.1"><csymbol cd="ambiguous" id="S3.p4.6.m4.1.1.1.cmml" xref="S3.p4.6.m4.1.1">subscript</csymbol><ci id="S3.p4.6.m4.1.1.2.cmml" xref="S3.p4.6.m4.1.1.2">Confidence</ci><ci id="S3.p4.6.m4.1.1.3.cmml" xref="S3.p4.6.m4.1.1.3">ùëå</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.6.m4.1c">\mathrm{Confidence}_{Y}</annotation></semantics></math> given <math id="S3.p4.7.m5.1" class="ltx_Math" alttext="\mathrm{Label}_{X}" display="inline"><semantics id="S3.p4.7.m5.1a"><msub id="S3.p4.7.m5.1.1" xref="S3.p4.7.m5.1.1.cmml"><mi id="S3.p4.7.m5.1.1.2" xref="S3.p4.7.m5.1.1.2.cmml">Label</mi><mi id="S3.p4.7.m5.1.1.3" xref="S3.p4.7.m5.1.1.3.cmml">X</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p4.7.m5.1b"><apply id="S3.p4.7.m5.1.1.cmml" xref="S3.p4.7.m5.1.1"><csymbol cd="ambiguous" id="S3.p4.7.m5.1.1.1.cmml" xref="S3.p4.7.m5.1.1">subscript</csymbol><ci id="S3.p4.7.m5.1.1.2.cmml" xref="S3.p4.7.m5.1.1.2">Label</ci><ci id="S3.p4.7.m5.1.1.3.cmml" xref="S3.p4.7.m5.1.1.3">ùëã</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.7.m5.1c">\mathrm{Label}_{X}</annotation></semantics></math>.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">The probabilistic framework enables users to process data for different analysis tasks using the standard operations of probability distributions,
including marginalization and conditioning¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.
The <span id="S3.p5.1.1" class="ltx_text ltx_font_bold">marginalization</span> operation discards variables in the distribution by integrating or summing them,
allowing users to focus on analyzing the variables of interest.
The <span id="S3.p5.1.2" class="ltx_text ltx_font_bold">conditioning</span> operation constrains variables to be of specific values or within specific intervals,
which enables users to analyze the subsets of interest.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Uni-Evaluator Visualization</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Based on the unified probability distribution, we developed three coordinated visualizations to explain model performance at different levels: 1) a matrix-based visualization (Fig.¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision</span></span>(b)) to provide an overview of model performance (<span id="S4.p1.1.1" class="ltx_text ltx_font_bold">T2</span>); 2) a table visualization (Fig.¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision</span></span>(c)) to identify problematic subsets (<span id="S4.p1.1.2" class="ltx_text ltx_font_bold">T3</span>); and 3) a grid visualization (Fig.¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision</span></span>(d)) to display the samples of interest (<span id="S4.p1.1.3" class="ltx_text ltx_font_bold">T4</span>).</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Matrix-based Visualization</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">To evaluate both classification and localization performance in a unified manner, it is essential to consider 1) the confusion between classes;
2) the sizes of predicted objects; and 3) the directions in which they are shifted from ground truth.
However, analyzing the three interdependent aspects simultaneously can be challenging.
Therefore, for more effective analysis, we disentangle them by utilizing the marginalization operation to discard irrelevant variables.
Accordingly, three evaluation modes are provided:
1) confusion mode for evaluating classification performance; 2) size mode for analyzing the sizes of predicted objects; 3) direction mode for analyzing the shifted directions of predicted objects.
Within each mode, we disentangle the associated variables from others by utilizing the marginalization operation to discard irrelevant variables.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">We employ a matrix-based visualization, an extension of Neo¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, to convey the information in the three modes. Specifically, the size/direction mode is enhanced with a carefully designed glyph to support the identification and diagnosis of size/direction errors.
The rows of the matrix represent ground-truth classes, and the columns represent predicted classes.
The class names are displayed left to the matrix and above the matrix.
When the number of classes is large (<em id="S4.SS1.p2.1.1" class="ltx_emph ltx_font_italic">e.g</em>.‚Äâ, 30 in a view with a resolution of <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="800\times 800" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mrow id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mn id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml">800</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p2.1.m1.1.1.1" xref="S4.SS1.p2.1.m1.1.1.1.cmml">√ó</mo><mn id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml">800</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><times id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1.1"></times><cn type="integer" id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2">800</cn><cn type="integer" id="S4.SS1.p2.1.m1.1.1.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3">800</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">800\times 800</annotation></semantics></math>), the matrix cells become small, which reduces their visibility.
To address this, we organized the classes hierarchically using hierarchical clustering algorithms¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> or based on their inherent hierarchical structure, and presented them as an indented tree.
Furthermore, users can enlarge the cells of interest with the drill-down and hovering interactions, which are described at the end of this section.
The summary statistics of each class (precision, recall,¬†<em id="S4.SS1.p2.1.2" class="ltx_emph ltx_font_italic">etc</em>.‚Äâ) is displayed as a list on the right side of the matrix.
The only difference between the three modes is the content within the matrix cells.
Next, we delve into each of these modes.</p>
</div>
<figure id="S4.SS1.1" class="ltx_figure ltx_align_floatleft"><img src="/html/2308.05168/assets/x5.png" id="S4.SS1.1.g1" class="ltx_graphics ltx_img_square" width="27" height="27" alt="[Uncaptioned image]">
</figure>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p"><em id="S4.SS1.p3.1.1" class="ltx_emph ltx_font_italic">Confusion mode</em>.
In confusion mode, the cell represents the number of samples in classification tasks and the number of objects in detection/segmentation tasks.
The frequency of the samples/objects confused between classes is encoded by the filling color in each cell, ranging from white to blue.
The darker the cell color, the greater the confusion between the associated classes.
Same to Neo¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, we place a light-gray dash in the cells with no samples/objects to make them more distinguishable from those with only a few samples/objects.
To make the confusion patterns more distinct, we employ the Optimal Leaf Ordering algorithm¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> to reorder the matrix.</p>
</div>
<figure id="S4.SS1.2" class="ltx_figure ltx_align_floatleft"><img src="/html/2308.05168/assets/x6.png" id="S4.SS1.2.g1" class="ltx_graphics ltx_img_portrait" width="27" height="72" alt="[Uncaptioned image]">
</figure>
<div id="S4.SS1.p4" class="ltx_para ltx_noindent">
<p id="S4.SS1.p4.1" class="ltx_p"><em id="S4.SS1.p4.1.1" class="ltx_emph ltx_font_italic">Size mode</em>.
Initially, we utilize two concentric circles to represent the size error.
The green circle represents the predicted objects with sizes smaller than the ground truth, and the yellow circle represents the predicted objects with larger sizes.
Their radii encode the number of associated objects.
The experts agree that this design is intuitive in identifying the main size errors in the cells.
However, they are also interested to know how many predicted objects are with precise sizes, which is not shown in the initial design.
To address this, we utilize a pie chart with three sectors to summarize the sizes of predicted objects in each cell.
The gray sector represents predicted objects with precise sizes, while the yellow/green represents those with larger/smaller sizes compared to ground truth.
To address the concern of color blindness, we provide the option for users to customize the color encoding themselves by clicking 
<span id="S4.SS1.p4.1.m1.1.1.1" class="ltx_inline-block ltx_markedasmath ltx_align_bottom">
<span id="S4.SS1.p4.1.m1.1.1.1.1" class="ltx_p"><img src="/html/2308.05168/assets/x7.png" id="S4.SS1.p4.1.m1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="1" height="1" alt="[Uncaptioned image]"></span>
</span>.
The angle of each sector is proportional to the number of predicted objects in this sector,
and the radius of the pie chart encodes the number of predicted objects in that cell.</p>
</div>
<figure id="S4.SS1.3" class="ltx_figure ltx_align_floatleft"><img src="/html/2308.05168/assets/x8.png" id="S4.SS1.3.g1" class="ltx_graphics ltx_img_portrait" width="27" height="71" alt="[Uncaptioned image]">
</figure>
<div id="S4.SS1.p5" class="ltx_para ltx_noindent">
<p id="S4.SS1.p5.1" class="ltx_p"><em id="S4.SS1.p5.1.1" class="ltx_emph ltx_font_italic">Direction mode</em>.
A straightforward way for encoding the shifted directions of predicted objects is to utilize a radar chart with eight spokes.
The length of each spoke encodes the number of predicted objects shifted in that direction.
A polygon is drawn to connect the spokes.
When showing this design to the experts,
they indicate that the polygon shapes immediately attract their attention.
However, the shapes provide less information in understanding the predictions compared with the lengths of the spokes.
Additionally, redundant lines in the radar chart, such as the boundary lines and the extended lines along the spokes, make the visual representation even more complex
and add extra cognitive load to users.
To address this issue, we replace the radar chart with eight arrows.
The length of each arrow encodes the number of predicted objects shifted in that direction.
We also add a circle in the middle to represent the predicted objects with precise positions.
Its radius encodes the number of such objects.</p>
</div>
<div id="S4.SS1.p6" class="ltx_para">
<p id="S4.SS1.p6.4" class="ltx_p">In the matrix-based visualization, several interactions are provided to facilitate exploration.
First, users can use the conditioning operation to drill down into sub-matrices of interest by clicking 
<span id="S4.SS1.p6.1.m1.1.1.1" class="ltx_inline-block ltx_markedasmath ltx_align_bottom">
<span id="S4.SS1.p6.1.m1.1.1.1.1" class="ltx_p"><img src="/html/2308.05168/assets/x9.png" id="S4.SS1.p6.1.m1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="1" height="1" alt="[Uncaptioned image]"></span>
</span> at the top of Fig¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision</span></span>(b).
In addition, when a class has much more samples/objects than the others, some patterns in the matrix may be hidden.
For example, in the confusion mode,
the cell colors of the dominant classes can overshadow other classes (<em id="S4.SS1.p6.4.1" class="ltx_emph ltx_font_italic">e.g</em>.‚Äâ, 
<span id="S4.SS1.p6.2.m2.1.1.1" class="ltx_inline-block ltx_markedasmath ltx_align_bottom">
<span id="S4.SS1.p6.2.m2.1.1.1.1" class="ltx_p"><img src="/html/2308.05168/assets/x10.png" id="S4.SS1.p6.2.m2.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="1" height="1" alt="[Uncaptioned image]"></span>
</span>), making it difficult to detect misclassifications in the other classes.
To address this issue, row/column normalization is supported (Fig.¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision</span></span>G).
For example, by performing row normalization on the matrix¬†
<span id="S4.SS1.p6.3.m3.1.1.1" class="ltx_inline-block ltx_markedasmath ltx_align_bottom">
<span id="S4.SS1.p6.3.m3.1.1.1.1" class="ltx_p"><img src="/html/2308.05168/assets/x11.png" id="S4.SS1.p6.3.m3.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="1" height="1" alt="[Uncaptioned image]"></span>
</span>, the confusion between the second and third classes appears (
<span id="S4.SS1.p6.4.m4.1.1.1" class="ltx_inline-block ltx_markedasmath ltx_align_bottom">
<span id="S4.SS1.p6.4.m4.1.1.1.1" class="ltx_p"><img src="/html/2308.05168/assets/x12.png" id="S4.SS1.p6.4.m4.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="1" height="1" alt="[Uncaptioned image]"></span>
</span>).
Moreover, in the size and the direction modes, the charts with few objects are usually small.
To ensure their visibility when needed, users can enlarge them upon hovering.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2308.05168/assets/x13.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="187" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.3.2" class="ltx_text" style="font-size:90%;">The matrix-based visualization of the COCO dataset in (a) the confusion mode and (b) the size mode; (c) the sub-matrix of super-class ‚Äúoutdoor;‚Äù (d) the predictions on the objects of ‚Äútraffic light.‚Äù</span></figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Table Visualization</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">To help users identify problematic subsets, a frequent pattern mining-based search method is first developed to mine the candidate subsets.
Then a table visualization is employed to convey these subsets in terms of their attributes.
The users can rank these subsets by one attribute or the combination of multiple attributes to find problematic subsets.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.8" class="ltx_p"><span id="S4.SS2.p2.8.1" class="ltx_text ltx_font_bold">Frequent pattern mining-based search</span>.
Since it is computationally infeasible to evaluate model performance on all possible subsets, a search method to find candidate subsets is required.
The state-of-the-art subset search method, DivExplorer¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>, employs a
frequent pattern mining-based search method¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>
to find the subsets that meet a minimum subset size.
Then it ranks the subsets based on model performance on each of these subsets.
With this ranking, users can identify the subsets where the model performs poorly.
This method works well for classification tasks as it supports the search of the subsets sliced along discrete attributes.
To apply this method to detection and segmentation tasks,
the continuous attributes need to be discretized.
Accordingly, we first employ the equal frequency discretization method¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> to divide the considered continuous attribute into <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mi id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><ci id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">ùëë</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">d</annotation></semantics></math> intervals, each of which contains a similar number of samples/objects.
This discretization method is utilized because it well balances robustness and accuracy, and is one of the most efficient discretization methods¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
In our implementation, the minimum subset size is set as <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="{\beta}S_{c}" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mrow id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml"><mi id="S4.SS2.p2.2.m2.1.1.2" xref="S4.SS2.p2.2.m2.1.1.2.cmml">Œ≤</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.2.m2.1.1.1" xref="S4.SS2.p2.2.m2.1.1.1.cmml">‚Äã</mo><msub id="S4.SS2.p2.2.m2.1.1.3" xref="S4.SS2.p2.2.m2.1.1.3.cmml"><mi id="S4.SS2.p2.2.m2.1.1.3.2" xref="S4.SS2.p2.2.m2.1.1.3.2.cmml">S</mi><mi id="S4.SS2.p2.2.m2.1.1.3.3" xref="S4.SS2.p2.2.m2.1.1.3.3.cmml">c</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><apply id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"><times id="S4.SS2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1.1"></times><ci id="S4.SS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2">ùõΩ</ci><apply id="S4.SS2.p2.2.m2.1.1.3.cmml" xref="S4.SS2.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p2.2.m2.1.1.3.1.cmml" xref="S4.SS2.p2.2.m2.1.1.3">subscript</csymbol><ci id="S4.SS2.p2.2.m2.1.1.3.2.cmml" xref="S4.SS2.p2.2.m2.1.1.3.2">ùëÜ</ci><ci id="S4.SS2.p2.2.m2.1.1.3.3.cmml" xref="S4.SS2.p2.2.m2.1.1.3.3">ùëê</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">{\beta}S_{c}</annotation></semantics></math> to avoid the selection of small subsets, which contribute little to overall model performance.
Here, <math id="S4.SS2.p2.3.m3.1" class="ltx_Math" alttext="S_{c}" display="inline"><semantics id="S4.SS2.p2.3.m3.1a"><msub id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml"><mi id="S4.SS2.p2.3.m3.1.1.2" xref="S4.SS2.p2.3.m3.1.1.2.cmml">S</mi><mi id="S4.SS2.p2.3.m3.1.1.3" xref="S4.SS2.p2.3.m3.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><apply id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.3.m3.1.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1">subscript</csymbol><ci id="S4.SS2.p2.3.m3.1.1.2.cmml" xref="S4.SS2.p2.3.m3.1.1.2">ùëÜ</ci><ci id="S4.SS2.p2.3.m3.1.1.3.cmml" xref="S4.SS2.p2.3.m3.1.1.3">ùëê</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">S_{c}</annotation></semantics></math> is the number of samples/objects in the class being explored.
<math id="S4.SS2.p2.4.m4.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S4.SS2.p2.4.m4.1a"><mi id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml">Œ≤</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.1b"><ci id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1">ùõΩ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.1c">\beta</annotation></semantics></math> is set as 0.1 by default, and the user can adjust it according to the task at hand.
<math id="S4.SS2.p2.5.m5.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S4.SS2.p2.5.m5.1a"><mi id="S4.SS2.p2.5.m5.1.1" xref="S4.SS2.p2.5.m5.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.5.m5.1b"><ci id="S4.SS2.p2.5.m5.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1">ùëë</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.5.m5.1c">d</annotation></semantics></math> is set as <math id="S4.SS2.p2.6.m6.1" class="ltx_Math" alttext="1/\beta" display="inline"><semantics id="S4.SS2.p2.6.m6.1a"><mrow id="S4.SS2.p2.6.m6.1.1" xref="S4.SS2.p2.6.m6.1.1.cmml"><mn id="S4.SS2.p2.6.m6.1.1.2" xref="S4.SS2.p2.6.m6.1.1.2.cmml">1</mn><mo id="S4.SS2.p2.6.m6.1.1.1" xref="S4.SS2.p2.6.m6.1.1.1.cmml">/</mo><mi id="S4.SS2.p2.6.m6.1.1.3" xref="S4.SS2.p2.6.m6.1.1.3.cmml">Œ≤</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.6.m6.1b"><apply id="S4.SS2.p2.6.m6.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1"><divide id="S4.SS2.p2.6.m6.1.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1.1"></divide><cn type="integer" id="S4.SS2.p2.6.m6.1.1.2.cmml" xref="S4.SS2.p2.6.m6.1.1.2">1</cn><ci id="S4.SS2.p2.6.m6.1.1.3.cmml" xref="S4.SS2.p2.6.m6.1.1.3">ùõΩ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.6.m6.1c">1/\beta</annotation></semantics></math> to ensure each interval contains approximately <math id="S4.SS2.p2.7.m7.1" class="ltx_Math" alttext="S/d={\beta}S" display="inline"><semantics id="S4.SS2.p2.7.m7.1a"><mrow id="S4.SS2.p2.7.m7.1.1" xref="S4.SS2.p2.7.m7.1.1.cmml"><mrow id="S4.SS2.p2.7.m7.1.1.2" xref="S4.SS2.p2.7.m7.1.1.2.cmml"><mi id="S4.SS2.p2.7.m7.1.1.2.2" xref="S4.SS2.p2.7.m7.1.1.2.2.cmml">S</mi><mo id="S4.SS2.p2.7.m7.1.1.2.1" xref="S4.SS2.p2.7.m7.1.1.2.1.cmml">/</mo><mi id="S4.SS2.p2.7.m7.1.1.2.3" xref="S4.SS2.p2.7.m7.1.1.2.3.cmml">d</mi></mrow><mo id="S4.SS2.p2.7.m7.1.1.1" xref="S4.SS2.p2.7.m7.1.1.1.cmml">=</mo><mrow id="S4.SS2.p2.7.m7.1.1.3" xref="S4.SS2.p2.7.m7.1.1.3.cmml"><mi id="S4.SS2.p2.7.m7.1.1.3.2" xref="S4.SS2.p2.7.m7.1.1.3.2.cmml">Œ≤</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.7.m7.1.1.3.1" xref="S4.SS2.p2.7.m7.1.1.3.1.cmml">‚Äã</mo><mi id="S4.SS2.p2.7.m7.1.1.3.3" xref="S4.SS2.p2.7.m7.1.1.3.3.cmml">S</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.7.m7.1b"><apply id="S4.SS2.p2.7.m7.1.1.cmml" xref="S4.SS2.p2.7.m7.1.1"><eq id="S4.SS2.p2.7.m7.1.1.1.cmml" xref="S4.SS2.p2.7.m7.1.1.1"></eq><apply id="S4.SS2.p2.7.m7.1.1.2.cmml" xref="S4.SS2.p2.7.m7.1.1.2"><divide id="S4.SS2.p2.7.m7.1.1.2.1.cmml" xref="S4.SS2.p2.7.m7.1.1.2.1"></divide><ci id="S4.SS2.p2.7.m7.1.1.2.2.cmml" xref="S4.SS2.p2.7.m7.1.1.2.2">ùëÜ</ci><ci id="S4.SS2.p2.7.m7.1.1.2.3.cmml" xref="S4.SS2.p2.7.m7.1.1.2.3">ùëë</ci></apply><apply id="S4.SS2.p2.7.m7.1.1.3.cmml" xref="S4.SS2.p2.7.m7.1.1.3"><times id="S4.SS2.p2.7.m7.1.1.3.1.cmml" xref="S4.SS2.p2.7.m7.1.1.3.1"></times><ci id="S4.SS2.p2.7.m7.1.1.3.2.cmml" xref="S4.SS2.p2.7.m7.1.1.3.2">ùõΩ</ci><ci id="S4.SS2.p2.7.m7.1.1.3.3.cmml" xref="S4.SS2.p2.7.m7.1.1.3.3">ùëÜ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.7.m7.1c">S/d={\beta}S</annotation></semantics></math> samples/objects,
where <math id="S4.SS2.p2.8.m8.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S4.SS2.p2.8.m8.1a"><mi id="S4.SS2.p2.8.m8.1.1" xref="S4.SS2.p2.8.m8.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.8.m8.1b"><ci id="S4.SS2.p2.8.m8.1.1.cmml" xref="S4.SS2.p2.8.m8.1.1">ùëÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.8.m8.1c">S</annotation></semantics></math> is the number of samples/objects in the dataset.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold">Identifying problematic subsets</span>.
We employ an interactive table visualization to visualize the candidate subsets and help identify the problematic ones.
In the table, each row represents a subset with all its attributes, such as the precision and average object size (Fig.¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision</span></span>(c)).
The discrete attributes are displayed as text, and the continuous attributes are displayed as bar items.
We provide several interactions, such as filtering and ranking, to explore the subsets.
For example, users can select the subsets of a specific class by filtering,
or rank the subsets by one attribute or the combination of multiple attributes.
Following Lineup¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, the combination is achieved by dragging the header of a column onto the header of another column.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Grid Visualization</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">To enable efficient examination of the relevant samples, a grid visualization is employed because of its effectiveness in exploring image content¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>.
The cells of the grid display selected samples with their predictions.
To support real-time exploration, we utilize the <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mi id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><ci id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">k</annotation></semantics></math>NN-based grid layout algorithm¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> to determine the position of each sample within the grid.
The algorithm preserves the proximity between samples
by first projecting them as a set of 2D points with t-SNE¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>,
and then matching these points with the cells by solving a linear assignment problem.
To clearly show the detected/segmented objects,
we crop the images and present them in the corresponding cells.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Interactive Model Evaluation</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">The three coordinated visualizations work together to support an interactive model evaluation
from a global overview to individual samples.
The matrix-based visualization provides users with an overview of model performance and helps identify the matrix cells with errors, such as classification errors.
The samples in such cells are then examined
in the grid visualization to help users analyze the main causes of the errors.
If the causes for the errors are challenging to discern in the grid visualization,
users can turn to the table visualization to analyze the causes at the subset level.
During the analysis,
users can rank the subsets based on different attributes, or utilize the Scented Widgets (
<span id="S4.SS4.p1.1.m1.1.1.1" class="ltx_inline-block ltx_markedasmath ltx_align_bottom">
<span id="S4.SS4.p1.1.m1.1.1.1.1" class="ltx_p"><img src="/html/2308.05168/assets/x14.png" id="S4.SS4.p1.1.m1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="2" height="2" alt="[Uncaptioned image]"></span>
</span>)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> in Fig.¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision</span></span>(a) to select samples of interest.
With the identified causes of the errors, users can then make informed improvements to the model and/or the associated data.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Case Studies</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">To demonstrate the effectiveness of Uni-Evaluator for evaluating and improving different computer vision models (<span id="S5.p1.1.1" class="ltx_text ltx_font_bold">T1</span>),
we conducted two case studies on object detection and instance segmentation.
Prior to the case studies, we briefly introduced Uni-Evaluator to the experts.
The visualizations of Uni-Evaluator were designed to be simple and familiar, allowing the experts to quickly understand its concepts and interactions within 20 minutes.
Throughout the case studies, we followed the pair analytics protocol¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, in which the experts led the exploration and analysis, and we navigated the tool.
This collaborative approach was chosen to enable the experts to fully focus on their analysis tasks, leading to enhanced efficiency and effectiveness.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Object Detection on COCO dataset</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.6" class="ltx_p">We collaborated with two experts (<math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="E_{1}" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><msub id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml"><mi id="S5.SS1.p1.1.m1.1.1.2" xref="S5.SS1.p1.1.m1.1.1.2.cmml">E</mi><mn id="S5.SS1.p1.1.m1.1.1.3" xref="S5.SS1.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><apply id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.p1.1.m1.1.1.2.cmml" xref="S5.SS1.p1.1.m1.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.p1.1.m1.1.1.3.cmml" xref="S5.SS1.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">E_{1}</annotation></semantics></math> and <math id="S5.SS1.p1.2.m2.1" class="ltx_Math" alttext="E_{2}" display="inline"><semantics id="S5.SS1.p1.2.m2.1a"><msub id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml"><mi id="S5.SS1.p1.2.m2.1.1.2" xref="S5.SS1.p1.2.m2.1.1.2.cmml">E</mi><mn id="S5.SS1.p1.2.m2.1.1.3" xref="S5.SS1.p1.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><apply id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.2.m2.1.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S5.SS1.p1.2.m2.1.1.2.cmml" xref="S5.SS1.p1.2.m2.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.p1.2.m2.1.1.3.cmml" xref="S5.SS1.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">E_{2}</annotation></semantics></math>) to evaluate a state-of-the-art object detection model, DINO¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>.
<math id="S5.SS1.p1.3.m3.1" class="ltx_Math" alttext="E_{1}" display="inline"><semantics id="S5.SS1.p1.3.m3.1a"><msub id="S5.SS1.p1.3.m3.1.1" xref="S5.SS1.p1.3.m3.1.1.cmml"><mi id="S5.SS1.p1.3.m3.1.1.2" xref="S5.SS1.p1.3.m3.1.1.2.cmml">E</mi><mn id="S5.SS1.p1.3.m3.1.1.3" xref="S5.SS1.p1.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.3.m3.1b"><apply id="S5.SS1.p1.3.m3.1.1.cmml" xref="S5.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.3.m3.1.1.1.cmml" xref="S5.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S5.SS1.p1.3.m3.1.1.2.cmml" xref="S5.SS1.p1.3.m3.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.p1.3.m3.1.1.3.cmml" xref="S5.SS1.p1.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.3.m3.1c">E_{1}</annotation></semantics></math> is a Ph.D. student who developed DINO with his teammates.
<math id="S5.SS1.p1.4.m4.1" class="ltx_Math" alttext="E_{2}" display="inline"><semantics id="S5.SS1.p1.4.m4.1a"><msub id="S5.SS1.p1.4.m4.1.1" xref="S5.SS1.p1.4.m4.1.1.cmml"><mi id="S5.SS1.p1.4.m4.1.1.2" xref="S5.SS1.p1.4.m4.1.1.2.cmml">E</mi><mn id="S5.SS1.p1.4.m4.1.1.3" xref="S5.SS1.p1.4.m4.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.4.m4.1b"><apply id="S5.SS1.p1.4.m4.1.1.cmml" xref="S5.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.4.m4.1.1.1.cmml" xref="S5.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S5.SS1.p1.4.m4.1.1.2.cmml" xref="S5.SS1.p1.4.m4.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.p1.4.m4.1.1.3.cmml" xref="S5.SS1.p1.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.4.m4.1c">E_{2}</annotation></semantics></math> is a researcher from a technology company who developed several object detection models and integrated them into their products.
A popular dataset for object detection, the COCO dataset¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, is utilized in this study.
This dataset consists of 118,287 training samples with 849,947 objects and 5,000 test samples with 36,335 objects.
The objects belong to 80 classes divided into 12 super-classes.
DINO with the ResNet-50 backbone¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> achieves mAPs of <math id="S5.SS1.p1.5.m5.1" class="ltx_Math" alttext="65.2\%" display="inline"><semantics id="S5.SS1.p1.5.m5.1a"><mrow id="S5.SS1.p1.5.m5.1.1" xref="S5.SS1.p1.5.m5.1.1.cmml"><mn id="S5.SS1.p1.5.m5.1.1.2" xref="S5.SS1.p1.5.m5.1.1.2.cmml">65.2</mn><mo id="S5.SS1.p1.5.m5.1.1.1" xref="S5.SS1.p1.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.5.m5.1b"><apply id="S5.SS1.p1.5.m5.1.1.cmml" xref="S5.SS1.p1.5.m5.1.1"><csymbol cd="latexml" id="S5.SS1.p1.5.m5.1.1.1.cmml" xref="S5.SS1.p1.5.m5.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.p1.5.m5.1.1.2.cmml" xref="S5.SS1.p1.5.m5.1.1.2">65.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.5.m5.1c">65.2\%</annotation></semantics></math> and <math id="S5.SS1.p1.6.m6.1" class="ltx_Math" alttext="50.8\%" display="inline"><semantics id="S5.SS1.p1.6.m6.1a"><mrow id="S5.SS1.p1.6.m6.1.1" xref="S5.SS1.p1.6.m6.1.1.cmml"><mn id="S5.SS1.p1.6.m6.1.1.2" xref="S5.SS1.p1.6.m6.1.1.2.cmml">50.8</mn><mo id="S5.SS1.p1.6.m6.1.1.1" xref="S5.SS1.p1.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.6.m6.1b"><apply id="S5.SS1.p1.6.m6.1.1.cmml" xref="S5.SS1.p1.6.m6.1.1"><csymbol cd="latexml" id="S5.SS1.p1.6.m6.1.1.1.cmml" xref="S5.SS1.p1.6.m6.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.p1.6.m6.1.1.2.cmml" xref="S5.SS1.p1.6.m6.1.1.2">50.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.6.m6.1c">50.8\%</annotation></semantics></math> on the training and test samples, respectively.
Although this model improved the mAP on COCO compared with previous object detection models,
the experts would like to examine what limited it from achieving a better performance.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.2" class="ltx_p">Following common practices, the experts evaluated the model on the test samples.
They began the analysis by examining overall performance in the matrix-based visualization.
The matrix was initially in the confusion mode with row normalization, showing the confusion between 12 super-classes.
Most of the off-diagonal cells had very light colors (Fig.¬†<a href="#S4.F4" title="Figure 4 ‚Ä£ 4.1 Matrix-based Visualization ‚Ä£ 4 Uni-Evaluator Visualization ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>(a)),
which indicated that the model had a high accuracy in classifying the objects of different super-classes.
Considering the high classification accuracy but low mAP,
the experts suspected that the model classified objects well but failed to localize them accurately.
As localization concerns both the size errors and shifted directions of the predictions¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>,
they decided to evaluate the model from these two perspectives.
<math id="S5.SS1.p2.1.m1.1" class="ltx_Math" alttext="E_{1}" display="inline"><semantics id="S5.SS1.p2.1.m1.1a"><msub id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml"><mi id="S5.SS1.p2.1.m1.1.1.2" xref="S5.SS1.p2.1.m1.1.1.2.cmml">E</mi><mn id="S5.SS1.p2.1.m1.1.1.3" xref="S5.SS1.p2.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><apply id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.p2.1.m1.1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.p2.1.m1.1.1.2.cmml" xref="S5.SS1.p2.1.m1.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.p2.1.m1.1.1.3.cmml" xref="S5.SS1.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">E_{1}</annotation></semantics></math> focused on analyzing the sizes of predicted objects,
and <math id="S5.SS1.p2.2.m2.1" class="ltx_Math" alttext="E_{2}" display="inline"><semantics id="S5.SS1.p2.2.m2.1a"><msub id="S5.SS1.p2.2.m2.1.1" xref="S5.SS1.p2.2.m2.1.1.cmml"><mi id="S5.SS1.p2.2.m2.1.1.2" xref="S5.SS1.p2.2.m2.1.1.2.cmml">E</mi><mn id="S5.SS1.p2.2.m2.1.1.3" xref="S5.SS1.p2.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.2.m2.1b"><apply id="S5.SS1.p2.2.m2.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.p2.2.m2.1.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="S5.SS1.p2.2.m2.1.1.2.cmml" xref="S5.SS1.p2.2.m2.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.p2.2.m2.1.1.3.cmml" xref="S5.SS1.p2.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.2.m2.1c">E_{2}</annotation></semantics></math> focused on analyzing the shifted directions of predicted objects.</p>
</div>
<section id="S5.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1 </span>Diagnosing Size Issues</h4>

<div id="S5.SS1.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS1.p1.2" class="ltx_p"><span id="S5.SS1.SSS1.p1.2.1" class="ltx_text ltx_font_bold">Performance overview (T2)</span>.
To investigate the potential size errors in the predictions,
<math id="S5.SS1.SSS1.p1.1.m1.1" class="ltx_Math" alttext="E_{1}" display="inline"><semantics id="S5.SS1.SSS1.p1.1.m1.1a"><msub id="S5.SS1.SSS1.p1.1.m1.1.1" xref="S5.SS1.SSS1.p1.1.m1.1.1.cmml"><mi id="S5.SS1.SSS1.p1.1.m1.1.1.2" xref="S5.SS1.SSS1.p1.1.m1.1.1.2.cmml">E</mi><mn id="S5.SS1.SSS1.p1.1.m1.1.1.3" xref="S5.SS1.SSS1.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p1.1.m1.1b"><apply id="S5.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S5.SS1.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.SSS1.p1.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.SSS1.p1.1.m1.1.1.2.cmml" xref="S5.SS1.SSS1.p1.1.m1.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.SSS1.p1.1.m1.1.1.3.cmml" xref="S5.SS1.SSS1.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p1.1.m1.1c">E_{1}</annotation></semantics></math> switched to the size mode.
In the matrix, he observed that all the large pie charts with large green and yellow sectors were on the diagonal.
This indicated that many objects were classified correctly but localized with size errors.
To investigate the cause of the size issue,
<math id="S5.SS1.SSS1.p1.2.m2.1" class="ltx_Math" alttext="E_{1}" display="inline"><semantics id="S5.SS1.SSS1.p1.2.m2.1a"><msub id="S5.SS1.SSS1.p1.2.m2.1.1" xref="S5.SS1.SSS1.p1.2.m2.1.1.cmml"><mi id="S5.SS1.SSS1.p1.2.m2.1.1.2" xref="S5.SS1.SSS1.p1.2.m2.1.1.2.cmml">E</mi><mn id="S5.SS1.SSS1.p1.2.m2.1.1.3" xref="S5.SS1.SSS1.p1.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p1.2.m2.1b"><apply id="S5.SS1.SSS1.p1.2.m2.1.1.cmml" xref="S5.SS1.SSS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS1.p1.2.m2.1.1.1.cmml" xref="S5.SS1.SSS1.p1.2.m2.1.1">subscript</csymbol><ci id="S5.SS1.SSS1.p1.2.m2.1.1.2.cmml" xref="S5.SS1.SSS1.p1.2.m2.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.SSS1.p1.2.m2.1.1.3.cmml" xref="S5.SS1.SSS1.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p1.2.m2.1c">E_{1}</annotation></semantics></math> decided to dive deeper into these diagonal cells.
The diagonal cell with the most size errors, super-class ‚Äúoutdoor‚Äù (Fig.¬†<a href="#S4.F4" title="Figure 4 ‚Ä£ 4.1 Matrix-based Visualization ‚Ä£ 4 Uni-Evaluator Visualization ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>A), was taken as an example to illustrate the idea.
The other diagonal cells can be analyzed in a similar way.</p>
</div>
<div id="S5.SS1.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS1.p2.1" class="ltx_p"><span id="S5.SS1.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Analyzing size issues in the subset of ‚Äúoutdoor‚Äù (T2, T3, T4)</span>.
To figure out which classes within the super-class ‚Äúoutdoor‚Äù contributed to the size errors, he expanded this cell to a sub-matrix (Fig.¬†<a href="#S4.F4" title="Figure 4 ‚Ä£ 4.1 Matrix-based Visualization ‚Ä£ 4 Uni-Evaluator Visualization ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>(c)).
In the sub-matrix,
he found the diagonal cell of ‚Äútraffic light‚Äù contributed the majority of the size errors (Fig.¬†<a href="#S4.F4" title="Figure 4 ‚Ä£ 4.1 Matrix-based Visualization ‚Ä£ 4 Uni-Evaluator Visualization ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>B).
However, in the grid visualization, the predictions on the associated objects did not show clear reasons for these size errors (Fig.¬†<a href="#S4.F4" title="Figure 4 ‚Ä£ 4.1 Matrix-based Visualization ‚Ä£ 4 Uni-Evaluator Visualization ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>(d)).
<math id="S5.SS1.SSS1.p2.1.m1.1" class="ltx_Math" alttext="E_{1}" display="inline"><semantics id="S5.SS1.SSS1.p2.1.m1.1a"><msub id="S5.SS1.SSS1.p2.1.m1.1.1" xref="S5.SS1.SSS1.p2.1.m1.1.1.cmml"><mi id="S5.SS1.SSS1.p2.1.m1.1.1.2" xref="S5.SS1.SSS1.p2.1.m1.1.1.2.cmml">E</mi><mn id="S5.SS1.SSS1.p2.1.m1.1.1.3" xref="S5.SS1.SSS1.p2.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p2.1.m1.1b"><apply id="S5.SS1.SSS1.p2.1.m1.1.1.cmml" xref="S5.SS1.SSS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS1.p2.1.m1.1.1.1.cmml" xref="S5.SS1.SSS1.p2.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.SSS1.p2.1.m1.1.1.2.cmml" xref="S5.SS1.SSS1.p2.1.m1.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.SSS1.p2.1.m1.1.1.3.cmml" xref="S5.SS1.SSS1.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p2.1.m1.1c">E_{1}</annotation></semantics></math> then turned to the table visualization to analyze performance on different subsets in this cell.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2308.05168/assets/x15.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="233" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S5.F5.3.2" class="ltx_text" style="font-size:90%;">(a) The table visualization shows subsets of ‚Äútraffic light;‚Äù (b) objects with smaller sizes; (c) objects with larger sizes.</span></figcaption>
</figure>
<div id="S5.SS1.SSS1.p3" class="ltx_para">
<p id="S5.SS1.SSS1.p3.2" class="ltx_p">To find problematic subsets in the table visualization,
<math id="S5.SS1.SSS1.p3.1.m1.1" class="ltx_Math" alttext="E_{1}" display="inline"><semantics id="S5.SS1.SSS1.p3.1.m1.1a"><msub id="S5.SS1.SSS1.p3.1.m1.1.1" xref="S5.SS1.SSS1.p3.1.m1.1.1.cmml"><mi id="S5.SS1.SSS1.p3.1.m1.1.1.2" xref="S5.SS1.SSS1.p3.1.m1.1.1.2.cmml">E</mi><mn id="S5.SS1.SSS1.p3.1.m1.1.1.3" xref="S5.SS1.SSS1.p3.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p3.1.m1.1b"><apply id="S5.SS1.SSS1.p3.1.m1.1.1.cmml" xref="S5.SS1.SSS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS1.p3.1.m1.1.1.1.cmml" xref="S5.SS1.SSS1.p3.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.SSS1.p3.1.m1.1.1.2.cmml" xref="S5.SS1.SSS1.p3.1.m1.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.SSS1.p3.1.m1.1.1.3.cmml" xref="S5.SS1.SSS1.p3.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p3.1.m1.1c">E_{1}</annotation></semantics></math> sorted the subsets by the recall scores in ascending order.
He identified many subsets with low recall scores (Fig.¬†<a href="#S5.F5" title="Figure 5 ‚Ä£ 5.1.1 Diagnosing Size Issues ‚Ä£ 5.1 Object Detection on COCO dataset ‚Ä£ 5 Case Studies ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>(a)).
<math id="S5.SS1.SSS1.p3.2.m2.1" class="ltx_Math" alttext="E_{1}" display="inline"><semantics id="S5.SS1.SSS1.p3.2.m2.1a"><msub id="S5.SS1.SSS1.p3.2.m2.1.1" xref="S5.SS1.SSS1.p3.2.m2.1.1.cmml"><mi id="S5.SS1.SSS1.p3.2.m2.1.1.2" xref="S5.SS1.SSS1.p3.2.m2.1.1.2.cmml">E</mi><mn id="S5.SS1.SSS1.p3.2.m2.1.1.3" xref="S5.SS1.SSS1.p3.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p3.2.m2.1b"><apply id="S5.SS1.SSS1.p3.2.m2.1.1.cmml" xref="S5.SS1.SSS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS1.p3.2.m2.1.1.1.cmml" xref="S5.SS1.SSS1.p3.2.m2.1.1">subscript</csymbol><ci id="S5.SS1.SSS1.p3.2.m2.1.1.2.cmml" xref="S5.SS1.SSS1.p3.2.m2.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.SSS1.p3.2.m2.1.1.3.cmml" xref="S5.SS1.SSS1.p3.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p3.2.m2.1c">E_{1}</annotation></semantics></math> selected the subset with the lowest recall score (Fig.¬†<a href="#S5.F5" title="Figure 5 ‚Ä£ 5.1.1 Diagnosing Size Issues ‚Ä£ 5.1 Object Detection on COCO dataset ‚Ä£ 5 Case Studies ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>A) and then turned to the matrix.
In the matrix, he clicked the green/yellow sector in the pie chart (Fig.¬†<a href="#S5.F5" title="Figure 5 ‚Ä£ 5.1.1 Diagnosing Size Issues ‚Ä£ 5.1 Object Detection on COCO dataset ‚Ä£ 5 Case Studies ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>B) to examine the predictions with smaller/larger sizes than ground truth in the grid visualization.</p>
</div>
<div id="S5.SS1.SSS1.p4" class="ltx_para">
<p id="S5.SS1.SSS1.p4.6" class="ltx_p">From examining the predictions with smaller sizes, <math id="S5.SS1.SSS1.p4.1.m1.1" class="ltx_Math" alttext="E_{1}" display="inline"><semantics id="S5.SS1.SSS1.p4.1.m1.1a"><msub id="S5.SS1.SSS1.p4.1.m1.1.1" xref="S5.SS1.SSS1.p4.1.m1.1.1.cmml"><mi id="S5.SS1.SSS1.p4.1.m1.1.1.2" xref="S5.SS1.SSS1.p4.1.m1.1.1.2.cmml">E</mi><mn id="S5.SS1.SSS1.p4.1.m1.1.1.3" xref="S5.SS1.SSS1.p4.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p4.1.m1.1b"><apply id="S5.SS1.SSS1.p4.1.m1.1.1.cmml" xref="S5.SS1.SSS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS1.p4.1.m1.1.1.1.cmml" xref="S5.SS1.SSS1.p4.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.SSS1.p4.1.m1.1.1.2.cmml" xref="S5.SS1.SSS1.p4.1.m1.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.SSS1.p4.1.m1.1.1.3.cmml" xref="S5.SS1.SSS1.p4.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p4.1.m1.1c">E_{1}</annotation></semantics></math> found that most of these predictions
existed in blurred images (Fig.¬†<a href="#S5.F5" title="Figure 5 ‚Ä£ 5.1.1 Diagnosing Size Issues ‚Ä£ 5.1 Object Detection on COCO dataset ‚Ä£ 5 Case Studies ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>(b)).
<math id="S5.SS1.SSS1.p4.2.m2.1" class="ltx_Math" alttext="E_{1}" display="inline"><semantics id="S5.SS1.SSS1.p4.2.m2.1a"><msub id="S5.SS1.SSS1.p4.2.m2.1.1" xref="S5.SS1.SSS1.p4.2.m2.1.1.cmml"><mi id="S5.SS1.SSS1.p4.2.m2.1.1.2" xref="S5.SS1.SSS1.p4.2.m2.1.1.2.cmml">E</mi><mn id="S5.SS1.SSS1.p4.2.m2.1.1.3" xref="S5.SS1.SSS1.p4.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p4.2.m2.1b"><apply id="S5.SS1.SSS1.p4.2.m2.1.1.cmml" xref="S5.SS1.SSS1.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS1.p4.2.m2.1.1.1.cmml" xref="S5.SS1.SSS1.p4.2.m2.1.1">subscript</csymbol><ci id="S5.SS1.SSS1.p4.2.m2.1.1.2.cmml" xref="S5.SS1.SSS1.p4.2.m2.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.SSS1.p4.2.m2.1.1.3.cmml" xref="S5.SS1.SSS1.p4.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p4.2.m2.1c">E_{1}</annotation></semantics></math> examined the training samples and found only a few such blurred images.
He concluded that this was the main reason why the model did not perform well on these blurred images.
To tackle this issue,
<math id="S5.SS1.SSS1.p4.3.m3.1" class="ltx_Math" alttext="E_{1}" display="inline"><semantics id="S5.SS1.SSS1.p4.3.m3.1a"><msub id="S5.SS1.SSS1.p4.3.m3.1.1" xref="S5.SS1.SSS1.p4.3.m3.1.1.cmml"><mi id="S5.SS1.SSS1.p4.3.m3.1.1.2" xref="S5.SS1.SSS1.p4.3.m3.1.1.2.cmml">E</mi><mn id="S5.SS1.SSS1.p4.3.m3.1.1.3" xref="S5.SS1.SSS1.p4.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p4.3.m3.1b"><apply id="S5.SS1.SSS1.p4.3.m3.1.1.cmml" xref="S5.SS1.SSS1.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS1.p4.3.m3.1.1.1.cmml" xref="S5.SS1.SSS1.p4.3.m3.1.1">subscript</csymbol><ci id="S5.SS1.SSS1.p4.3.m3.1.1.2.cmml" xref="S5.SS1.SSS1.p4.3.m3.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.SSS1.p4.3.m3.1.1.3.cmml" xref="S5.SS1.SSS1.p4.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p4.3.m3.1c">E_{1}</annotation></semantics></math> applied Gaussian noise data augmentation to <math id="S5.SS1.SSS1.p4.4.m4.2" class="ltx_Math" alttext="4,139" display="inline"><semantics id="S5.SS1.SSS1.p4.4.m4.2a"><mrow id="S5.SS1.SSS1.p4.4.m4.2.3.2" xref="S5.SS1.SSS1.p4.4.m4.2.3.1.cmml"><mn id="S5.SS1.SSS1.p4.4.m4.1.1" xref="S5.SS1.SSS1.p4.4.m4.1.1.cmml">4</mn><mo id="S5.SS1.SSS1.p4.4.m4.2.3.2.1" xref="S5.SS1.SSS1.p4.4.m4.2.3.1.cmml">,</mo><mn id="S5.SS1.SSS1.p4.4.m4.2.2" xref="S5.SS1.SSS1.p4.4.m4.2.2.cmml">139</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p4.4.m4.2b"><list id="S5.SS1.SSS1.p4.4.m4.2.3.1.cmml" xref="S5.SS1.SSS1.p4.4.m4.2.3.2"><cn type="integer" id="S5.SS1.SSS1.p4.4.m4.1.1.cmml" xref="S5.SS1.SSS1.p4.4.m4.1.1">4</cn><cn type="integer" id="S5.SS1.SSS1.p4.4.m4.2.2.cmml" xref="S5.SS1.SSS1.p4.4.m4.2.2">139</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p4.4.m4.2c">4,139</annotation></semantics></math> training samples with traffic lights.
After fine-tuning with the augmented samples,
the AP of ‚Äútraffic light‚Äù was increased from <math id="S5.SS1.SSS1.p4.5.m5.1" class="ltx_Math" alttext="32.7\%" display="inline"><semantics id="S5.SS1.SSS1.p4.5.m5.1a"><mrow id="S5.SS1.SSS1.p4.5.m5.1.1" xref="S5.SS1.SSS1.p4.5.m5.1.1.cmml"><mn id="S5.SS1.SSS1.p4.5.m5.1.1.2" xref="S5.SS1.SSS1.p4.5.m5.1.1.2.cmml">32.7</mn><mo id="S5.SS1.SSS1.p4.5.m5.1.1.1" xref="S5.SS1.SSS1.p4.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p4.5.m5.1b"><apply id="S5.SS1.SSS1.p4.5.m5.1.1.cmml" xref="S5.SS1.SSS1.p4.5.m5.1.1"><csymbol cd="latexml" id="S5.SS1.SSS1.p4.5.m5.1.1.1.cmml" xref="S5.SS1.SSS1.p4.5.m5.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.SSS1.p4.5.m5.1.1.2.cmml" xref="S5.SS1.SSS1.p4.5.m5.1.1.2">32.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p4.5.m5.1c">32.7\%</annotation></semantics></math> to <math id="S5.SS1.SSS1.p4.6.m6.1" class="ltx_Math" alttext="33.1\%" display="inline"><semantics id="S5.SS1.SSS1.p4.6.m6.1a"><mrow id="S5.SS1.SSS1.p4.6.m6.1.1" xref="S5.SS1.SSS1.p4.6.m6.1.1.cmml"><mn id="S5.SS1.SSS1.p4.6.m6.1.1.2" xref="S5.SS1.SSS1.p4.6.m6.1.1.2.cmml">33.1</mn><mo id="S5.SS1.SSS1.p4.6.m6.1.1.1" xref="S5.SS1.SSS1.p4.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p4.6.m6.1b"><apply id="S5.SS1.SSS1.p4.6.m6.1.1.cmml" xref="S5.SS1.SSS1.p4.6.m6.1.1"><csymbol cd="latexml" id="S5.SS1.SSS1.p4.6.m6.1.1.1.cmml" xref="S5.SS1.SSS1.p4.6.m6.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.SSS1.p4.6.m6.1.1.2.cmml" xref="S5.SS1.SSS1.p4.6.m6.1.1.2">33.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p4.6.m6.1c">33.1\%</annotation></semantics></math>.</p>
</div>
<div id="S5.SS1.SSS1.p5" class="ltx_para">
<p id="S5.SS1.SSS1.p5.6" class="ltx_p">From examining the predictions with larger sizes, <math id="S5.SS1.SSS1.p5.1.m1.1" class="ltx_Math" alttext="E_{1}" display="inline"><semantics id="S5.SS1.SSS1.p5.1.m1.1a"><msub id="S5.SS1.SSS1.p5.1.m1.1.1" xref="S5.SS1.SSS1.p5.1.m1.1.1.cmml"><mi id="S5.SS1.SSS1.p5.1.m1.1.1.2" xref="S5.SS1.SSS1.p5.1.m1.1.1.2.cmml">E</mi><mn id="S5.SS1.SSS1.p5.1.m1.1.1.3" xref="S5.SS1.SSS1.p5.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p5.1.m1.1b"><apply id="S5.SS1.SSS1.p5.1.m1.1.1.cmml" xref="S5.SS1.SSS1.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS1.p5.1.m1.1.1.1.cmml" xref="S5.SS1.SSS1.p5.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.SSS1.p5.1.m1.1.1.2.cmml" xref="S5.SS1.SSS1.p5.1.m1.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.SSS1.p5.1.m1.1.1.3.cmml" xref="S5.SS1.SSS1.p5.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p5.1.m1.1c">E_{1}</annotation></semantics></math> found that
the model had already made precise predictions on these objects (Fig.¬†<a href="#S5.F5" title="Figure 5 ‚Ä£ 5.1.1 Diagnosing Size Issues ‚Ä£ 5.1 Object Detection on COCO dataset ‚Ä£ 5 Case Studies ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>(c)).
However, some annotations were imprecise, which caused small overlaps between predictions and annotations.
<math id="S5.SS1.SSS1.p5.2.m2.1" class="ltx_Math" alttext="E_{1}" display="inline"><semantics id="S5.SS1.SSS1.p5.2.m2.1a"><msub id="S5.SS1.SSS1.p5.2.m2.1.1" xref="S5.SS1.SSS1.p5.2.m2.1.1.cmml"><mi id="S5.SS1.SSS1.p5.2.m2.1.1.2" xref="S5.SS1.SSS1.p5.2.m2.1.1.2.cmml">E</mi><mn id="S5.SS1.SSS1.p5.2.m2.1.1.3" xref="S5.SS1.SSS1.p5.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p5.2.m2.1b"><apply id="S5.SS1.SSS1.p5.2.m2.1.1.cmml" xref="S5.SS1.SSS1.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS1.p5.2.m2.1.1.1.cmml" xref="S5.SS1.SSS1.p5.2.m2.1.1">subscript</csymbol><ci id="S5.SS1.SSS1.p5.2.m2.1.1.2.cmml" xref="S5.SS1.SSS1.p5.2.m2.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.SSS1.p5.2.m2.1.1.3.cmml" xref="S5.SS1.SSS1.p5.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p5.2.m2.1c">E_{1}</annotation></semantics></math> continued to examine other subsets of ‚Äútraffic light‚Äù and found similar issues caused by imprecise annotations.
He commented that the imprecise annotations in the test samples would mislead the model evaluation.
Interested in assessing actual performance on ‚Äútraffic light,‚Äù
<math id="S5.SS1.SSS1.p5.3.m3.1" class="ltx_Math" alttext="E_{1}" display="inline"><semantics id="S5.SS1.SSS1.p5.3.m3.1a"><msub id="S5.SS1.SSS1.p5.3.m3.1.1" xref="S5.SS1.SSS1.p5.3.m3.1.1.cmml"><mi id="S5.SS1.SSS1.p5.3.m3.1.1.2" xref="S5.SS1.SSS1.p5.3.m3.1.1.2.cmml">E</mi><mn id="S5.SS1.SSS1.p5.3.m3.1.1.3" xref="S5.SS1.SSS1.p5.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p5.3.m3.1b"><apply id="S5.SS1.SSS1.p5.3.m3.1.1.cmml" xref="S5.SS1.SSS1.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS1.p5.3.m3.1.1.1.cmml" xref="S5.SS1.SSS1.p5.3.m3.1.1">subscript</csymbol><ci id="S5.SS1.SSS1.p5.3.m3.1.1.2.cmml" xref="S5.SS1.SSS1.p5.3.m3.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.SSS1.p5.3.m3.1.1.3.cmml" xref="S5.SS1.SSS1.p5.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p5.3.m3.1c">E_{1}</annotation></semantics></math> hired annotators to re-annotate the bounding boxes of ‚Äútraffic light‚Äù on test samples.
<math id="S5.SS1.SSS1.p5.4.m4.1" class="ltx_Math" alttext="E_{1}" display="inline"><semantics id="S5.SS1.SSS1.p5.4.m4.1a"><msub id="S5.SS1.SSS1.p5.4.m4.1.1" xref="S5.SS1.SSS1.p5.4.m4.1.1.cmml"><mi id="S5.SS1.SSS1.p5.4.m4.1.1.2" xref="S5.SS1.SSS1.p5.4.m4.1.1.2.cmml">E</mi><mn id="S5.SS1.SSS1.p5.4.m4.1.1.3" xref="S5.SS1.SSS1.p5.4.m4.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p5.4.m4.1b"><apply id="S5.SS1.SSS1.p5.4.m4.1.1.cmml" xref="S5.SS1.SSS1.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS1.p5.4.m4.1.1.1.cmml" xref="S5.SS1.SSS1.p5.4.m4.1.1">subscript</csymbol><ci id="S5.SS1.SSS1.p5.4.m4.1.1.2.cmml" xref="S5.SS1.SSS1.p5.4.m4.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.SSS1.p5.4.m4.1.1.3.cmml" xref="S5.SS1.SSS1.p5.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p5.4.m4.1c">E_{1}</annotation></semantics></math> then re-evaluated the model using the re-annotated test samples.
The AP of ‚Äútraffic light‚Äù was increased from <math id="S5.SS1.SSS1.p5.5.m5.1" class="ltx_Math" alttext="33.1\%" display="inline"><semantics id="S5.SS1.SSS1.p5.5.m5.1a"><mrow id="S5.SS1.SSS1.p5.5.m5.1.1" xref="S5.SS1.SSS1.p5.5.m5.1.1.cmml"><mn id="S5.SS1.SSS1.p5.5.m5.1.1.2" xref="S5.SS1.SSS1.p5.5.m5.1.1.2.cmml">33.1</mn><mo id="S5.SS1.SSS1.p5.5.m5.1.1.1" xref="S5.SS1.SSS1.p5.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p5.5.m5.1b"><apply id="S5.SS1.SSS1.p5.5.m5.1.1.cmml" xref="S5.SS1.SSS1.p5.5.m5.1.1"><csymbol cd="latexml" id="S5.SS1.SSS1.p5.5.m5.1.1.1.cmml" xref="S5.SS1.SSS1.p5.5.m5.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.SSS1.p5.5.m5.1.1.2.cmml" xref="S5.SS1.SSS1.p5.5.m5.1.1.2">33.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p5.5.m5.1c">33.1\%</annotation></semantics></math> to <math id="S5.SS1.SSS1.p5.6.m6.1" class="ltx_Math" alttext="42.4\%" display="inline"><semantics id="S5.SS1.SSS1.p5.6.m6.1a"><mrow id="S5.SS1.SSS1.p5.6.m6.1.1" xref="S5.SS1.SSS1.p5.6.m6.1.1.cmml"><mn id="S5.SS1.SSS1.p5.6.m6.1.1.2" xref="S5.SS1.SSS1.p5.6.m6.1.1.2.cmml">42.4</mn><mo id="S5.SS1.SSS1.p5.6.m6.1.1.1" xref="S5.SS1.SSS1.p5.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p5.6.m6.1b"><apply id="S5.SS1.SSS1.p5.6.m6.1.1.cmml" xref="S5.SS1.SSS1.p5.6.m6.1.1"><csymbol cd="latexml" id="S5.SS1.SSS1.p5.6.m6.1.1.1.cmml" xref="S5.SS1.SSS1.p5.6.m6.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.SSS1.p5.6.m6.1.1.2.cmml" xref="S5.SS1.SSS1.p5.6.m6.1.1.2">42.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p5.6.m6.1c">42.4\%</annotation></semantics></math>.</p>
</div>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2308.05168/assets/x16.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="282" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S5.F6.3.2" class="ltx_text" style="font-size:90%;">The subsets ranked by (a) the precision score and (b) the combination of the precision score and the ground-truth aspect ratio; (c) inconsistent annotations of ‚Äúbroccoli.‚Äù</span></figcaption>
</figure>
<div id="S5.SS1.SSS1.p6" class="ltx_para">
<p id="S5.SS1.SSS1.p6.3" class="ltx_p">After exploring the subsets with low recall scores,
<math id="S5.SS1.SSS1.p6.1.m1.1" class="ltx_Math" alttext="E_{1}" display="inline"><semantics id="S5.SS1.SSS1.p6.1.m1.1a"><msub id="S5.SS1.SSS1.p6.1.m1.1.1" xref="S5.SS1.SSS1.p6.1.m1.1.1.cmml"><mi id="S5.SS1.SSS1.p6.1.m1.1.1.2" xref="S5.SS1.SSS1.p6.1.m1.1.1.2.cmml">E</mi><mn id="S5.SS1.SSS1.p6.1.m1.1.1.3" xref="S5.SS1.SSS1.p6.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p6.1.m1.1b"><apply id="S5.SS1.SSS1.p6.1.m1.1.1.cmml" xref="S5.SS1.SSS1.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS1.p6.1.m1.1.1.1.cmml" xref="S5.SS1.SSS1.p6.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.SSS1.p6.1.m1.1.1.2.cmml" xref="S5.SS1.SSS1.p6.1.m1.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.SSS1.p6.1.m1.1.1.3.cmml" xref="S5.SS1.SSS1.p6.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p6.1.m1.1c">E_{1}</annotation></semantics></math> was also interested in the subsets of ‚Äútraffic light‚Äù with low precision scores.
He sorted the subsets by the precision scores in ascending order (Fig.¬†<a href="#S5.F6" title="Figure 6 ‚Ä£ 5.1.1 Diagnosing Size Issues ‚Ä£ 5.1 Object Detection on COCO dataset ‚Ä£ 5 Case Studies ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>(a))
and noticed that
many subsets with low precision scores contained the objects of
small sizes (Fig.¬†<a href="#S5.F6" title="Figure 6 ‚Ä£ 5.1.1 Diagnosing Size Issues ‚Ä£ 5.1 Object Detection on COCO dataset ‚Ä£ 5 Case Studies ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>A) or small aspect ratios (Fig.¬†<a href="#S5.F6" title="Figure 6 ‚Ä£ 5.1.1 Diagnosing Size Issues ‚Ä£ 5.1 Object Detection on COCO dataset ‚Ä£ 5 Case Studies ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>B).
Here, the aspect ratio of an object is
the ratio between the minimum and maximum of the width and the height,
which ranges between 0 and 1¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.
<math id="S5.SS1.SSS1.p6.2.m2.1" class="ltx_Math" alttext="E_{1}" display="inline"><semantics id="S5.SS1.SSS1.p6.2.m2.1a"><msub id="S5.SS1.SSS1.p6.2.m2.1.1" xref="S5.SS1.SSS1.p6.2.m2.1.1.cmml"><mi id="S5.SS1.SSS1.p6.2.m2.1.1.2" xref="S5.SS1.SSS1.p6.2.m2.1.1.2.cmml">E</mi><mn id="S5.SS1.SSS1.p6.2.m2.1.1.3" xref="S5.SS1.SSS1.p6.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p6.2.m2.1b"><apply id="S5.SS1.SSS1.p6.2.m2.1.1.cmml" xref="S5.SS1.SSS1.p6.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS1.p6.2.m2.1.1.1.cmml" xref="S5.SS1.SSS1.p6.2.m2.1.1">subscript</csymbol><ci id="S5.SS1.SSS1.p6.2.m2.1.1.2.cmml" xref="S5.SS1.SSS1.p6.2.m2.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.SSS1.p6.2.m2.1.1.3.cmml" xref="S5.SS1.SSS1.p6.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p6.2.m2.1c">E_{1}</annotation></semantics></math> explained that this is reasonable because these characteristics represented some extremely hard objects in detection,
and therefore would lead to a low precision score in predictions.
Conversely, objects of large sizes or large aspect ratios were usually detected with high precision scores because they were easy to detect.
Inspired by this finding,
<math id="S5.SS1.SSS1.p6.3.m3.1" class="ltx_Math" alttext="E_{1}" display="inline"><semantics id="S5.SS1.SSS1.p6.3.m3.1a"><msub id="S5.SS1.SSS1.p6.3.m3.1.1" xref="S5.SS1.SSS1.p6.3.m3.1.1.cmml"><mi id="S5.SS1.SSS1.p6.3.m3.1.1.2" xref="S5.SS1.SSS1.p6.3.m3.1.1.2.cmml">E</mi><mn id="S5.SS1.SSS1.p6.3.m3.1.1.3" xref="S5.SS1.SSS1.p6.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p6.3.m3.1b"><apply id="S5.SS1.SSS1.p6.3.m3.1.1.cmml" xref="S5.SS1.SSS1.p6.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS1.p6.3.m3.1.1.1.cmml" xref="S5.SS1.SSS1.p6.3.m3.1.1">subscript</csymbol><ci id="S5.SS1.SSS1.p6.3.m3.1.1.2.cmml" xref="S5.SS1.SSS1.p6.3.m3.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.SSS1.p6.3.m3.1.1.3.cmml" xref="S5.SS1.SSS1.p6.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p6.3.m3.1c">E_{1}</annotation></semantics></math> would like to examine the subsets that were low in precision but without such hard objects.
Next, we took the subsets with low precision scores but large aspect ratios as examples for demonstration.</p>
</div>
<div id="S5.SS1.SSS1.p7" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS1.p7.12" class="ltx_p"><span id="S5.SS1.SSS1.p7.12.1" class="ltx_text ltx_font_bold">Analyzing other low-precision subsets (T3, T4)</span>.
<math id="S5.SS1.SSS1.p7.1.m1.1" class="ltx_Math" alttext="E_{1}" display="inline"><semantics id="S5.SS1.SSS1.p7.1.m1.1a"><msub id="S5.SS1.SSS1.p7.1.m1.1.1" xref="S5.SS1.SSS1.p7.1.m1.1.1.cmml"><mi id="S5.SS1.SSS1.p7.1.m1.1.1.2" xref="S5.SS1.SSS1.p7.1.m1.1.1.2.cmml">E</mi><mn id="S5.SS1.SSS1.p7.1.m1.1.1.3" xref="S5.SS1.SSS1.p7.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p7.1.m1.1b"><apply id="S5.SS1.SSS1.p7.1.m1.1.1.cmml" xref="S5.SS1.SSS1.p7.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS1.p7.1.m1.1.1.1.cmml" xref="S5.SS1.SSS1.p7.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.SSS1.p7.1.m1.1.1.2.cmml" xref="S5.SS1.SSS1.p7.1.m1.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.SSS1.p7.1.m1.1.1.3.cmml" xref="S5.SS1.SSS1.p7.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p7.1.m1.1c">E_{1}</annotation></semantics></math> re-ranked the subsets by combining the precision score and the aspect ratio of ground truth to find some subsets with low precision scores but large aspect ratios (Fig.¬†<a href="#S5.F6" title="Figure 6 ‚Ä£ 5.1.1 Diagnosing Size Issues ‚Ä£ 5.1 Object Detection on COCO dataset ‚Ä£ 5 Case Studies ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>(b)).
Among the subsets, a subset of ‚Äúbroccoli‚Äù was ranked at the top (Fig.¬†<a href="#S5.F6" title="Figure 6 ‚Ä£ 5.1.1 Diagnosing Size Issues ‚Ä£ 5.1 Object Detection on COCO dataset ‚Ä£ 5 Case Studies ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>C).
He examined its associated objects and saw many ‚Äúbroccoli‚Äù objects were annotated with a confusing criterion (Fig.¬†<a href="#S5.F6" title="Figure 6 ‚Ä£ 5.1.1 Diagnosing Size Issues ‚Ä£ 5.1 Object Detection on COCO dataset ‚Ä£ 5 Case Studies ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>(c)).
Some contained only one floret of broccoli,
while others might include two or more florets of broccoli.
From these findings,
he realized that the inconsistent annotations caused some correct predictions considered to be wrong during model evaluation.
Similar patterns were also found in other food-related classes, including ‚Äúapple,‚Äù ‚Äúorange,‚Äù ‚Äúbanana,‚Äù and ‚Äúcarrot.‚Äù
For more accurate model evaluation,
<math id="S5.SS1.SSS1.p7.2.m2.1" class="ltx_Math" alttext="E_{1}" display="inline"><semantics id="S5.SS1.SSS1.p7.2.m2.1a"><msub id="S5.SS1.SSS1.p7.2.m2.1.1" xref="S5.SS1.SSS1.p7.2.m2.1.1.cmml"><mi id="S5.SS1.SSS1.p7.2.m2.1.1.2" xref="S5.SS1.SSS1.p7.2.m2.1.1.2.cmml">E</mi><mn id="S5.SS1.SSS1.p7.2.m2.1.1.3" xref="S5.SS1.SSS1.p7.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p7.2.m2.1b"><apply id="S5.SS1.SSS1.p7.2.m2.1.1.cmml" xref="S5.SS1.SSS1.p7.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS1.p7.2.m2.1.1.1.cmml" xref="S5.SS1.SSS1.p7.2.m2.1.1">subscript</csymbol><ci id="S5.SS1.SSS1.p7.2.m2.1.1.2.cmml" xref="S5.SS1.SSS1.p7.2.m2.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.SSS1.p7.2.m2.1.1.3.cmml" xref="S5.SS1.SSS1.p7.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p7.2.m2.1c">E_{1}</annotation></semantics></math> decided to re-annotate these five classes with a consistent criterion.
After the re-annotation, the APs of ‚Äúbroccoli,‚Äù ‚Äúapple,‚Äù ‚Äúbanana,‚Äù ‚Äúorange,‚Äù and ‚Äúcarrot‚Äù were increased from
<math id="S5.SS1.SSS1.p7.3.m3.1" class="ltx_Math" alttext="26.9\%" display="inline"><semantics id="S5.SS1.SSS1.p7.3.m3.1a"><mrow id="S5.SS1.SSS1.p7.3.m3.1.1" xref="S5.SS1.SSS1.p7.3.m3.1.1.cmml"><mn id="S5.SS1.SSS1.p7.3.m3.1.1.2" xref="S5.SS1.SSS1.p7.3.m3.1.1.2.cmml">26.9</mn><mo id="S5.SS1.SSS1.p7.3.m3.1.1.1" xref="S5.SS1.SSS1.p7.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p7.3.m3.1b"><apply id="S5.SS1.SSS1.p7.3.m3.1.1.cmml" xref="S5.SS1.SSS1.p7.3.m3.1.1"><csymbol cd="latexml" id="S5.SS1.SSS1.p7.3.m3.1.1.1.cmml" xref="S5.SS1.SSS1.p7.3.m3.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.SSS1.p7.3.m3.1.1.2.cmml" xref="S5.SS1.SSS1.p7.3.m3.1.1.2">26.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p7.3.m3.1c">26.9\%</annotation></semantics></math> to <math id="S5.SS1.SSS1.p7.4.m4.1" class="ltx_Math" alttext="62.1\%" display="inline"><semantics id="S5.SS1.SSS1.p7.4.m4.1a"><mrow id="S5.SS1.SSS1.p7.4.m4.1.1" xref="S5.SS1.SSS1.p7.4.m4.1.1.cmml"><mn id="S5.SS1.SSS1.p7.4.m4.1.1.2" xref="S5.SS1.SSS1.p7.4.m4.1.1.2.cmml">62.1</mn><mo id="S5.SS1.SSS1.p7.4.m4.1.1.1" xref="S5.SS1.SSS1.p7.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p7.4.m4.1b"><apply id="S5.SS1.SSS1.p7.4.m4.1.1.cmml" xref="S5.SS1.SSS1.p7.4.m4.1.1"><csymbol cd="latexml" id="S5.SS1.SSS1.p7.4.m4.1.1.1.cmml" xref="S5.SS1.SSS1.p7.4.m4.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.SSS1.p7.4.m4.1.1.2.cmml" xref="S5.SS1.SSS1.p7.4.m4.1.1.2">62.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p7.4.m4.1c">62.1\%</annotation></semantics></math>, <math id="S5.SS1.SSS1.p7.5.m5.1" class="ltx_Math" alttext="30.3\%" display="inline"><semantics id="S5.SS1.SSS1.p7.5.m5.1a"><mrow id="S5.SS1.SSS1.p7.5.m5.1.1" xref="S5.SS1.SSS1.p7.5.m5.1.1.cmml"><mn id="S5.SS1.SSS1.p7.5.m5.1.1.2" xref="S5.SS1.SSS1.p7.5.m5.1.1.2.cmml">30.3</mn><mo id="S5.SS1.SSS1.p7.5.m5.1.1.1" xref="S5.SS1.SSS1.p7.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p7.5.m5.1b"><apply id="S5.SS1.SSS1.p7.5.m5.1.1.cmml" xref="S5.SS1.SSS1.p7.5.m5.1.1"><csymbol cd="latexml" id="S5.SS1.SSS1.p7.5.m5.1.1.1.cmml" xref="S5.SS1.SSS1.p7.5.m5.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.SSS1.p7.5.m5.1.1.2.cmml" xref="S5.SS1.SSS1.p7.5.m5.1.1.2">30.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p7.5.m5.1c">30.3\%</annotation></semantics></math> to <math id="S5.SS1.SSS1.p7.6.m6.1" class="ltx_Math" alttext="38.7\%" display="inline"><semantics id="S5.SS1.SSS1.p7.6.m6.1a"><mrow id="S5.SS1.SSS1.p7.6.m6.1.1" xref="S5.SS1.SSS1.p7.6.m6.1.1.cmml"><mn id="S5.SS1.SSS1.p7.6.m6.1.1.2" xref="S5.SS1.SSS1.p7.6.m6.1.1.2.cmml">38.7</mn><mo id="S5.SS1.SSS1.p7.6.m6.1.1.1" xref="S5.SS1.SSS1.p7.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p7.6.m6.1b"><apply id="S5.SS1.SSS1.p7.6.m6.1.1.cmml" xref="S5.SS1.SSS1.p7.6.m6.1.1"><csymbol cd="latexml" id="S5.SS1.SSS1.p7.6.m6.1.1.1.cmml" xref="S5.SS1.SSS1.p7.6.m6.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.SSS1.p7.6.m6.1.1.2.cmml" xref="S5.SS1.SSS1.p7.6.m6.1.1.2">38.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p7.6.m6.1c">38.7\%</annotation></semantics></math>,
<math id="S5.SS1.SSS1.p7.7.m7.1" class="ltx_Math" alttext="32.9\%" display="inline"><semantics id="S5.SS1.SSS1.p7.7.m7.1a"><mrow id="S5.SS1.SSS1.p7.7.m7.1.1" xref="S5.SS1.SSS1.p7.7.m7.1.1.cmml"><mn id="S5.SS1.SSS1.p7.7.m7.1.1.2" xref="S5.SS1.SSS1.p7.7.m7.1.1.2.cmml">32.9</mn><mo id="S5.SS1.SSS1.p7.7.m7.1.1.1" xref="S5.SS1.SSS1.p7.7.m7.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p7.7.m7.1b"><apply id="S5.SS1.SSS1.p7.7.m7.1.1.cmml" xref="S5.SS1.SSS1.p7.7.m7.1.1"><csymbol cd="latexml" id="S5.SS1.SSS1.p7.7.m7.1.1.1.cmml" xref="S5.SS1.SSS1.p7.7.m7.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.SSS1.p7.7.m7.1.1.2.cmml" xref="S5.SS1.SSS1.p7.7.m7.1.1.2">32.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p7.7.m7.1c">32.9\%</annotation></semantics></math> to <math id="S5.SS1.SSS1.p7.8.m8.1" class="ltx_Math" alttext="51.0\%" display="inline"><semantics id="S5.SS1.SSS1.p7.8.m8.1a"><mrow id="S5.SS1.SSS1.p7.8.m8.1.1" xref="S5.SS1.SSS1.p7.8.m8.1.1.cmml"><mn id="S5.SS1.SSS1.p7.8.m8.1.1.2" xref="S5.SS1.SSS1.p7.8.m8.1.1.2.cmml">51.0</mn><mo id="S5.SS1.SSS1.p7.8.m8.1.1.1" xref="S5.SS1.SSS1.p7.8.m8.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p7.8.m8.1b"><apply id="S5.SS1.SSS1.p7.8.m8.1.1.cmml" xref="S5.SS1.SSS1.p7.8.m8.1.1"><csymbol cd="latexml" id="S5.SS1.SSS1.p7.8.m8.1.1.1.cmml" xref="S5.SS1.SSS1.p7.8.m8.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.SSS1.p7.8.m8.1.1.2.cmml" xref="S5.SS1.SSS1.p7.8.m8.1.1.2">51.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p7.8.m8.1c">51.0\%</annotation></semantics></math>, <math id="S5.SS1.SSS1.p7.9.m9.1" class="ltx_Math" alttext="38.7\%" display="inline"><semantics id="S5.SS1.SSS1.p7.9.m9.1a"><mrow id="S5.SS1.SSS1.p7.9.m9.1.1" xref="S5.SS1.SSS1.p7.9.m9.1.1.cmml"><mn id="S5.SS1.SSS1.p7.9.m9.1.1.2" xref="S5.SS1.SSS1.p7.9.m9.1.1.2.cmml">38.7</mn><mo id="S5.SS1.SSS1.p7.9.m9.1.1.1" xref="S5.SS1.SSS1.p7.9.m9.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p7.9.m9.1b"><apply id="S5.SS1.SSS1.p7.9.m9.1.1.cmml" xref="S5.SS1.SSS1.p7.9.m9.1.1"><csymbol cd="latexml" id="S5.SS1.SSS1.p7.9.m9.1.1.1.cmml" xref="S5.SS1.SSS1.p7.9.m9.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.SSS1.p7.9.m9.1.1.2.cmml" xref="S5.SS1.SSS1.p7.9.m9.1.1.2">38.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p7.9.m9.1c">38.7\%</annotation></semantics></math> to <math id="S5.SS1.SSS1.p7.10.m10.1" class="ltx_Math" alttext="78.4\%" display="inline"><semantics id="S5.SS1.SSS1.p7.10.m10.1a"><mrow id="S5.SS1.SSS1.p7.10.m10.1.1" xref="S5.SS1.SSS1.p7.10.m10.1.1.cmml"><mn id="S5.SS1.SSS1.p7.10.m10.1.1.2" xref="S5.SS1.SSS1.p7.10.m10.1.1.2.cmml">78.4</mn><mo id="S5.SS1.SSS1.p7.10.m10.1.1.1" xref="S5.SS1.SSS1.p7.10.m10.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p7.10.m10.1b"><apply id="S5.SS1.SSS1.p7.10.m10.1.1.cmml" xref="S5.SS1.SSS1.p7.10.m10.1.1"><csymbol cd="latexml" id="S5.SS1.SSS1.p7.10.m10.1.1.1.cmml" xref="S5.SS1.SSS1.p7.10.m10.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.SSS1.p7.10.m10.1.1.2.cmml" xref="S5.SS1.SSS1.p7.10.m10.1.1.2">78.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p7.10.m10.1c">78.4\%</annotation></semantics></math>,
<math id="S5.SS1.SSS1.p7.11.m11.1" class="ltx_Math" alttext="29.3\%" display="inline"><semantics id="S5.SS1.SSS1.p7.11.m11.1a"><mrow id="S5.SS1.SSS1.p7.11.m11.1.1" xref="S5.SS1.SSS1.p7.11.m11.1.1.cmml"><mn id="S5.SS1.SSS1.p7.11.m11.1.1.2" xref="S5.SS1.SSS1.p7.11.m11.1.1.2.cmml">29.3</mn><mo id="S5.SS1.SSS1.p7.11.m11.1.1.1" xref="S5.SS1.SSS1.p7.11.m11.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p7.11.m11.1b"><apply id="S5.SS1.SSS1.p7.11.m11.1.1.cmml" xref="S5.SS1.SSS1.p7.11.m11.1.1"><csymbol cd="latexml" id="S5.SS1.SSS1.p7.11.m11.1.1.1.cmml" xref="S5.SS1.SSS1.p7.11.m11.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.SSS1.p7.11.m11.1.1.2.cmml" xref="S5.SS1.SSS1.p7.11.m11.1.1.2">29.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p7.11.m11.1c">29.3\%</annotation></semantics></math> to <math id="S5.SS1.SSS1.p7.12.m12.1" class="ltx_Math" alttext="44.4\%" display="inline"><semantics id="S5.SS1.SSS1.p7.12.m12.1a"><mrow id="S5.SS1.SSS1.p7.12.m12.1.1" xref="S5.SS1.SSS1.p7.12.m12.1.1.cmml"><mn id="S5.SS1.SSS1.p7.12.m12.1.1.2" xref="S5.SS1.SSS1.p7.12.m12.1.1.2.cmml">44.4</mn><mo id="S5.SS1.SSS1.p7.12.m12.1.1.1" xref="S5.SS1.SSS1.p7.12.m12.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p7.12.m12.1b"><apply id="S5.SS1.SSS1.p7.12.m12.1.1.cmml" xref="S5.SS1.SSS1.p7.12.m12.1.1"><csymbol cd="latexml" id="S5.SS1.SSS1.p7.12.m12.1.1.1.cmml" xref="S5.SS1.SSS1.p7.12.m12.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.SSS1.p7.12.m12.1.1.2.cmml" xref="S5.SS1.SSS1.p7.12.m12.1.1.2">44.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p7.12.m12.1c">44.4\%</annotation></semantics></math>, respectively.</p>
</div>
</section>
<section id="S5.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2 </span>Diagnosing Direction Issues</h4>

<div id="S5.SS1.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS2.p1.1" class="ltx_p"><span id="S5.SS1.SSS2.p1.1.1" class="ltx_text ltx_font_bold">Performance overview (T2)</span>.
<math id="S5.SS1.SSS2.p1.1.m1.1" class="ltx_Math" alttext="E_{2}" display="inline"><semantics id="S5.SS1.SSS2.p1.1.m1.1a"><msub id="S5.SS1.SSS2.p1.1.m1.1.1" xref="S5.SS1.SSS2.p1.1.m1.1.1.cmml"><mi id="S5.SS1.SSS2.p1.1.m1.1.1.2" xref="S5.SS1.SSS2.p1.1.m1.1.1.2.cmml">E</mi><mn id="S5.SS1.SSS2.p1.1.m1.1.1.3" xref="S5.SS1.SSS2.p1.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS2.p1.1.m1.1b"><apply id="S5.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S5.SS1.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS2.p1.1.m1.1.1.1.cmml" xref="S5.SS1.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.SSS2.p1.1.m1.1.1.2.cmml" xref="S5.SS1.SSS2.p1.1.m1.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.SSS2.p1.1.m1.1.1.3.cmml" xref="S5.SS1.SSS2.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS2.p1.1.m1.1c">E_{2}</annotation></semantics></math> continued the evaluation of localization performance in terms of shifted directions.
He examined the matrix in the direction mode.
In the matrix (Fig.¬†<a href="#S5.F7" title="Figure 7 ‚Ä£ 5.1.2 Diagnosing Direction Issues ‚Ä£ 5.1 Object Detection on COCO dataset ‚Ä£ 5 Case Studies ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>(a)),
he found four cells with apparently longer arrows in specific directions (Figs.¬†<a href="#S5.F7" title="Figure 7 ‚Ä£ 5.1.2 Diagnosing Direction Issues ‚Ä£ 5.1 Object Detection on COCO dataset ‚Ä£ 5 Case Studies ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>A,¬†<a href="#S5.F7" title="Figure 7 ‚Ä£ 5.1.2 Diagnosing Direction Issues ‚Ä£ 5.1 Object Detection on COCO dataset ‚Ä£ 5 Case Studies ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>B,¬†<a href="#S5.F7" title="Figure 7 ‚Ä£ 5.1.2 Diagnosing Direction Issues ‚Ä£ 5.1 Object Detection on COCO dataset ‚Ä£ 5 Case Studies ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>C, and¬†<a href="#S5.F7" title="Figure 7 ‚Ä£ 5.1.2 Diagnosing Direction Issues ‚Ä£ 5.1 Object Detection on COCO dataset ‚Ä£ 5 Case Studies ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>D).
He decided to examine them one by one.</p>
</div>
<div id="S5.SS1.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS2.p2.4" class="ltx_p"><span id="S5.SS1.SSS2.p2.4.1" class="ltx_text ltx_font_bold">Analyzing the imprecise direction issue in class ‚Äúperson‚Äù (T2, T4)</span>.
<math id="S5.SS1.SSS2.p2.1.m1.1" class="ltx_Math" alttext="E_{2}" display="inline"><semantics id="S5.SS1.SSS2.p2.1.m1.1a"><msub id="S5.SS1.SSS2.p2.1.m1.1.1" xref="S5.SS1.SSS2.p2.1.m1.1.1.cmml"><mi id="S5.SS1.SSS2.p2.1.m1.1.1.2" xref="S5.SS1.SSS2.p2.1.m1.1.1.2.cmml">E</mi><mn id="S5.SS1.SSS2.p2.1.m1.1.1.3" xref="S5.SS1.SSS2.p2.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS2.p2.1.m1.1b"><apply id="S5.SS1.SSS2.p2.1.m1.1.1.cmml" xref="S5.SS1.SSS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS2.p2.1.m1.1.1.1.cmml" xref="S5.SS1.SSS2.p2.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.SSS2.p2.1.m1.1.1.2.cmml" xref="S5.SS1.SSS2.p2.1.m1.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.SSS2.p2.1.m1.1.1.3.cmml" xref="S5.SS1.SSS2.p2.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS2.p2.1.m1.1c">E_{2}</annotation></semantics></math> began his analysis on cell A (Fig.¬†<a href="#S5.F7" title="Figure 7 ‚Ä£ 5.1.2 Diagnosing Direction Issues ‚Ä£ 5.1 Object Detection on COCO dataset ‚Ä£ 5 Case Studies ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>A),
where many predicted objects were shifted downward.
To determine the reason for the direction errors,
he expanded the cell to a sub-matrix (Fig.¬†<a href="#S5.F7" title="Figure 7 ‚Ä£ 5.1.2 Diagnosing Direction Issues ‚Ä£ 5.1 Object Detection on COCO dataset ‚Ä£ 5 Case Studies ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>(b)).
In the sub-matrix,
he noticed cell E (Fig.¬†<a href="#S5.F7" title="Figure 7 ‚Ä£ 5.1.2 Diagnosing Direction Issues ‚Ä£ 5.1 Object Detection on COCO dataset ‚Ä£ 5 Case Studies ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>E), where ‚Äúperson‚Äù was confused with ‚Äúskis,‚Äù contributed the majority of the direction errors.
A large proportion of predictions in this cell were shifted downward in position.
He clicked the downward arrow to further check the associated objects.
In the grid visualization,
he noticed that the predicted ‚Äúskis‚Äù were precisely localized,
but the corresponding annotations were absent (Fig.¬†<a href="#S5.F7" title="Figure 7 ‚Ä£ 5.1.2 Diagnosing Direction Issues ‚Ä£ 5.1 Object Detection on COCO dataset ‚Ä£ 5 Case Studies ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>(c)).
As a result, the predictions were matched with the ground truth ‚Äúperson.‚Äù
<math id="S5.SS1.SSS2.p2.2.m2.1" class="ltx_Math" alttext="E_{2}" display="inline"><semantics id="S5.SS1.SSS2.p2.2.m2.1a"><msub id="S5.SS1.SSS2.p2.2.m2.1.1" xref="S5.SS1.SSS2.p2.2.m2.1.1.cmml"><mi id="S5.SS1.SSS2.p2.2.m2.1.1.2" xref="S5.SS1.SSS2.p2.2.m2.1.1.2.cmml">E</mi><mn id="S5.SS1.SSS2.p2.2.m2.1.1.3" xref="S5.SS1.SSS2.p2.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS2.p2.2.m2.1b"><apply id="S5.SS1.SSS2.p2.2.m2.1.1.cmml" xref="S5.SS1.SSS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS2.p2.2.m2.1.1.1.cmml" xref="S5.SS1.SSS2.p2.2.m2.1.1">subscript</csymbol><ci id="S5.SS1.SSS2.p2.2.m2.1.1.2.cmml" xref="S5.SS1.SSS2.p2.2.m2.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.SSS2.p2.2.m2.1.1.3.cmml" xref="S5.SS1.SSS2.p2.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS2.p2.2.m2.1c">E_{2}</annotation></semantics></math> decided to add the missing annotations of ‚Äúskis‚Äù in test samples.
After that, the AP of ‚Äúskis‚Äù was increased from <math id="S5.SS1.SSS2.p2.3.m3.1" class="ltx_Math" alttext="35.4\%" display="inline"><semantics id="S5.SS1.SSS2.p2.3.m3.1a"><mrow id="S5.SS1.SSS2.p2.3.m3.1.1" xref="S5.SS1.SSS2.p2.3.m3.1.1.cmml"><mn id="S5.SS1.SSS2.p2.3.m3.1.1.2" xref="S5.SS1.SSS2.p2.3.m3.1.1.2.cmml">35.4</mn><mo id="S5.SS1.SSS2.p2.3.m3.1.1.1" xref="S5.SS1.SSS2.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS2.p2.3.m3.1b"><apply id="S5.SS1.SSS2.p2.3.m3.1.1.cmml" xref="S5.SS1.SSS2.p2.3.m3.1.1"><csymbol cd="latexml" id="S5.SS1.SSS2.p2.3.m3.1.1.1.cmml" xref="S5.SS1.SSS2.p2.3.m3.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.SSS2.p2.3.m3.1.1.2.cmml" xref="S5.SS1.SSS2.p2.3.m3.1.1.2">35.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS2.p2.3.m3.1c">35.4\%</annotation></semantics></math> to <math id="S5.SS1.SSS2.p2.4.m4.1" class="ltx_Math" alttext="36.8\%" display="inline"><semantics id="S5.SS1.SSS2.p2.4.m4.1a"><mrow id="S5.SS1.SSS2.p2.4.m4.1.1" xref="S5.SS1.SSS2.p2.4.m4.1.1.cmml"><mn id="S5.SS1.SSS2.p2.4.m4.1.1.2" xref="S5.SS1.SSS2.p2.4.m4.1.1.2.cmml">36.8</mn><mo id="S5.SS1.SSS2.p2.4.m4.1.1.1" xref="S5.SS1.SSS2.p2.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS2.p2.4.m4.1b"><apply id="S5.SS1.SSS2.p2.4.m4.1.1.cmml" xref="S5.SS1.SSS2.p2.4.m4.1.1"><csymbol cd="latexml" id="S5.SS1.SSS2.p2.4.m4.1.1.1.cmml" xref="S5.SS1.SSS2.p2.4.m4.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.SSS2.p2.4.m4.1.1.2.cmml" xref="S5.SS1.SSS2.p2.4.m4.1.1.2">36.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS2.p2.4.m4.1c">36.8\%</annotation></semantics></math>.</p>
</div>
<div id="S5.SS1.SSS2.p3" class="ltx_para">
<p id="S5.SS1.SSS2.p3.5" class="ltx_p">Similarly, <math id="S5.SS1.SSS2.p3.1.m1.1" class="ltx_Math" alttext="E_{2}" display="inline"><semantics id="S5.SS1.SSS2.p3.1.m1.1a"><msub id="S5.SS1.SSS2.p3.1.m1.1.1" xref="S5.SS1.SSS2.p3.1.m1.1.1.cmml"><mi id="S5.SS1.SSS2.p3.1.m1.1.1.2" xref="S5.SS1.SSS2.p3.1.m1.1.1.2.cmml">E</mi><mn id="S5.SS1.SSS2.p3.1.m1.1.1.3" xref="S5.SS1.SSS2.p3.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS2.p3.1.m1.1b"><apply id="S5.SS1.SSS2.p3.1.m1.1.1.cmml" xref="S5.SS1.SSS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS2.p3.1.m1.1.1.1.cmml" xref="S5.SS1.SSS2.p3.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.SSS2.p3.1.m1.1.1.2.cmml" xref="S5.SS1.SSS2.p3.1.m1.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.SSS2.p3.1.m1.1.1.3.cmml" xref="S5.SS1.SSS2.p3.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS2.p3.1.m1.1c">E_{2}</annotation></semantics></math> examined the other cells with longer arrows in specific directions (Figs.¬†<a href="#S5.F7" title="Figure 7 ‚Ä£ 5.1.2 Diagnosing Direction Issues ‚Ä£ 5.1 Object Detection on COCO dataset ‚Ä£ 5 Case Studies ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>B,¬†<a href="#S5.F7" title="Figure 7 ‚Ä£ 5.1.2 Diagnosing Direction Issues ‚Ä£ 5.1 Object Detection on COCO dataset ‚Ä£ 5 Case Studies ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>C and <a href="#S5.F7" title="Figure 7 ‚Ä£ 5.1.2 Diagnosing Direction Issues ‚Ä£ 5.1 Object Detection on COCO dataset ‚Ä£ 5 Case Studies ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>D).
Analyzing the predictions,
he discovered similar issues as ‚Äúskis‚Äù
and decided to add the missing annotations of ‚Äúpotted plant‚Äù and ‚Äúvase‚Äù in test samples.
After that, the APs of ‚Äúpotted plant‚Äù and ‚Äúvase‚Äù were increased from <math id="S5.SS1.SSS2.p3.2.m2.1" class="ltx_Math" alttext="33.6\%" display="inline"><semantics id="S5.SS1.SSS2.p3.2.m2.1a"><mrow id="S5.SS1.SSS2.p3.2.m2.1.1" xref="S5.SS1.SSS2.p3.2.m2.1.1.cmml"><mn id="S5.SS1.SSS2.p3.2.m2.1.1.2" xref="S5.SS1.SSS2.p3.2.m2.1.1.2.cmml">33.6</mn><mo id="S5.SS1.SSS2.p3.2.m2.1.1.1" xref="S5.SS1.SSS2.p3.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS2.p3.2.m2.1b"><apply id="S5.SS1.SSS2.p3.2.m2.1.1.cmml" xref="S5.SS1.SSS2.p3.2.m2.1.1"><csymbol cd="latexml" id="S5.SS1.SSS2.p3.2.m2.1.1.1.cmml" xref="S5.SS1.SSS2.p3.2.m2.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.SSS2.p3.2.m2.1.1.2.cmml" xref="S5.SS1.SSS2.p3.2.m2.1.1.2">33.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS2.p3.2.m2.1c">33.6\%</annotation></semantics></math> to <math id="S5.SS1.SSS2.p3.3.m3.1" class="ltx_Math" alttext="37.0\%" display="inline"><semantics id="S5.SS1.SSS2.p3.3.m3.1a"><mrow id="S5.SS1.SSS2.p3.3.m3.1.1" xref="S5.SS1.SSS2.p3.3.m3.1.1.cmml"><mn id="S5.SS1.SSS2.p3.3.m3.1.1.2" xref="S5.SS1.SSS2.p3.3.m3.1.1.2.cmml">37.0</mn><mo id="S5.SS1.SSS2.p3.3.m3.1.1.1" xref="S5.SS1.SSS2.p3.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS2.p3.3.m3.1b"><apply id="S5.SS1.SSS2.p3.3.m3.1.1.cmml" xref="S5.SS1.SSS2.p3.3.m3.1.1"><csymbol cd="latexml" id="S5.SS1.SSS2.p3.3.m3.1.1.1.cmml" xref="S5.SS1.SSS2.p3.3.m3.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.SSS2.p3.3.m3.1.1.2.cmml" xref="S5.SS1.SSS2.p3.3.m3.1.1.2">37.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS2.p3.3.m3.1c">37.0\%</annotation></semantics></math>, and <math id="S5.SS1.SSS2.p3.4.m4.1" class="ltx_Math" alttext="44.1\%" display="inline"><semantics id="S5.SS1.SSS2.p3.4.m4.1a"><mrow id="S5.SS1.SSS2.p3.4.m4.1.1" xref="S5.SS1.SSS2.p3.4.m4.1.1.cmml"><mn id="S5.SS1.SSS2.p3.4.m4.1.1.2" xref="S5.SS1.SSS2.p3.4.m4.1.1.2.cmml">44.1</mn><mo id="S5.SS1.SSS2.p3.4.m4.1.1.1" xref="S5.SS1.SSS2.p3.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS2.p3.4.m4.1b"><apply id="S5.SS1.SSS2.p3.4.m4.1.1.cmml" xref="S5.SS1.SSS2.p3.4.m4.1.1"><csymbol cd="latexml" id="S5.SS1.SSS2.p3.4.m4.1.1.1.cmml" xref="S5.SS1.SSS2.p3.4.m4.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.SSS2.p3.4.m4.1.1.2.cmml" xref="S5.SS1.SSS2.p3.4.m4.1.1.2">44.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS2.p3.4.m4.1c">44.1\%</annotation></semantics></math> to <math id="S5.SS1.SSS2.p3.5.m5.1" class="ltx_Math" alttext="46.2\%" display="inline"><semantics id="S5.SS1.SSS2.p3.5.m5.1a"><mrow id="S5.SS1.SSS2.p3.5.m5.1.1" xref="S5.SS1.SSS2.p3.5.m5.1.1.cmml"><mn id="S5.SS1.SSS2.p3.5.m5.1.1.2" xref="S5.SS1.SSS2.p3.5.m5.1.1.2.cmml">46.2</mn><mo id="S5.SS1.SSS2.p3.5.m5.1.1.1" xref="S5.SS1.SSS2.p3.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS2.p3.5.m5.1b"><apply id="S5.SS1.SSS2.p3.5.m5.1.1.cmml" xref="S5.SS1.SSS2.p3.5.m5.1.1"><csymbol cd="latexml" id="S5.SS1.SSS2.p3.5.m5.1.1.1.cmml" xref="S5.SS1.SSS2.p3.5.m5.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.SSS2.p3.5.m5.1.1.2.cmml" xref="S5.SS1.SSS2.p3.5.m5.1.1.2">46.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS2.p3.5.m5.1c">46.2\%</annotation></semantics></math>, respectively.</p>
</div>
<div id="S5.SS1.SSS2.p4" class="ltx_para">
<p id="S5.SS1.SSS2.p4.6" class="ltx_p">In summary, <math id="S5.SS1.SSS2.p4.1.m1.1" class="ltx_Math" alttext="E_{1}" display="inline"><semantics id="S5.SS1.SSS2.p4.1.m1.1a"><msub id="S5.SS1.SSS2.p4.1.m1.1.1" xref="S5.SS1.SSS2.p4.1.m1.1.1.cmml"><mi id="S5.SS1.SSS2.p4.1.m1.1.1.2" xref="S5.SS1.SSS2.p4.1.m1.1.1.2.cmml">E</mi><mn id="S5.SS1.SSS2.p4.1.m1.1.1.3" xref="S5.SS1.SSS2.p4.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS2.p4.1.m1.1b"><apply id="S5.SS1.SSS2.p4.1.m1.1.1.cmml" xref="S5.SS1.SSS2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS2.p4.1.m1.1.1.1.cmml" xref="S5.SS1.SSS2.p4.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.SSS2.p4.1.m1.1.1.2.cmml" xref="S5.SS1.SSS2.p4.1.m1.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.SSS2.p4.1.m1.1.1.3.cmml" xref="S5.SS1.SSS2.p4.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS2.p4.1.m1.1c">E_{1}</annotation></semantics></math> and <math id="S5.SS1.SSS2.p4.2.m2.1" class="ltx_Math" alttext="E_{2}" display="inline"><semantics id="S5.SS1.SSS2.p4.2.m2.1a"><msub id="S5.SS1.SSS2.p4.2.m2.1.1" xref="S5.SS1.SSS2.p4.2.m2.1.1.cmml"><mi id="S5.SS1.SSS2.p4.2.m2.1.1.2" xref="S5.SS1.SSS2.p4.2.m2.1.1.2.cmml">E</mi><mn id="S5.SS1.SSS2.p4.2.m2.1.1.3" xref="S5.SS1.SSS2.p4.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS2.p4.2.m2.1b"><apply id="S5.SS1.SSS2.p4.2.m2.1.1.cmml" xref="S5.SS1.SSS2.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS2.p4.2.m2.1.1.1.cmml" xref="S5.SS1.SSS2.p4.2.m2.1.1">subscript</csymbol><ci id="S5.SS1.SSS2.p4.2.m2.1.1.2.cmml" xref="S5.SS1.SSS2.p4.2.m2.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.SSS2.p4.2.m2.1.1.3.cmml" xref="S5.SS1.SSS2.p4.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS2.p4.2.m2.1c">E_{2}</annotation></semantics></math> applied Gaussian noise data augmentation to <math id="S5.SS1.SSS2.p4.3.m3.2" class="ltx_Math" alttext="4,139" display="inline"><semantics id="S5.SS1.SSS2.p4.3.m3.2a"><mrow id="S5.SS1.SSS2.p4.3.m3.2.3.2" xref="S5.SS1.SSS2.p4.3.m3.2.3.1.cmml"><mn id="S5.SS1.SSS2.p4.3.m3.1.1" xref="S5.SS1.SSS2.p4.3.m3.1.1.cmml">4</mn><mo id="S5.SS1.SSS2.p4.3.m3.2.3.2.1" xref="S5.SS1.SSS2.p4.3.m3.2.3.1.cmml">,</mo><mn id="S5.SS1.SSS2.p4.3.m3.2.2" xref="S5.SS1.SSS2.p4.3.m3.2.2.cmml">139</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS2.p4.3.m3.2b"><list id="S5.SS1.SSS2.p4.3.m3.2.3.1.cmml" xref="S5.SS1.SSS2.p4.3.m3.2.3.2"><cn type="integer" id="S5.SS1.SSS2.p4.3.m3.1.1.cmml" xref="S5.SS1.SSS2.p4.3.m3.1.1">4</cn><cn type="integer" id="S5.SS1.SSS2.p4.3.m3.2.2.cmml" xref="S5.SS1.SSS2.p4.3.m3.2.2">139</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS2.p4.3.m3.2c">4,139</annotation></semantics></math> training samples with traffic lights and re-annotated the objects of nine classes.
The overall mAP on the re-annotated test samples was improved from <math id="S5.SS1.SSS2.p4.4.m4.1" class="ltx_Math" alttext="50.8\%" display="inline"><semantics id="S5.SS1.SSS2.p4.4.m4.1a"><mrow id="S5.SS1.SSS2.p4.4.m4.1.1" xref="S5.SS1.SSS2.p4.4.m4.1.1.cmml"><mn id="S5.SS1.SSS2.p4.4.m4.1.1.2" xref="S5.SS1.SSS2.p4.4.m4.1.1.2.cmml">50.8</mn><mo id="S5.SS1.SSS2.p4.4.m4.1.1.1" xref="S5.SS1.SSS2.p4.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS2.p4.4.m4.1b"><apply id="S5.SS1.SSS2.p4.4.m4.1.1.cmml" xref="S5.SS1.SSS2.p4.4.m4.1.1"><csymbol cd="latexml" id="S5.SS1.SSS2.p4.4.m4.1.1.1.cmml" xref="S5.SS1.SSS2.p4.4.m4.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.SSS2.p4.4.m4.1.1.2.cmml" xref="S5.SS1.SSS2.p4.4.m4.1.1.2">50.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS2.p4.4.m4.1c">50.8\%</annotation></semantics></math> to <math id="S5.SS1.SSS2.p4.5.m5.1" class="ltx_Math" alttext="52.5\%" display="inline"><semantics id="S5.SS1.SSS2.p4.5.m5.1a"><mrow id="S5.SS1.SSS2.p4.5.m5.1.1" xref="S5.SS1.SSS2.p4.5.m5.1.1.cmml"><mn id="S5.SS1.SSS2.p4.5.m5.1.1.2" xref="S5.SS1.SSS2.p4.5.m5.1.1.2.cmml">52.5</mn><mo id="S5.SS1.SSS2.p4.5.m5.1.1.1" xref="S5.SS1.SSS2.p4.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS2.p4.5.m5.1b"><apply id="S5.SS1.SSS2.p4.5.m5.1.1.cmml" xref="S5.SS1.SSS2.p4.5.m5.1.1"><csymbol cd="latexml" id="S5.SS1.SSS2.p4.5.m5.1.1.1.cmml" xref="S5.SS1.SSS2.p4.5.m5.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.SSS2.p4.5.m5.1.1.2.cmml" xref="S5.SS1.SSS2.p4.5.m5.1.1.2">52.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS2.p4.5.m5.1c">52.5\%</annotation></semantics></math>.
<math id="S5.SS1.SSS2.p4.6.m6.1" class="ltx_Math" alttext="E_{2}" display="inline"><semantics id="S5.SS1.SSS2.p4.6.m6.1a"><msub id="S5.SS1.SSS2.p4.6.m6.1.1" xref="S5.SS1.SSS2.p4.6.m6.1.1.cmml"><mi id="S5.SS1.SSS2.p4.6.m6.1.1.2" xref="S5.SS1.SSS2.p4.6.m6.1.1.2.cmml">E</mi><mn id="S5.SS1.SSS2.p4.6.m6.1.1.3" xref="S5.SS1.SSS2.p4.6.m6.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS2.p4.6.m6.1b"><apply id="S5.SS1.SSS2.p4.6.m6.1.1.cmml" xref="S5.SS1.SSS2.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS2.p4.6.m6.1.1.1.cmml" xref="S5.SS1.SSS2.p4.6.m6.1.1">subscript</csymbol><ci id="S5.SS1.SSS2.p4.6.m6.1.1.2.cmml" xref="S5.SS1.SSS2.p4.6.m6.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.SSS2.p4.6.m6.1.1.3.cmml" xref="S5.SS1.SSS2.p4.6.m6.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS2.p4.6.m6.1c">E_{2}</annotation></semantics></math> was impressed by the improvement and thus conducted a post-analysis on the re-annotated test samples.</p>
</div>
<figure id="S5.F7" class="ltx_figure"><img src="/html/2308.05168/assets/x17.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="408" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S5.F7.3.2" class="ltx_text" style="font-size:90%;">(a) The matrix-based visualization in the direction mode; (b) the sub-matrix of ‚Äúperson‚Äù and ‚Äúsports;‚Äù (c) missing annotations in ‚Äúskis.‚Äù</span></figcaption>
</figure>
</section>
<section id="S5.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.3 </span>Post Analysis</h4>

<div id="S5.SS1.SSS3.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS3.p1.5" class="ltx_p"><span id="S5.SS1.SSS3.p1.5.1" class="ltx_text ltx_font_bold">More accurately evaluating model performance</span>.
The previous study has shown that larger models usually achieve higher mAPs than small models on the original COCO test samples¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.
<math id="S5.SS1.SSS3.p1.1.m1.1" class="ltx_Math" alttext="E_{2}" display="inline"><semantics id="S5.SS1.SSS3.p1.1.m1.1a"><msub id="S5.SS1.SSS3.p1.1.m1.1.1" xref="S5.SS1.SSS3.p1.1.m1.1.1.cmml"><mi id="S5.SS1.SSS3.p1.1.m1.1.1.2" xref="S5.SS1.SSS3.p1.1.m1.1.1.2.cmml">E</mi><mn id="S5.SS1.SSS3.p1.1.m1.1.1.3" xref="S5.SS1.SSS3.p1.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS3.p1.1.m1.1b"><apply id="S5.SS1.SSS3.p1.1.m1.1.1.cmml" xref="S5.SS1.SSS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS3.p1.1.m1.1.1.1.cmml" xref="S5.SS1.SSS3.p1.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.SSS3.p1.1.m1.1.1.2.cmml" xref="S5.SS1.SSS3.p1.1.m1.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.SSS3.p1.1.m1.1.1.3.cmml" xref="S5.SS1.SSS3.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS3.p1.1.m1.1c">E_{2}</annotation></semantics></math> sought to determine whether they truly perform better or merely overfit the annotation errors.
Therefore, <math id="S5.SS1.SSS3.p1.2.m2.1" class="ltx_Math" alttext="E_{2}" display="inline"><semantics id="S5.SS1.SSS3.p1.2.m2.1a"><msub id="S5.SS1.SSS3.p1.2.m2.1.1" xref="S5.SS1.SSS3.p1.2.m2.1.1.cmml"><mi id="S5.SS1.SSS3.p1.2.m2.1.1.2" xref="S5.SS1.SSS3.p1.2.m2.1.1.2.cmml">E</mi><mn id="S5.SS1.SSS3.p1.2.m2.1.1.3" xref="S5.SS1.SSS3.p1.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS3.p1.2.m2.1b"><apply id="S5.SS1.SSS3.p1.2.m2.1.1.cmml" xref="S5.SS1.SSS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS3.p1.2.m2.1.1.1.cmml" xref="S5.SS1.SSS3.p1.2.m2.1.1">subscript</csymbol><ci id="S5.SS1.SSS3.p1.2.m2.1.1.2.cmml" xref="S5.SS1.SSS3.p1.2.m2.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.SSS3.p1.2.m2.1.1.3.cmml" xref="S5.SS1.SSS3.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS3.p1.2.m2.1c">E_{2}</annotation></semantics></math> evaluated performance of InternImage¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, a large pre-trained model that achieves the highest mAP (<math id="S5.SS1.SSS3.p1.3.m3.1" class="ltx_Math" alttext="65.0\%" display="inline"><semantics id="S5.SS1.SSS3.p1.3.m3.1a"><mrow id="S5.SS1.SSS3.p1.3.m3.1.1" xref="S5.SS1.SSS3.p1.3.m3.1.1.cmml"><mn id="S5.SS1.SSS3.p1.3.m3.1.1.2" xref="S5.SS1.SSS3.p1.3.m3.1.1.2.cmml">65.0</mn><mo id="S5.SS1.SSS3.p1.3.m3.1.1.1" xref="S5.SS1.SSS3.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS3.p1.3.m3.1b"><apply id="S5.SS1.SSS3.p1.3.m3.1.1.cmml" xref="S5.SS1.SSS3.p1.3.m3.1.1"><csymbol cd="latexml" id="S5.SS1.SSS3.p1.3.m3.1.1.1.cmml" xref="S5.SS1.SSS3.p1.3.m3.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.SSS3.p1.3.m3.1.1.2.cmml" xref="S5.SS1.SSS3.p1.3.m3.1.1.2">65.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS3.p1.3.m3.1c">65.0\%</annotation></semantics></math>) on the original test samples.
On the re-annotated test samples, the gain in mAP was <math id="S5.SS1.SSS3.p1.4.m4.1" class="ltx_Math" alttext="2.3\%" display="inline"><semantics id="S5.SS1.SSS3.p1.4.m4.1a"><mrow id="S5.SS1.SSS3.p1.4.m4.1.1" xref="S5.SS1.SSS3.p1.4.m4.1.1.cmml"><mn id="S5.SS1.SSS3.p1.4.m4.1.1.2" xref="S5.SS1.SSS3.p1.4.m4.1.1.2.cmml">2.3</mn><mo id="S5.SS1.SSS3.p1.4.m4.1.1.1" xref="S5.SS1.SSS3.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS3.p1.4.m4.1b"><apply id="S5.SS1.SSS3.p1.4.m4.1.1.cmml" xref="S5.SS1.SSS3.p1.4.m4.1.1"><csymbol cd="latexml" id="S5.SS1.SSS3.p1.4.m4.1.1.1.cmml" xref="S5.SS1.SSS3.p1.4.m4.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.SSS3.p1.4.m4.1.1.2.cmml" xref="S5.SS1.SSS3.p1.4.m4.1.1.2">2.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS3.p1.4.m4.1c">2.3\%</annotation></semantics></math>, which exceeded that of DINO (<math id="S5.SS1.SSS3.p1.5.m5.1" class="ltx_Math" alttext="1.8\%" display="inline"><semantics id="S5.SS1.SSS3.p1.5.m5.1a"><mrow id="S5.SS1.SSS3.p1.5.m5.1.1" xref="S5.SS1.SSS3.p1.5.m5.1.1.cmml"><mn id="S5.SS1.SSS3.p1.5.m5.1.1.2" xref="S5.SS1.SSS3.p1.5.m5.1.1.2.cmml">1.8</mn><mo id="S5.SS1.SSS3.p1.5.m5.1.1.1" xref="S5.SS1.SSS3.p1.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS3.p1.5.m5.1b"><apply id="S5.SS1.SSS3.p1.5.m5.1.1.cmml" xref="S5.SS1.SSS3.p1.5.m5.1.1"><csymbol cd="latexml" id="S5.SS1.SSS3.p1.5.m5.1.1.1.cmml" xref="S5.SS1.SSS3.p1.5.m5.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.SSS3.p1.5.m5.1.1.2.cmml" xref="S5.SS1.SSS3.p1.5.m5.1.1.2">1.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS3.p1.5.m5.1c">1.8\%</annotation></semantics></math>).
This showed that the higher performance on large models was not due to overfitting annotation errors even though the numbers of their parameters were very large.
This is also verified by the recent research¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>.</p>
</div>
<div id="S5.SS1.SSS3.p2" class="ltx_para">
<p id="S5.SS1.SSS3.p2.5" class="ltx_p">Meanwhile, <math id="S5.SS1.SSS3.p2.1.m1.1" class="ltx_Math" alttext="E_{2}" display="inline"><semantics id="S5.SS1.SSS3.p2.1.m1.1a"><msub id="S5.SS1.SSS3.p2.1.m1.1.1" xref="S5.SS1.SSS3.p2.1.m1.1.1.cmml"><mi id="S5.SS1.SSS3.p2.1.m1.1.1.2" xref="S5.SS1.SSS3.p2.1.m1.1.1.2.cmml">E</mi><mn id="S5.SS1.SSS3.p2.1.m1.1.1.3" xref="S5.SS1.SSS3.p2.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS3.p2.1.m1.1b"><apply id="S5.SS1.SSS3.p2.1.m1.1.1.cmml" xref="S5.SS1.SSS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS3.p2.1.m1.1.1.1.cmml" xref="S5.SS1.SSS3.p2.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.SSS3.p2.1.m1.1.1.2.cmml" xref="S5.SS1.SSS3.p2.1.m1.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.SSS3.p2.1.m1.1.1.3.cmml" xref="S5.SS1.SSS3.p2.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS3.p2.1.m1.1c">E_{2}</annotation></semantics></math> noted that the current annotations might not be accurate enough to evaluate object detection models.
‚ÄúConsidering the <math id="S5.SS1.SSS3.p2.2.m2.1" class="ltx_Math" alttext="2.3\%" display="inline"><semantics id="S5.SS1.SSS3.p2.2.m2.1a"><mrow id="S5.SS1.SSS3.p2.2.m2.1.1" xref="S5.SS1.SSS3.p2.2.m2.1.1.cmml"><mn id="S5.SS1.SSS3.p2.2.m2.1.1.2" xref="S5.SS1.SSS3.p2.2.m2.1.1.2.cmml">2.3</mn><mo id="S5.SS1.SSS3.p2.2.m2.1.1.1" xref="S5.SS1.SSS3.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS3.p2.2.m2.1b"><apply id="S5.SS1.SSS3.p2.2.m2.1.1.cmml" xref="S5.SS1.SSS3.p2.2.m2.1.1"><csymbol cd="latexml" id="S5.SS1.SSS3.p2.2.m2.1.1.1.cmml" xref="S5.SS1.SSS3.p2.2.m2.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.SSS3.p2.2.m2.1.1.2.cmml" xref="S5.SS1.SSS3.p2.2.m2.1.1.2">2.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS3.p2.2.m2.1c">2.3\%</annotation></semantics></math> mAP improvement by revising the annotations of only nine classes, I expect the gain to reach around <math id="S5.SS1.SSS3.p2.3.m3.1" class="ltx_Math" alttext="15.0\%" display="inline"><semantics id="S5.SS1.SSS3.p2.3.m3.1a"><mrow id="S5.SS1.SSS3.p2.3.m3.1.1" xref="S5.SS1.SSS3.p2.3.m3.1.1.cmml"><mn id="S5.SS1.SSS3.p2.3.m3.1.1.2" xref="S5.SS1.SSS3.p2.3.m3.1.1.2.cmml">15.0</mn><mo id="S5.SS1.SSS3.p2.3.m3.1.1.1" xref="S5.SS1.SSS3.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS3.p2.3.m3.1b"><apply id="S5.SS1.SSS3.p2.3.m3.1.1.cmml" xref="S5.SS1.SSS3.p2.3.m3.1.1"><csymbol cd="latexml" id="S5.SS1.SSS3.p2.3.m3.1.1.1.cmml" xref="S5.SS1.SSS3.p2.3.m3.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.SSS3.p2.3.m3.1.1.2.cmml" xref="S5.SS1.SSS3.p2.3.m3.1.1.2">15.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS3.p2.3.m3.1c">15.0\%</annotation></semantics></math> if the other 71 classes were re-annotated.
It makes me largely underestimate performance of object detection models.‚Äù <math id="S5.SS1.SSS3.p2.4.m4.1" class="ltx_Math" alttext="E_{2}" display="inline"><semantics id="S5.SS1.SSS3.p2.4.m4.1a"><msub id="S5.SS1.SSS3.p2.4.m4.1.1" xref="S5.SS1.SSS3.p2.4.m4.1.1.cmml"><mi id="S5.SS1.SSS3.p2.4.m4.1.1.2" xref="S5.SS1.SSS3.p2.4.m4.1.1.2.cmml">E</mi><mn id="S5.SS1.SSS3.p2.4.m4.1.1.3" xref="S5.SS1.SSS3.p2.4.m4.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS3.p2.4.m4.1b"><apply id="S5.SS1.SSS3.p2.4.m4.1.1.cmml" xref="S5.SS1.SSS3.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS3.p2.4.m4.1.1.1.cmml" xref="S5.SS1.SSS3.p2.4.m4.1.1">subscript</csymbol><ci id="S5.SS1.SSS3.p2.4.m4.1.1.2.cmml" xref="S5.SS1.SSS3.p2.4.m4.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.SSS3.p2.4.m4.1.1.3.cmml" xref="S5.SS1.SSS3.p2.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS3.p2.4.m4.1c">E_{2}</annotation></semantics></math> said.
To accurately evaluate models, <math id="S5.SS1.SSS3.p2.5.m5.1" class="ltx_Math" alttext="E_{2}" display="inline"><semantics id="S5.SS1.SSS3.p2.5.m5.1a"><msub id="S5.SS1.SSS3.p2.5.m5.1.1" xref="S5.SS1.SSS3.p2.5.m5.1.1.cmml"><mi id="S5.SS1.SSS3.p2.5.m5.1.1.2" xref="S5.SS1.SSS3.p2.5.m5.1.1.2.cmml">E</mi><mn id="S5.SS1.SSS3.p2.5.m5.1.1.3" xref="S5.SS1.SSS3.p2.5.m5.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS3.p2.5.m5.1b"><apply id="S5.SS1.SSS3.p2.5.m5.1.1.cmml" xref="S5.SS1.SSS3.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS3.p2.5.m5.1.1.1.cmml" xref="S5.SS1.SSS3.p2.5.m5.1.1">subscript</csymbol><ci id="S5.SS1.SSS3.p2.5.m5.1.1.2.cmml" xref="S5.SS1.SSS3.p2.5.m5.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.SSS3.p2.5.m5.1.1.3.cmml" xref="S5.SS1.SSS3.p2.5.m5.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS3.p2.5.m5.1c">E_{2}</annotation></semantics></math> emphasized the importance of re-annotating the remaining 71 classes.
The findings in the case study provide guidance for the re-annotation process.
For example, when numerous objects of the same class are present in close proximity and delineating them is difficult even for humans (<em id="S5.SS1.SSS3.p2.5.1" class="ltx_emph ltx_font_italic">e.g</em>.‚Äâ, the broccoli in Fig.¬†<a href="#S5.F6" title="Figure 6 ‚Ä£ 5.1.1 Diagnosing Size Issues ‚Ä£ 5.1 Object Detection on COCO dataset ‚Ä£ 5 Case Studies ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>(c)),
these objects can be annotated as a whole with the label ‚Äúis crowd‚Äù to avoid inaccurate annotations.</p>
</div>
<div id="S5.SS1.SSS3.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS3.p3.6" class="ltx_p"><span id="S5.SS1.SSS3.p3.6.1" class="ltx_text ltx_font_bold">Distilling large models</span>.
As inaccurate annotations were identified in the test samples,
<math id="S5.SS1.SSS3.p3.1.m1.1" class="ltx_Math" alttext="E_{2}" display="inline"><semantics id="S5.SS1.SSS3.p3.1.m1.1a"><msub id="S5.SS1.SSS3.p3.1.m1.1.1" xref="S5.SS1.SSS3.p3.1.m1.1.1.cmml"><mi id="S5.SS1.SSS3.p3.1.m1.1.1.2" xref="S5.SS1.SSS3.p3.1.m1.1.1.2.cmml">E</mi><mn id="S5.SS1.SSS3.p3.1.m1.1.1.3" xref="S5.SS1.SSS3.p3.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS3.p3.1.m1.1b"><apply id="S5.SS1.SSS3.p3.1.m1.1.1.cmml" xref="S5.SS1.SSS3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS3.p3.1.m1.1.1.1.cmml" xref="S5.SS1.SSS3.p3.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.SSS3.p3.1.m1.1.1.2.cmml" xref="S5.SS1.SSS3.p3.1.m1.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.SSS3.p3.1.m1.1.1.3.cmml" xref="S5.SS1.SSS3.p3.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS3.p3.1.m1.1c">E_{2}</annotation></semantics></math> suspected the annotations of the training samples were also inaccurate.
The inaccurate annotations of the training images will degrade model performance, especially for small machine learning models running on resource-limited devices¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>.
As re-annotating the training samples is prohibitively expensive, <math id="S5.SS1.SSS3.p3.2.m2.1" class="ltx_Math" alttext="E_{2}" display="inline"><semantics id="S5.SS1.SSS3.p3.2.m2.1a"><msub id="S5.SS1.SSS3.p3.2.m2.1.1" xref="S5.SS1.SSS3.p3.2.m2.1.1.cmml"><mi id="S5.SS1.SSS3.p3.2.m2.1.1.2" xref="S5.SS1.SSS3.p3.2.m2.1.1.2.cmml">E</mi><mn id="S5.SS1.SSS3.p3.2.m2.1.1.3" xref="S5.SS1.SSS3.p3.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS3.p3.2.m2.1b"><apply id="S5.SS1.SSS3.p3.2.m2.1.1.cmml" xref="S5.SS1.SSS3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS3.p3.2.m2.1.1.1.cmml" xref="S5.SS1.SSS3.p3.2.m2.1.1">subscript</csymbol><ci id="S5.SS1.SSS3.p3.2.m2.1.1.2.cmml" xref="S5.SS1.SSS3.p3.2.m2.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.SSS3.p3.2.m2.1.1.3.cmml" xref="S5.SS1.SSS3.p3.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS3.p3.2.m2.1c">E_{2}</annotation></semantics></math> suggested using distillation to enhance the performance of small models.
Specifically, a large model is trained on the training samples and then utilized to detect the objects in all the training samples.
Then these detected objects are used to train the small models.
To illustrate the idea, <math id="S5.SS1.SSS3.p3.3.m3.1" class="ltx_Math" alttext="E_{2}" display="inline"><semantics id="S5.SS1.SSS3.p3.3.m3.1a"><msub id="S5.SS1.SSS3.p3.3.m3.1.1" xref="S5.SS1.SSS3.p3.3.m3.1.1.cmml"><mi id="S5.SS1.SSS3.p3.3.m3.1.1.2" xref="S5.SS1.SSS3.p3.3.m3.1.1.2.cmml">E</mi><mn id="S5.SS1.SSS3.p3.3.m3.1.1.3" xref="S5.SS1.SSS3.p3.3.m3.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS3.p3.3.m3.1b"><apply id="S5.SS1.SSS3.p3.3.m3.1.1.cmml" xref="S5.SS1.SSS3.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS3.p3.3.m3.1.1.1.cmml" xref="S5.SS1.SSS3.p3.3.m3.1.1">subscript</csymbol><ci id="S5.SS1.SSS3.p3.3.m3.1.1.2.cmml" xref="S5.SS1.SSS3.p3.3.m3.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.SSS3.p3.3.m3.1.1.3.cmml" xref="S5.SS1.SSS3.p3.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS3.p3.3.m3.1c">E_{2}</annotation></semantics></math> used DINO with Swin backbone¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> as the large model and DINO with ResNet-50 backbone¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> as the small model for distillation.
On the re-annotated test samples, the mAP was improved from <math id="S5.SS1.SSS3.p3.4.m4.1" class="ltx_Math" alttext="52.5\%" display="inline"><semantics id="S5.SS1.SSS3.p3.4.m4.1a"><mrow id="S5.SS1.SSS3.p3.4.m4.1.1" xref="S5.SS1.SSS3.p3.4.m4.1.1.cmml"><mn id="S5.SS1.SSS3.p3.4.m4.1.1.2" xref="S5.SS1.SSS3.p3.4.m4.1.1.2.cmml">52.5</mn><mo id="S5.SS1.SSS3.p3.4.m4.1.1.1" xref="S5.SS1.SSS3.p3.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS3.p3.4.m4.1b"><apply id="S5.SS1.SSS3.p3.4.m4.1.1.cmml" xref="S5.SS1.SSS3.p3.4.m4.1.1"><csymbol cd="latexml" id="S5.SS1.SSS3.p3.4.m4.1.1.1.cmml" xref="S5.SS1.SSS3.p3.4.m4.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.SSS3.p3.4.m4.1.1.2.cmml" xref="S5.SS1.SSS3.p3.4.m4.1.1.2">52.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS3.p3.4.m4.1c">52.5\%</annotation></semantics></math> to <math id="S5.SS1.SSS3.p3.5.m5.1" class="ltx_Math" alttext="53.4\%" display="inline"><semantics id="S5.SS1.SSS3.p3.5.m5.1a"><mrow id="S5.SS1.SSS3.p3.5.m5.1.1" xref="S5.SS1.SSS3.p3.5.m5.1.1.cmml"><mn id="S5.SS1.SSS3.p3.5.m5.1.1.2" xref="S5.SS1.SSS3.p3.5.m5.1.1.2.cmml">53.4</mn><mo id="S5.SS1.SSS3.p3.5.m5.1.1.1" xref="S5.SS1.SSS3.p3.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS3.p3.5.m5.1b"><apply id="S5.SS1.SSS3.p3.5.m5.1.1.cmml" xref="S5.SS1.SSS3.p3.5.m5.1.1"><csymbol cd="latexml" id="S5.SS1.SSS3.p3.5.m5.1.1.1.cmml" xref="S5.SS1.SSS3.p3.5.m5.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.SSS3.p3.5.m5.1.1.2.cmml" xref="S5.SS1.SSS3.p3.5.m5.1.1.2">53.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS3.p3.5.m5.1c">53.4\%</annotation></semantics></math>.
‚ÄúThe improvement is remarkable, particularly because it required no additional annotation efforts.‚Äù <math id="S5.SS1.SSS3.p3.6.m6.1" class="ltx_Math" alttext="E_{2}" display="inline"><semantics id="S5.SS1.SSS3.p3.6.m6.1a"><msub id="S5.SS1.SSS3.p3.6.m6.1.1" xref="S5.SS1.SSS3.p3.6.m6.1.1.cmml"><mi id="S5.SS1.SSS3.p3.6.m6.1.1.2" xref="S5.SS1.SSS3.p3.6.m6.1.1.2.cmml">E</mi><mn id="S5.SS1.SSS3.p3.6.m6.1.1.3" xref="S5.SS1.SSS3.p3.6.m6.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS3.p3.6.m6.1b"><apply id="S5.SS1.SSS3.p3.6.m6.1.1.cmml" xref="S5.SS1.SSS3.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS3.p3.6.m6.1.1.1.cmml" xref="S5.SS1.SSS3.p3.6.m6.1.1">subscript</csymbol><ci id="S5.SS1.SSS3.p3.6.m6.1.1.2.cmml" xref="S5.SS1.SSS3.p3.6.m6.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS1.SSS3.p3.6.m6.1.1.3.cmml" xref="S5.SS1.SSS3.p3.6.m6.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS3.p3.6.m6.1c">E_{2}</annotation></semantics></math> commented.</p>
</div>
</section>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Instance Segmentation on iSAID dataset</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.5" class="ltx_p">In this case study, we invited the expert <math id="S5.SS2.p1.1.m1.1" class="ltx_Math" alttext="E_{3}" display="inline"><semantics id="S5.SS2.p1.1.m1.1a"><msub id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml"><mi id="S5.SS2.p1.1.m1.1.1.2" xref="S5.SS2.p1.1.m1.1.1.2.cmml">E</mi><mn id="S5.SS2.p1.1.m1.1.1.3" xref="S5.SS2.p1.1.m1.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><apply id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p1.1.m1.1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S5.SS2.p1.1.m1.1.1.2.cmml" xref="S5.SS2.p1.1.m1.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS2.p1.1.m1.1.1.3.cmml" xref="S5.SS2.p1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">E_{3}</annotation></semantics></math> to evaluate an instance segmentation model on the iSAID dataset¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>, an aerial image dataset for instance segmentation.
This dataset contains 36,038 training samples with 716,640 objects and 11,752 test samples with 233,625 objects.
The objects belong to 15 classes divided into two super-classes, ‚Äútransport‚Äù and ‚Äúland.‚Äù
<math id="S5.SS2.p1.2.m2.1" class="ltx_Math" alttext="E_{3}" display="inline"><semantics id="S5.SS2.p1.2.m2.1a"><msub id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml"><mi id="S5.SS2.p1.2.m2.1.1.2" xref="S5.SS2.p1.2.m2.1.1.2.cmml">E</mi><mn id="S5.SS2.p1.2.m2.1.1.3" xref="S5.SS2.p1.2.m2.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><apply id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS2.p1.2.m2.1.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S5.SS2.p1.2.m2.1.1.2.cmml" xref="S5.SS2.p1.2.m2.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS2.p1.2.m2.1.1.3.cmml" xref="S5.SS2.p1.2.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">E_{3}</annotation></semantics></math> employed CATNet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, a state-of-the-art instance segmentation model for aerial images.
With the ResNet-50 backbone¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, it achieved an mAP of <math id="S5.SS2.p1.3.m3.1" class="ltx_Math" alttext="51.7\%" display="inline"><semantics id="S5.SS2.p1.3.m3.1a"><mrow id="S5.SS2.p1.3.m3.1.1" xref="S5.SS2.p1.3.m3.1.1.cmml"><mn id="S5.SS2.p1.3.m3.1.1.2" xref="S5.SS2.p1.3.m3.1.1.2.cmml">51.7</mn><mo id="S5.SS2.p1.3.m3.1.1.1" xref="S5.SS2.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.3.m3.1b"><apply id="S5.SS2.p1.3.m3.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1"><csymbol cd="latexml" id="S5.SS2.p1.3.m3.1.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p1.3.m3.1.1.2.cmml" xref="S5.SS2.p1.3.m3.1.1.2">51.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.3.m3.1c">51.7\%</annotation></semantics></math> on the training samples and an mAP of <math id="S5.SS2.p1.4.m4.1" class="ltx_Math" alttext="39.1\%" display="inline"><semantics id="S5.SS2.p1.4.m4.1a"><mrow id="S5.SS2.p1.4.m4.1.1" xref="S5.SS2.p1.4.m4.1.1.cmml"><mn id="S5.SS2.p1.4.m4.1.1.2" xref="S5.SS2.p1.4.m4.1.1.2.cmml">39.1</mn><mo id="S5.SS2.p1.4.m4.1.1.1" xref="S5.SS2.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.4.m4.1b"><apply id="S5.SS2.p1.4.m4.1.1.cmml" xref="S5.SS2.p1.4.m4.1.1"><csymbol cd="latexml" id="S5.SS2.p1.4.m4.1.1.1.cmml" xref="S5.SS2.p1.4.m4.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p1.4.m4.1.1.2.cmml" xref="S5.SS2.p1.4.m4.1.1.2">39.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.4.m4.1c">39.1\%</annotation></semantics></math> on the test samples.
<math id="S5.SS2.p1.5.m5.1" class="ltx_Math" alttext="E_{3}" display="inline"><semantics id="S5.SS2.p1.5.m5.1a"><msub id="S5.SS2.p1.5.m5.1.1" xref="S5.SS2.p1.5.m5.1.1.cmml"><mi id="S5.SS2.p1.5.m5.1.1.2" xref="S5.SS2.p1.5.m5.1.1.2.cmml">E</mi><mn id="S5.SS2.p1.5.m5.1.1.3" xref="S5.SS2.p1.5.m5.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.5.m5.1b"><apply id="S5.SS2.p1.5.m5.1.1.cmml" xref="S5.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S5.SS2.p1.5.m5.1.1.1.cmml" xref="S5.SS2.p1.5.m5.1.1">subscript</csymbol><ci id="S5.SS2.p1.5.m5.1.1.2.cmml" xref="S5.SS2.p1.5.m5.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS2.p1.5.m5.1.1.3.cmml" xref="S5.SS2.p1.5.m5.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.5.m5.1c">E_{3}</annotation></semantics></math> wanted to improve model performance, so he used Uni-Evaluator to evaluate the model on the test samples.
Here, we take the super-class ‚Äútransport‚Äù as an example to illustrate the idea.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.p2.3" class="ltx_p"><span id="S5.SS2.p2.3.1" class="ltx_text ltx_font_bold">Performance overview (T2)</span>.
Initially, the matrix was in the confusion mode with the super-class ‚Äútransport‚Äù expanded,
displaying the confusion between seven classes (Fig.¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision</span></span>(b)).
In the matrix, he discovered several regions of interest:
1) in region A, ‚Äúlarge vehicle‚Äù and ‚Äúsmall vehicle‚Äù were confused with each other (Fig.¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision</span></span>A);
2) in region B, many objects of ‚Äúship,‚Äù ‚Äúlarge vehicle,‚Äù ‚Äúsmall vehicle,‚Äù ‚Äústorage tank,‚Äù and ‚Äúhelicopter‚Äù failed to be segmented (Fig.¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision</span></span>B);
3) in region C, many helicopters were misclassified as planes (Fig.¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision</span></span>C); and
4) in region D, some backgrounds were misclassified as ‚Äúlarge vehicle‚Äù and ‚Äúsmall vehicle‚Äù (Fig.¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision</span></span>D).
Since the classification and the segmentation were interrelated in this multi-task scenario,
<math id="S5.SS2.p2.1.m1.1" class="ltx_Math" alttext="E_{3}" display="inline"><semantics id="S5.SS2.p2.1.m1.1a"><msub id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml"><mi id="S5.SS2.p2.1.m1.1.1.2" xref="S5.SS2.p2.1.m1.1.1.2.cmml">E</mi><mn id="S5.SS2.p2.1.m1.1.1.3" xref="S5.SS2.p2.1.m1.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><apply id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.1.m1.1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S5.SS2.p2.1.m1.1.1.2.cmml" xref="S5.SS2.p2.1.m1.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS2.p2.1.m1.1.1.3.cmml" xref="S5.SS2.p2.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">E_{3}</annotation></semantics></math> further examined the sizes and the shifted directions of the segmented objects with the size mode and direction mode, respectively.
In the size mode, he found two regions of interest:
in region E, some objects of ‚Äúsmall vehicle‚Äù were confused with ‚Äúlarge vehicle‚Äù (Fig.¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision</span></span>E); and in region F, some objects of ‚Äúharbor‚Äù were correctly classified but with size errors (Fig.¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision</span></span>F).
However, <math id="S5.SS2.p2.2.m2.1" class="ltx_Math" alttext="E_{3}" display="inline"><semantics id="S5.SS2.p2.2.m2.1a"><msub id="S5.SS2.p2.2.m2.1.1" xref="S5.SS2.p2.2.m2.1.1.cmml"><mi id="S5.SS2.p2.2.m2.1.1.2" xref="S5.SS2.p2.2.m2.1.1.2.cmml">E</mi><mn id="S5.SS2.p2.2.m2.1.1.3" xref="S5.SS2.p2.2.m2.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m2.1b"><apply id="S5.SS2.p2.2.m2.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.2.m2.1.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1">subscript</csymbol><ci id="S5.SS2.p2.2.m2.1.1.2.cmml" xref="S5.SS2.p2.2.m2.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS2.p2.2.m2.1.1.3.cmml" xref="S5.SS2.p2.2.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m2.1c">E_{3}</annotation></semantics></math> found no obvious patterns in the direction mode.
He explained, ‚ÄúThis is reasonable because aerial images are insensitive to direction changes.‚Äù
<math id="S5.SS2.p2.3.m3.1" class="ltx_Math" alttext="E_{3}" display="inline"><semantics id="S5.SS2.p2.3.m3.1a"><msub id="S5.SS2.p2.3.m3.1.1" xref="S5.SS2.p2.3.m3.1.1.cmml"><mi id="S5.SS2.p2.3.m3.1.1.2" xref="S5.SS2.p2.3.m3.1.1.2.cmml">E</mi><mn id="S5.SS2.p2.3.m3.1.1.3" xref="S5.SS2.p2.3.m3.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.3.m3.1b"><apply id="S5.SS2.p2.3.m3.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.3.m3.1.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1">subscript</csymbol><ci id="S5.SS2.p2.3.m3.1.1.2.cmml" xref="S5.SS2.p2.3.m3.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS2.p2.3.m3.1.1.3.cmml" xref="S5.SS2.p2.3.m3.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.3.m3.1c">E_{3}</annotation></semantics></math> decided to analyze each identified region of interest separately.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para ltx_noindent">
<p id="S5.SS2.p3.18" class="ltx_p"><span id="S5.SS2.p3.18.1" class="ltx_text ltx_font_bold">Analyzing misclassification in ‚Äúlarge vehicle‚Äù and ‚Äúsmall vehicle‚Äù (T2, T4)</span>.
<math id="S5.SS2.p3.1.m1.1" class="ltx_Math" alttext="E_{3}" display="inline"><semantics id="S5.SS2.p3.1.m1.1a"><msub id="S5.SS2.p3.1.m1.1.1" xref="S5.SS2.p3.1.m1.1.1.cmml"><mi id="S5.SS2.p3.1.m1.1.1.2" xref="S5.SS2.p3.1.m1.1.1.2.cmml">E</mi><mn id="S5.SS2.p3.1.m1.1.1.3" xref="S5.SS2.p3.1.m1.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.1.m1.1b"><apply id="S5.SS2.p3.1.m1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p3.1.m1.1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1">subscript</csymbol><ci id="S5.SS2.p3.1.m1.1.1.2.cmml" xref="S5.SS2.p3.1.m1.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS2.p3.1.m1.1.1.3.cmml" xref="S5.SS2.p3.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.1.m1.1c">E_{3}</annotation></semantics></math> first analyzed region A, where ‚Äúlarge vehicle‚Äù and ‚Äúsmall vehicle‚Äù were confused with each other (Fig.¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision</span></span>A).
To understand what caused the confusion, he switched to the size mode and observed that in
cell E (Fig.¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision</span></span>E),
the yellow sector of the pie chart occupied a large proportion.
This indicated that many segmented small vehicles were larger than ground truth.
To investigate this further, <math id="S5.SS2.p3.2.m2.1" class="ltx_Math" alttext="E_{3}" display="inline"><semantics id="S5.SS2.p3.2.m2.1a"><msub id="S5.SS2.p3.2.m2.1.1" xref="S5.SS2.p3.2.m2.1.1.cmml"><mi id="S5.SS2.p3.2.m2.1.1.2" xref="S5.SS2.p3.2.m2.1.1.2.cmml">E</mi><mn id="S5.SS2.p3.2.m2.1.1.3" xref="S5.SS2.p3.2.m2.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.2.m2.1b"><apply id="S5.SS2.p3.2.m2.1.1.cmml" xref="S5.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS2.p3.2.m2.1.1.1.cmml" xref="S5.SS2.p3.2.m2.1.1">subscript</csymbol><ci id="S5.SS2.p3.2.m2.1.1.2.cmml" xref="S5.SS2.p3.2.m2.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS2.p3.2.m2.1.1.3.cmml" xref="S5.SS2.p3.2.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.2.m2.1c">E_{3}</annotation></semantics></math> clicked the yellow sector to check the associated objects (Fig.¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision</span></span>(d)).
He noticed that the small vehicles were so small that most of the predictions masked two or more small vehicles as a whole.
Such wrong segmentation results were also found in the training samples.
<math id="S5.SS2.p3.3.m3.1" class="ltx_Math" alttext="E_{3}" display="inline"><semantics id="S5.SS2.p3.3.m3.1a"><msub id="S5.SS2.p3.3.m3.1.1" xref="S5.SS2.p3.3.m3.1.1.cmml"><mi id="S5.SS2.p3.3.m3.1.1.2" xref="S5.SS2.p3.3.m3.1.1.2.cmml">E</mi><mn id="S5.SS2.p3.3.m3.1.1.3" xref="S5.SS2.p3.3.m3.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.3.m3.1b"><apply id="S5.SS2.p3.3.m3.1.1.cmml" xref="S5.SS2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS2.p3.3.m3.1.1.1.cmml" xref="S5.SS2.p3.3.m3.1.1">subscript</csymbol><ci id="S5.SS2.p3.3.m3.1.1.2.cmml" xref="S5.SS2.p3.3.m3.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS2.p3.3.m3.1.1.3.cmml" xref="S5.SS2.p3.3.m3.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.3.m3.1c">E_{3}</annotation></semantics></math> hypothesized that the model was poor at predicting such small objects.
To verify this, he selected the small objects by filtering and found that most of these objects belonged to five classes: ‚Äúship,‚Äù ‚Äústorage tank,‚Äù ‚Äúsmall vehicle,‚Äù ‚Äúlarge vehicle,‚Äù and ‚Äúhelicopter.‚Äù
Many objects of these five classes were misclassified (Fig.¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision</span></span>A) or failed to be segmented (Fig.¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision</span></span>B).
Upon this observation, he examined the model and found that the images were downsampled in the model.
The small objects were hard to be distinguished in the downsampled images, and thus led to the wrong segmentation.
To address this issue, <math id="S5.SS2.p3.4.m4.1" class="ltx_Math" alttext="E_{3}" display="inline"><semantics id="S5.SS2.p3.4.m4.1a"><msub id="S5.SS2.p3.4.m4.1.1" xref="S5.SS2.p3.4.m4.1.1.cmml"><mi id="S5.SS2.p3.4.m4.1.1.2" xref="S5.SS2.p3.4.m4.1.1.2.cmml">E</mi><mn id="S5.SS2.p3.4.m4.1.1.3" xref="S5.SS2.p3.4.m4.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.4.m4.1b"><apply id="S5.SS2.p3.4.m4.1.1.cmml" xref="S5.SS2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S5.SS2.p3.4.m4.1.1.1.cmml" xref="S5.SS2.p3.4.m4.1.1">subscript</csymbol><ci id="S5.SS2.p3.4.m4.1.1.2.cmml" xref="S5.SS2.p3.4.m4.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS2.p3.4.m4.1.1.3.cmml" xref="S5.SS2.p3.4.m4.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.4.m4.1c">E_{3}</annotation></semantics></math> increased the resolution of the images from <math id="S5.SS2.p3.5.m5.1" class="ltx_Math" alttext="512\times 512" display="inline"><semantics id="S5.SS2.p3.5.m5.1a"><mrow id="S5.SS2.p3.5.m5.1.1" xref="S5.SS2.p3.5.m5.1.1.cmml"><mn id="S5.SS2.p3.5.m5.1.1.2" xref="S5.SS2.p3.5.m5.1.1.2.cmml">512</mn><mo lspace="0.222em" rspace="0.222em" id="S5.SS2.p3.5.m5.1.1.1" xref="S5.SS2.p3.5.m5.1.1.1.cmml">√ó</mo><mn id="S5.SS2.p3.5.m5.1.1.3" xref="S5.SS2.p3.5.m5.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.5.m5.1b"><apply id="S5.SS2.p3.5.m5.1.1.cmml" xref="S5.SS2.p3.5.m5.1.1"><times id="S5.SS2.p3.5.m5.1.1.1.cmml" xref="S5.SS2.p3.5.m5.1.1.1"></times><cn type="integer" id="S5.SS2.p3.5.m5.1.1.2.cmml" xref="S5.SS2.p3.5.m5.1.1.2">512</cn><cn type="integer" id="S5.SS2.p3.5.m5.1.1.3.cmml" xref="S5.SS2.p3.5.m5.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.5.m5.1c">512\times 512</annotation></semantics></math> to <math id="S5.SS2.p3.6.m6.1" class="ltx_Math" alttext="1024\times 1024" display="inline"><semantics id="S5.SS2.p3.6.m6.1a"><mrow id="S5.SS2.p3.6.m6.1.1" xref="S5.SS2.p3.6.m6.1.1.cmml"><mn id="S5.SS2.p3.6.m6.1.1.2" xref="S5.SS2.p3.6.m6.1.1.2.cmml">1024</mn><mo lspace="0.222em" rspace="0.222em" id="S5.SS2.p3.6.m6.1.1.1" xref="S5.SS2.p3.6.m6.1.1.1.cmml">√ó</mo><mn id="S5.SS2.p3.6.m6.1.1.3" xref="S5.SS2.p3.6.m6.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.6.m6.1b"><apply id="S5.SS2.p3.6.m6.1.1.cmml" xref="S5.SS2.p3.6.m6.1.1"><times id="S5.SS2.p3.6.m6.1.1.1.cmml" xref="S5.SS2.p3.6.m6.1.1.1"></times><cn type="integer" id="S5.SS2.p3.6.m6.1.1.2.cmml" xref="S5.SS2.p3.6.m6.1.1.2">1024</cn><cn type="integer" id="S5.SS2.p3.6.m6.1.1.3.cmml" xref="S5.SS2.p3.6.m6.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.6.m6.1c">1024\times 1024</annotation></semantics></math> and retrained the model.
Subsequently, the APs of ‚Äúship,‚Äù ‚Äústorage tank,‚Äù ‚Äúsmall vehicle,‚Äù ‚Äúlarge vehicle,‚Äù and ‚Äúhelicopter‚Äù were increased from
<math id="S5.SS2.p3.7.m7.1" class="ltx_Math" alttext="49.9\%" display="inline"><semantics id="S5.SS2.p3.7.m7.1a"><mrow id="S5.SS2.p3.7.m7.1.1" xref="S5.SS2.p3.7.m7.1.1.cmml"><mn id="S5.SS2.p3.7.m7.1.1.2" xref="S5.SS2.p3.7.m7.1.1.2.cmml">49.9</mn><mo id="S5.SS2.p3.7.m7.1.1.1" xref="S5.SS2.p3.7.m7.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.7.m7.1b"><apply id="S5.SS2.p3.7.m7.1.1.cmml" xref="S5.SS2.p3.7.m7.1.1"><csymbol cd="latexml" id="S5.SS2.p3.7.m7.1.1.1.cmml" xref="S5.SS2.p3.7.m7.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p3.7.m7.1.1.2.cmml" xref="S5.SS2.p3.7.m7.1.1.2">49.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.7.m7.1c">49.9\%</annotation></semantics></math> to <math id="S5.SS2.p3.8.m8.1" class="ltx_Math" alttext="51.5\%" display="inline"><semantics id="S5.SS2.p3.8.m8.1a"><mrow id="S5.SS2.p3.8.m8.1.1" xref="S5.SS2.p3.8.m8.1.1.cmml"><mn id="S5.SS2.p3.8.m8.1.1.2" xref="S5.SS2.p3.8.m8.1.1.2.cmml">51.5</mn><mo id="S5.SS2.p3.8.m8.1.1.1" xref="S5.SS2.p3.8.m8.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.8.m8.1b"><apply id="S5.SS2.p3.8.m8.1.1.cmml" xref="S5.SS2.p3.8.m8.1.1"><csymbol cd="latexml" id="S5.SS2.p3.8.m8.1.1.1.cmml" xref="S5.SS2.p3.8.m8.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p3.8.m8.1.1.2.cmml" xref="S5.SS2.p3.8.m8.1.1.2">51.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.8.m8.1c">51.5\%</annotation></semantics></math>, <math id="S5.SS2.p3.9.m9.1" class="ltx_Math" alttext="39.9\%" display="inline"><semantics id="S5.SS2.p3.9.m9.1a"><mrow id="S5.SS2.p3.9.m9.1.1" xref="S5.SS2.p3.9.m9.1.1.cmml"><mn id="S5.SS2.p3.9.m9.1.1.2" xref="S5.SS2.p3.9.m9.1.1.2.cmml">39.9</mn><mo id="S5.SS2.p3.9.m9.1.1.1" xref="S5.SS2.p3.9.m9.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.9.m9.1b"><apply id="S5.SS2.p3.9.m9.1.1.cmml" xref="S5.SS2.p3.9.m9.1.1"><csymbol cd="latexml" id="S5.SS2.p3.9.m9.1.1.1.cmml" xref="S5.SS2.p3.9.m9.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p3.9.m9.1.1.2.cmml" xref="S5.SS2.p3.9.m9.1.1.2">39.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.9.m9.1c">39.9\%</annotation></semantics></math> to <math id="S5.SS2.p3.10.m10.1" class="ltx_Math" alttext="42.8\%" display="inline"><semantics id="S5.SS2.p3.10.m10.1a"><mrow id="S5.SS2.p3.10.m10.1.1" xref="S5.SS2.p3.10.m10.1.1.cmml"><mn id="S5.SS2.p3.10.m10.1.1.2" xref="S5.SS2.p3.10.m10.1.1.2.cmml">42.8</mn><mo id="S5.SS2.p3.10.m10.1.1.1" xref="S5.SS2.p3.10.m10.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.10.m10.1b"><apply id="S5.SS2.p3.10.m10.1.1.cmml" xref="S5.SS2.p3.10.m10.1.1"><csymbol cd="latexml" id="S5.SS2.p3.10.m10.1.1.1.cmml" xref="S5.SS2.p3.10.m10.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p3.10.m10.1.1.2.cmml" xref="S5.SS2.p3.10.m10.1.1.2">42.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.10.m10.1c">42.8\%</annotation></semantics></math>,
<math id="S5.SS2.p3.11.m11.1" class="ltx_Math" alttext="16.5\%" display="inline"><semantics id="S5.SS2.p3.11.m11.1a"><mrow id="S5.SS2.p3.11.m11.1.1" xref="S5.SS2.p3.11.m11.1.1.cmml"><mn id="S5.SS2.p3.11.m11.1.1.2" xref="S5.SS2.p3.11.m11.1.1.2.cmml">16.5</mn><mo id="S5.SS2.p3.11.m11.1.1.1" xref="S5.SS2.p3.11.m11.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.11.m11.1b"><apply id="S5.SS2.p3.11.m11.1.1.cmml" xref="S5.SS2.p3.11.m11.1.1"><csymbol cd="latexml" id="S5.SS2.p3.11.m11.1.1.1.cmml" xref="S5.SS2.p3.11.m11.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p3.11.m11.1.1.2.cmml" xref="S5.SS2.p3.11.m11.1.1.2">16.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.11.m11.1c">16.5\%</annotation></semantics></math> to <math id="S5.SS2.p3.12.m12.1" class="ltx_Math" alttext="19.2\%" display="inline"><semantics id="S5.SS2.p3.12.m12.1a"><mrow id="S5.SS2.p3.12.m12.1.1" xref="S5.SS2.p3.12.m12.1.1.cmml"><mn id="S5.SS2.p3.12.m12.1.1.2" xref="S5.SS2.p3.12.m12.1.1.2.cmml">19.2</mn><mo id="S5.SS2.p3.12.m12.1.1.1" xref="S5.SS2.p3.12.m12.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.12.m12.1b"><apply id="S5.SS2.p3.12.m12.1.1.cmml" xref="S5.SS2.p3.12.m12.1.1"><csymbol cd="latexml" id="S5.SS2.p3.12.m12.1.1.1.cmml" xref="S5.SS2.p3.12.m12.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p3.12.m12.1.1.2.cmml" xref="S5.SS2.p3.12.m12.1.1.2">19.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.12.m12.1c">19.2\%</annotation></semantics></math>, <math id="S5.SS2.p3.13.m13.1" class="ltx_Math" alttext="39.5\%" display="inline"><semantics id="S5.SS2.p3.13.m13.1a"><mrow id="S5.SS2.p3.13.m13.1.1" xref="S5.SS2.p3.13.m13.1.1.cmml"><mn id="S5.SS2.p3.13.m13.1.1.2" xref="S5.SS2.p3.13.m13.1.1.2.cmml">39.5</mn><mo id="S5.SS2.p3.13.m13.1.1.1" xref="S5.SS2.p3.13.m13.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.13.m13.1b"><apply id="S5.SS2.p3.13.m13.1.1.cmml" xref="S5.SS2.p3.13.m13.1.1"><csymbol cd="latexml" id="S5.SS2.p3.13.m13.1.1.1.cmml" xref="S5.SS2.p3.13.m13.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p3.13.m13.1.1.2.cmml" xref="S5.SS2.p3.13.m13.1.1.2">39.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.13.m13.1c">39.5\%</annotation></semantics></math> to <math id="S5.SS2.p3.14.m14.1" class="ltx_Math" alttext="41.6\%" display="inline"><semantics id="S5.SS2.p3.14.m14.1a"><mrow id="S5.SS2.p3.14.m14.1.1" xref="S5.SS2.p3.14.m14.1.1.cmml"><mn id="S5.SS2.p3.14.m14.1.1.2" xref="S5.SS2.p3.14.m14.1.1.2.cmml">41.6</mn><mo id="S5.SS2.p3.14.m14.1.1.1" xref="S5.SS2.p3.14.m14.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.14.m14.1b"><apply id="S5.SS2.p3.14.m14.1.1.cmml" xref="S5.SS2.p3.14.m14.1.1"><csymbol cd="latexml" id="S5.SS2.p3.14.m14.1.1.1.cmml" xref="S5.SS2.p3.14.m14.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p3.14.m14.1.1.2.cmml" xref="S5.SS2.p3.14.m14.1.1.2">41.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.14.m14.1c">41.6\%</annotation></semantics></math>,
<math id="S5.SS2.p3.15.m15.1" class="ltx_Math" alttext="6.3\%" display="inline"><semantics id="S5.SS2.p3.15.m15.1a"><mrow id="S5.SS2.p3.15.m15.1.1" xref="S5.SS2.p3.15.m15.1.1.cmml"><mn id="S5.SS2.p3.15.m15.1.1.2" xref="S5.SS2.p3.15.m15.1.1.2.cmml">6.3</mn><mo id="S5.SS2.p3.15.m15.1.1.1" xref="S5.SS2.p3.15.m15.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.15.m15.1b"><apply id="S5.SS2.p3.15.m15.1.1.cmml" xref="S5.SS2.p3.15.m15.1.1"><csymbol cd="latexml" id="S5.SS2.p3.15.m15.1.1.1.cmml" xref="S5.SS2.p3.15.m15.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p3.15.m15.1.1.2.cmml" xref="S5.SS2.p3.15.m15.1.1.2">6.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.15.m15.1c">6.3\%</annotation></semantics></math> to <math id="S5.SS2.p3.16.m16.1" class="ltx_Math" alttext="7.5\%" display="inline"><semantics id="S5.SS2.p3.16.m16.1a"><mrow id="S5.SS2.p3.16.m16.1.1" xref="S5.SS2.p3.16.m16.1.1.cmml"><mn id="S5.SS2.p3.16.m16.1.1.2" xref="S5.SS2.p3.16.m16.1.1.2.cmml">7.5</mn><mo id="S5.SS2.p3.16.m16.1.1.1" xref="S5.SS2.p3.16.m16.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.16.m16.1b"><apply id="S5.SS2.p3.16.m16.1.1.cmml" xref="S5.SS2.p3.16.m16.1.1"><csymbol cd="latexml" id="S5.SS2.p3.16.m16.1.1.1.cmml" xref="S5.SS2.p3.16.m16.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p3.16.m16.1.1.2.cmml" xref="S5.SS2.p3.16.m16.1.1.2">7.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.16.m16.1c">7.5\%</annotation></semantics></math>, respectively,
and the overall mAP was increased from <math id="S5.SS2.p3.17.m17.1" class="ltx_Math" alttext="39.1\%" display="inline"><semantics id="S5.SS2.p3.17.m17.1a"><mrow id="S5.SS2.p3.17.m17.1.1" xref="S5.SS2.p3.17.m17.1.1.cmml"><mn id="S5.SS2.p3.17.m17.1.1.2" xref="S5.SS2.p3.17.m17.1.1.2.cmml">39.1</mn><mo id="S5.SS2.p3.17.m17.1.1.1" xref="S5.SS2.p3.17.m17.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.17.m17.1b"><apply id="S5.SS2.p3.17.m17.1.1.cmml" xref="S5.SS2.p3.17.m17.1.1"><csymbol cd="latexml" id="S5.SS2.p3.17.m17.1.1.1.cmml" xref="S5.SS2.p3.17.m17.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p3.17.m17.1.1.2.cmml" xref="S5.SS2.p3.17.m17.1.1.2">39.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.17.m17.1c">39.1\%</annotation></semantics></math> to <math id="S5.SS2.p3.18.m18.1" class="ltx_Math" alttext="40.3\%" display="inline"><semantics id="S5.SS2.p3.18.m18.1a"><mrow id="S5.SS2.p3.18.m18.1.1" xref="S5.SS2.p3.18.m18.1.1.cmml"><mn id="S5.SS2.p3.18.m18.1.1.2" xref="S5.SS2.p3.18.m18.1.1.2.cmml">40.3</mn><mo id="S5.SS2.p3.18.m18.1.1.1" xref="S5.SS2.p3.18.m18.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.18.m18.1b"><apply id="S5.SS2.p3.18.m18.1.1.cmml" xref="S5.SS2.p3.18.m18.1.1"><csymbol cd="latexml" id="S5.SS2.p3.18.m18.1.1.1.cmml" xref="S5.SS2.p3.18.m18.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p3.18.m18.1.1.2.cmml" xref="S5.SS2.p3.18.m18.1.1.2">40.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.18.m18.1c">40.3\%</annotation></semantics></math>.</p>
</div>
<figure id="S5.F8" class="ltx_figure"><img src="/html/2308.05168/assets/x18.png" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="191" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S5.F8.3.2" class="ltx_text" style="font-size:90%;">(a) A helicopter is well segmented but misclassified; (b) a helicopter and a plane share a similar appearance; (c) the middle part of a harbor is segmented, but its bars are not.</span></figcaption>
</figure>
<div id="S5.SS2.p4" class="ltx_para ltx_noindent">
<p id="S5.SS2.p4.3" class="ltx_p"><span id="S5.SS2.p4.3.1" class="ltx_text ltx_font_bold">Analyzing misclassification in ‚Äúhelicopter‚Äù and background (T2, T4)</span>.
<math id="S5.SS2.p4.1.m1.1" class="ltx_Math" alttext="E_{3}" display="inline"><semantics id="S5.SS2.p4.1.m1.1a"><msub id="S5.SS2.p4.1.m1.1.1" xref="S5.SS2.p4.1.m1.1.1.cmml"><mi id="S5.SS2.p4.1.m1.1.1.2" xref="S5.SS2.p4.1.m1.1.1.2.cmml">E</mi><mn id="S5.SS2.p4.1.m1.1.1.3" xref="S5.SS2.p4.1.m1.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p4.1.m1.1b"><apply id="S5.SS2.p4.1.m1.1.1.cmml" xref="S5.SS2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p4.1.m1.1.1.1.cmml" xref="S5.SS2.p4.1.m1.1.1">subscript</csymbol><ci id="S5.SS2.p4.1.m1.1.1.2.cmml" xref="S5.SS2.p4.1.m1.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS2.p4.1.m1.1.1.3.cmml" xref="S5.SS2.p4.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p4.1.m1.1c">E_{3}</annotation></semantics></math> proceeded to analyze region C and found that some objects of ‚Äúhelicopter‚Äù was misclassified as ‚Äúplane.‚Äù
Examining the associated objects in the grid visualization, he observed that these helicopters were well segmented but still misclassified (Fig.¬†<a href="#S5.F8" title="Figure 8 ‚Ä£ 5.2 Instance Segmentation on iSAID dataset ‚Ä£ 5 Case Studies ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>(a)).
After examining helicopters and planes in the training samples, <math id="S5.SS2.p4.2.m2.1" class="ltx_Math" alttext="E_{3}" display="inline"><semantics id="S5.SS2.p4.2.m2.1a"><msub id="S5.SS2.p4.2.m2.1.1" xref="S5.SS2.p4.2.m2.1.1.cmml"><mi id="S5.SS2.p4.2.m2.1.1.2" xref="S5.SS2.p4.2.m2.1.1.2.cmml">E</mi><mn id="S5.SS2.p4.2.m2.1.1.3" xref="S5.SS2.p4.2.m2.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p4.2.m2.1b"><apply id="S5.SS2.p4.2.m2.1.1.cmml" xref="S5.SS2.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS2.p4.2.m2.1.1.1.cmml" xref="S5.SS2.p4.2.m2.1.1">subscript</csymbol><ci id="S5.SS2.p4.2.m2.1.1.2.cmml" xref="S5.SS2.p4.2.m2.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS2.p4.2.m2.1.1.3.cmml" xref="S5.SS2.p4.2.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p4.2.m2.1c">E_{3}</annotation></semantics></math> commented, ‚ÄúThis misclassification occurs because helicopters and planes have a similar appearance in aerial images (Fig.¬†<a href="#S5.F8" title="Figure 8 ‚Ä£ 5.2 Instance Segmentation on iSAID dataset ‚Ä£ 5 Case Studies ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>(b)), and the training samples contain more instances of planes (19,720) than helicopters (1,402).‚Äù
To address this issue, <math id="S5.SS2.p4.3.m3.1" class="ltx_Math" alttext="E_{3}" display="inline"><semantics id="S5.SS2.p4.3.m3.1a"><msub id="S5.SS2.p4.3.m3.1.1" xref="S5.SS2.p4.3.m3.1.1.cmml"><mi id="S5.SS2.p4.3.m3.1.1.2" xref="S5.SS2.p4.3.m3.1.1.2.cmml">E</mi><mn id="S5.SS2.p4.3.m3.1.1.3" xref="S5.SS2.p4.3.m3.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p4.3.m3.1b"><apply id="S5.SS2.p4.3.m3.1.1.cmml" xref="S5.SS2.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS2.p4.3.m3.1.1.1.cmml" xref="S5.SS2.p4.3.m3.1.1">subscript</csymbol><ci id="S5.SS2.p4.3.m3.1.1.2.cmml" xref="S5.SS2.p4.3.m3.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS2.p4.3.m3.1.1.3.cmml" xref="S5.SS2.p4.3.m3.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p4.3.m3.1c">E_{3}</annotation></semantics></math> first collected 1,077 aerial images with 1,561 helicopters,
then applied the widely-used Copy-Paste augmentation strategy¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> to generate more helicopters in the training samples.
The augmentation was carried out by pasting the helicopters from their original images into other images, which resulted in a total of 5,063 helicopters.</p>
</div>
<div id="S5.SS2.p5" class="ltx_para">
<p id="S5.SS2.p5.3" class="ltx_p">Next, <math id="S5.SS2.p5.1.m1.1" class="ltx_Math" alttext="E_{3}" display="inline"><semantics id="S5.SS2.p5.1.m1.1a"><msub id="S5.SS2.p5.1.m1.1.1" xref="S5.SS2.p5.1.m1.1.1.cmml"><mi id="S5.SS2.p5.1.m1.1.1.2" xref="S5.SS2.p5.1.m1.1.1.2.cmml">E</mi><mn id="S5.SS2.p5.1.m1.1.1.3" xref="S5.SS2.p5.1.m1.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p5.1.m1.1b"><apply id="S5.SS2.p5.1.m1.1.1.cmml" xref="S5.SS2.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p5.1.m1.1.1.1.cmml" xref="S5.SS2.p5.1.m1.1.1">subscript</csymbol><ci id="S5.SS2.p5.1.m1.1.1.2.cmml" xref="S5.SS2.p5.1.m1.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS2.p5.1.m1.1.1.3.cmml" xref="S5.SS2.p5.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p5.1.m1.1c">E_{3}</annotation></semantics></math> analyzed region D, where some backgrounds were misclassified as ‚Äúlarge vehicle‚Äù or ‚Äúsmall vehicle‚Äù (Fig.¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision</span></span>D).
<math id="S5.SS2.p5.2.m2.1" class="ltx_Math" alttext="E_{3}" display="inline"><semantics id="S5.SS2.p5.2.m2.1a"><msub id="S5.SS2.p5.2.m2.1.1" xref="S5.SS2.p5.2.m2.1.1.cmml"><mi id="S5.SS2.p5.2.m2.1.1.2" xref="S5.SS2.p5.2.m2.1.1.2.cmml">E</mi><mn id="S5.SS2.p5.2.m2.1.1.3" xref="S5.SS2.p5.2.m2.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p5.2.m2.1b"><apply id="S5.SS2.p5.2.m2.1.1.cmml" xref="S5.SS2.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS2.p5.2.m2.1.1.1.cmml" xref="S5.SS2.p5.2.m2.1.1">subscript</csymbol><ci id="S5.SS2.p5.2.m2.1.1.2.cmml" xref="S5.SS2.p5.2.m2.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS2.p5.2.m2.1.1.3.cmml" xref="S5.SS2.p5.2.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p5.2.m2.1c">E_{3}</annotation></semantics></math> considered this was reasonable since vehicles were small and obscure in aerial images, making them difficult to be distinguished
from the background by the model.
This was also consistent with the previous studies¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.
As this misclassification was a long-standing problem in instance segmentation, <math id="S5.SS2.p5.3.m3.1" class="ltx_Math" alttext="E_{3}" display="inline"><semantics id="S5.SS2.p5.3.m3.1a"><msub id="S5.SS2.p5.3.m3.1.1" xref="S5.SS2.p5.3.m3.1.1.cmml"><mi id="S5.SS2.p5.3.m3.1.1.2" xref="S5.SS2.p5.3.m3.1.1.2.cmml">E</mi><mn id="S5.SS2.p5.3.m3.1.1.3" xref="S5.SS2.p5.3.m3.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p5.3.m3.1b"><apply id="S5.SS2.p5.3.m3.1.1.cmml" xref="S5.SS2.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS2.p5.3.m3.1.1.1.cmml" xref="S5.SS2.p5.3.m3.1.1">subscript</csymbol><ci id="S5.SS2.p5.3.m3.1.1.2.cmml" xref="S5.SS2.p5.3.m3.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS2.p5.3.m3.1.1.3.cmml" xref="S5.SS2.p5.3.m3.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p5.3.m3.1c">E_{3}</annotation></semantics></math> decided to explore it further in future research.</p>
</div>
<div id="S5.SS2.p6" class="ltx_para">
<p id="S5.SS2.p6.4" class="ltx_p">After fine-tuning the model with the augmented samples of ‚Äúhelicopter‚Äù, the AP of ‚Äúhelicopter‚Äù was increased from <math id="S5.SS2.p6.1.m1.1" class="ltx_Math" alttext="7.5\%" display="inline"><semantics id="S5.SS2.p6.1.m1.1a"><mrow id="S5.SS2.p6.1.m1.1.1" xref="S5.SS2.p6.1.m1.1.1.cmml"><mn id="S5.SS2.p6.1.m1.1.1.2" xref="S5.SS2.p6.1.m1.1.1.2.cmml">7.5</mn><mo id="S5.SS2.p6.1.m1.1.1.1" xref="S5.SS2.p6.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p6.1.m1.1b"><apply id="S5.SS2.p6.1.m1.1.1.cmml" xref="S5.SS2.p6.1.m1.1.1"><csymbol cd="latexml" id="S5.SS2.p6.1.m1.1.1.1.cmml" xref="S5.SS2.p6.1.m1.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p6.1.m1.1.1.2.cmml" xref="S5.SS2.p6.1.m1.1.1.2">7.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p6.1.m1.1c">7.5\%</annotation></semantics></math> to <math id="S5.SS2.p6.2.m2.1" class="ltx_Math" alttext="10.4\%" display="inline"><semantics id="S5.SS2.p6.2.m2.1a"><mrow id="S5.SS2.p6.2.m2.1.1" xref="S5.SS2.p6.2.m2.1.1.cmml"><mn id="S5.SS2.p6.2.m2.1.1.2" xref="S5.SS2.p6.2.m2.1.1.2.cmml">10.4</mn><mo id="S5.SS2.p6.2.m2.1.1.1" xref="S5.SS2.p6.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p6.2.m2.1b"><apply id="S5.SS2.p6.2.m2.1.1.cmml" xref="S5.SS2.p6.2.m2.1.1"><csymbol cd="latexml" id="S5.SS2.p6.2.m2.1.1.1.cmml" xref="S5.SS2.p6.2.m2.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p6.2.m2.1.1.2.cmml" xref="S5.SS2.p6.2.m2.1.1.2">10.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p6.2.m2.1c">10.4\%</annotation></semantics></math>.
The overall mAP was increased from <math id="S5.SS2.p6.3.m3.1" class="ltx_Math" alttext="40.3\%" display="inline"><semantics id="S5.SS2.p6.3.m3.1a"><mrow id="S5.SS2.p6.3.m3.1.1" xref="S5.SS2.p6.3.m3.1.1.cmml"><mn id="S5.SS2.p6.3.m3.1.1.2" xref="S5.SS2.p6.3.m3.1.1.2.cmml">40.3</mn><mo id="S5.SS2.p6.3.m3.1.1.1" xref="S5.SS2.p6.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p6.3.m3.1b"><apply id="S5.SS2.p6.3.m3.1.1.cmml" xref="S5.SS2.p6.3.m3.1.1"><csymbol cd="latexml" id="S5.SS2.p6.3.m3.1.1.1.cmml" xref="S5.SS2.p6.3.m3.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p6.3.m3.1.1.2.cmml" xref="S5.SS2.p6.3.m3.1.1.2">40.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p6.3.m3.1c">40.3\%</annotation></semantics></math> to <math id="S5.SS2.p6.4.m4.1" class="ltx_Math" alttext="40.4\%" display="inline"><semantics id="S5.SS2.p6.4.m4.1a"><mrow id="S5.SS2.p6.4.m4.1.1" xref="S5.SS2.p6.4.m4.1.1.cmml"><mn id="S5.SS2.p6.4.m4.1.1.2" xref="S5.SS2.p6.4.m4.1.1.2.cmml">40.4</mn><mo id="S5.SS2.p6.4.m4.1.1.1" xref="S5.SS2.p6.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p6.4.m4.1b"><apply id="S5.SS2.p6.4.m4.1.1.cmml" xref="S5.SS2.p6.4.m4.1.1"><csymbol cd="latexml" id="S5.SS2.p6.4.m4.1.1.1.cmml" xref="S5.SS2.p6.4.m4.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p6.4.m4.1.1.2.cmml" xref="S5.SS2.p6.4.m4.1.1.2">40.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p6.4.m4.1c">40.4\%</annotation></semantics></math>.</p>
</div>
<div id="S5.SS2.p7" class="ltx_para ltx_noindent">
<p id="S5.SS2.p7.9" class="ltx_p"><span id="S5.SS2.p7.9.1" class="ltx_text ltx_font_bold">Analyzing the imprecise size issue in ‚Äúharbor‚Äù (T2, T4)</span>.
<math id="S5.SS2.p7.1.m1.1" class="ltx_Math" alttext="E_{3}" display="inline"><semantics id="S5.SS2.p7.1.m1.1a"><msub id="S5.SS2.p7.1.m1.1.1" xref="S5.SS2.p7.1.m1.1.1.cmml"><mi id="S5.SS2.p7.1.m1.1.1.2" xref="S5.SS2.p7.1.m1.1.1.2.cmml">E</mi><mn id="S5.SS2.p7.1.m1.1.1.3" xref="S5.SS2.p7.1.m1.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p7.1.m1.1b"><apply id="S5.SS2.p7.1.m1.1.1.cmml" xref="S5.SS2.p7.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p7.1.m1.1.1.1.cmml" xref="S5.SS2.p7.1.m1.1.1">subscript</csymbol><ci id="S5.SS2.p7.1.m1.1.1.2.cmml" xref="S5.SS2.p7.1.m1.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS2.p7.1.m1.1.1.3.cmml" xref="S5.SS2.p7.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p7.1.m1.1c">E_{3}</annotation></semantics></math> continued to analyze the diagonal cell of ‚Äúharbor‚Äù (Fig.¬†<span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision</span></span>F).
The green sector of this cell indicated that many harbors were predicted with a smaller size.
Intrigued by this observation,
he clicked the green sector to check the associated objects in the grid visualization.
He found that the middle parts of the harbors were correctly segmented, but the bars around them were not (Fig.¬†<a href="#S5.F8" title="Figure 8 ‚Ä£ 5.2 Instance Segmentation on iSAID dataset ‚Ä£ 5 Case Studies ‚Ä£ A Unified Interactive Model Evaluation for Classification, Object Detection, and Instance Segmentation in Computer Vision" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>(c)).
Similar patterns were also found in the training samples.
From the finding,
he realized that the currently used binary cross entropy loss function is area-based, and the model with this loss function was unable to precisely segment the bars due to their small sizes.
It was necessary to consider the boundaries of harbors during training.
Additionally, <math id="S5.SS2.p7.2.m2.1" class="ltx_Math" alttext="E_{3}" display="inline"><semantics id="S5.SS2.p7.2.m2.1a"><msub id="S5.SS2.p7.2.m2.1.1" xref="S5.SS2.p7.2.m2.1.1.cmml"><mi id="S5.SS2.p7.2.m2.1.1.2" xref="S5.SS2.p7.2.m2.1.1.2.cmml">E</mi><mn id="S5.SS2.p7.2.m2.1.1.3" xref="S5.SS2.p7.2.m2.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p7.2.m2.1b"><apply id="S5.SS2.p7.2.m2.1.1.cmml" xref="S5.SS2.p7.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS2.p7.2.m2.1.1.1.cmml" xref="S5.SS2.p7.2.m2.1.1">subscript</csymbol><ci id="S5.SS2.p7.2.m2.1.1.2.cmml" xref="S5.SS2.p7.2.m2.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS2.p7.2.m2.1.1.3.cmml" xref="S5.SS2.p7.2.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p7.2.m2.1c">E_{3}</annotation></semantics></math> found two other classes, ‚Äúplane‚Äù and ‚Äúhelicopter,‚Äù with the similar issue of complex boundaries.
To address this issue, he combined a boundary-based loss¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> with the original binary cross entropy loss for the three classes.
After fine-tuning the model, the APs of ‚Äúharbor,‚Äù ‚Äúplane,‚Äù and ‚Äúhelicopter‚Äù were increased from <math id="S5.SS2.p7.3.m3.1" class="ltx_Math" alttext="29.6\%" display="inline"><semantics id="S5.SS2.p7.3.m3.1a"><mrow id="S5.SS2.p7.3.m3.1.1" xref="S5.SS2.p7.3.m3.1.1.cmml"><mn id="S5.SS2.p7.3.m3.1.1.2" xref="S5.SS2.p7.3.m3.1.1.2.cmml">29.6</mn><mo id="S5.SS2.p7.3.m3.1.1.1" xref="S5.SS2.p7.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p7.3.m3.1b"><apply id="S5.SS2.p7.3.m3.1.1.cmml" xref="S5.SS2.p7.3.m3.1.1"><csymbol cd="latexml" id="S5.SS2.p7.3.m3.1.1.1.cmml" xref="S5.SS2.p7.3.m3.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p7.3.m3.1.1.2.cmml" xref="S5.SS2.p7.3.m3.1.1.2">29.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p7.3.m3.1c">29.6\%</annotation></semantics></math> to <math id="S5.SS2.p7.4.m4.1" class="ltx_Math" alttext="31.7\%" display="inline"><semantics id="S5.SS2.p7.4.m4.1a"><mrow id="S5.SS2.p7.4.m4.1.1" xref="S5.SS2.p7.4.m4.1.1.cmml"><mn id="S5.SS2.p7.4.m4.1.1.2" xref="S5.SS2.p7.4.m4.1.1.2.cmml">31.7</mn><mo id="S5.SS2.p7.4.m4.1.1.1" xref="S5.SS2.p7.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p7.4.m4.1b"><apply id="S5.SS2.p7.4.m4.1.1.cmml" xref="S5.SS2.p7.4.m4.1.1"><csymbol cd="latexml" id="S5.SS2.p7.4.m4.1.1.1.cmml" xref="S5.SS2.p7.4.m4.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p7.4.m4.1.1.2.cmml" xref="S5.SS2.p7.4.m4.1.1.2">31.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p7.4.m4.1c">31.7\%</annotation></semantics></math>, <math id="S5.SS2.p7.5.m5.1" class="ltx_Math" alttext="51.5\%" display="inline"><semantics id="S5.SS2.p7.5.m5.1a"><mrow id="S5.SS2.p7.5.m5.1.1" xref="S5.SS2.p7.5.m5.1.1.cmml"><mn id="S5.SS2.p7.5.m5.1.1.2" xref="S5.SS2.p7.5.m5.1.1.2.cmml">51.5</mn><mo id="S5.SS2.p7.5.m5.1.1.1" xref="S5.SS2.p7.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p7.5.m5.1b"><apply id="S5.SS2.p7.5.m5.1.1.cmml" xref="S5.SS2.p7.5.m5.1.1"><csymbol cd="latexml" id="S5.SS2.p7.5.m5.1.1.1.cmml" xref="S5.SS2.p7.5.m5.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p7.5.m5.1.1.2.cmml" xref="S5.SS2.p7.5.m5.1.1.2">51.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p7.5.m5.1c">51.5\%</annotation></semantics></math> to <math id="S5.SS2.p7.6.m6.1" class="ltx_Math" alttext="52.6\%" display="inline"><semantics id="S5.SS2.p7.6.m6.1a"><mrow id="S5.SS2.p7.6.m6.1.1" xref="S5.SS2.p7.6.m6.1.1.cmml"><mn id="S5.SS2.p7.6.m6.1.1.2" xref="S5.SS2.p7.6.m6.1.1.2.cmml">52.6</mn><mo id="S5.SS2.p7.6.m6.1.1.1" xref="S5.SS2.p7.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p7.6.m6.1b"><apply id="S5.SS2.p7.6.m6.1.1.cmml" xref="S5.SS2.p7.6.m6.1.1"><csymbol cd="latexml" id="S5.SS2.p7.6.m6.1.1.1.cmml" xref="S5.SS2.p7.6.m6.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p7.6.m6.1.1.2.cmml" xref="S5.SS2.p7.6.m6.1.1.2">52.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p7.6.m6.1c">52.6\%</annotation></semantics></math>, and <math id="S5.SS2.p7.7.m7.1" class="ltx_Math" alttext="10.4\%" display="inline"><semantics id="S5.SS2.p7.7.m7.1a"><mrow id="S5.SS2.p7.7.m7.1.1" xref="S5.SS2.p7.7.m7.1.1.cmml"><mn id="S5.SS2.p7.7.m7.1.1.2" xref="S5.SS2.p7.7.m7.1.1.2.cmml">10.4</mn><mo id="S5.SS2.p7.7.m7.1.1.1" xref="S5.SS2.p7.7.m7.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p7.7.m7.1b"><apply id="S5.SS2.p7.7.m7.1.1.cmml" xref="S5.SS2.p7.7.m7.1.1"><csymbol cd="latexml" id="S5.SS2.p7.7.m7.1.1.1.cmml" xref="S5.SS2.p7.7.m7.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p7.7.m7.1.1.2.cmml" xref="S5.SS2.p7.7.m7.1.1.2">10.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p7.7.m7.1c">10.4\%</annotation></semantics></math> to <math id="S5.SS2.p7.8.m8.1" class="ltx_Math" alttext="11.3\%" display="inline"><semantics id="S5.SS2.p7.8.m8.1a"><mrow id="S5.SS2.p7.8.m8.1.1" xref="S5.SS2.p7.8.m8.1.1.cmml"><mn id="S5.SS2.p7.8.m8.1.1.2" xref="S5.SS2.p7.8.m8.1.1.2.cmml">11.3</mn><mo id="S5.SS2.p7.8.m8.1.1.1" xref="S5.SS2.p7.8.m8.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p7.8.m8.1b"><apply id="S5.SS2.p7.8.m8.1.1.cmml" xref="S5.SS2.p7.8.m8.1.1"><csymbol cd="latexml" id="S5.SS2.p7.8.m8.1.1.1.cmml" xref="S5.SS2.p7.8.m8.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p7.8.m8.1.1.2.cmml" xref="S5.SS2.p7.8.m8.1.1.2">11.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p7.8.m8.1c">11.3\%</annotation></semantics></math>, respectively.
This resulted in an overall mAP increase to <math id="S5.SS2.p7.9.m9.1" class="ltx_Math" alttext="40.6\%" display="inline"><semantics id="S5.SS2.p7.9.m9.1a"><mrow id="S5.SS2.p7.9.m9.1.1" xref="S5.SS2.p7.9.m9.1.1.cmml"><mn id="S5.SS2.p7.9.m9.1.1.2" xref="S5.SS2.p7.9.m9.1.1.2.cmml">40.6</mn><mo id="S5.SS2.p7.9.m9.1.1.1" xref="S5.SS2.p7.9.m9.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p7.9.m9.1b"><apply id="S5.SS2.p7.9.m9.1.1.cmml" xref="S5.SS2.p7.9.m9.1.1"><csymbol cd="latexml" id="S5.SS2.p7.9.m9.1.1.1.cmml" xref="S5.SS2.p7.9.m9.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p7.9.m9.1.1.2.cmml" xref="S5.SS2.p7.9.m9.1.1.2">40.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p7.9.m9.1c">40.6\%</annotation></semantics></math>.</p>
</div>
<div id="S5.SS2.p8" class="ltx_para">
<p id="S5.SS2.p8.6" class="ltx_p">In summary, <math id="S5.SS2.p8.1.m1.1" class="ltx_Math" alttext="E_{3}" display="inline"><semantics id="S5.SS2.p8.1.m1.1a"><msub id="S5.SS2.p8.1.m1.1.1" xref="S5.SS2.p8.1.m1.1.1.cmml"><mi id="S5.SS2.p8.1.m1.1.1.2" xref="S5.SS2.p8.1.m1.1.1.2.cmml">E</mi><mn id="S5.SS2.p8.1.m1.1.1.3" xref="S5.SS2.p8.1.m1.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p8.1.m1.1b"><apply id="S5.SS2.p8.1.m1.1.1.cmml" xref="S5.SS2.p8.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p8.1.m1.1.1.1.cmml" xref="S5.SS2.p8.1.m1.1.1">subscript</csymbol><ci id="S5.SS2.p8.1.m1.1.1.2.cmml" xref="S5.SS2.p8.1.m1.1.1.2">ùê∏</ci><cn type="integer" id="S5.SS2.p8.1.m1.1.1.3.cmml" xref="S5.SS2.p8.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p8.1.m1.1c">E_{3}</annotation></semantics></math>:
1) increased the resolution of all images;
2) added <math id="S5.SS2.p8.2.m2.2" class="ltx_Math" alttext="1,077" display="inline"><semantics id="S5.SS2.p8.2.m2.2a"><mrow id="S5.SS2.p8.2.m2.2.3.2" xref="S5.SS2.p8.2.m2.2.3.1.cmml"><mn id="S5.SS2.p8.2.m2.1.1" xref="S5.SS2.p8.2.m2.1.1.cmml">1</mn><mo id="S5.SS2.p8.2.m2.2.3.2.1" xref="S5.SS2.p8.2.m2.2.3.1.cmml">,</mo><mn id="S5.SS2.p8.2.m2.2.2" xref="S5.SS2.p8.2.m2.2.2.cmml">077</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p8.2.m2.2b"><list id="S5.SS2.p8.2.m2.2.3.1.cmml" xref="S5.SS2.p8.2.m2.2.3.2"><cn type="integer" id="S5.SS2.p8.2.m2.1.1.cmml" xref="S5.SS2.p8.2.m2.1.1">1</cn><cn type="integer" id="S5.SS2.p8.2.m2.2.2.cmml" xref="S5.SS2.p8.2.m2.2.2">077</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p8.2.m2.2c">1,077</annotation></semantics></math> samples with <math id="S5.SS2.p8.3.m3.2" class="ltx_Math" alttext="1,561" display="inline"><semantics id="S5.SS2.p8.3.m3.2a"><mrow id="S5.SS2.p8.3.m3.2.3.2" xref="S5.SS2.p8.3.m3.2.3.1.cmml"><mn id="S5.SS2.p8.3.m3.1.1" xref="S5.SS2.p8.3.m3.1.1.cmml">1</mn><mo id="S5.SS2.p8.3.m3.2.3.2.1" xref="S5.SS2.p8.3.m3.2.3.1.cmml">,</mo><mn id="S5.SS2.p8.3.m3.2.2" xref="S5.SS2.p8.3.m3.2.2.cmml">561</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p8.3.m3.2b"><list id="S5.SS2.p8.3.m3.2.3.1.cmml" xref="S5.SS2.p8.3.m3.2.3.2"><cn type="integer" id="S5.SS2.p8.3.m3.1.1.cmml" xref="S5.SS2.p8.3.m3.1.1">1</cn><cn type="integer" id="S5.SS2.p8.3.m3.2.2.cmml" xref="S5.SS2.p8.3.m3.2.2">561</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p8.3.m3.2c">1,561</annotation></semantics></math> helicopters and utilized Copy-Paste augmentation to obtain a total of <math id="S5.SS2.p8.4.m4.2" class="ltx_Math" alttext="5,063" display="inline"><semantics id="S5.SS2.p8.4.m4.2a"><mrow id="S5.SS2.p8.4.m4.2.3.2" xref="S5.SS2.p8.4.m4.2.3.1.cmml"><mn id="S5.SS2.p8.4.m4.1.1" xref="S5.SS2.p8.4.m4.1.1.cmml">5</mn><mo id="S5.SS2.p8.4.m4.2.3.2.1" xref="S5.SS2.p8.4.m4.2.3.1.cmml">,</mo><mn id="S5.SS2.p8.4.m4.2.2" xref="S5.SS2.p8.4.m4.2.2.cmml">063</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p8.4.m4.2b"><list id="S5.SS2.p8.4.m4.2.3.1.cmml" xref="S5.SS2.p8.4.m4.2.3.2"><cn type="integer" id="S5.SS2.p8.4.m4.1.1.cmml" xref="S5.SS2.p8.4.m4.1.1">5</cn><cn type="integer" id="S5.SS2.p8.4.m4.2.2.cmml" xref="S5.SS2.p8.4.m4.2.2">063</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p8.4.m4.2c">5,063</annotation></semantics></math> helicopters;
3) and added a boundary-based loss for ‚Äúharbor,‚Äù ‚Äúplane,‚Äù and ‚Äúhelicopter.‚Äù
The overall mAP was improved from <math id="S5.SS2.p8.5.m5.1" class="ltx_Math" alttext="39.1\%" display="inline"><semantics id="S5.SS2.p8.5.m5.1a"><mrow id="S5.SS2.p8.5.m5.1.1" xref="S5.SS2.p8.5.m5.1.1.cmml"><mn id="S5.SS2.p8.5.m5.1.1.2" xref="S5.SS2.p8.5.m5.1.1.2.cmml">39.1</mn><mo id="S5.SS2.p8.5.m5.1.1.1" xref="S5.SS2.p8.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p8.5.m5.1b"><apply id="S5.SS2.p8.5.m5.1.1.cmml" xref="S5.SS2.p8.5.m5.1.1"><csymbol cd="latexml" id="S5.SS2.p8.5.m5.1.1.1.cmml" xref="S5.SS2.p8.5.m5.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p8.5.m5.1.1.2.cmml" xref="S5.SS2.p8.5.m5.1.1.2">39.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p8.5.m5.1c">39.1\%</annotation></semantics></math> to <math id="S5.SS2.p8.6.m6.1" class="ltx_Math" alttext="40.6\%" display="inline"><semantics id="S5.SS2.p8.6.m6.1a"><mrow id="S5.SS2.p8.6.m6.1.1" xref="S5.SS2.p8.6.m6.1.1.cmml"><mn id="S5.SS2.p8.6.m6.1.1.2" xref="S5.SS2.p8.6.m6.1.1.2.cmml">40.6</mn><mo id="S5.SS2.p8.6.m6.1.1.1" xref="S5.SS2.p8.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p8.6.m6.1b"><apply id="S5.SS2.p8.6.m6.1.1.cmml" xref="S5.SS2.p8.6.m6.1.1"><csymbol cd="latexml" id="S5.SS2.p8.6.m6.1.1.1.cmml" xref="S5.SS2.p8.6.m6.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p8.6.m6.1.1.2.cmml" xref="S5.SS2.p8.6.m6.1.1.2">40.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p8.6.m6.1c">40.6\%</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Expert Feedback and Discussion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.6" class="ltx_p">Following the case studies, we conducted six interviews to gather feedback from a group of experts, which included the three experts (<math id="S6.p1.1.m1.1" class="ltx_Math" alttext="E_{1}" display="inline"><semantics id="S6.p1.1.m1.1a"><msub id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml"><mi id="S6.p1.1.m1.1.1.2" xref="S6.p1.1.m1.1.1.2.cmml">E</mi><mn id="S6.p1.1.m1.1.1.3" xref="S6.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><apply id="S6.p1.1.m1.1.1.cmml" xref="S6.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S6.p1.1.m1.1.1.1.cmml" xref="S6.p1.1.m1.1.1">subscript</csymbol><ci id="S6.p1.1.m1.1.1.2.cmml" xref="S6.p1.1.m1.1.1.2">ùê∏</ci><cn type="integer" id="S6.p1.1.m1.1.1.3.cmml" xref="S6.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">E_{1}</annotation></semantics></math>, <math id="S6.p1.2.m2.1" class="ltx_Math" alttext="E_{2}" display="inline"><semantics id="S6.p1.2.m2.1a"><msub id="S6.p1.2.m2.1.1" xref="S6.p1.2.m2.1.1.cmml"><mi id="S6.p1.2.m2.1.1.2" xref="S6.p1.2.m2.1.1.2.cmml">E</mi><mn id="S6.p1.2.m2.1.1.3" xref="S6.p1.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S6.p1.2.m2.1b"><apply id="S6.p1.2.m2.1.1.cmml" xref="S6.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S6.p1.2.m2.1.1.1.cmml" xref="S6.p1.2.m2.1.1">subscript</csymbol><ci id="S6.p1.2.m2.1.1.2.cmml" xref="S6.p1.2.m2.1.1.2">ùê∏</ci><cn type="integer" id="S6.p1.2.m2.1.1.3.cmml" xref="S6.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.2.m2.1c">E_{2}</annotation></semantics></math>, <math id="S6.p1.3.m3.1" class="ltx_Math" alttext="E_{3}" display="inline"><semantics id="S6.p1.3.m3.1a"><msub id="S6.p1.3.m3.1.1" xref="S6.p1.3.m3.1.1.cmml"><mi id="S6.p1.3.m3.1.1.2" xref="S6.p1.3.m3.1.1.2.cmml">E</mi><mn id="S6.p1.3.m3.1.1.3" xref="S6.p1.3.m3.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S6.p1.3.m3.1b"><apply id="S6.p1.3.m3.1.1.cmml" xref="S6.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S6.p1.3.m3.1.1.1.cmml" xref="S6.p1.3.m3.1.1">subscript</csymbol><ci id="S6.p1.3.m3.1.1.2.cmml" xref="S6.p1.3.m3.1.1.2">ùê∏</ci><cn type="integer" id="S6.p1.3.m3.1.1.3.cmml" xref="S6.p1.3.m3.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.3.m3.1c">E_{3}</annotation></semantics></math>) who collaborated with us in the case studies and three additional experts we invited (<math id="S6.p1.4.m4.1" class="ltx_Math" alttext="E_{4}" display="inline"><semantics id="S6.p1.4.m4.1a"><msub id="S6.p1.4.m4.1.1" xref="S6.p1.4.m4.1.1.cmml"><mi id="S6.p1.4.m4.1.1.2" xref="S6.p1.4.m4.1.1.2.cmml">E</mi><mn id="S6.p1.4.m4.1.1.3" xref="S6.p1.4.m4.1.1.3.cmml">4</mn></msub><annotation-xml encoding="MathML-Content" id="S6.p1.4.m4.1b"><apply id="S6.p1.4.m4.1.1.cmml" xref="S6.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S6.p1.4.m4.1.1.1.cmml" xref="S6.p1.4.m4.1.1">subscript</csymbol><ci id="S6.p1.4.m4.1.1.2.cmml" xref="S6.p1.4.m4.1.1.2">ùê∏</ci><cn type="integer" id="S6.p1.4.m4.1.1.3.cmml" xref="S6.p1.4.m4.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.4.m4.1c">E_{4}</annotation></semantics></math>, <math id="S6.p1.5.m5.1" class="ltx_Math" alttext="E_{5}" display="inline"><semantics id="S6.p1.5.m5.1a"><msub id="S6.p1.5.m5.1.1" xref="S6.p1.5.m5.1.1.cmml"><mi id="S6.p1.5.m5.1.1.2" xref="S6.p1.5.m5.1.1.2.cmml">E</mi><mn id="S6.p1.5.m5.1.1.3" xref="S6.p1.5.m5.1.1.3.cmml">5</mn></msub><annotation-xml encoding="MathML-Content" id="S6.p1.5.m5.1b"><apply id="S6.p1.5.m5.1.1.cmml" xref="S6.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S6.p1.5.m5.1.1.1.cmml" xref="S6.p1.5.m5.1.1">subscript</csymbol><ci id="S6.p1.5.m5.1.1.2.cmml" xref="S6.p1.5.m5.1.1.2">ùê∏</ci><cn type="integer" id="S6.p1.5.m5.1.1.3.cmml" xref="S6.p1.5.m5.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.5.m5.1c">E_{5}</annotation></semantics></math>, <math id="S6.p1.6.m6.1" class="ltx_Math" alttext="E_{6}" display="inline"><semantics id="S6.p1.6.m6.1a"><msub id="S6.p1.6.m6.1.1" xref="S6.p1.6.m6.1.1.cmml"><mi id="S6.p1.6.m6.1.1.2" xref="S6.p1.6.m6.1.1.2.cmml">E</mi><mn id="S6.p1.6.m6.1.1.3" xref="S6.p1.6.m6.1.1.3.cmml">6</mn></msub><annotation-xml encoding="MathML-Content" id="S6.p1.6.m6.1b"><apply id="S6.p1.6.m6.1.1.cmml" xref="S6.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S6.p1.6.m6.1.1.1.cmml" xref="S6.p1.6.m6.1.1">subscript</csymbol><ci id="S6.p1.6.m6.1.1.2.cmml" xref="S6.p1.6.m6.1.1.2">ùê∏</ci><cn type="integer" id="S6.p1.6.m6.1.1.3.cmml" xref="S6.p1.6.m6.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.6.m6.1c">E_{6}</annotation></semantics></math>).
The newly invited experts were Ph.D.¬†students who had over two years of experience in computer vision research.
Each interview lasted between 40 to 65 minutes.
Overall, the expert feedback was positive regarding the usability of Uni-Evaluator.
However, some limitations were also identified, highlighting areas requiring further investigation in the future.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Usability</h3>

<div id="S6.SS1.p1" class="ltx_para ltx_noindent">
<p id="S6.SS1.p1.1" class="ltx_p"><span id="S6.SS1.p1.1.1" class="ltx_text ltx_font_bold">Generalization to other tasks</span>.
In the current implementation, Uni-Evaluator supports three main tasks in computer vision: classification, object detection, and instance segmentation.
According to the experts, Uni-Evaluator can also be utilized for evaluating models in other computer vision tasks.
Here we take the visual generation task as an example to illustrate the extension idea.
This task involves encoding training samples into latent vectors and then reconstructing the original training samples.
When evaluating models in such tasks, the experts are interested in analyzing the reconstruction errors of samples.
To do the analysis, they can cluster samples and then analyze the errors between different clusters in the matrix-based visualization.
Meanwhile, the table visualization and the grid visualization are directly applicable to exploring the subsets and samples of interest.
The semantic segmentation task can also be analyzed in Uni-Evaluator as a pixel-level classification task.
However, since this task usually has a significant class imbalance, such a pixel-level analysis may hinder the poor performance analysis on minority classes.
In addition, focusing only on pixel-level information results in a loss of object-level information (the shapes of objects), which is also critical for evaluating semantic segmentation models¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. Therefore, it is essential to study metrics that are insensitive to class imbalance (<em id="S6.SS1.p1.1.2" class="ltx_emph ltx_font_italic">e.g</em>.‚Äâ, mIoU) and integrate pixel- and object-level information in Uni-Evaluator, which is a promising research topic in the future.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para ltx_noindent">
<p id="S6.SS1.p2.2" class="ltx_p"><span id="S6.SS1.p2.2.1" class="ltx_text ltx_font_bold">Diagnosing causes of poor performance</span>.
All the experts commented positively on the design of the three modes in the matrix-based visualization,
which enables the identification of the specific reasons for poor performance.
In the instance segmentation case study, using the three modes, <math id="S6.SS1.p2.1.m1.1" class="ltx_Math" alttext="E_{3}" display="inline"><semantics id="S6.SS1.p2.1.m1.1a"><msub id="S6.SS1.p2.1.m1.1.1" xref="S6.SS1.p2.1.m1.1.1.cmml"><mi id="S6.SS1.p2.1.m1.1.1.2" xref="S6.SS1.p2.1.m1.1.1.2.cmml">E</mi><mn id="S6.SS1.p2.1.m1.1.1.3" xref="S6.SS1.p2.1.m1.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.1.m1.1b"><apply id="S6.SS1.p2.1.m1.1.1.cmml" xref="S6.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S6.SS1.p2.1.m1.1.1.1.cmml" xref="S6.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S6.SS1.p2.1.m1.1.1.2.cmml" xref="S6.SS1.p2.1.m1.1.1.2">ùê∏</ci><cn type="integer" id="S6.SS1.p2.1.m1.1.1.3.cmml" xref="S6.SS1.p2.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.1.m1.1c">E_{3}</annotation></semantics></math> successfully identified that the confusion between ‚Äúsmall vehicle‚Äù and ‚Äúlarge vehicle‚Äù was due to the small sizes of the ‚Äúsmall vehicle.‚Äù
Furthermore, <math id="S6.SS1.p2.2.m2.1" class="ltx_Math" alttext="E_{1}" display="inline"><semantics id="S6.SS1.p2.2.m2.1a"><msub id="S6.SS1.p2.2.m2.1.1" xref="S6.SS1.p2.2.m2.1.1.cmml"><mi id="S6.SS1.p2.2.m2.1.1.2" xref="S6.SS1.p2.2.m2.1.1.2.cmml">E</mi><mn id="S6.SS1.p2.2.m2.1.1.3" xref="S6.SS1.p2.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.2.m2.1b"><apply id="S6.SS1.p2.2.m2.1.1.cmml" xref="S6.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S6.SS1.p2.2.m2.1.1.1.cmml" xref="S6.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="S6.SS1.p2.2.m2.1.1.2.cmml" xref="S6.SS1.p2.2.m2.1.1.2">ùê∏</ci><cn type="integer" id="S6.SS1.p2.2.m2.1.1.3.cmml" xref="S6.SS1.p2.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.2.m2.1c">E_{1}</annotation></semantics></math> noted that the table visualization facilitated diagnosing the different causes of poor performance in different subsets.
‚ÄúUsing the table visualization, I quickly identified that the size errors of class ‚Äòtraffic light‚Äô were caused by both the blurred images and the annotation errors in the test samples,‚Äù he commented.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Limitations and Future Work</h3>

<div id="S6.SS2.p1" class="ltx_para ltx_noindent">
<p id="S6.SS2.p1.1" class="ltx_p"><span id="S6.SS2.p1.1.1" class="ltx_text ltx_font_bold">Identifying data subsets by semantic attributes</span>.
Uni-Evaluator currently slices the subsets along the low-level attributes of objects, such as sizes and aspect ratios.
It does not support identifying problematic subsets based on semantic attributes that describe visual appearances.
For example, <math id="S6.SS2.p1.1.m1.1" class="ltx_Math" alttext="E_{2}" display="inline"><semantics id="S6.SS2.p1.1.m1.1a"><msub id="S6.SS2.p1.1.m1.1.1" xref="S6.SS2.p1.1.m1.1.1.cmml"><mi id="S6.SS2.p1.1.m1.1.1.2" xref="S6.SS2.p1.1.m1.1.1.2.cmml">E</mi><mn id="S6.SS2.p1.1.m1.1.1.3" xref="S6.SS2.p1.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.1.m1.1b"><apply id="S6.SS2.p1.1.m1.1.1.cmml" xref="S6.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S6.SS2.p1.1.m1.1.1.1.cmml" xref="S6.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S6.SS2.p1.1.m1.1.1.2.cmml" xref="S6.SS2.p1.1.m1.1.1.2">ùê∏</ci><cn type="integer" id="S6.SS2.p1.1.m1.1.1.3.cmml" xref="S6.SS2.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.1.m1.1c">E_{2}</annotation></semantics></math> observed that their object detection models often failed to detect objects with some specific appearances,
such as a person on a bridge.
However, such semantic attributes usually require expensive manual annotations¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>.
One potential method is to use disentangled representation learning to extract candidate semantic attributes and then allow users to select meaningful ones¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para ltx_noindent">
<p id="S6.SS2.p2.2" class="ltx_p"><span id="S6.SS2.p2.2.1" class="ltx_text ltx_font_bold">Effective model comparison</span>.
Uni-Evaluator is currently designed for evaluating a single model. However, the experts also want to compare different models for various purposes¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>.
For example, <math id="S6.SS2.p2.1.m1.1" class="ltx_Math" alttext="E_{4}" display="inline"><semantics id="S6.SS2.p2.1.m1.1a"><msub id="S6.SS2.p2.1.m1.1.1" xref="S6.SS2.p2.1.m1.1.1.cmml"><mi id="S6.SS2.p2.1.m1.1.1.2" xref="S6.SS2.p2.1.m1.1.1.2.cmml">E</mi><mn id="S6.SS2.p2.1.m1.1.1.3" xref="S6.SS2.p2.1.m1.1.1.3.cmml">4</mn></msub><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.1.m1.1b"><apply id="S6.SS2.p2.1.m1.1.1.cmml" xref="S6.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S6.SS2.p2.1.m1.1.1.1.cmml" xref="S6.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S6.SS2.p2.1.m1.1.1.2.cmml" xref="S6.SS2.p2.1.m1.1.1.2">ùê∏</ci><cn type="integer" id="S6.SS2.p2.1.m1.1.1.3.cmml" xref="S6.SS2.p2.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.1.m1.1c">E_{4}</annotation></semantics></math> indicated that Transformer-based object detection models usually outperformed CNN-based object detection models,
whereas the latter often exhibited higher inference speeds in real-world applications¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>.
To exploit the strengths of both, <math id="S6.SS2.p2.2.m2.1" class="ltx_Math" alttext="E_{4}" display="inline"><semantics id="S6.SS2.p2.2.m2.1a"><msub id="S6.SS2.p2.2.m2.1.1" xref="S6.SS2.p2.2.m2.1.1.cmml"><mi id="S6.SS2.p2.2.m2.1.1.2" xref="S6.SS2.p2.2.m2.1.1.2.cmml">E</mi><mn id="S6.SS2.p2.2.m2.1.1.3" xref="S6.SS2.p2.2.m2.1.1.3.cmml">4</mn></msub><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.2.m2.1b"><apply id="S6.SS2.p2.2.m2.1.1.cmml" xref="S6.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S6.SS2.p2.2.m2.1.1.1.cmml" xref="S6.SS2.p2.2.m2.1.1">subscript</csymbol><ci id="S6.SS2.p2.2.m2.1.1.2.cmml" xref="S6.SS2.p2.2.m2.1.1.2">ùê∏</ci><cn type="integer" id="S6.SS2.p2.2.m2.1.1.3.cmml" xref="S6.SS2.p2.2.m2.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.2.m2.1c">E_{4}</annotation></semantics></math> wanted to compare their performance, identify what made the Transformer-based models perform better, and then integrate it into the CNN-based models.
Therefore, investigating how Uni-Evaluator can effectively support model comparison is a promising future research direction.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">We present Uni-Evaluator, a visual analysis tool that supports a unified interactive model evaluation for computer vision tasks.
From a survey conducted with 151 computer vision experts, we distill three design goals for a unified model evaluation and then derive four tasks from the design goals.
Based on the derived tasks,
we propose a unified probability distribution method that models both continuous and discrete predictions in a unified manner.
With the unified probabilistic modeling,
we develop three coordinated visualizations to facilitate a comprehensive model evaluation from a global overview to detailed samples.
Two case studies are conducted to demonstrate the effectiveness of Uni-Evaluator in
improving the model and the associated data in object detection and instance segmentation.

</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
This work was supported by the National Natural Science Foundation of China under grants U21A20469, 61936002, and 92248303, the National Key R&amp;D Program of China under Grant 2020YFB2104100, grants from the Institute Guo Qiang, THUIBCS, and BLBCI, and in part by Tsinghua-Kuaishou Institute of Future Media Data.
The authors would like to thank Dr.¬†Xizhou Zhu, Dr.¬†Mengchen Liu, Guo-Ye Yang, Dr.¬†Liuyu Xiang, and Dr.¬†Xiaohan Wang for their contributions to the case studies, Lanxi Xiao for her valuable comments on the visualization design, Zhen Li for implementing parts of the matrix-based visualization and the filtering function, and Yiwei Hou for voicing our video.



</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
M.¬†Abadi, A.¬†Agarwal, P.¬†Barham, E.¬†Brevdo, Z.¬†Chen, C.¬†Citro, G.¬†S. Corrado,
A.¬†Davis, J.¬†Dean, M.¬†Devin, et¬†al.

</span>
<span class="ltx_bibblock">Tensorflow: Large-scale machine learning on heterogeneous distributed
systems.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1603.04467, 2016.
<a target="_blank" href="https://doi.org/10.48550/arXiv.1603.04467" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ48550/arXiv‚ÄÜ.‚ÄÜ1603‚ÄÜ.‚ÄÜ04467</a>

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
B.¬†Alsallakh, A.¬†Hanbury, H.¬†Hauser, S.¬†Miksch, and A.¬†Rauber.

</span>
<span class="ltx_bibblock">Visual methods for analyzing probabilistic classification data.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>,
20(12):1703‚Äì1712, 2014. <a target="_blank" href="https://doi.org/10.1109/TVCG.2014.2346660" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1109/TVCG‚ÄÜ.‚ÄÜ2014‚ÄÜ.‚ÄÜ2346660</a>

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
S.¬†Amershi, M.¬†Chickering, S.¬†M. Drucker, B.¬†Lee, P.¬†Simard, and J.¬†Suh.

</span>
<span class="ltx_bibblock">ModelTracker: Redesigning performance analysis tools for machine
learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Proceedings of the ACM CHI Conference on Human Factors in
Computing Systems</span>, pp. 337‚Äì346. Seoul, 2015.
<a target="_blank" href="https://doi.org/10.1145/2702123.2702509" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1145/2702123‚ÄÜ.‚ÄÜ2702509</a>

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
R.¬†Arias-Hernandez, L.¬†T. Kaastra, T.¬†M. Green, and B.¬†Fisher.

</span>
<span class="ltx_bibblock">Pair analytics: Capturing reasoning processes in collaborative visual
analytics.

</span>
<span class="ltx_bibblock">In <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">IEEE Hawaii International Conference on System Sciences</span>, pp.
1‚Äì10. Koloa, Kauai, 2011. <a target="_blank" href="https://doi.org/10.1109/HICSS.2011.339" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1109/HICSS‚ÄÜ.‚ÄÜ2011‚ÄÜ.‚ÄÜ339</a>

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Z.¬†Bar-Joseph, D.¬†K. Gifford, and T.¬†S. Jaakkola.

</span>
<span class="ltx_bibblock">Fast optimal leaf ordering for hierarchical clustering.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Bioinformatics</span>, 17(suppl_1):S22‚ÄìS29, 2001.
<a target="_blank" href="https://doi.org/10.1093/bioinformatics/17.suppl_1.S22" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1093/bioinformatics/17‚ÄÜ.‚ÄÜsuppl_1‚ÄÜ.‚ÄÜS22</a>

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
D.¬†Bertucci, M.¬†M. Hamid, Y.¬†Anand, A.¬†Ruangrotsakun, D.¬†Tabatabai, M.¬†Perez,
and M.¬†Kahng.

</span>
<span class="ltx_bibblock">DendroMap: Visual exploration of large-scale image datasets for
machine learning with treemaps.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>,
29(1):320‚Äì330, 2023. <a target="_blank" href="https://doi.org/10.1109/TVCG.2022.3209425" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1109/TVCG‚ÄÜ.‚ÄÜ2022‚ÄÜ.‚ÄÜ3209425</a>

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
L.¬†Biewald.

</span>
<span class="ltx_bibblock">Weights &amp; Biases.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://wandb.ai/site/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://wandb.ai/site/</a>, 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A.¬†Bilal, A.¬†Jourabloo, M.¬†Ye, X.¬†Liu, and L.¬†Ren.

</span>
<span class="ltx_bibblock">Do convolutional neural networks learn class hierarchy?

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>,
24(1):152‚Äì162, 2018. <a target="_blank" href="https://doi.org/10.1109/TVCG.2017.2744683" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1109/TVCG‚ÄÜ.‚ÄÜ2017‚ÄÜ.‚ÄÜ2744683</a>

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
D.¬†Bolya, S.¬†Foley, J.¬†Hays, and J.¬†Hoffman.

</span>
<span class="ltx_bibblock">TIDE: A general toolbox for identifying object detection errors.

</span>
<span class="ltx_bibblock">In <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision</span>,
pp. 558‚Äì573. Glasgow, 2020.
<a target="_blank" href="https://doi.org/10.1007/978-3-030-58580-8_33" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1007/978-3-030-58580-8_33</a>

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
A.¬†Borji and S.¬†M. Iranmanesh.

</span>
<span class="ltx_bibblock">Empirical upper bound in object detection and more.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1911.12451, 2019.
<a target="_blank" href="https://doi.org/10.48550/arXiv.1911.12451" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ48550/arXiv‚ÄÜ.‚ÄÜ1911‚ÄÜ.‚ÄÜ12451</a>

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
M.¬†Boull√©.

</span>
<span class="ltx_bibblock">MODL: A Bayes optimal discretization method for continuous
attributes.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Machine Learning</span>, 65:131‚Äì165, 2006.
<a target="_blank" href="https://doi.org/10.1007/s10994-006-8364-x" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1007/s10994-006-8364-x</a>

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
J.¬†Cao, H.¬†Cholakkal, R.¬†M. Anwer, F.¬†S. Khan, Y.¬†Pang, and L.¬†Shao.

</span>
<span class="ltx_bibblock">D2Det: Towards high quality object detection and instance
segmentation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pp. 11485‚Äì11494. Seattle, 2020.
<a target="_blank" href="https://doi.org/10.1109/cvpr42600.2020.01150" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1109/cvpr42600‚ÄÜ.‚ÄÜ2020‚ÄÜ.‚ÄÜ01150</a>

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
C.¬†Chen, Z.¬†Wang, J.¬†Wu, X.¬†Wang, L.-Z. Guo, Y.-F. Li, and S.¬†Liu.

</span>
<span class="ltx_bibblock">Interactive graph construction for graph-based semi-supervised
learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>,
27(9):3701‚Äì3716, 2021. <a target="_blank" href="https://doi.org/10.1109/TVCG.2021.3084694" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1109/TVCG‚ÄÜ.‚ÄÜ2021‚ÄÜ.‚ÄÜ3084694</a>

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
C.¬†Chen, J.¬†Wu, X.¬†Wang, S.¬†Xiang, S.-H. Zhang, Q.¬†Tang, and S.¬†Liu.

</span>
<span class="ltx_bibblock">Towards better caption supervision for object detection.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>,
28(4):1941‚Äì1954, 2022. <a target="_blank" href="https://doi.org/10.1109/TVCG.2021.3138933" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1109/TVCG‚ÄÜ.‚ÄÜ2021‚ÄÜ.‚ÄÜ3138933</a>

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
C.¬†Chen, J.¬†Yuan, Y.¬†Lu, Y.¬†Liu, H.¬†Su, S.¬†Yuan, and S.¬†Liu.

</span>
<span class="ltx_bibblock">OoDAnalyzer: Interactive analysis of out-of-distribution samples.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>,
27(7):3335‚Äì3349, 2021. <a target="_blank" href="https://doi.org/10.1109/TVCG.2020.2973258" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1109/TVCG‚ÄÜ.‚ÄÜ2020‚ÄÜ.‚ÄÜ2973258</a>

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
J.¬†F. DeRose, J.¬†Wang, and M.¬†Berger.

</span>
<span class="ltx_bibblock">Attention flows: Analyzing and comparing attention mechanisms in
language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>,
27(2):1160‚Äì1170, 2021. <a target="_blank" href="https://doi.org/10.1109/TVCG.2020.3028976" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1109/TVCG‚ÄÜ.‚ÄÜ2020‚ÄÜ.‚ÄÜ3028976</a>

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Y.¬†Fang, W.¬†Wang, B.¬†Xie, Q.¬†Sun, L.¬†Wu, X.¬†Wang, T.¬†Huang, X.¬†Wang, and
Y.¬†Cao.

</span>
<span class="ltx_bibblock">EVA: Exploring the limits of masked visual representation learning
at scale.

</span>
<span class="ltx_bibblock">In <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pp. 19358‚Äì19369. Vancouver, 2023.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
G.¬†Ghiasi, Y.¬†Cui, A.¬†Srinivas, R.¬†Qian, T.-Y. Lin, E.¬†D. Cubuk, Q.¬†V. Le, and
B.¬†Zoph.

</span>
<span class="ltx_bibblock">Simple copy-paste is a strong data augmentation method for instance
segmentation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pp. 2918‚Äì2928, 2021.
<a target="_blank" href="https://doi.org/10.1109/cvpr46437.2021.00294" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1109/cvpr46437‚ÄÜ.‚ÄÜ2021‚ÄÜ.‚ÄÜ00294</a>

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
R.¬†Girshick.

</span>
<span class="ltx_bibblock">Fast R-CNN.

</span>
<span class="ltx_bibblock">In <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision</span>, pp. 1440‚Äì1448. Santiago, 2015.
<a target="_blank" href="https://doi.org/10.1109/ICCV.2015.169" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1109/ICCV‚ÄÜ.‚ÄÜ2015‚ÄÜ.‚ÄÜ169</a>

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
M.¬†Gleicher, A.¬†Barve, X.¬†Yu, and F.¬†Heimerl.

</span>
<span class="ltx_bibblock">Boxer: Interactive comparison of classifier results.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Computer Graphics Forum</span>, 39(3):181‚Äì193, 2020.
<a target="_blank" href="https://doi.org/10.1111/cgf.13972" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1111/cgf‚ÄÜ.‚ÄÜ13972</a>

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
J.¬†G√∂rtler, F.¬†Hohman, D.¬†Moritz, K.¬†Wongsuphasawat, D.¬†Ren, R.¬†Nair,
M.¬†Kirchner, and K.¬†Patel.

</span>
<span class="ltx_bibblock">Neo: Generalizing confusion matrix visualization to hierarchical and
multi-output labels.

</span>
<span class="ltx_bibblock">In <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Proceedings of the ACM CHI Conference on Human Factors in
Computing Systems</span>, pp. 1‚Äì13. New Orleans, 2022.
<a target="_blank" href="https://doi.org/10.1145/3491102.3501823" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1145/3491102‚ÄÜ.‚ÄÜ3501823</a>

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
L.¬†Gou, L.¬†Zou, N.¬†Li, M.¬†Hofmann, A.¬†K. Shekar, A.¬†Wendt, and L.¬†Ren.

</span>
<span class="ltx_bibblock">VATLD: A visual analytics system to assess, understand and improve
traffic light detection.

</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>,
27(2):261‚Äì271, 2021. <a target="_blank" href="https://doi.org/10.1109/TVCG.2020.3030350" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1109/TVCG‚ÄÜ.‚ÄÜ2020‚ÄÜ.‚ÄÜ3030350</a>

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
S.¬†Gratzl, A.¬†Lex, N.¬†Gehlenborg, H.¬†Pfister, and M.¬†Streit.

</span>
<span class="ltx_bibblock">LineUp: Visual analysis of multi-attribute rankings.

</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>,
19(12):2277‚Äì2286, 2013. <a target="_blank" href="https://doi.org/10.1109/TVCG.2013.173" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1109/TVCG‚ÄÜ.‚ÄÜ2013‚ÄÜ.‚ÄÜ173</a>

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
J.¬†Han, H.¬†Cheng, D.¬†Xin, and X.¬†Yan.

</span>
<span class="ltx_bibblock">Frequent pattern mining: current status and future directions.

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Data Mining and Knowledge Discovery</span>, 15(1):55‚Äì86, 2007.
<a target="_blank" href="https://doi.org/10.1007/s10618-006-0059-1" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1007/s10618-006-0059-1</a>

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
K.¬†He, X.¬†Zhang, S.¬†Ren, and J.¬†Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pp. 770‚Äì778. Las Vegas, 2016.
<a target="_blank" href="https://doi.org/10.1109/CVPR.2016.90" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1109/CVPR‚ÄÜ.‚ÄÜ2016‚ÄÜ.‚ÄÜ90</a>

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
W.¬†He, L.¬†Zou, A.¬†K. Shekar, L.¬†Gou, and L.¬†Ren.

</span>
<span class="ltx_bibblock">Where can we help? a visual analytics approach to diagnosing and
improving semantic segmentation of movable objects.

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>,
28(1):1040‚Äì1050, 2021. <a target="_blank" href="https://doi.org/10.1109/TVCG.2021.3114855" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1109/TVCG‚ÄÜ.‚ÄÜ2021‚ÄÜ.‚ÄÜ3114855</a>

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
F.¬†Heyen, T.¬†Munz, M.¬†Neumann, D.¬†Ortega, N.¬†T. Vu, D.¬†Weiskopf, and
M.¬†Sedlmair.

</span>
<span class="ltx_bibblock">ClaVis: An interactive visual comparison system for classifiers.

</span>
<span class="ltx_bibblock">In <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Proceedings of the ACM International Conference on Advanced
Visual Interfaces</span>, pp. 1‚Äì9. Island of Ischia, 2020.
<a target="_blank" href="https://doi.org/10.1145/3399715.3399814" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1145/3399715‚ÄÜ.‚ÄÜ3399814</a>

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
A.¬†Hinterreiter, P.¬†Ruch, H.¬†Stitz, M.¬†Ennemoser, J.¬†Bernard, H.¬†Strobelt, and
M.¬†Streit.

</span>
<span class="ltx_bibblock">ConfusionFlow: A model-agnostic visualization for temporal analysis
of classifier confusion.

</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>,
28(2):1222‚Äì1236, 2022. <a target="_blank" href="https://doi.org/10.1109/TVCG.2020.3012063" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1109/TVCG‚ÄÜ.‚ÄÜ2020‚ÄÜ.‚ÄÜ3012063</a>

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
F.¬†Hohman, M.¬†Kahng, R.¬†Pienta, and D.¬†H. Chau.

</span>
<span class="ltx_bibblock">Visual analytics in deep learning: An interrogative survey for the
next frontiers.

</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>,
25(8):2674‚Äì2693, 2019. <a target="_blank" href="https://doi.org/10.1109/TVCG.2018.2843369" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1109/TVCG‚ÄÜ.‚ÄÜ2018‚ÄÜ.‚ÄÜ2843369</a>

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
D.¬†Hoiem, Y.¬†Chodpathumwan, and Q.¬†Dai.

</span>
<span class="ltx_bibblock">Diagnosing error in object detectors.

</span>
<span class="ltx_bibblock">In <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision</span>,
pp. 340‚Äì353. Florence, 2012.
<a target="_blank" href="https://doi.org/10.1007/978-3-642-33712-3_25" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1007/978-3-642-33712-3_25</a>

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
L.¬†Hou, K.¬†Lu, J.¬†Xue, and Y.¬†Li.

</span>
<span class="ltx_bibblock">Shape-adaptive selection and measurement for oriented object
detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</span>, pp. 923‚Äì932, 2022.
<a target="_blank" href="https://doi.org/10.1609/aaai.v36i1.19975" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1609/aaai‚ÄÜ.‚ÄÜv36i1‚ÄÜ.‚ÄÜ19975</a>

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
D.¬†Jadhav and T.¬†Ramanathan.

</span>
<span class="ltx_bibblock">Parametric and non-parametric estimation of value-at-risk.

</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">The Journal of Risk Model Validation</span>, 3(1):51, 2009.
<a target="_blank" href="https://doi.org/doi.org/10.5539/ijbm.v8n11p103" title="" class="ltx_ref ltx_href">doi: doi‚ÄÜ.‚ÄÜorg/10‚ÄÜ.‚ÄÜ5539/ijbm‚ÄÜ.‚ÄÜv8n11p103</a>

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
J.¬†M. Johnson and T.¬†M. Khoshgoftaar.

</span>
<span class="ltx_bibblock">Survey on deep learning with class imbalance.

</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">Journal of Big Data</span>, 6(1):1‚Äì54, 2019.
<a target="_blank" href="https://doi.org/10.1186/s40537-019-0192-5" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1186/s40537-019-0192-5</a>

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
H.¬†Kervadec, J.¬†Bouchtiba, C.¬†Desrosiers, E.¬†Granger, J.¬†Dolz, and I.¬†B. Ayed.

</span>
<span class="ltx_bibblock">Boundary loss for highly unbalanced segmentation.

</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">Medical Image Analysis</span>, 67:101851, 2021.
<a target="_blank" href="https://doi.org/10.1016/j.media.2020.101851" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1016/j‚ÄÜ.‚ÄÜmedia‚ÄÜ.‚ÄÜ2020‚ÄÜ.‚ÄÜ101851</a>

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
N.¬†Lei, D.¬†An, Y.¬†Guo, K.¬†Su, S.¬†Liu, Z.¬†Luo, S.-T. Yau, and X.¬†Gu.

</span>
<span class="ltx_bibblock">A geometric understanding of deep learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">Engineering</span>, 6(3):361‚Äì374, 2020.
<a target="_blank" href="https://doi.org/10.1016/j.eng.2019.09.010" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1016/j‚ÄÜ.‚ÄÜeng‚ÄÜ.‚ÄÜ2019‚ÄÜ.‚ÄÜ09‚ÄÜ.‚ÄÜ010</a>

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Z.¬†Li, X.¬†Wang, W.¬†Yang, J.¬†Wu, Z.¬†Zhang, Z.¬†Liu, M.¬†Sun, H.¬†Zhang, and S.¬†Liu.

</span>
<span class="ltx_bibblock">A unified understanding of deep NLP models for text classification.

</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>,
28(12):4980‚Äì4994, 2022. <a target="_blank" href="https://doi.org/10.1109/TVCG.2022.3184186" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1109/TVCG‚ÄÜ.‚ÄÜ2022‚ÄÜ.‚ÄÜ3184186</a>

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
T.-Y. Lin, M.¬†Maire, S.¬†Belongie, J.¬†Hays, P.¬†Perona, D.¬†Ramanan,
P.¬†Doll√°r, and C.¬†L. Zitnick.

</span>
<span class="ltx_bibblock">Microsoft COCO: Common objects in context.

</span>
<span class="ltx_bibblock">In <span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision</span>,
pp. 740‚Äì755. Z√ºrich, 2014.
<a target="_blank" href="https://doi.org/10.1007/978-3-319-10602-1_48" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1007/978-3-319-10602-1_48</a>

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
L.¬†Liu, W.¬†Ouyang, X.¬†Wang, P.¬†Fieguth, J.¬†Chen, X.¬†Liu, and
M.¬†Pietik√§inen.

</span>
<span class="ltx_bibblock">Deep learning for generic object detection: A survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">International Journal of Computer Vision</span>, 128:261‚Äì318, 2020.
<a target="_blank" href="https://doi.org/10.1007/s11263-019-01247-4" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1007/s11263-019-01247-4</a>

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
M.¬†Liu, S.¬†Liu, H.¬†Su, K.¬†Cao, and J.¬†Zhu.

</span>
<span class="ltx_bibblock">Analyzing the noise robustness of deep neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Visual Analytics
Science and Technology</span>, pp. 60‚Äì71. Berlin, 2018.
<a target="_blank" href="https://doi.org/10.1109/VAST.2018.8802509" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1109/VAST‚ÄÜ.‚ÄÜ2018‚ÄÜ.‚ÄÜ8802509</a>

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
M.¬†Liu, J.¬†Shi, Z.¬†Li, C.¬†Li, J.¬†Zhu, and S.¬†Liu.

</span>
<span class="ltx_bibblock">Towards better analysis of deep convolutional neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>,
23(1):91‚Äì100, 2017. <a target="_blank" href="https://doi.org/10.1109/TVCG.2016.2598831" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1109/TVCG‚ÄÜ.‚ÄÜ2016‚ÄÜ.‚ÄÜ2598831</a>

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Y.¬†Liu, H.¬†Li, C.¬†Hu, S.¬†Luo, H.¬†Shen, and C.¬†W. Chen.

</span>
<span class="ltx_bibblock">Learning to aggregate multi-scale context for instance segmentation
in remote sensing images.

</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/2111.11057, 2021.
<a target="_blank" href="https://doi.org/10.48550/arXiv.2111.11057" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ48550/arXiv‚ÄÜ.‚ÄÜ2111‚ÄÜ.‚ÄÜ11057</a>

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Z.¬†Liu, Y.¬†Lin, Y.¬†Cao, H.¬†Hu, Y.¬†Wei, Z.¬†Zhang, S.¬†Lin, and B.¬†Guo.

</span>
<span class="ltx_bibblock">Swin transformer: Hierarchical vision transformer using shifted
windows.

</span>
<span class="ltx_bibblock">In <span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span>, pp. 10012‚Äì10022. Montreal, 2021.
<a target="_blank" href="https://doi.org/10.1109/iccv48922.2021.00986" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1109/iccv48922‚ÄÜ.‚ÄÜ2021‚ÄÜ.‚ÄÜ00986</a>

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Z.¬†Liu, H.¬†Mao, C.-Y. Wu, C.¬†Feichtenhofer, T.¬†Darrell, and S.¬†Xie.

</span>
<span class="ltx_bibblock">A ConvNet for the 2020s.

</span>
<span class="ltx_bibblock">In <span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pp. 11976‚Äì11986. New Orleans, 2022.
<a target="_blank" href="https://doi.org/10.1109/cvpr52688.2022.01167" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1109/cvpr52688‚ÄÜ.‚ÄÜ2022‚ÄÜ.‚ÄÜ01167</a>

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
L.¬†v.¬†d. Maaten and G.¬†Hinton.

</span>
<span class="ltx_bibblock">Visualizing data using t-SNE.

</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text ltx_font_italic">Journal of Machine Learning Research</span>, 9(86):2579‚Äì2605, 2008.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
S.¬†Minaee, Y.¬†Y. Boykov, F.¬†Porikli, A.¬†J. Plaza, N.¬†Kehtarnavaz, and
D.¬†Terzopoulos.

</span>
<span class="ltx_bibblock">Image segmentation using deep learning: A survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>,
44(7):3523‚Äì3542, 2022. <a target="_blank" href="https://doi.org/10.1109/TPAMI.2021.3059968" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1109/TPAMI‚ÄÜ.‚ÄÜ2021‚ÄÜ.‚ÄÜ3059968</a>

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
F.¬†Murtagh and P.¬†Contreras.

</span>
<span class="ltx_bibblock">Algorithms for hierarchical clustering: an overview.

</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">Wiley Interdisciplinary Reviews: Data Mining and Knowledge
Discovery</span>, 2(1):86‚Äì97, 2012. <a target="_blank" href="https://doi.org/10.1002/widm.53" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1002/widm‚ÄÜ.‚ÄÜ53</a>

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
P.¬†Nakkiran, G.¬†Kaplun, Y.¬†Bansal, T.¬†Yang, B.¬†Barak, and I.¬†Sutskever.

</span>
<span class="ltx_bibblock">Deep double descent: Where bigger models and more data hurt.

</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text ltx_font_italic">Journal of Statistical Mechanics: Theory and Experiment</span>,
2021(12):124003, 2021. <a target="_blank" href="https://doi.org/10.1088/1742-5468/ac3a74" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1088/1742-5468/ac3a74</a>

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
E.¬†Pastor, L.¬†de¬†Alfaro, and E.¬†Baralis.

</span>
<span class="ltx_bibblock">Looking for trouble: Analyzing classifier behavior via pattern
divergence.

</span>
<span class="ltx_bibblock">In <span id="bib.bib48.1.1" class="ltx_text ltx_font_italic">Proceedings of the ACM International Conference on Management
of Data</span>, pp. 1400‚Äì1412. Xi‚Äôan, 2021.
<a target="_blank" href="https://doi.org/10.1145/3448016.3457284" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1145/3448016‚ÄÜ.‚ÄÜ3457284</a>

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
D.¬†W. Pentico.

</span>
<span class="ltx_bibblock">Assignment problems: A golden anniversary survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">European Journal of Operational Research</span>, 176(2):774‚Äì793,
2007. <a target="_blank" href="https://doi.org/10.1016/j.ejor.2005.09.014" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1016/j‚ÄÜ.‚ÄÜejor‚ÄÜ.‚ÄÜ2005‚ÄÜ.‚ÄÜ09‚ÄÜ.‚ÄÜ014</a>

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
D.¬†Ren, S.¬†Amershi, B.¬†Lee, J.¬†Suh, and J.¬†D. Williams.

</span>
<span class="ltx_bibblock">Squares: Supporting interactive performance analysis for multiclass
classifiers.

</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>,
23(1):61‚Äì70, 2017. <a target="_blank" href="https://doi.org/10.1109/TVCG.2016.2598828" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1109/TVCG‚ÄÜ.‚ÄÜ2016‚ÄÜ.‚ÄÜ2598828</a>

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
P.¬†Rottmann, M.¬†Wallinger, A.¬†Bonerath, S.¬†Gedicke, M.¬†N√∂llenburg, and J.-H.
Haunert.

</span>
<span class="ltx_bibblock">MosaicSets: Embedding set systems into grid graphs.

</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>,
29(1):875‚Äì885, 2023. <a target="_blank" href="https://doi.org/10.1109/TVCG.2022.3209485" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1109/TVCG‚ÄÜ.‚ÄÜ2022‚ÄÜ.‚ÄÜ3209485</a>

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
W.¬†Schroeder.

</span>
<span class="ltx_bibblock">Switching between tools in complex applications.

</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text ltx_font_italic">Journal of Usability Studies</span>, 3(4):173‚Äì188, 2008.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
R.¬†Sevastjanova, E.¬†Cakmak, S.¬†Ravfogel, R.¬†Cotterell, and M.¬†El-Assady.

</span>
<span class="ltx_bibblock">Visual comparison of language model adaptation.

</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>,
29(1):1178‚Äì1188, 2023. <a target="_blank" href="https://doi.org/10.1109/TVCG.2022.3209458" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1109/TVCG‚ÄÜ.‚ÄÜ2022‚ÄÜ.‚ÄÜ3209458</a>

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
J.¬†T. Townsend.

</span>
<span class="ltx_bibblock">Theoretical analysis of an alphabetic confusion matrix.

</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text ltx_font_italic">Perception &amp; Psychophysics</span>, 9:40‚Äì50, 1971.
<a target="_blank" href="https://doi.org/10.3758/BF03213026" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ3758/BF03213026</a>

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Q.¬†Wang, S.¬†L‚ÄôYi, and N.¬†Gehlenborg.

</span>
<span class="ltx_bibblock">DRAVA: Aligning human concepts with machine learning latent
dimensions for the visual exploration of small multiples.

</span>
<span class="ltx_bibblock">In <span id="bib.bib55.1.1" class="ltx_text ltx_font_italic">Proceedings of the ACM CHI Conference on Human Factors in
Computing Systems</span>, pp. 1‚Äì15. Hamburg, 2023.
<a target="_blank" href="https://doi.org/10.1145/3544548.3581127" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1145/3544548‚ÄÜ.‚ÄÜ3581127</a>

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
W.¬†Wang, J.¬†Dai, Z.¬†Chen, Z.¬†Huang, Z.¬†Li, X.¬†Zhu, X.¬†Hu, T.¬†Lu, L.¬†Lu, H.¬†Li,
et¬†al.

</span>
<span class="ltx_bibblock">InternImage: Exploring large-scale vision foundation models with
deformable convolutions.

</span>
<span class="ltx_bibblock">In <span id="bib.bib56.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pp. 14408‚Äì14419. Vancouver, 2023.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
S.¬†Waqas¬†Zamir, A.¬†Arora, A.¬†Gupta, S.¬†Khan, G.¬†Sun, F.¬†Shahbaz¬†Khan, F.¬†Zhu,
L.¬†Shao, G.-S. Xia, and X.¬†Bai.

</span>
<span class="ltx_bibblock">iSAID: A large-scale dataset for instance segmentation in aerial
images.

</span>
<span class="ltx_bibblock">In <span id="bib.bib57.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition Workshops</span>, pp. 28‚Äì37. Long Beach, 2019.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
W.¬†Willett, J.¬†Heer, and M.¬†Agrawala.

</span>
<span class="ltx_bibblock">Scented widgets: Improving navigation cues with embedded
visualizations.

</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>,
13(6):1129‚Äì1136, 2007. <a target="_blank" href="https://doi.org/10.1109/TVCG.2007.70589" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1109/TVCG‚ÄÜ.‚ÄÜ2007‚ÄÜ.‚ÄÜ70589</a>

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
A.¬†K. Wong and D.¬†K. Chiu.

</span>
<span class="ltx_bibblock">Synthesizing statistical knowledge from incomplete mixed-mode data.

</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>,
9(6):796‚Äì805, 1987. <a target="_blank" href="https://doi.org/10.1109/TPAMI.1987.4767986" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1109/TPAMI‚ÄÜ.‚ÄÜ1987‚ÄÜ.‚ÄÜ4767986</a>

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
P.¬†Xenopoulos, J.¬†Rulff, L.¬†G. Nonato, B.¬†Barr, and C.¬†Silva.

</span>
<span class="ltx_bibblock">Calibrate: Interactive analysis of probabilistic model output.

</span>
<span class="ltx_bibblock"><span id="bib.bib60.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>,
29(1):853‚Äì863, 2023. <a target="_blank" href="https://doi.org/10.1109/TVCG.2022.3209489" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1109/TVCG‚ÄÜ.‚ÄÜ2022‚ÄÜ.‚ÄÜ3209489</a>

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
J.¬†Yang, C.¬†Li, X.¬†Dai, and J.¬†Gao.

</span>
<span class="ltx_bibblock">Focal modulation networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib61.1.1" class="ltx_text ltx_font_italic">Proceedings of the Advances in Neural Information Processing
Systems</span>, pp. 4203‚Äì4217. New Orleans, 2022.
<a target="_blank" href="https://doi.org/10.31525/ct1-nct03946618" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ31525/ct1-nct03946618</a>

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
W.¬†Yang, Z.¬†Li, M.¬†Liu, Y.¬†Lu, K.¬†Cao, R.¬†Maciejewski, and
S.¬†Liu.

</span>
<span class="ltx_bibblock">Diagnosing concept drift with visual analytics.

</span>
<span class="ltx_bibblock">In <span id="bib.bib62.1.1" class="ltx_text ltx_font_italic">Proceedings of IEEE Conference on Visual Analytics Science
and Technology</span>, pp. 12‚Äì23, 2020.
<a target="_blank" href="https://doi.org/10.1109/vast50239.2020.00007" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1109/vast50239‚ÄÜ.‚ÄÜ2020‚ÄÜ.‚ÄÜ00007</a>

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
W.¬†Yang, X.¬†Wang, J.¬†Lu, W.¬†Dou, and S.¬†Liu.

</span>
<span class="ltx_bibblock">Interactive steering of hierarchical clustering.

</span>
<span class="ltx_bibblock"><span id="bib.bib63.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>,
27(10):3953‚Äì3967, 2021. <a target="_blank" href="https://doi.org/10.1109/tvcg.2020.2995100" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1109/tvcg‚ÄÜ.‚ÄÜ2020‚ÄÜ.‚ÄÜ2995100</a>

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
W.¬†Yang, X.¬†Ye, X.¬†Zhang, L.¬†Xiao, J.¬†Xia, Z.¬†Wang, J.¬†Zhu, H.¬†Pfister, and
S.¬†Liu.

</span>
<span class="ltx_bibblock">Diagnosing ensemble few-shot classifiers.

</span>
<span class="ltx_bibblock"><span id="bib.bib64.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>,
28(9):3292‚Äì3306, 2022. <a target="_blank" href="https://doi.org/10.1109/TVCG.2022.3182488" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1109/TVCG‚ÄÜ.‚ÄÜ2022‚ÄÜ.‚ÄÜ3182488</a>

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
J.¬†Yuan, C.¬†Chen, W.¬†Yang, M.¬†Liu, J.¬†Xia, and S.¬†Liu.

</span>
<span class="ltx_bibblock">A survey of visual analytics techniques for machine learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib65.1.1" class="ltx_text ltx_font_italic">Computational Visual Media</span>, 7(1):3‚Äì36, 2021.
<a target="_blank" href="https://doi.org/10.1007/s41095-020-0191-7" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1007/s41095-020-0191-7</a>

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
H.¬†Zhang, F.¬†Li, S.¬†Liu, L.¬†Zhang, H.¬†Su, J.¬†Zhu, L.¬†Ni, and H.¬†Shum.

</span>
<span class="ltx_bibblock">DINO: DETR with improved denoising anchor boxes for end-to-end
object detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib66.1.1" class="ltx_text ltx_font_italic">Proceedings of the International Conference on Learning
Representations</span>, pp. 1‚Äì9. Kigali, 2023.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
X.¬†Zhang, J.¬†P. Ono, H.¬†Song, L.¬†Gou, K.-L. Ma, and L.¬†Ren.

</span>
<span class="ltx_bibblock">SliceTeller: A data slice-driven approach for machine learning
model validation.

</span>
<span class="ltx_bibblock"><span id="bib.bib67.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>,
29(1):842‚Äì852, 2023. <a target="_blank" href="https://doi.org/10.1109/TVCG.2022.3209465" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1109/TVCG‚ÄÜ.‚ÄÜ2022‚ÄÜ.‚ÄÜ3209465</a>

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Y.¬†Zhang and Q.¬†Yang.

</span>
<span class="ltx_bibblock">An overview of multi-task learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib68.1.1" class="ltx_text ltx_font_italic">National Science Review</span>, 5(1):30‚Äì43, 2018.
<a target="_blank" href="https://doi.org/10.1093/nsr/nwx105" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1093/nsr/nwx105</a>

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Y.¬†Zhang and Q.¬†Yang.

</span>
<span class="ltx_bibblock">A survey on multi-task learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib69.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Knowledge and Data Engineering</span>,
34(12):5586‚Äì5609, 2022. <a target="_blank" href="https://doi.org/10.1109/TKDE.2021.3070203" title="" class="ltx_ref ltx_href">doi: 10‚ÄÜ.‚ÄÜ1109/TKDE‚ÄÜ.‚ÄÜ2021‚ÄÜ.‚ÄÜ3070203</a>

</span>
</li>
</ul>
</section>
<div id="p9" class="ltx_para">
<p id="p9.1" class="ltx_p">See pages - of <a href="Appendix" title="" class="ltx_ref">Appendix</a></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2308.05167" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2308.05168" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2308.05168">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2308.05168" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2308.05169" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 12:57:28 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
