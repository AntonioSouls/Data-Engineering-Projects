<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2212.00585] Soft Labels for Rapid Satellite Object Detection</title><meta property="og:description" content="Soft labels in image classification are vector representations of an image’s true classification. In this paper, we investigate soft labels in the context of satellite object detection. We propose using detections as t…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Soft Labels for Rapid Satellite Object Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Soft Labels for Rapid Satellite Object Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2212.00585">

<!--Generated on Fri Mar  1 08:33:52 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Soft Labels for Rapid Satellite Object Detection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id1.1.id1" class="ltx_text" style="font-size:90%;">Matthew Ciolino</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id2.2.id1" class="ltx_text" style="font-size:80%;">PeopleTec, Inc, 4901 Corporate Dr NW, Huntsville, AL 35805, USA</span>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id3.1.id1" class="ltx_text" style="font-size:90%;">Grant Rosario</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id4.2.id1" class="ltx_text" style="font-size:80%;">PeopleTec, Inc, 4901 Corporate Dr NW, Huntsville, AL 35805, USA</span>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id5.1.id1" class="ltx_text" style="font-size:90%;">David Noever</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id6.2.id1" class="ltx_text" style="font-size:80%;">PeopleTec, Inc, 4901 Corporate Dr NW, Huntsville, AL 35805, USA</span>
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id7.id1" class="ltx_p"><span id="id7.id1.1" class="ltx_text" style="font-size:80%;">Soft labels in image classification are vector representations of an image’s true classification. In this paper, we investigate soft labels in the context of satellite object detection. We propose using detections as the basis for a new dataset of soft labels. Much of the effort in creating a high-quality model is gathering and annotating the training data. If we could use a model to generate a dataset for us, we could not only rapidly create datasets, but also supplement existing open-source datasets. Using a subset of the xView dataset, we train a YOLOv5 model to detect cars, planes, and ships. We then use that model to generate soft labels for the second training set which we then train and compare to the original model. We show that soft labels can be used to train a model that is almost as accurate as a model trained on the original data.</span></p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
<span id="id8.id1" class="ltx_text" style="font-size:80%;">
Soft Labels, Object Detection, Datasets
</span>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:80%;">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p"><span id="S1.p1.1.1" class="ltx_text" style="font-size:80%;">Learning representations is a powerful tool in artificial intelligence. Deep learning has always been used to learn representations of images, text, and audio but furthermore can be used for transfer learning </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p1.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a><span id="S1.p1.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S1.p1.1.4" class="ltx_text" style="font-size:80%;"> and pre-training </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p1.1.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib2" title="" class="ltx_ref">2</a><span id="S1.p1.1.6.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S1.p1.1.7" class="ltx_text" style="font-size:80%;">. In this way one models strength can be used to improve performance on another task. As is commonplace for many object detection backbones, a pre-trained feature extraction network </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p1.1.8.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib3" title="" class="ltx_ref">3</a><span id="S1.p1.1.9.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S1.p1.1.10" class="ltx_text" style="font-size:80%;"> is used to initialize the backbone of a model. This stabilizes training, improves convergence speed, and improves performance </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p1.1.11.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib4" title="" class="ltx_ref">4</a><span id="S1.p1.1.12.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S1.p1.1.13" class="ltx_text" style="font-size:80%;">. Soft labels attempt this by abstracting the transfer of information to the dataset level.</span></p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p"><span id="S1.p2.1.1" class="ltx_text" style="font-size:80%;">Soft labels use a well-trained model to completely generate the training data for another model. A clear use of soft labels is in model distillation </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p2.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="S1.p2.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S1.p2.1.4" class="ltx_text" style="font-size:80%;"> where the final layer in a large neural network containing the class probabilities is used as the ground truth to train a smaller network on. This effectively teaches a smaller model what a large model learned in a teacher-student </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p2.1.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib6" title="" class="ltx_ref">6</a><span id="S1.p2.1.6.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S1.p2.1.7" class="ltx_text" style="font-size:80%;"> relationship. In this paper, we use soft labels to train an object detection model on satellite imagery and then rapidly create a dataset of soft labels.</span></p>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:80%;">
<span class="ltx_tag ltx_tag_subsection"><span id="S1.SS1.5.1.1" class="ltx_text">I-A</span> </span><span id="S1.SS1.6.2" class="ltx_text ltx_font_italic">Background</span>
</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p"><span id="S1.SS1.p1.1.1" class="ltx_text" style="font-size:80%;">Dataset creation can be a time-consuming and costly endeavor </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.SS1.p1.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib7" title="" class="ltx_ref">7</a><span id="S1.SS1.p1.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S1.SS1.p1.1.4" class="ltx_text" style="font-size:80%;">. While a handful of high-quality satellite object detection datasets exist </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.SS1.p1.1.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib8" title="" class="ltx_ref">8</a><span id="S1.SS1.p1.1.6.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S1.SS1.p1.1.7" class="ltx_text" style="font-size:80%;">, they may not be sufficient for the task at hand. By automating the process of annotations through soft labels, we can cut down on the time it takes to make a dataset. Various problems and solutions arise with soft labels and here are some examples:</span></p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2212.00585/assets/Figures/gt_vs_soft_airport_10.png" id="S1.F1.sf1.g1" class="ltx_graphics ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf1.4.1.1" class="ltx_text" style="font-size:113%;">((a))</span> </span><span id="S1.F1.sf1.5.2" class="ltx_text" style="font-size:113%;">Train Set 1</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2212.00585/assets/Figures/gt_vs_soft_ship_01.png" id="S1.F1.sf2.g1" class="ltx_graphics ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf2.4.1.1" class="ltx_text" style="font-size:113%;">((b))</span> </span><span id="S1.F1.sf2.5.2" class="ltx_text" style="font-size:113%;">Train Set 2</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.4.1.1" class="ltx_text" style="font-size:113%;">Figure 1</span>: </span><span id="S1.F1.5.2" class="ltx_text" style="font-size:113%;">Ground Truth Labels (green) vs Soft Labels (red)</span></figcaption>
</figure>
<section id="S1.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="font-size:80%;">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S1.SS1.SSS1.5.1.1" class="ltx_text">I-A</span>1 </span>Missing Objects</h4>

<div id="S1.SS1.SSS1.p1" class="ltx_para">
<p id="S1.SS1.SSS1.p1.1" class="ltx_p"><span id="S1.SS1.SSS1.p1.1.1" class="ltx_text" style="font-size:80%;">One intuitive problem with soft labels for object detection is that low confidence detections will be filtered not allowing a model to learn finer grain details for objects. While this would leave the objects in question to be designated as background objects, Wu et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.SS1.SSS1.p1.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib9" title="" class="ltx_ref">9</a><span id="S1.SS1.SSS1.p1.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S1.SS1.SSS1.p1.1.4" class="ltx_text" style="font-size:80%;">, show that with the right modification to the bounding box loss function, a model can improve instead of worsening. To prove this, they dropped 30% of the ground truth labels and found a 5% drop in performance while when they weigh high-quality bounding boxes higher an increase of 3% is found as compared to a baseline.</span></p>
</div>
</section>
<section id="S1.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="font-size:80%;">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S1.SS1.SSS2.5.1.1" class="ltx_text">I-A</span>2 </span>Dataset Creation</h4>

<div id="S1.SS1.SSS2.p1" class="ltx_para">
<p id="S1.SS1.SSS2.p1.1" class="ltx_p"><span id="S1.SS1.SSS2.p1.1.1" class="ltx_text" style="font-size:80%;">One effective use of soft labels for object detection is subtyping. Subtyping is the process of taking a class and breaking it down into smaller classes. For example, a car can be broken down into a sedan, truck, and SUV. In our past work, Rosario et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.SS1.SSS2.p1.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S1.SS1.SSS2.p1.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S1.SS1.SSS2.p1.1.4" class="ltx_text" style="font-size:80%;"> show that with a simple car detection model and classifying cars by color, we can create a dataset of soft labels for car colors.</span></p>
</div>
</section>
<section id="S1.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="font-size:80%;">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S1.SS1.SSS3.5.1.1" class="ltx_text">I-A</span>3 </span>Overfitting</h4>

<div id="S1.SS1.SSS3.p1" class="ltx_para">
<p id="S1.SS1.SSS3.p1.1" class="ltx_p"><span id="S1.SS1.SSS3.p1.1.1" class="ltx_text" style="font-size:80%;">Overfitting is a common problem in deep learning. While there are many ways to combat overfitting, Zhang et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.SS1.SSS3.p1.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S1.SS1.SSS3.p1.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S1.SS1.SSS3.p1.1.4" class="ltx_text" style="font-size:80%;"> proposed using an online label smoothing to generate soft labels more reliably. In each epoch during training, they mix hard labels and the previous epochs soft labels to iteratively improve the soft labels of each object detected. That mixture is governed by the cross-entropy classification loss with the original distribution of soft labels being uniform. This method is shown to better define and separate classes on image datasets bringing a 2.1% gain on performance for top-1 error on VOC </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.SS1.SSS3.p1.1.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="S1.SS1.SSS3.p1.1.6.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S1.SS1.SSS3.p1.1.7" class="ltx_text" style="font-size:80%;"> and COCO.</span></p>
</div>
</section>
<section id="S1.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="font-size:80%;">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S1.SS1.SSS4.5.1.1" class="ltx_text">I-A</span>4 </span>Student/Teacher Models</h4>

<div id="S1.SS1.SSS4.p1" class="ltx_para">
<p id="S1.SS1.SSS4.p1.1" class="ltx_p"><span id="S1.SS1.SSS4.p1.1.1" class="ltx_text" style="font-size:80%;">Using the soft labels from a teacher model, a student model can outperform models trained on partially labeled COCO data. Xu et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.SS1.SSS4.p1.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib13" title="" class="ltx_ref">13</a><span id="S1.SS1.SSS4.p1.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S1.SS1.SSS4.p1.1.4" class="ltx_text" style="font-size:80%;"> in this semi-supervised object detection (SSOD) paper, presented 2 techniques: soft teacher, where the teacher model is actually updated by the student using an exponential moving average, and a training strategy where the student/teacher models are trained using images with different augmentations. This use of soft labels improves state-of-the-art performance by 8.43% on 1% to 10% labeled coco data.</span></p>
</div>
</section>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:80%;">
<span class="ltx_tag ltx_tag_subsection"><span id="S1.SS2.5.1.1" class="ltx_text">I-B</span> </span><span id="S1.SS2.6.2" class="ltx_text ltx_font_italic">Contributions</span>
</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p"><span id="S1.SS2.p1.1.1" class="ltx_text" style="font-size:80%;">The above methods are all examples of how soft labels can be used to improve a model. In this paper, we approach a far simpler task of simply investigating the performance drop-off using 100% soft labels as compared to the complete dataset. Ground truth versus soft label dataset can be seen in [Figure </span><a href="#S1.F1" title="Figure 1 ‣ I-A Background ‣ I Introduction ‣ Soft Labels for Rapid Satellite Object Detection" class="ltx_ref" style="font-size:80%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S1.SS2.p1.1.2" class="ltx_text" style="font-size:80%;">]. In this paper we attempt to answer 3 research questions that pertained to the nuances of soft labels:</span></p>
</div>
<div id="S1.SS2.p2" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p"><span id="S1.I1.i1.p1.1.1" class="ltx_text" style="font-size:80%;">How does the soft labeling affect performance?</span></p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p"><span id="S1.I1.i2.p1.1.1" class="ltx_text" style="font-size:80%;">Can this be used to categorize datasets automatically?</span></p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p"><span id="S1.I1.i3.p1.1.1" class="ltx_text" style="font-size:80%;">What confidence value trains the best soft label model?</span></p>
</div>
</li>
</ul>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2212.00585/assets/Figures/exp_flow_2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="290" height="134" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.4.1.1" class="ltx_text" style="font-size:113%;">Figure 2</span>: </span><span id="S1.F2.5.2" class="ltx_text" style="font-size:113%;">Expierment Flow</span></figcaption>
</figure>
<figure id="S1.T1" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S1.T1.4.1.1" class="ltx_text" style="font-size:113%;">TABLE I</span>: </span><span id="S1.T1.5.2" class="ltx_text" style="font-size:113%;">Test Set Metrics</span></figcaption>
<table id="S1.T1.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S1.T1.6.1.1" class="ltx_tr">
<th id="S1.T1.6.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_ll ltx_border_r ltx_border_t"><span id="S1.T1.6.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Model</span></th>
<td id="S1.T1.6.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Conf</span></td>
<td id="S1.T1.6.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">mAP50</span></td>
<td id="S1.T1.6.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">mAP95</span></td>
<td id="S1.T1.6.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.1.1.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">F1 Score</span></td>
<td id="S1.T1.6.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.1.1.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">box_loss</span></td>
<td id="S1.T1.6.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.1.1.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">obj_loss</span></td>
<td id="S1.T1.6.1.1.8" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S1.T1.6.1.1.8.1" class="ltx_text ltx_font_bold" style="font-size:80%;">class_loss</span></td>
</tr>
<tr id="S1.T1.6.2.2" class="ltx_tr">
<th id="S1.T1.6.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_ll ltx_border_r ltx_border_t"><span id="S1.T1.6.2.2.1.1" class="ltx_text" style="font-size:80%;">Train Set 1</span></th>
<td id="S1.T1.6.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.2.2.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S1.T1.6.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.2.2.3.1" class="ltx_text" style="font-size:80%;">0.76251</span></td>
<td id="S1.T1.6.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.2.2.4.1" class="ltx_text" style="font-size:80%;">0.45069</span></td>
<td id="S1.T1.6.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.2.2.5.1" class="ltx_text" style="font-size:80%;">.71 @ .419c</span></td>
<td id="S1.T1.6.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.2.2.6.1" class="ltx_text" style="font-size:80%;">0.052841</span></td>
<td id="S1.T1.6.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.2.2.7.1" class="ltx_text" style="font-size:80%;">0.03327</span></td>
<td id="S1.T1.6.2.2.8" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S1.T1.6.2.2.8.1" class="ltx_text" style="font-size:80%;">0.0058541</span></td>
</tr>
<tr id="S1.T1.6.3.3" class="ltx_tr">
<th id="S1.T1.6.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_ll ltx_border_r ltx_border_t"><span id="S1.T1.6.3.3.1.1" class="ltx_text" style="font-size:80%;">Train Set 1 Soft</span></th>
<td id="S1.T1.6.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.3.3.2.1" class="ltx_text" style="font-size:80%;">0.3</span></td>
<td id="S1.T1.6.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.3.3.3.1" class="ltx_text" style="font-size:80%;">0.73275</span></td>
<td id="S1.T1.6.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.3.3.4.1" class="ltx_text" style="font-size:80%;">0.42357</span></td>
<td id="S1.T1.6.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.3.3.5.1" class="ltx_text" style="font-size:80%;">.70 @ .482c</span></td>
<td id="S1.T1.6.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.3.3.6.1" class="ltx_text" style="font-size:80%;">0.042647 (-21.35%)</span></td>
<td id="S1.T1.6.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.3.3.7.1" class="ltx_text" style="font-size:80%;">0.029469 (-12.11%)</span></td>
<td id="S1.T1.6.3.3.8" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S1.T1.6.3.3.8.1" class="ltx_text" style="font-size:80%;">0.0033020 (-55.74%)</span></td>
</tr>
<tr id="S1.T1.6.4.4" class="ltx_tr">
<th id="S1.T1.6.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_ll ltx_border_r ltx_border_t"><span id="S1.T1.6.4.4.1.1" class="ltx_text" style="font-size:80%;">Train Set 1 Soft</span></th>
<td id="S1.T1.6.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.4.4.2.1" class="ltx_text" style="font-size:80%;">0.5</span></td>
<td id="S1.T1.6.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.4.4.3.1" class="ltx_text" style="font-size:80%;">0.70798</span></td>
<td id="S1.T1.6.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.4.4.4.1" class="ltx_text" style="font-size:80%;">0.40687</span></td>
<td id="S1.T1.6.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.4.4.5.1" class="ltx_text" style="font-size:80%;">.69 @ .318c</span></td>
<td id="S1.T1.6.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.4.4.6.1" class="ltx_text" style="font-size:80%;">0.044096 (-18.04%)</span></td>
<td id="S1.T1.6.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.4.4.7.1" class="ltx_text" style="font-size:80%;">0.030910 (-7.35%)</span></td>
<td id="S1.T1.6.4.4.8" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S1.T1.6.4.4.8.1" class="ltx_text" style="font-size:80%;">0.0041233 (-34.69%)</span></td>
</tr>
<tr id="S1.T1.6.5.5" class="ltx_tr">
<th id="S1.T1.6.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_ll ltx_border_r ltx_border_t"><span id="S1.T1.6.5.5.1.1" class="ltx_text" style="font-size:80%;">Train Set 2</span></th>
<td id="S1.T1.6.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.5.5.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S1.T1.6.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.5.5.3.1" class="ltx_text" style="font-size:80%;">0.77470</span></td>
<td id="S1.T1.6.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.5.5.4.1" class="ltx_text" style="font-size:80%;">0.46090</span></td>
<td id="S1.T1.6.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.5.5.5.1" class="ltx_text" style="font-size:80%;">.72 @ .421c</span></td>
<td id="S1.T1.6.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.5.5.6.1" class="ltx_text" style="font-size:80%;">0.0415</span></td>
<td id="S1.T1.6.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.5.5.7.1" class="ltx_text" style="font-size:80%;">0.027994</span></td>
<td id="S1.T1.6.5.5.8" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S1.T1.6.5.5.8.1" class="ltx_text" style="font-size:80%;">0.002451</span></td>
</tr>
<tr id="S1.T1.6.6.6" class="ltx_tr">
<th id="S1.T1.6.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_ll ltx_border_r ltx_border_t"><span id="S1.T1.6.6.6.1.1" class="ltx_text" style="font-size:80%;">Train Set 2 Soft</span></th>
<td id="S1.T1.6.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.6.6.2.1" class="ltx_text" style="font-size:80%;">0.3</span></td>
<td id="S1.T1.6.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.6.6.3.1" class="ltx_text" style="font-size:80%;">0.72932</span></td>
<td id="S1.T1.6.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.6.6.4.1" class="ltx_text" style="font-size:80%;">0.42006</span></td>
<td id="S1.T1.6.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.6.6.5.1" class="ltx_text" style="font-size:80%;">.70 @ .503c</span></td>
<td id="S1.T1.6.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.6.6.6.1" class="ltx_text" style="font-size:80%;">0.042767 (+3.01%)</span></td>
<td id="S1.T1.6.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.6.6.6.7.1" class="ltx_text" style="font-size:80%;">0.031548 (+11.93%)</span></td>
<td id="S1.T1.6.6.6.8" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S1.T1.6.6.6.8.1" class="ltx_text" style="font-size:80%;">0.0033173 (+30.04%)</span></td>
</tr>
<tr id="S1.T1.6.7.7" class="ltx_tr">
<th id="S1.T1.6.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_ll ltx_border_r ltx_border_t"><span id="S1.T1.6.7.7.1.1" class="ltx_text" style="font-size:80%;">Train Set 2 Soft</span></th>
<td id="S1.T1.6.7.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S1.T1.6.7.7.2.1" class="ltx_text" style="font-size:80%;">0.5</span></td>
<td id="S1.T1.6.7.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S1.T1.6.7.7.3.1" class="ltx_text" style="font-size:80%;">0.71681</span></td>
<td id="S1.T1.6.7.7.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S1.T1.6.7.7.4.1" class="ltx_text" style="font-size:80%;">0.41578</span></td>
<td id="S1.T1.6.7.7.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S1.T1.6.7.7.5.1" class="ltx_text" style="font-size:80%;">.70 @ .308c</span></td>
<td id="S1.T1.6.7.7.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S1.T1.6.7.7.6.1" class="ltx_text" style="font-size:80%;">0.044652 (+7.32%)</span></td>
<td id="S1.T1.6.7.7.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S1.T1.6.7.7.7.1" class="ltx_text" style="font-size:80%;">0.030846 (+9.69%)</span></td>
<td id="S1.T1.6.7.7.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t"><span id="S1.T1.6.7.7.8.1" class="ltx_text" style="font-size:80%;">0.0044479 (+57.89%)</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S1.F3" class="ltx_figure"><img src="/html/2212.00585/assets/Figures/per_class_metrics_4.png" id="S1.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="589" height="234" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span id="S1.F3.4.1.1" class="ltx_text" style="font-size:113%;">Figure 3</span>: </span><span id="S1.F3.5.2" class="ltx_text" style="font-size:113%;">Per Class Metrics Plot - Color/Linetype to Class/Metric</span></figcaption>
</figure>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:80%;">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Experiment</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text" style="font-size:80%;">To answer these questions, we lay out the following experiments [Figure </span><a href="#S1.F2" title="Figure 2 ‣ I-B Contributions ‣ I Introduction ‣ Soft Labels for Rapid Satellite Object Detection" class="ltx_ref" style="font-size:80%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S2.p1.1.2" class="ltx_text" style="font-size:80%;">] involving a subset of the xView dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.3.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib14" title="" class="ltx_ref">14</a><span id="S2.p1.1.4.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S2.p1.1.5" class="ltx_text" style="font-size:80%;">. We sub-select xView for moving objects (vehicles, planes, and ships) leaving 34,252 images collected from parking lots, marine ports, and airports. Approximately one-third of the pictures were background, meaning they contained no examples but contributed to the training ”null” set. Our data contains 37,712 instances of ships, 174,779 instances of cars, and 18,052 instances of planes. These counts are randomly split across 3 sets for train_1/train_2/valid sets containing 40/40/20 percent of the data respectively. More dataset information can be found in the appendix.</span></p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text" style="font-size:80%;">YOLOv5s </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p2.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib15" title="" class="ltx_ref">15</a><span id="S2.p2.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S2.p2.1.4" class="ltx_text" style="font-size:80%;"> containing 7,027,720 parameters was trained on the train_1/train_2 set for 3 epochs with a batch size of 16 using the pretrained coco </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p2.1.5.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S2.p2.1.6.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S2.p2.1.7" class="ltx_text" style="font-size:80%;"> model as a checkpoint. Using those models, we soft-labeled the opposite training set to generate soft-labeled train_2/train_1. All models trained used the unseen valid set as the test set to compare metrics. We use the mAP, F1, and losses to evaluate the models.</span></p>
</div>
<div id="S2.p3" class="ltx_para">
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><span id="S2.I1.i1.p1.1.1" class="ltx_text" style="font-size:80%;">mAP is the mean of the average precision value for recall value over 0 to 1 for each class at a given IoU. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.I1.i1.p1.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib17" title="" class="ltx_ref">17</a><span id="S2.I1.i1.p1.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S2.I1.i1.p1.1.4" class="ltx_text" style="font-size:80%;"></span></p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><span id="S2.I1.i2.p1.1.1" class="ltx_text" style="font-size:80%;">The IoU threshold is the intersection over union of the predicted bounding box and the ground truth bounding box. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.I1.i2.p1.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a><span id="S2.I1.i2.p1.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S2.I1.i2.p1.1.4" class="ltx_text" style="font-size:80%;"></span></p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p"><span id="S2.I1.i3.p1.1.1" class="ltx_text" style="font-size:80%;">F1 is the harmonic mean of precision and recall at different confidence thresholds. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.I1.i3.p1.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib19" title="" class="ltx_ref">19</a><span id="S2.I1.i3.p1.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S2.I1.i3.p1.1.4" class="ltx_text" style="font-size:80%;"></span></p>
</div>
</li>
</ul>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text" style="font-size:80%;">We also look at train/test loss. For loss, YOLOv5 uses a combination </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p4.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib20" title="" class="ltx_ref">20</a><span id="S2.p4.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S2.p4.1.4" class="ltx_text" style="font-size:80%;"> of bounding box loss, objectness loss, and classification loss to train the model.</span></p>
</div>
<div id="S2.p5" class="ltx_para">
<ul id="S2.I2" class="ltx_itemize">
<li id="S2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i1.p1" class="ltx_para">
<p id="S2.I2.i1.p1.1" class="ltx_p"><span id="S2.I2.i1.p1.1.1" class="ltx_text" style="font-size:80%;">The bounding box loss is the mean squared error between the predicted bounding box and the ground truth bounding box.</span></p>
</div>
</li>
<li id="S2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i2.p1" class="ltx_para">
<p id="S2.I2.i2.p1.1" class="ltx_p"><span id="S2.I2.i2.p1.1.1" class="ltx_text" style="font-size:80%;">The objectness loss measures the probability that an object exists in a proposed region of interest through binary cross entropy.</span></p>
</div>
</li>
<li id="S2.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i3.p1" class="ltx_para">
<p id="S2.I2.i3.p1.1" class="ltx_p"><span id="S2.I2.i3.p1.1.1" class="ltx_text" style="font-size:80%;">The classification loss is the cross-entropy loss between the predicted class and the ground truth class.</span></p>
</div>
</li>
</ul>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p"><span id="S2.p6.1.1" class="ltx_text" style="font-size:80%;">A detailed summary of YOLOv5 can be found here </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p6.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib21" title="" class="ltx_ref">21</a><span id="S2.p6.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S2.p6.1.4" class="ltx_text" style="font-size:80%;">. The test set which the results are all based on is broken down as follows: Across the 7,896 images and 58,165 instances in the test set, 8,710 instances (15%) are ships, 45,025 instances (77%) are cars and 4,430 instances (8%) are planes.</span></p>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p"><span id="S2.p7.1.1" class="ltx_text" style="font-size:80%;">In addition, during experimentation OWL-ViT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p7.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib22" title="" class="ltx_ref">22</a><span id="S2.p7.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S2.p7.1.4" class="ltx_text" style="font-size:80%;"> was released. This Open-Vocabulary Object Detection model can be used to soft label. In the appendix section, we compare YOLOv5 models with the COCO 2017 validation set with ground truth, 0.1 confidence OWL-ViT inference, and 0.25 confidence OWL-ViT inference.</span></p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:80%;">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Results</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p"><span id="S3.p1.1.1" class="ltx_text" style="font-size:80%;">While ground truth models performed the best [Table </span><a href="#S1.T1" title="TABLE I ‣ I-B Contributions ‣ I Introduction ‣ Soft Labels for Rapid Satellite Object Detection" class="ltx_ref" style="font-size:80%;"><span class="ltx_text ltx_ref_tag">I</span></a><span id="S3.p1.1.2" class="ltx_text" style="font-size:80%;">], the soft label models consistently came within a 6% difference for mAP and F1 score. Interestingly, the F1 score remains very consistent (.68 to .70) with the optimal confidence value acting inversely to the confidence threshold of the soft-labeled training set. When looking at losses of the test set after the final epoch of training, we see a more interesting story that might tell us why the metrics are lower.</span></p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text" style="font-size:80%;">While the soft label trained model losses are both higher and lower than the ground truth model, we can derive insight from the relative differences. Both bounding box loss and objectness loss seem relatively consistent at an average difference of 11.35%. This is in stark contrast to the classification loss which averages a difference of 44.59%. Since classification loss has the highest change (still the lowest component of the loss) it can be said that the unlabeled objects in the soft label training sets account for the loss of performance.</span></p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p"><span id="S3.p3.1.1" class="ltx_text" style="font-size:80%;">We can take a look at the per-class metrics [Figure </span><a href="#S1.F3" title="Figure 3 ‣ I-B Contributions ‣ I Introduction ‣ Soft Labels for Rapid Satellite Object Detection" class="ltx_ref" style="font-size:80%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S3.p3.1.2" class="ltx_text" style="font-size:80%;">] to get a better look at what might be going on. As shown, the majority of the decrease in performance comes from soft-labeled planes. For planes trained on 0.5 confidence soft labels, all metrics dropped on average 8.49%. Interestingly, cars, which are the smallest but most abundant object in the dataset performed only 0.65% worse across all metrics than the ground truth models. Overall, per-class metrics dropped 4.44% across all metrics.</span></p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:80%;">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Discussion</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p"><span id="S4.p1.1.1" class="ltx_text" style="font-size:80%;">While this performance drop is statistically significant in certain cases, i.e., planes, the benefits of soft-labels outweigh this loss in performance by providing additional data in a low-cost and efficient manner as well as potentially increasing model knowledge in the cases of transfer learning. Moreover, this loss in performance can usually be remedied by either balancing the dataset or increasing data for lagging labels, as evidenced by the statistically insignificant drop in car labeling performance.</span></p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:80%;">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p"><span id="S5.p1.1.1" class="ltx_text" style="font-size:80%;">Models can be trained exclusively on soft labels with a less than 6% drop in mAP as compared to ground truth labels on the same dataset. Regardless of the confidence threshold used to create the soft labels, mAP/F1 scores remain within 1% of each other. This suggests that the soft labels are not overfitting to the training data. These results validate that rapid object detection datasets can be created with soft labels and that soft labels can be used to train models with a high degree of accuracy.</span></p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section" style="font-size:80%;">Future Work</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p"><span id="Sx1.p1.1.1" class="ltx_text" style="font-size:80%;">In future work, we would like to use soft labels to improve the performance of the model on the test set by supplementing existing data instead of training solely on the soft labels. We would also like to use soft labels from one generalized model that could categorize any satellite image you are looking at into a dataset.</span></p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section" style="font-size:80%;">Acknowledgment</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p"><span id="Sx2.p1.1.1" class="ltx_text" style="font-size:80%;">The authors would like to thank the PeopleTec Technical Fellows program for encouragement and project assistance.</span></p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:80%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:80%;">
F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong, and Q. He, “A
comprehensive survey on transfer learning,” </span><em id="bib.bib1.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">Proceedings of the IEEE</em><span id="bib.bib1.3.3" class="ltx_text" style="font-size:80%;">,
vol. 109, no. 1, pp. 43–76, 2020.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:80%;">
H. H. Mao, “A survey on self-supervised pre-training for sequential transfer
learning in neural networks,” </span><em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">arXiv preprint arXiv:2007.00800</em><span id="bib.bib2.3.3" class="ltx_text" style="font-size:80%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:80%;">
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learningfor image
recognition,” </span><em id="bib.bib3.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">ComputerScience</em><span id="bib.bib3.3.3" class="ltx_text" style="font-size:80%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:80%;">
Y. Chen, T. Yang, X. Zhang, G. Meng, X. Xiao, and J. Sun, “Detnas: Backbone
search for object detection,” </span><em id="bib.bib4.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">Advances in Neural Information
Processing Systems</em><span id="bib.bib4.3.3" class="ltx_text" style="font-size:80%;">, vol. 32, 2019.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:80%;">
G. Chen, W. Choi, X. Yu, T. Han, and M. Chandraker, “Learning efficient object
detection models with knowledge distillation,” </span><em id="bib.bib5.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">Advances in neural
information processing systems</em><span id="bib.bib5.3.3" class="ltx_text" style="font-size:80%;">, vol. 30, 2017.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:80%;">
C. H. Nguyen, T. C. Nguyen, T. N. Tang, and N. L. Phan, “Improving object
detection by label assignment distillation,” in </span><em id="bib.bib6.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">Proceedings of the
IEEE/CVF Winter Conference on Applications of Computer Vision</em><span id="bib.bib6.3.3" class="ltx_text" style="font-size:80%;">, 2022, pp.
1005–1014.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:80%;">
R. Incze, “The cost of machine learning projects,” Sep 2019. [Online].
Available:
</span><a target="_blank" href="https://medium.com/cognifeed/the-cost-of-machine-learning-projects-7ca3aea03a5c" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:80%;">https://medium.com/cognifeed/the-cost-of-machine-learning-projects-7ca3aea03a5c</a><span id="bib.bib7.2.2" class="ltx_text" style="font-size:80%;">
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:80%;">
Chrieke, “Chrieke/awesome-satellite-imagery-datasets: list of satellite image
training datasets with annotations for computer vision and deep learning.”
[Online]. Available:
</span><a target="_blank" href="https://github.com/chrieke/awesome-satellite-imagery-datasets" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:80%;">https://github.com/chrieke/awesome-satellite-imagery-datasets</a><span id="bib.bib8.2.2" class="ltx_text" style="font-size:80%;">
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:80%;">
Z. Wu, N. Bodla, B. Singh, M. Najibi, R. Chellappa, and L. S. Davis, “Soft
sampling for robust object detection,” </span><em id="bib.bib9.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">arXiv preprint
arXiv:1806.06986</em><span id="bib.bib9.3.3" class="ltx_text" style="font-size:80%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:80%;">
G. Rosario, D. Noever, and M. Ciolino, “Soft-labeling strategies for rapid
sub-typing,” </span><em id="bib.bib10.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">arXiv preprint arXiv:2209.12684</em><span id="bib.bib10.3.3" class="ltx_text" style="font-size:80%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:80%;">
C.-B. Zhang, P.-T. Jiang, Q. Hou, Y. Wei, Q. Han, Z. Li, and M.-M. Cheng,
“Delving deep into label smoothing,” </span><em id="bib.bib11.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">IEEE Transactions on Image
Processing</em><span id="bib.bib11.3.3" class="ltx_text" style="font-size:80%;">, vol. 30, pp. 5984–5996, 2021.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:80%;">
M. Everingham, L. V. Gool, C. K. I. Williams, J. M. Winn, and A. Zisserman,
“The pascal visual object classes (voc) challenge.” </span><em id="bib.bib12.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">Int. J. Comput.
Vis.</em><span id="bib.bib12.3.3" class="ltx_text" style="font-size:80%;">, vol. 88, no. 2, pp. 303–338, 2010. [Online]. Available:
</span><a target="_blank" href="http://dblp.uni-trier.de/db/journals/ijcv/ijcv88.html#EveringhamGWWZ10" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:80%;">http://dblp.uni-trier.de/db/journals/ijcv/ijcv88.html#EveringhamGWWZ10</a><span id="bib.bib12.4.4" class="ltx_text" style="font-size:80%;">
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:80%;">
M. Xu, Z. Zhang, H. Hu, J. Wang, L. Wang, F. Wei, X. Bai, and Z. Liu,
“End-to-end semi-supervised object detection with soft teacher,” in
</span><em id="bib.bib13.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">Proceedings of the IEEE/CVF International Conference on Computer
Vision</em><span id="bib.bib13.3.3" class="ltx_text" style="font-size:80%;">, 2021, pp. 3060–3069.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:80%;">
D. Lam, R. Kuzma, K. McGee, S. Dooley, M. Laielli, M. Klaric, Y. Bulatov, and
B. McCord, “xview: Objects in context in overhead imagery,” </span><em id="bib.bib14.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">arXiv
preprint arXiv:1802.07856</em><span id="bib.bib14.3.3" class="ltx_text" style="font-size:80%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:80%;">
G. Jocher, A. Chaurasia, A. Stoken, J. Borovec, Y. Kwon, K. Michael, and
J. Fang, “ultralytics/yolov5: v6. 2-yolov5 classification models, apple m1,
reproducibility, clearml and deci. ai integrations,” </span><em id="bib.bib15.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">Zenodo. org</em><span id="bib.bib15.3.3" class="ltx_text" style="font-size:80%;">,
2022.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:80%;">
T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Dollár, and C. L. Zitnick, “Microsoft coco: Common objects in
context,” in </span><em id="bib.bib16.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">European conference on computer vision</em><span id="bib.bib16.3.3" class="ltx_text" style="font-size:80%;">.   Springer, 2014, pp. 740–755.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:80%;">
J. Hui, “Map (mean average precision) for object detection,” Apr 2019.
[Online]. Available:
</span><a target="_blank" href="https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:80%;">https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173</a><span id="bib.bib17.2.2" class="ltx_text" style="font-size:80%;">
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:80%;">
Baeldung, “Intersection over union for object detection,” Sep 2022. [Online].
Available:
</span><a target="_blank" href="https://www.baeldung.com/cs/object-detection-intersection-vs-union" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:80%;">https://www.baeldung.com/cs/object-detection-intersection-vs-union</a><span id="bib.bib18.2.2" class="ltx_text" style="font-size:80%;">
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:80%;">
“F-score,” May 2019. [Online]. Available:
</span><a target="_blank" href="https://deepai.org/machine-learning-glossary-and-terms/f-score" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:80%;">https://deepai.org/machine-learning-glossary-and-terms/f-score</a><span id="bib.bib19.2.2" class="ltx_text" style="font-size:80%;">
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:80%;">
P. Lihi Gur Arie, “The practical guide for object detection with yolov5
algorithm,” Apr 2022. [Online]. Available:
</span><a target="_blank" href="https://towardsdatascience.com/the-practical-guide-for-object-detection-with-yolov5-algorithm-74c04aac4843" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:80%;">https://towardsdatascience.com/the-practical-guide-for-object-detection-with-yolov5-algorithm-74c04aac4843</a><span id="bib.bib20.2.2" class="ltx_text" style="font-size:80%;">
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:80%;">
Ultralytics, “Yolov5 (6.0/6.1) brief summary · issue 6998 ·
ultralytics/yolov5.” [Online]. Available:
</span><a target="_blank" href="https://github.com/ultralytics/yolov5/issues/6998" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:80%;">https://github.com/ultralytics/yolov5/issues/6998</a><span id="bib.bib21.2.2" class="ltx_text" style="font-size:80%;">
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:80%;">
M. Minderer, A. Gritsenko, A. Stone, M. Neumann, D. Weissenborn,
A. Dosovitskiy, A. Mahendran, A. Arnab, M. Dehghani, Z. Shen </span><em id="bib.bib22.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib22.3.3" class="ltx_text" style="font-size:80%;">,
“Simple open-vocabulary object detection with vision transformers,”
</span><em id="bib.bib22.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">arXiv preprint arXiv:2205.06230</em><span id="bib.bib22.5.5" class="ltx_text" style="font-size:80%;">, 2022.
</span>
</span>
</li>
</ul>
</section>
<section id="Sx3" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section" style="font-size:80%;">Appendix</h2>

<div id="Sx3.p1" class="ltx_para">
<p id="Sx3.p1.1" class="ltx_p"><span id="Sx3.p1.1.1" class="ltx_text" style="font-size:80%;">Dataset information from the training sets. Instance counts bounding boxes, bounding boxes, box center locations, and box width and height.</span></p>
</div>
<figure id="Sx3.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="Sx3.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2212.00585/assets/Figures/labels_train.jpg" id="Sx3.F4.sf1.g1" class="ltx_graphics ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span id="Sx3.F4.sf1.4.1.1" class="ltx_text" style="font-size:113%;">((a))</span> </span><span id="Sx3.F4.sf1.5.2" class="ltx_text" style="font-size:113%;">Train Set 1 Dataset Information</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="Sx3.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2212.00585/assets/Figures/train_val_batch2_pred.jpg" id="Sx3.F4.sf2.g1" class="ltx_graphics ltx_img_square" width="598" height="542" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span id="Sx3.F4.sf2.4.1.1" class="ltx_text" style="font-size:113%;">((b))</span> </span><span id="Sx3.F4.sf2.5.2" class="ltx_text" style="font-size:113%;">Train Set 1 Soft Labels</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span id="Sx3.F4.4.1.1" class="ltx_text" style="font-size:113%;">Figure 4</span>: </span><span id="Sx3.F4.5.2" class="ltx_text" style="font-size:113%;">Dataset Information (a) and Example Predictions (b)</span></figcaption>
</figure>
<figure id="Sx3.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2212.00585/assets/Figures/owlvit.png" id="Sx3.F5.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="509" height="709" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span id="Sx3.F5.4.1.1" class="ltx_text" style="font-size:113%;">Figure 5</span>: </span><span id="Sx3.F5.5.2" class="ltx_text" style="font-size:113%;">OWL-ViT YOLOv5 COCO-VAL GT, 0.1, 0.25 Training Metrics</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="Sx3.F5.6" class="ltx_p ltx_figure_panel ltx_align_center"><span id="Sx3.F5.6.1" class="ltx_text" style="font-size:80%;">As shown, mAP at 0.5 IoU for the ground truth is 0.2225, OWL-ViT at 0.25 confidence is .1844 (-17.12%), and OWL-ViT at 0.10 confidence is .1534 (-31.05%). Results for this experiment can be found at this public tensorboard: </span><a target="_blank" href="https://tensorboard.dev/experiment/J5prk5YeRKqZEuLPigU7jw/#scalars" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:80%;">https://tensorboard.dev/experiment/J5prk5YeRKqZEuLPigU7jw/#scalars</a><span id="Sx3.F5.6.2" class="ltx_text" style="font-size:80%;"></span></p>
</div>
</div>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2212.00584" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2212.00585" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2212.00585">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2212.00585" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2212.00586" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 08:33:52 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
