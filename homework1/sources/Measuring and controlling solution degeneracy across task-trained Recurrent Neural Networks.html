<!DOCTYPE html><html lang="en" data-theme="light"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks</title>
<!--Generated on Fri Oct  4 22:49:51 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">




<base href="https://arxiv.org/html/2410.03972v1/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2410.03972v1">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" title="Toggle dark/light mode" aria-label="System preference">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        
        
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="https://arxiv.org/html/2410.03972v1/#myForm">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2410.03972v1">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2410.03972v1" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" title="Toggle dark/light mode">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        
        
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist"><li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#abstract" title="Abstract">
      <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref"></span>
        Abstract
      </span>
    </a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S1" title="In Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S1.SS1" title="In 1 Introduction ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>Contributions</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S2" title="In Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S2.SS1" title="In 2 Methods ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Model Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S2.SS2" title="In 2 Methods ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Tasks</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S3" title="In Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S3.SS1" title="In 3 Results ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Characterizing tasks complexity and neural dynamics (RNN hidden states)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S3.SS2" title="In 3 Results ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Characterizing the degeneracy of task-trained RNNs</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S3.SS2.SSS1" title="In 3.2 Characterizing the degeneracy of task-trained RNNs ‣ 3 Results ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Degeneracy in dynamics decreases with task complexity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S3.SS2.SSS2" title="In 3.2 Characterizing the degeneracy of task-trained RNNs ‣ 3 Results ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Degeneracy in weights increases with task complexity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S3.SS2.SSS3" title="In 3.2 Characterizing the degeneracy of task-trained RNNs ‣ 3 Results ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.3 </span>Degeneracy in out-of-distribution generalization behaviors decreases with task complexity</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S3.SS3" title="In 3 Results ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Controlling degeneracy of task-trained RNNs</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S4" title="In Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Related work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S5" title="In Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#A1" title="In Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Task and training details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#A1.SS1" title="In Appendix A Task and training details ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>N-Bit Flip Flip</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#A1.SS2" title="In Appendix A Task and training details ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Delayed Discrimination</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#A1.SS3" title="In Appendix A Task and training details ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Sine Wave Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#A1.SS4" title="In Appendix A Task and training details ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4 </span>Path Integration</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#A2" title="In Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Characterizing the Attractorness of the Tasks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#A3" title="In Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Dynamical Similarity Analysis (DSA)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#A4" title="In Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Permutation-independent distance between weights</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib" title="References">
      <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref"></span>
        References
      </span>
    </a></li></ol></nav>

<div class="ltx_page_content"><div id="target-section" class="section"><a id="license-tr" href="https://info.arxiv.org/help/license/index.html#licenses-available">License: CC BY-NC-ND 4.0</a><div id="watermark-tr">arXiv:2410.03972v1 [cs.LG] 04 Oct 2024</div></div>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Measuring and controlling 
<br class="ltx_break">solution degeneracy across task-trained 
<br class="ltx_break">Recurrent Neural Networks</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ann Huang, Satpreet H. Singh, Kanaka Rajan 
<br class="ltx_break">Department of Neurobiology, Harvard Medical School 
<br class="ltx_break">Kempner Institute, Harvard University 
<br class="ltx_break">Boston, MA, USA 
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">{annhuang@g, satpreet_singh@hms, kanaka_rajan@hms}.harvard.edu</span>
</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract" id="abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id2.id1">Task-trained recurrent neural networks (RNNs) are versatile models of dynamical processes widely used in machine learning and neuroscience.
While RNNs are easily trained to perform a wide range of tasks, the nature and extent of the degeneracy in the resultant solutions (i.e., the variability across trained RNNs) remain poorly understood.
Here, we provide a unified framework for analyzing degeneracy across three levels: behavior, neural dynamics, and weight space.
We analyzed RNNs trained on diverse tasks across machine learning and neuroscience domains, including N-bit flip-flop, sine wave generation, delayed discrimination, and path integration.
Our key finding is that the variability across RNN solutions, quantified on the basis of neural dynamics and trained weights, depends primarily on network capacity and task characteristics such as complexity.
We introduce information-theoretic measures to quantify task complexity and demonstrate that increasing task complexity consistently reduces degeneracy in neural dynamics and generalization behavior while increasing degeneracy in weight space.
These relationships hold across diverse tasks and can be used to control the degeneracy of the solution space of task-trained RNNs.
Furthermore, we provide several strategies to control solution degeneracy, enabling task-trained RNNs to learn more consistent or diverse solutions as needed.
We envision that these insights will lead to more reliable machine learning models and could inspire strategies to better understand and control degeneracy observed in neuroscience experiments.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Recurrent neural networks (RNNs) are widely used across machine learning and computational neuroscience for modeling dynamic processes.
They can be efficiently trained using standard nonconvex optimization techniques and have proven useful for understanding neural dynamics during task performance <cite class="ltx_cite ltx_citemacro_citep">(Sussillo, <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib53" title="">2014</a>; Rajan et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib49" title="">2016</a>; Barak, <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib3" title="">2017</a>; Mastrogiuseppe &amp; Ostojic, <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib41" title="">2018</a>; Vyas et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib58" title="">2020</a>; Driscoll et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib14" title="">2024</a>)</cite>.
While it is well-known that in feedforward networks, differences in weight initialization and randomness during training (stochastic gradients, mini-batch sample variability, etc.)
can cause different final solutions, less is known about how such differences manifest when training RNNs
<cite class="ltx_cite ltx_citemacro_citep">(Das &amp; Fiete, <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib13" title="">2020</a>; Turner &amp; Barak, <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib56" title="">2024</a>; Collins et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib9" title="">2022</a>; Glorot &amp; Bengio, <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib26" title="">2010</a>; Fort et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib18" title="">2019</a>; Goodfellow et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib28" title="">2015</a>; Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib38" title="">2018</a>; Jastrzebski et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib35" title="">2018</a>; Chaudhari et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib8" title="">2017</a>; Frankle &amp; Carbin, <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib19" title="">2019</a>; Kornblith et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib37" title="">2019</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Traditionally, the study of task-trained RNNs has often focused on analyzing models trained using a single approach, implicitly assuming that multiple RNNs trained on the same task would converge to similar solutions, even when trained differently or when starting from different initial conditions.
However, recent work has shown that this assumption may not hold universally.
For instance, <cite class="ltx_cite ltx_citemacro_cite">Maheswaranathan et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib40" title="">2019</a>)</cite> found that while trained RNNs may share certain topological features, their internal representation geometry can vary widely.
Similarly, <cite class="ltx_cite ltx_citemacro_cite">Turner et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib57" title="">2021</a>)</cite> discovered that task-trained networks can develop qualitatively distinct dynamics.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">These contrasting findings raise a fundamental question about the degeneracy of task-trained RNN solutions: when do networks trained on the same task converge to similar dynamics and representations, and when do they exhibit substantial degeneracy?
Previous studies offer conflicting perspectives, with some suggesting that task-trained RNNs exhibit dynamics that are universal across instances <cite class="ltx_cite ltx_citemacro_citep">(Maheswaranathan et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib40" title="">2019</a>)</cite>, while others emphasize the degeneracy across individual solutions <cite class="ltx_cite ltx_citemacro_citep">(Turner et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib57" title="">2021</a>; Galgali et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib21" title="">2023</a>; Gholamrezaei &amp; Whishaw, <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib24" title="">2009</a>; Gao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib22" title="">2017a</a>; Mehrer et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib42" title="">2020</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this paper, we reconcile these differing views by providing a unified framework for analyzing degeneracy at three levels: behavior, neural dynamics, and weight space.
We hypothesize that the variation in solutions of trained RNNs is influenced by <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">task under-specification</span>.
In other words, when constraints do not uniquely or adequately determine the network’s solution to a given task, we observe greater variability across trained networks <cite class="ltx_cite ltx_citemacro_citep">(D’Amour et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib11" title="">2020</a>)</cite>. To test this hypothesis, we investigate how different task characteristics, particularly at the level of complexity of the inputs and outputs, affect degeneracy of the solutions found by task-trained RNNs at three levels (across behavioral, neural, and weight space).
We simulated four tasks – N-Bits Flip Flip, Delayed Discrimination, Sine Wave Generation, and Path Integration <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S2.F1" title="Figure 1 ‣ 2.2 Tasks ‣ 2 Methods ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">1</span></a>. Our key finding is that as task complexity increases, the solution space becomes more constrained, reducing degeneracy in both behavior and neural dynamics but increasing degeneracy in the underlying network weights.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Our simulations confirm the above hypothesis at the behavioral and neural-dynamical levels: with increasing task complexity, the degeneracy of solutions – measured at the behavioral level by the coefficient of variation of the out-of-distribution performance and at the dynamical level by the Dynamical Similarity Analysis distance
– decreases consistently across all four tasks we tested <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S3.F3" title="Figure 3 ‣ 3.2.1 Degeneracy in dynamics decreases with task complexity ‣ 3.2 Characterizing the degeneracy of task-trained RNNs ‣ 3 Results ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">3</span></a> <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S3.F5" title="Figure 5 ‣ 3.2.3 Degeneracy in out-of-distribution generalization behaviors decreases with task complexity ‣ 3.2 Characterizing the degeneracy of task-trained RNNs ‣ 3 Results ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">5</span></a> <cite class="ltx_cite ltx_citemacro_citep">(Ostrow et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib46" title="">2023</a>)</cite>.
Interestingly, at the weight level, we observe the opposite trend: the degeneracy of solutions increases with task complexity.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">After quantifying degeneracy at behavioral, neural, and weight-levels, next we propose practical strategies for controlling the level of degeneracy in task-trained RNNs, including altering task complexity, incorporating auxiliary loss functions, and applying structural constraints during training. These methods provide flexibility for researchers aiming to tailor the training process to their specific needs, whether they are seeking more consistent <cite class="ltx_cite ltx_citemacro_citep">(Kepple et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib36" title="">2022</a>)</cite> or more diverse RNN solutions <cite class="ltx_cite ltx_citemacro_citep">(Liebana&nbsp;Garcia et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib39" title="">2023</a>; Fascianelli et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib17" title="">2024</a>; Pan-Vazquez et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib47" title="">2024</a>; Kepple et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib36" title="">2022</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Contributions</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S1.SS1.p1">
<p class="ltx_p" id="S1.SS1.p1.1">Our contributions with this paper are as follows:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">Information-theoretic quantification of task complexity</span>: We introduce a new framework for assessing task complexity by quantifying the information content in the input and target output time series of a task. This measure effectively captures the extent to which input-output signals constrain neural dynamics. Furthermore, this measure correlates well with the neural-dynamical degeneracy we observed both across different complexity levels within the same task and across different tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">Quantifying the degeneracy of solution spaces</span>: We measure degeneracy at the behavioral, dynamical, and weight levels in populations of trained networks across four different tasks of wide applicability to both ML and neuroscience. Our findings reveal that, across all tasks, increased task complexity leads to lower degeneracy at the behavioral and neural dynamical levels, but concomitantly higher degeneracy at the weight level.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">Methods for controlling degeneracy</span>: We propose five techniques for controlling degeneracy in task-trained RNNs, including manipulating task complexity, applying specific regularization during training through auxiliary losses, and structural constraints during training. Notably, we demonstrate that both neural dynamical degeneracy and weight degeneracy can be manipulated to change either in the same (“covariant”) or opposite directions (“contravariance”), providing flexibility for researchers to tailor network solutions to their specific needs <cite class="ltx_cite ltx_citemacro_citep">(Cao &amp; Yamins, <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib7" title="">2024</a>; Kepple et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib36" title="">2022</a>; Fascianelli et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib17" title="">2024</a>; Maheswaranathan et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib40" title="">2019</a>; Durstewitz et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib15" title="">2023</a>; Gilpin, <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib25" title="">2024</a>)</cite> .</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methods</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Model Architecture</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.8">We used discrete-time nonlinear <em class="ltx_emph ltx_font_italic" id="S2.SS1.p1.8.1">vanilla</em> recurrent neural networks (RNNs), where the update rule is defined as:
<math alttext="\displaystyle\mathbf{h}_{t}=F(\mathbf{h}_{t-1},\mathbf{x}_{t})=\tanh\left(%
\mathbf{W}_{h}\mathbf{h}_{t-1}+\mathbf{W}_{x}\mathbf{x}_{t}+\mathbf{b}\right)," class="ltx_Math" display="inline" id="S2.SS1.p1.1.m1.2"><semantics id="S2.SS1.p1.1.m1.2a"><mrow id="S2.SS1.p1.1.m1.2.2.1" xref="S2.SS1.p1.1.m1.2.2.1.1.cmml"><mrow id="S2.SS1.p1.1.m1.2.2.1.1" xref="S2.SS1.p1.1.m1.2.2.1.1.cmml"><msub id="S2.SS1.p1.1.m1.2.2.1.1.5" xref="S2.SS1.p1.1.m1.2.2.1.1.5.cmml"><mi id="S2.SS1.p1.1.m1.2.2.1.1.5.2" xref="S2.SS1.p1.1.m1.2.2.1.1.5.2.cmml">𝐡</mi><mi id="S2.SS1.p1.1.m1.2.2.1.1.5.3" xref="S2.SS1.p1.1.m1.2.2.1.1.5.3.cmml">t</mi></msub><mo id="S2.SS1.p1.1.m1.2.2.1.1.6" xref="S2.SS1.p1.1.m1.2.2.1.1.6.cmml">=</mo><mrow id="S2.SS1.p1.1.m1.2.2.1.1.2" xref="S2.SS1.p1.1.m1.2.2.1.1.2.cmml"><mi id="S2.SS1.p1.1.m1.2.2.1.1.2.4" xref="S2.SS1.p1.1.m1.2.2.1.1.2.4.cmml">F</mi><mo id="S2.SS1.p1.1.m1.2.2.1.1.2.3" xref="S2.SS1.p1.1.m1.2.2.1.1.2.3.cmml">⁢</mo><mrow id="S2.SS1.p1.1.m1.2.2.1.1.2.2.2" xref="S2.SS1.p1.1.m1.2.2.1.1.2.2.3.cmml"><mo id="S2.SS1.p1.1.m1.2.2.1.1.2.2.2.3" stretchy="false" xref="S2.SS1.p1.1.m1.2.2.1.1.2.2.3.cmml">(</mo><msub id="S2.SS1.p1.1.m1.2.2.1.1.1.1.1.1" xref="S2.SS1.p1.1.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S2.SS1.p1.1.m1.2.2.1.1.1.1.1.1.2" xref="S2.SS1.p1.1.m1.2.2.1.1.1.1.1.1.2.cmml">𝐡</mi><mrow id="S2.SS1.p1.1.m1.2.2.1.1.1.1.1.1.3" xref="S2.SS1.p1.1.m1.2.2.1.1.1.1.1.1.3.cmml"><mi id="S2.SS1.p1.1.m1.2.2.1.1.1.1.1.1.3.2" xref="S2.SS1.p1.1.m1.2.2.1.1.1.1.1.1.3.2.cmml">t</mi><mo id="S2.SS1.p1.1.m1.2.2.1.1.1.1.1.1.3.1" xref="S2.SS1.p1.1.m1.2.2.1.1.1.1.1.1.3.1.cmml">−</mo><mn id="S2.SS1.p1.1.m1.2.2.1.1.1.1.1.1.3.3" xref="S2.SS1.p1.1.m1.2.2.1.1.1.1.1.1.3.3.cmml">1</mn></mrow></msub><mo id="S2.SS1.p1.1.m1.2.2.1.1.2.2.2.4" xref="S2.SS1.p1.1.m1.2.2.1.1.2.2.3.cmml">,</mo><msub id="S2.SS1.p1.1.m1.2.2.1.1.2.2.2.2" xref="S2.SS1.p1.1.m1.2.2.1.1.2.2.2.2.cmml"><mi id="S2.SS1.p1.1.m1.2.2.1.1.2.2.2.2.2" xref="S2.SS1.p1.1.m1.2.2.1.1.2.2.2.2.2.cmml">𝐱</mi><mi id="S2.SS1.p1.1.m1.2.2.1.1.2.2.2.2.3" xref="S2.SS1.p1.1.m1.2.2.1.1.2.2.2.2.3.cmml">t</mi></msub><mo id="S2.SS1.p1.1.m1.2.2.1.1.2.2.2.5" stretchy="false" xref="S2.SS1.p1.1.m1.2.2.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S2.SS1.p1.1.m1.2.2.1.1.7" xref="S2.SS1.p1.1.m1.2.2.1.1.7.cmml">=</mo><mrow id="S2.SS1.p1.1.m1.2.2.1.1.3.1" xref="S2.SS1.p1.1.m1.2.2.1.1.3.2.cmml"><mi id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">tanh</mi><mo id="S2.SS1.p1.1.m1.2.2.1.1.3.1a" xref="S2.SS1.p1.1.m1.2.2.1.1.3.2.cmml">⁡</mo><mrow id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1" xref="S2.SS1.p1.1.m1.2.2.1.1.3.2.cmml"><mo id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.2" xref="S2.SS1.p1.1.m1.2.2.1.1.3.2.cmml">(</mo><mrow id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.cmml"><mrow id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.cmml"><msub id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.2" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.2.cmml"><mi id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.2.2" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.2.2.cmml">𝐖</mi><mi id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.2.3" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.2.3.cmml">h</mi></msub><mo id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.1" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.1.cmml">⁢</mo><msub id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.3" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.3.cmml"><mi id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.3.2" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.3.2.cmml">𝐡</mi><mrow id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.3.3" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.3.3.cmml"><mi id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.3.3.2" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.3.3.2.cmml">t</mi><mo id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.3.3.1" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.3.3.1.cmml">−</mo><mn id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.3.3.3" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.3.3.3.cmml">1</mn></mrow></msub></mrow><mo id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.1" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.1.cmml">+</mo><mrow id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3.cmml"><msub id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3.2" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3.2.cmml"><mi id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3.2.2" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3.2.2.cmml">𝐖</mi><mi id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3.2.3" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3.2.3.cmml">x</mi></msub><mo id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3.1" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3.1.cmml">⁢</mo><msub id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3.3" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3.3.cmml"><mi id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3.3.2" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3.3.2.cmml">𝐱</mi><mi id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3.3.3" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3.3.3.cmml">t</mi></msub></mrow><mo id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.1a" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.1.cmml">+</mo><mi id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.4" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.4.cmml">𝐛</mi></mrow><mo id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.3" xref="S2.SS1.p1.1.m1.2.2.1.1.3.2.cmml">)</mo></mrow></mrow></mrow><mo id="S2.SS1.p1.1.m1.2.2.1.2" xref="S2.SS1.p1.1.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.2b"><apply id="S2.SS1.p1.1.m1.2.2.1.1.cmml" xref="S2.SS1.p1.1.m1.2.2.1"><and id="S2.SS1.p1.1.m1.2.2.1.1a.cmml" xref="S2.SS1.p1.1.m1.2.2.1"></and><apply id="S2.SS1.p1.1.m1.2.2.1.1b.cmml" xref="S2.SS1.p1.1.m1.2.2.1"><eq id="S2.SS1.p1.1.m1.2.2.1.1.6.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.6"></eq><apply id="S2.SS1.p1.1.m1.2.2.1.1.5.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.5"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.2.2.1.1.5.1.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.5">subscript</csymbol><ci id="S2.SS1.p1.1.m1.2.2.1.1.5.2.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.5.2">𝐡</ci><ci id="S2.SS1.p1.1.m1.2.2.1.1.5.3.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.5.3">𝑡</ci></apply><apply id="S2.SS1.p1.1.m1.2.2.1.1.2.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.2"><times id="S2.SS1.p1.1.m1.2.2.1.1.2.3.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.2.3"></times><ci id="S2.SS1.p1.1.m1.2.2.1.1.2.4.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.2.4">𝐹</ci><interval closure="open" id="S2.SS1.p1.1.m1.2.2.1.1.2.2.3.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.2.2.2"><apply id="S2.SS1.p1.1.m1.2.2.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S2.SS1.p1.1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.1.1.1.1.2">𝐡</ci><apply id="S2.SS1.p1.1.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.1.1.1.1.3"><minus id="S2.SS1.p1.1.m1.2.2.1.1.1.1.1.1.3.1.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.1.1.1.1.3.1"></minus><ci id="S2.SS1.p1.1.m1.2.2.1.1.1.1.1.1.3.2.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.1.1.1.1.3.2">𝑡</ci><cn id="S2.SS1.p1.1.m1.2.2.1.1.1.1.1.1.3.3.cmml" type="integer" xref="S2.SS1.p1.1.m1.2.2.1.1.1.1.1.1.3.3">1</cn></apply></apply><apply id="S2.SS1.p1.1.m1.2.2.1.1.2.2.2.2.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.2.2.1.1.2.2.2.2.1.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.2.2.2.2">subscript</csymbol><ci id="S2.SS1.p1.1.m1.2.2.1.1.2.2.2.2.2.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.2.2.2.2.2">𝐱</ci><ci id="S2.SS1.p1.1.m1.2.2.1.1.2.2.2.2.3.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.2.2.2.2.3">𝑡</ci></apply></interval></apply></apply><apply id="S2.SS1.p1.1.m1.2.2.1.1c.cmml" xref="S2.SS1.p1.1.m1.2.2.1"><eq id="S2.SS1.p1.1.m1.2.2.1.1.7.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.7"></eq><share href="https://arxiv.org/html/2410.03972v1#S2.SS1.p1.1.m1.2.2.1.1.2.cmml" id="S2.SS1.p1.1.m1.2.2.1.1d.cmml" xref="S2.SS1.p1.1.m1.2.2.1"></share><apply id="S2.SS1.p1.1.m1.2.2.1.1.3.2.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1"><tanh id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1"></tanh><apply id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1"><plus id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.1"></plus><apply id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2"><times id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.1.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.1"></times><apply id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.2.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.2.1.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.2">subscript</csymbol><ci id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.2.2.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.2.2">𝐖</ci><ci id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.2.3.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.2.3">ℎ</ci></apply><apply id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.3.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.3"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.3.1.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.3">subscript</csymbol><ci id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.3.2.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.3.2">𝐡</ci><apply id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.3.3.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.3.3"><minus id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.3.3.1.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.3.3.1"></minus><ci id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.3.3.2.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.3.3.2">𝑡</ci><cn id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.3.3.3.cmml" type="integer" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.2.3.3.3">1</cn></apply></apply></apply><apply id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3"><times id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3.1.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3.1"></times><apply id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3.2.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3.2.1.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3.2">subscript</csymbol><ci id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3.2.2.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3.2.2">𝐖</ci><ci id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3.2.3.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3.2.3">𝑥</ci></apply><apply id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3.3.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3.3"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3.3.1.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3.3">subscript</csymbol><ci id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3.3.2.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3.3.2">𝐱</ci><ci id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3.3.3.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.3.3.3">𝑡</ci></apply></apply><ci id="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.4.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.3.1.1.1.4">𝐛</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.2c">\displaystyle\mathbf{h}_{t}=F(\mathbf{h}_{t-1},\mathbf{x}_{t})=\tanh\left(%
\mathbf{W}_{h}\mathbf{h}_{t-1}+\mathbf{W}_{x}\mathbf{x}_{t}+\mathbf{b}\right),</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.1.m1.2d">bold_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_F ( bold_h start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = roman_tanh ( bold_W start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT bold_h start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT + bold_W start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + bold_b ) ,</annotation></semantics></math>
where <math alttext="\mathbf{h}_{t}\in\mathbb{R}^{n}" class="ltx_Math" display="inline" id="S2.SS1.p1.2.m2.1"><semantics id="S2.SS1.p1.2.m2.1a"><mrow id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml"><msub id="S2.SS1.p1.2.m2.1.1.2" xref="S2.SS1.p1.2.m2.1.1.2.cmml"><mi id="S2.SS1.p1.2.m2.1.1.2.2" xref="S2.SS1.p1.2.m2.1.1.2.2.cmml">𝐡</mi><mi id="S2.SS1.p1.2.m2.1.1.2.3" xref="S2.SS1.p1.2.m2.1.1.2.3.cmml">t</mi></msub><mo id="S2.SS1.p1.2.m2.1.1.1" xref="S2.SS1.p1.2.m2.1.1.1.cmml">∈</mo><msup id="S2.SS1.p1.2.m2.1.1.3" xref="S2.SS1.p1.2.m2.1.1.3.cmml"><mi id="S2.SS1.p1.2.m2.1.1.3.2" xref="S2.SS1.p1.2.m2.1.1.3.2.cmml">ℝ</mi><mi id="S2.SS1.p1.2.m2.1.1.3.3" xref="S2.SS1.p1.2.m2.1.1.3.3.cmml">n</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><apply id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1"><in id="S2.SS1.p1.2.m2.1.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1.1"></in><apply id="S2.SS1.p1.2.m2.1.1.2.cmml" xref="S2.SS1.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.1.1.2.1.cmml" xref="S2.SS1.p1.2.m2.1.1.2">subscript</csymbol><ci id="S2.SS1.p1.2.m2.1.1.2.2.cmml" xref="S2.SS1.p1.2.m2.1.1.2.2">𝐡</ci><ci id="S2.SS1.p1.2.m2.1.1.2.3.cmml" xref="S2.SS1.p1.2.m2.1.1.2.3">𝑡</ci></apply><apply id="S2.SS1.p1.2.m2.1.1.3.cmml" xref="S2.SS1.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.1.1.3.1.cmml" xref="S2.SS1.p1.2.m2.1.1.3">superscript</csymbol><ci id="S2.SS1.p1.2.m2.1.1.3.2.cmml" xref="S2.SS1.p1.2.m2.1.1.3.2">ℝ</ci><ci id="S2.SS1.p1.2.m2.1.1.3.3.cmml" xref="S2.SS1.p1.2.m2.1.1.3.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">\mathbf{h}_{t}\in\mathbb{R}^{n}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.2.m2.1d">bold_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT</annotation></semantics></math> is the hidden state at time <math alttext="t" class="ltx_Math" display="inline" id="S2.SS1.p1.3.m3.1"><semantics id="S2.SS1.p1.3.m3.1a"><mi id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><ci id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">t</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.3.m3.1d">italic_t</annotation></semantics></math>, <math alttext="\mathbf{x}_{t}\in\mathbb{R}^{m}" class="ltx_Math" display="inline" id="S2.SS1.p1.4.m4.1"><semantics id="S2.SS1.p1.4.m4.1a"><mrow id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml"><msub id="S2.SS1.p1.4.m4.1.1.2" xref="S2.SS1.p1.4.m4.1.1.2.cmml"><mi id="S2.SS1.p1.4.m4.1.1.2.2" xref="S2.SS1.p1.4.m4.1.1.2.2.cmml">𝐱</mi><mi id="S2.SS1.p1.4.m4.1.1.2.3" xref="S2.SS1.p1.4.m4.1.1.2.3.cmml">t</mi></msub><mo id="S2.SS1.p1.4.m4.1.1.1" xref="S2.SS1.p1.4.m4.1.1.1.cmml">∈</mo><msup id="S2.SS1.p1.4.m4.1.1.3" xref="S2.SS1.p1.4.m4.1.1.3.cmml"><mi id="S2.SS1.p1.4.m4.1.1.3.2" xref="S2.SS1.p1.4.m4.1.1.3.2.cmml">ℝ</mi><mi id="S2.SS1.p1.4.m4.1.1.3.3" xref="S2.SS1.p1.4.m4.1.1.3.3.cmml">m</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><apply id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1"><in id="S2.SS1.p1.4.m4.1.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1.1"></in><apply id="S2.SS1.p1.4.m4.1.1.2.cmml" xref="S2.SS1.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p1.4.m4.1.1.2.1.cmml" xref="S2.SS1.p1.4.m4.1.1.2">subscript</csymbol><ci id="S2.SS1.p1.4.m4.1.1.2.2.cmml" xref="S2.SS1.p1.4.m4.1.1.2.2">𝐱</ci><ci id="S2.SS1.p1.4.m4.1.1.2.3.cmml" xref="S2.SS1.p1.4.m4.1.1.2.3">𝑡</ci></apply><apply id="S2.SS1.p1.4.m4.1.1.3.cmml" xref="S2.SS1.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.4.m4.1.1.3.1.cmml" xref="S2.SS1.p1.4.m4.1.1.3">superscript</csymbol><ci id="S2.SS1.p1.4.m4.1.1.3.2.cmml" xref="S2.SS1.p1.4.m4.1.1.3.2">ℝ</ci><ci id="S2.SS1.p1.4.m4.1.1.3.3.cmml" xref="S2.SS1.p1.4.m4.1.1.3.3">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">\mathbf{x}_{t}\in\mathbb{R}^{m}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.4.m4.1d">bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT</annotation></semantics></math> is the input at time <math alttext="t" class="ltx_Math" display="inline" id="S2.SS1.p1.5.m5.1"><semantics id="S2.SS1.p1.5.m5.1a"><mi id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><ci id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">t</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.5.m5.1d">italic_t</annotation></semantics></math>, <math alttext="\mathbf{W}_{h}\in\mathbb{R}^{n\times n}" class="ltx_Math" display="inline" id="S2.SS1.p1.6.m6.1"><semantics id="S2.SS1.p1.6.m6.1a"><mrow id="S2.SS1.p1.6.m6.1.1" xref="S2.SS1.p1.6.m6.1.1.cmml"><msub id="S2.SS1.p1.6.m6.1.1.2" xref="S2.SS1.p1.6.m6.1.1.2.cmml"><mi id="S2.SS1.p1.6.m6.1.1.2.2" xref="S2.SS1.p1.6.m6.1.1.2.2.cmml">𝐖</mi><mi id="S2.SS1.p1.6.m6.1.1.2.3" xref="S2.SS1.p1.6.m6.1.1.2.3.cmml">h</mi></msub><mo id="S2.SS1.p1.6.m6.1.1.1" xref="S2.SS1.p1.6.m6.1.1.1.cmml">∈</mo><msup id="S2.SS1.p1.6.m6.1.1.3" xref="S2.SS1.p1.6.m6.1.1.3.cmml"><mi id="S2.SS1.p1.6.m6.1.1.3.2" xref="S2.SS1.p1.6.m6.1.1.3.2.cmml">ℝ</mi><mrow id="S2.SS1.p1.6.m6.1.1.3.3" xref="S2.SS1.p1.6.m6.1.1.3.3.cmml"><mi id="S2.SS1.p1.6.m6.1.1.3.3.2" xref="S2.SS1.p1.6.m6.1.1.3.3.2.cmml">n</mi><mo id="S2.SS1.p1.6.m6.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.SS1.p1.6.m6.1.1.3.3.1.cmml">×</mo><mi id="S2.SS1.p1.6.m6.1.1.3.3.3" xref="S2.SS1.p1.6.m6.1.1.3.3.3.cmml">n</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m6.1b"><apply id="S2.SS1.p1.6.m6.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1"><in id="S2.SS1.p1.6.m6.1.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1.1"></in><apply id="S2.SS1.p1.6.m6.1.1.2.cmml" xref="S2.SS1.p1.6.m6.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p1.6.m6.1.1.2.1.cmml" xref="S2.SS1.p1.6.m6.1.1.2">subscript</csymbol><ci id="S2.SS1.p1.6.m6.1.1.2.2.cmml" xref="S2.SS1.p1.6.m6.1.1.2.2">𝐖</ci><ci id="S2.SS1.p1.6.m6.1.1.2.3.cmml" xref="S2.SS1.p1.6.m6.1.1.2.3">ℎ</ci></apply><apply id="S2.SS1.p1.6.m6.1.1.3.cmml" xref="S2.SS1.p1.6.m6.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.6.m6.1.1.3.1.cmml" xref="S2.SS1.p1.6.m6.1.1.3">superscript</csymbol><ci id="S2.SS1.p1.6.m6.1.1.3.2.cmml" xref="S2.SS1.p1.6.m6.1.1.3.2">ℝ</ci><apply id="S2.SS1.p1.6.m6.1.1.3.3.cmml" xref="S2.SS1.p1.6.m6.1.1.3.3"><times id="S2.SS1.p1.6.m6.1.1.3.3.1.cmml" xref="S2.SS1.p1.6.m6.1.1.3.3.1"></times><ci id="S2.SS1.p1.6.m6.1.1.3.3.2.cmml" xref="S2.SS1.p1.6.m6.1.1.3.3.2">𝑛</ci><ci id="S2.SS1.p1.6.m6.1.1.3.3.3.cmml" xref="S2.SS1.p1.6.m6.1.1.3.3.3">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m6.1c">\mathbf{W}_{h}\in\mathbb{R}^{n\times n}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.6.m6.1d">bold_W start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_n end_POSTSUPERSCRIPT</annotation></semantics></math> is the recurrent weight matrix, <math alttext="\mathbf{W}_{x}\in\mathbb{R}^{n\times m}" class="ltx_Math" display="inline" id="S2.SS1.p1.7.m7.1"><semantics id="S2.SS1.p1.7.m7.1a"><mrow id="S2.SS1.p1.7.m7.1.1" xref="S2.SS1.p1.7.m7.1.1.cmml"><msub id="S2.SS1.p1.7.m7.1.1.2" xref="S2.SS1.p1.7.m7.1.1.2.cmml"><mi id="S2.SS1.p1.7.m7.1.1.2.2" xref="S2.SS1.p1.7.m7.1.1.2.2.cmml">𝐖</mi><mi id="S2.SS1.p1.7.m7.1.1.2.3" xref="S2.SS1.p1.7.m7.1.1.2.3.cmml">x</mi></msub><mo id="S2.SS1.p1.7.m7.1.1.1" xref="S2.SS1.p1.7.m7.1.1.1.cmml">∈</mo><msup id="S2.SS1.p1.7.m7.1.1.3" xref="S2.SS1.p1.7.m7.1.1.3.cmml"><mi id="S2.SS1.p1.7.m7.1.1.3.2" xref="S2.SS1.p1.7.m7.1.1.3.2.cmml">ℝ</mi><mrow id="S2.SS1.p1.7.m7.1.1.3.3" xref="S2.SS1.p1.7.m7.1.1.3.3.cmml"><mi id="S2.SS1.p1.7.m7.1.1.3.3.2" xref="S2.SS1.p1.7.m7.1.1.3.3.2.cmml">n</mi><mo id="S2.SS1.p1.7.m7.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.SS1.p1.7.m7.1.1.3.3.1.cmml">×</mo><mi id="S2.SS1.p1.7.m7.1.1.3.3.3" xref="S2.SS1.p1.7.m7.1.1.3.3.3.cmml">m</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.7.m7.1b"><apply id="S2.SS1.p1.7.m7.1.1.cmml" xref="S2.SS1.p1.7.m7.1.1"><in id="S2.SS1.p1.7.m7.1.1.1.cmml" xref="S2.SS1.p1.7.m7.1.1.1"></in><apply id="S2.SS1.p1.7.m7.1.1.2.cmml" xref="S2.SS1.p1.7.m7.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p1.7.m7.1.1.2.1.cmml" xref="S2.SS1.p1.7.m7.1.1.2">subscript</csymbol><ci id="S2.SS1.p1.7.m7.1.1.2.2.cmml" xref="S2.SS1.p1.7.m7.1.1.2.2">𝐖</ci><ci id="S2.SS1.p1.7.m7.1.1.2.3.cmml" xref="S2.SS1.p1.7.m7.1.1.2.3">𝑥</ci></apply><apply id="S2.SS1.p1.7.m7.1.1.3.cmml" xref="S2.SS1.p1.7.m7.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.7.m7.1.1.3.1.cmml" xref="S2.SS1.p1.7.m7.1.1.3">superscript</csymbol><ci id="S2.SS1.p1.7.m7.1.1.3.2.cmml" xref="S2.SS1.p1.7.m7.1.1.3.2">ℝ</ci><apply id="S2.SS1.p1.7.m7.1.1.3.3.cmml" xref="S2.SS1.p1.7.m7.1.1.3.3"><times id="S2.SS1.p1.7.m7.1.1.3.3.1.cmml" xref="S2.SS1.p1.7.m7.1.1.3.3.1"></times><ci id="S2.SS1.p1.7.m7.1.1.3.3.2.cmml" xref="S2.SS1.p1.7.m7.1.1.3.3.2">𝑛</ci><ci id="S2.SS1.p1.7.m7.1.1.3.3.3.cmml" xref="S2.SS1.p1.7.m7.1.1.3.3.3">𝑚</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.7.m7.1c">\mathbf{W}_{x}\in\mathbb{R}^{n\times m}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.7.m7.1d">bold_W start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_m end_POSTSUPERSCRIPT</annotation></semantics></math> is the input weight matrix, and <math alttext="\mathbf{b}\in\mathbb{R}^{n}" class="ltx_Math" display="inline" id="S2.SS1.p1.8.m8.1"><semantics id="S2.SS1.p1.8.m8.1a"><mrow id="S2.SS1.p1.8.m8.1.1" xref="S2.SS1.p1.8.m8.1.1.cmml"><mi id="S2.SS1.p1.8.m8.1.1.2" xref="S2.SS1.p1.8.m8.1.1.2.cmml">𝐛</mi><mo id="S2.SS1.p1.8.m8.1.1.1" xref="S2.SS1.p1.8.m8.1.1.1.cmml">∈</mo><msup id="S2.SS1.p1.8.m8.1.1.3" xref="S2.SS1.p1.8.m8.1.1.3.cmml"><mi id="S2.SS1.p1.8.m8.1.1.3.2" xref="S2.SS1.p1.8.m8.1.1.3.2.cmml">ℝ</mi><mi id="S2.SS1.p1.8.m8.1.1.3.3" xref="S2.SS1.p1.8.m8.1.1.3.3.cmml">n</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.8.m8.1b"><apply id="S2.SS1.p1.8.m8.1.1.cmml" xref="S2.SS1.p1.8.m8.1.1"><in id="S2.SS1.p1.8.m8.1.1.1.cmml" xref="S2.SS1.p1.8.m8.1.1.1"></in><ci id="S2.SS1.p1.8.m8.1.1.2.cmml" xref="S2.SS1.p1.8.m8.1.1.2">𝐛</ci><apply id="S2.SS1.p1.8.m8.1.1.3.cmml" xref="S2.SS1.p1.8.m8.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.8.m8.1.1.3.1.cmml" xref="S2.SS1.p1.8.m8.1.1.3">superscript</csymbol><ci id="S2.SS1.p1.8.m8.1.1.3.2.cmml" xref="S2.SS1.p1.8.m8.1.1.3.2">ℝ</ci><ci id="S2.SS1.p1.8.m8.1.1.3.3.cmml" xref="S2.SS1.p1.8.m8.1.1.3.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.8.m8.1c">\mathbf{b}\in\mathbb{R}^{n}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.8.m8.1d">bold_b ∈ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT</annotation></semantics></math> is the bias vector.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">A linear readout layer is applied on top of the RNN hidden state to produce the model’s prediction at each time step. The RNNs use Backpropagation Through Time (BPTT) as the learning rule where the RNN is unrolled over time, allowing gradients to be computed for each time step in the sequence <cite class="ltx_cite ltx_citemacro_citep">(Werbos, <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib59" title="">1990</a>)</cite>. All networks are trained using supervised learning via the Adam optimizer, with a learning rate specific to each task determined via hyperparameter tuning (a list of all training-related hyperparameters can be found in Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#A1" title="Appendix A Task and training details ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">A</span></a>). For each task, we train 50 networks using different initializations, until they achieve near-asymptotic loss on the test set. For the N-Bit Flip-Flop and Path Integration tasks <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S2.F1" title="Figure 1 ‣ 2.2 Tasks ‣ 2 Methods ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">1</span></a>, we use RNNs with 64 hidden units and input/output dimensions appropriate to each task’s specific requirements.
For the Delayed Discrimination and Sine Wave Generation tasks <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S2.F1" title="Figure 1 ‣ 2.2 Tasks ‣ 2 Methods ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">1</span></a>, we use RNNs with 128 hidden units.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Tasks</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="157" id="S2.F1.g1" src="https://arxiv.org/html/2410.03972v1/extracted/5902981/figures/ICLR-01.png" width="598">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
<span class="ltx_text ltx_font_bold" id="S2.F1.6.1">Task suite:</span>
We train RNNs on a diverse set of four tasks:
<span class="ltx_text ltx_font_bold" id="S2.F1.7.2">N-Bit flip-flop</span>: Networks are trained to remember the last non-zero input for each of the N channels.
<span class="ltx_text ltx_font_bold" id="S2.F1.8.3">Delayed Discrimination</span>: Networks compare the magnitude of two temporally separated pulses across N channels.
<span class="ltx_text ltx_font_bold" id="S2.F1.9.4">Sine Wave Generation</span>: Networks receive static input indicating frequency and produce sine waves of the specified frequency across each of the N channels.
<span class="ltx_text ltx_font_bold" id="S2.F1.10.5">Path Integration</span>: Networks integrate velocity inputs to track position in a bounded arena in 2D or 3D space. Schematic only shows 2D environment.
</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.5"><span class="ltx_text ltx_font_bold" id="S2.SS2.p1.5.1">N-Bit Flip-Flop Task</span>
In this task, RNNs are provided with <math alttext="N" class="ltx_Math" display="inline" id="S2.SS2.p1.1.m1.1"><semantics id="S2.SS2.p1.1.m1.1a"><mi id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><ci id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.1.m1.1d">italic_N</annotation></semantics></math> independent input channels, each taking discrete values from <math alttext="\{-1,0,+1\}" class="ltx_Math" display="inline" id="S2.SS2.p1.2.m2.3"><semantics id="S2.SS2.p1.2.m2.3a"><mrow id="S2.SS2.p1.2.m2.3.3.2" xref="S2.SS2.p1.2.m2.3.3.3.cmml"><mo id="S2.SS2.p1.2.m2.3.3.2.3" stretchy="false" xref="S2.SS2.p1.2.m2.3.3.3.cmml">{</mo><mrow id="S2.SS2.p1.2.m2.2.2.1.1" xref="S2.SS2.p1.2.m2.2.2.1.1.cmml"><mo id="S2.SS2.p1.2.m2.2.2.1.1a" xref="S2.SS2.p1.2.m2.2.2.1.1.cmml">−</mo><mn id="S2.SS2.p1.2.m2.2.2.1.1.2" xref="S2.SS2.p1.2.m2.2.2.1.1.2.cmml">1</mn></mrow><mo id="S2.SS2.p1.2.m2.3.3.2.4" xref="S2.SS2.p1.2.m2.3.3.3.cmml">,</mo><mn id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml">0</mn><mo id="S2.SS2.p1.2.m2.3.3.2.5" xref="S2.SS2.p1.2.m2.3.3.3.cmml">,</mo><mrow id="S2.SS2.p1.2.m2.3.3.2.2" xref="S2.SS2.p1.2.m2.3.3.2.2.cmml"><mo id="S2.SS2.p1.2.m2.3.3.2.2a" xref="S2.SS2.p1.2.m2.3.3.2.2.cmml">+</mo><mn id="S2.SS2.p1.2.m2.3.3.2.2.2" xref="S2.SS2.p1.2.m2.3.3.2.2.2.cmml">1</mn></mrow><mo id="S2.SS2.p1.2.m2.3.3.2.6" stretchy="false" xref="S2.SS2.p1.2.m2.3.3.3.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.3b"><set id="S2.SS2.p1.2.m2.3.3.3.cmml" xref="S2.SS2.p1.2.m2.3.3.2"><apply id="S2.SS2.p1.2.m2.2.2.1.1.cmml" xref="S2.SS2.p1.2.m2.2.2.1.1"><minus id="S2.SS2.p1.2.m2.2.2.1.1.1.cmml" xref="S2.SS2.p1.2.m2.2.2.1.1"></minus><cn id="S2.SS2.p1.2.m2.2.2.1.1.2.cmml" type="integer" xref="S2.SS2.p1.2.m2.2.2.1.1.2">1</cn></apply><cn id="S2.SS2.p1.2.m2.1.1.cmml" type="integer" xref="S2.SS2.p1.2.m2.1.1">0</cn><apply id="S2.SS2.p1.2.m2.3.3.2.2.cmml" xref="S2.SS2.p1.2.m2.3.3.2.2"><plus id="S2.SS2.p1.2.m2.3.3.2.2.1.cmml" xref="S2.SS2.p1.2.m2.3.3.2.2"></plus><cn id="S2.SS2.p1.2.m2.3.3.2.2.2.cmml" type="integer" xref="S2.SS2.p1.2.m2.3.3.2.2.2">1</cn></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.3c">\{-1,0,+1\}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.2.m2.3d">{ - 1 , 0 , + 1 }</annotation></semantics></math>, with a fixed probability of switching <math alttext="p_{switch}" class="ltx_Math" display="inline" id="S2.SS2.p1.3.m3.1"><semantics id="S2.SS2.p1.3.m3.1a"><msub id="S2.SS2.p1.3.m3.1.1" xref="S2.SS2.p1.3.m3.1.1.cmml"><mi id="S2.SS2.p1.3.m3.1.1.2" xref="S2.SS2.p1.3.m3.1.1.2.cmml">p</mi><mrow id="S2.SS2.p1.3.m3.1.1.3" xref="S2.SS2.p1.3.m3.1.1.3.cmml"><mi id="S2.SS2.p1.3.m3.1.1.3.2" xref="S2.SS2.p1.3.m3.1.1.3.2.cmml">s</mi><mo id="S2.SS2.p1.3.m3.1.1.3.1" xref="S2.SS2.p1.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S2.SS2.p1.3.m3.1.1.3.3" xref="S2.SS2.p1.3.m3.1.1.3.3.cmml">w</mi><mo id="S2.SS2.p1.3.m3.1.1.3.1a" xref="S2.SS2.p1.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S2.SS2.p1.3.m3.1.1.3.4" xref="S2.SS2.p1.3.m3.1.1.3.4.cmml">i</mi><mo id="S2.SS2.p1.3.m3.1.1.3.1b" xref="S2.SS2.p1.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S2.SS2.p1.3.m3.1.1.3.5" xref="S2.SS2.p1.3.m3.1.1.3.5.cmml">t</mi><mo id="S2.SS2.p1.3.m3.1.1.3.1c" xref="S2.SS2.p1.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S2.SS2.p1.3.m3.1.1.3.6" xref="S2.SS2.p1.3.m3.1.1.3.6.cmml">c</mi><mo id="S2.SS2.p1.3.m3.1.1.3.1d" xref="S2.SS2.p1.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S2.SS2.p1.3.m3.1.1.3.7" xref="S2.SS2.p1.3.m3.1.1.3.7.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.1b"><apply id="S2.SS2.p1.3.m3.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.3.m3.1.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S2.SS2.p1.3.m3.1.1.2.cmml" xref="S2.SS2.p1.3.m3.1.1.2">𝑝</ci><apply id="S2.SS2.p1.3.m3.1.1.3.cmml" xref="S2.SS2.p1.3.m3.1.1.3"><times id="S2.SS2.p1.3.m3.1.1.3.1.cmml" xref="S2.SS2.p1.3.m3.1.1.3.1"></times><ci id="S2.SS2.p1.3.m3.1.1.3.2.cmml" xref="S2.SS2.p1.3.m3.1.1.3.2">𝑠</ci><ci id="S2.SS2.p1.3.m3.1.1.3.3.cmml" xref="S2.SS2.p1.3.m3.1.1.3.3">𝑤</ci><ci id="S2.SS2.p1.3.m3.1.1.3.4.cmml" xref="S2.SS2.p1.3.m3.1.1.3.4">𝑖</ci><ci id="S2.SS2.p1.3.m3.1.1.3.5.cmml" xref="S2.SS2.p1.3.m3.1.1.3.5">𝑡</ci><ci id="S2.SS2.p1.3.m3.1.1.3.6.cmml" xref="S2.SS2.p1.3.m3.1.1.3.6">𝑐</ci><ci id="S2.SS2.p1.3.m3.1.1.3.7.cmml" xref="S2.SS2.p1.3.m3.1.1.3.7">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.1c">p_{switch}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.3.m3.1d">italic_p start_POSTSUBSCRIPT italic_s italic_w italic_i italic_t italic_c italic_h end_POSTSUBSCRIPT</annotation></semantics></math>.
The network has <math alttext="N" class="ltx_Math" display="inline" id="S2.SS2.p1.4.m4.1"><semantics id="S2.SS2.p1.4.m4.1a"><mi id="S2.SS2.p1.4.m4.1.1" xref="S2.SS2.p1.4.m4.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.4.m4.1b"><ci id="S2.SS2.p1.4.m4.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.4.m4.1c">N</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.4.m4.1d">italic_N</annotation></semantics></math> output channels, each of which is required to remember the value of the last nonzero input received on its respective input channel.
We vary the complexity of the task by changing the number of input and/or output channels <math alttext="N" class="ltx_Math" display="inline" id="S2.SS2.p1.5.m5.1"><semantics id="S2.SS2.p1.5.m5.1a"><mi id="S2.SS2.p1.5.m5.1.1" xref="S2.SS2.p1.5.m5.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.5.m5.1b"><ci id="S2.SS2.p1.5.m5.1.1.cmml" xref="S2.SS2.p1.5.m5.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.5.m5.1c">N</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.5.m5.1d">italic_N</annotation></semantics></math>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.7"><span class="ltx_text ltx_font_bold" id="S2.SS2.p2.7.1">Delayed Discrimination Task</span>
The network is presented with two pulses of varying amplitudes <math alttext="f_{1},f_{2}\in[2,10]" class="ltx_Math" display="inline" id="S2.SS2.p2.1.m1.4"><semantics id="S2.SS2.p2.1.m1.4a"><mrow id="S2.SS2.p2.1.m1.4.4" xref="S2.SS2.p2.1.m1.4.4.cmml"><mrow id="S2.SS2.p2.1.m1.4.4.2.2" xref="S2.SS2.p2.1.m1.4.4.2.3.cmml"><msub id="S2.SS2.p2.1.m1.3.3.1.1.1" xref="S2.SS2.p2.1.m1.3.3.1.1.1.cmml"><mi id="S2.SS2.p2.1.m1.3.3.1.1.1.2" xref="S2.SS2.p2.1.m1.3.3.1.1.1.2.cmml">f</mi><mn id="S2.SS2.p2.1.m1.3.3.1.1.1.3" xref="S2.SS2.p2.1.m1.3.3.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS2.p2.1.m1.4.4.2.2.3" xref="S2.SS2.p2.1.m1.4.4.2.3.cmml">,</mo><msub id="S2.SS2.p2.1.m1.4.4.2.2.2" xref="S2.SS2.p2.1.m1.4.4.2.2.2.cmml"><mi id="S2.SS2.p2.1.m1.4.4.2.2.2.2" xref="S2.SS2.p2.1.m1.4.4.2.2.2.2.cmml">f</mi><mn id="S2.SS2.p2.1.m1.4.4.2.2.2.3" xref="S2.SS2.p2.1.m1.4.4.2.2.2.3.cmml">2</mn></msub></mrow><mo id="S2.SS2.p2.1.m1.4.4.3" xref="S2.SS2.p2.1.m1.4.4.3.cmml">∈</mo><mrow id="S2.SS2.p2.1.m1.4.4.4.2" xref="S2.SS2.p2.1.m1.4.4.4.1.cmml"><mo id="S2.SS2.p2.1.m1.4.4.4.2.1" stretchy="false" xref="S2.SS2.p2.1.m1.4.4.4.1.cmml">[</mo><mn id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml">2</mn><mo id="S2.SS2.p2.1.m1.4.4.4.2.2" xref="S2.SS2.p2.1.m1.4.4.4.1.cmml">,</mo><mn id="S2.SS2.p2.1.m1.2.2" xref="S2.SS2.p2.1.m1.2.2.cmml">10</mn><mo id="S2.SS2.p2.1.m1.4.4.4.2.3" stretchy="false" xref="S2.SS2.p2.1.m1.4.4.4.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.4b"><apply id="S2.SS2.p2.1.m1.4.4.cmml" xref="S2.SS2.p2.1.m1.4.4"><in id="S2.SS2.p2.1.m1.4.4.3.cmml" xref="S2.SS2.p2.1.m1.4.4.3"></in><list id="S2.SS2.p2.1.m1.4.4.2.3.cmml" xref="S2.SS2.p2.1.m1.4.4.2.2"><apply id="S2.SS2.p2.1.m1.3.3.1.1.1.cmml" xref="S2.SS2.p2.1.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.1.m1.3.3.1.1.1.1.cmml" xref="S2.SS2.p2.1.m1.3.3.1.1.1">subscript</csymbol><ci id="S2.SS2.p2.1.m1.3.3.1.1.1.2.cmml" xref="S2.SS2.p2.1.m1.3.3.1.1.1.2">𝑓</ci><cn id="S2.SS2.p2.1.m1.3.3.1.1.1.3.cmml" type="integer" xref="S2.SS2.p2.1.m1.3.3.1.1.1.3">1</cn></apply><apply id="S2.SS2.p2.1.m1.4.4.2.2.2.cmml" xref="S2.SS2.p2.1.m1.4.4.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.p2.1.m1.4.4.2.2.2.1.cmml" xref="S2.SS2.p2.1.m1.4.4.2.2.2">subscript</csymbol><ci id="S2.SS2.p2.1.m1.4.4.2.2.2.2.cmml" xref="S2.SS2.p2.1.m1.4.4.2.2.2.2">𝑓</ci><cn id="S2.SS2.p2.1.m1.4.4.2.2.2.3.cmml" type="integer" xref="S2.SS2.p2.1.m1.4.4.2.2.2.3">2</cn></apply></list><interval closure="closed" id="S2.SS2.p2.1.m1.4.4.4.1.cmml" xref="S2.SS2.p2.1.m1.4.4.4.2"><cn id="S2.SS2.p2.1.m1.1.1.cmml" type="integer" xref="S2.SS2.p2.1.m1.1.1">2</cn><cn id="S2.SS2.p2.1.m1.2.2.cmml" type="integer" xref="S2.SS2.p2.1.m1.2.2">10</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.4c">f_{1},f_{2}\in[2,10]</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.1.m1.4d">italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ∈ [ 2 , 10 ]</annotation></semantics></math>, separated in time by a variable delay <math alttext="t\in[5,20]" class="ltx_Math" display="inline" id="S2.SS2.p2.2.m2.2"><semantics id="S2.SS2.p2.2.m2.2a"><mrow id="S2.SS2.p2.2.m2.2.3" xref="S2.SS2.p2.2.m2.2.3.cmml"><mi id="S2.SS2.p2.2.m2.2.3.2" xref="S2.SS2.p2.2.m2.2.3.2.cmml">t</mi><mo id="S2.SS2.p2.2.m2.2.3.1" xref="S2.SS2.p2.2.m2.2.3.1.cmml">∈</mo><mrow id="S2.SS2.p2.2.m2.2.3.3.2" xref="S2.SS2.p2.2.m2.2.3.3.1.cmml"><mo id="S2.SS2.p2.2.m2.2.3.3.2.1" stretchy="false" xref="S2.SS2.p2.2.m2.2.3.3.1.cmml">[</mo><mn id="S2.SS2.p2.2.m2.1.1" xref="S2.SS2.p2.2.m2.1.1.cmml">5</mn><mo id="S2.SS2.p2.2.m2.2.3.3.2.2" xref="S2.SS2.p2.2.m2.2.3.3.1.cmml">,</mo><mn id="S2.SS2.p2.2.m2.2.2" xref="S2.SS2.p2.2.m2.2.2.cmml">20</mn><mo id="S2.SS2.p2.2.m2.2.3.3.2.3" stretchy="false" xref="S2.SS2.p2.2.m2.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.2.m2.2b"><apply id="S2.SS2.p2.2.m2.2.3.cmml" xref="S2.SS2.p2.2.m2.2.3"><in id="S2.SS2.p2.2.m2.2.3.1.cmml" xref="S2.SS2.p2.2.m2.2.3.1"></in><ci id="S2.SS2.p2.2.m2.2.3.2.cmml" xref="S2.SS2.p2.2.m2.2.3.2">𝑡</ci><interval closure="closed" id="S2.SS2.p2.2.m2.2.3.3.1.cmml" xref="S2.SS2.p2.2.m2.2.3.3.2"><cn id="S2.SS2.p2.2.m2.1.1.cmml" type="integer" xref="S2.SS2.p2.2.m2.1.1">5</cn><cn id="S2.SS2.p2.2.m2.2.2.cmml" type="integer" xref="S2.SS2.p2.2.m2.2.2">20</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.2.m2.2c">t\in[5,20]</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.2.m2.2d">italic_t ∈ [ 5 , 20 ]</annotation></semantics></math> time steps.
The network is required to output the sign of the difference between the two amplitudes, i.e., <math alttext="\operatorname{sign}(f_{2}-f_{1})" class="ltx_Math" display="inline" id="S2.SS2.p2.3.m3.2"><semantics id="S2.SS2.p2.3.m3.2a"><mrow id="S2.SS2.p2.3.m3.2.2.1" xref="S2.SS2.p2.3.m3.2.2.2.cmml"><mi id="S2.SS2.p2.3.m3.1.1" xref="S2.SS2.p2.3.m3.1.1.cmml">sign</mi><mo id="S2.SS2.p2.3.m3.2.2.1a" xref="S2.SS2.p2.3.m3.2.2.2.cmml">⁡</mo><mrow id="S2.SS2.p2.3.m3.2.2.1.1" xref="S2.SS2.p2.3.m3.2.2.2.cmml"><mo id="S2.SS2.p2.3.m3.2.2.1.1.2" stretchy="false" xref="S2.SS2.p2.3.m3.2.2.2.cmml">(</mo><mrow id="S2.SS2.p2.3.m3.2.2.1.1.1" xref="S2.SS2.p2.3.m3.2.2.1.1.1.cmml"><msub id="S2.SS2.p2.3.m3.2.2.1.1.1.2" xref="S2.SS2.p2.3.m3.2.2.1.1.1.2.cmml"><mi id="S2.SS2.p2.3.m3.2.2.1.1.1.2.2" xref="S2.SS2.p2.3.m3.2.2.1.1.1.2.2.cmml">f</mi><mn id="S2.SS2.p2.3.m3.2.2.1.1.1.2.3" xref="S2.SS2.p2.3.m3.2.2.1.1.1.2.3.cmml">2</mn></msub><mo id="S2.SS2.p2.3.m3.2.2.1.1.1.1" xref="S2.SS2.p2.3.m3.2.2.1.1.1.1.cmml">−</mo><msub id="S2.SS2.p2.3.m3.2.2.1.1.1.3" xref="S2.SS2.p2.3.m3.2.2.1.1.1.3.cmml"><mi id="S2.SS2.p2.3.m3.2.2.1.1.1.3.2" xref="S2.SS2.p2.3.m3.2.2.1.1.1.3.2.cmml">f</mi><mn id="S2.SS2.p2.3.m3.2.2.1.1.1.3.3" xref="S2.SS2.p2.3.m3.2.2.1.1.1.3.3.cmml">1</mn></msub></mrow><mo id="S2.SS2.p2.3.m3.2.2.1.1.3" stretchy="false" xref="S2.SS2.p2.3.m3.2.2.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.3.m3.2b"><apply id="S2.SS2.p2.3.m3.2.2.2.cmml" xref="S2.SS2.p2.3.m3.2.2.1"><ci id="S2.SS2.p2.3.m3.1.1.cmml" xref="S2.SS2.p2.3.m3.1.1">sign</ci><apply id="S2.SS2.p2.3.m3.2.2.1.1.1.cmml" xref="S2.SS2.p2.3.m3.2.2.1.1.1"><minus id="S2.SS2.p2.3.m3.2.2.1.1.1.1.cmml" xref="S2.SS2.p2.3.m3.2.2.1.1.1.1"></minus><apply id="S2.SS2.p2.3.m3.2.2.1.1.1.2.cmml" xref="S2.SS2.p2.3.m3.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.p2.3.m3.2.2.1.1.1.2.1.cmml" xref="S2.SS2.p2.3.m3.2.2.1.1.1.2">subscript</csymbol><ci id="S2.SS2.p2.3.m3.2.2.1.1.1.2.2.cmml" xref="S2.SS2.p2.3.m3.2.2.1.1.1.2.2">𝑓</ci><cn id="S2.SS2.p2.3.m3.2.2.1.1.1.2.3.cmml" type="integer" xref="S2.SS2.p2.3.m3.2.2.1.1.1.2.3">2</cn></apply><apply id="S2.SS2.p2.3.m3.2.2.1.1.1.3.cmml" xref="S2.SS2.p2.3.m3.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.p2.3.m3.2.2.1.1.1.3.1.cmml" xref="S2.SS2.p2.3.m3.2.2.1.1.1.3">subscript</csymbol><ci id="S2.SS2.p2.3.m3.2.2.1.1.1.3.2.cmml" xref="S2.SS2.p2.3.m3.2.2.1.1.1.3.2">𝑓</ci><cn id="S2.SS2.p2.3.m3.2.2.1.1.1.3.3.cmml" type="integer" xref="S2.SS2.p2.3.m3.2.2.1.1.1.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.3.m3.2c">\operatorname{sign}(f_{2}-f_{1})</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.3.m3.2d">roman_sign ( italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT )</annotation></semantics></math>.
This task can be extended to have <math alttext="N" class="ltx_Math" display="inline" id="S2.SS2.p2.4.m4.1"><semantics id="S2.SS2.p2.4.m4.1a"><mi id="S2.SS2.p2.4.m4.1.1" xref="S2.SS2.p2.4.m4.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.4.m4.1b"><ci id="S2.SS2.p2.4.m4.1.1.cmml" xref="S2.SS2.p2.4.m4.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.4.m4.1c">N</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.4.m4.1d">italic_N</annotation></semantics></math> independent input and output channels, where the network must compare <math alttext="f_{1}" class="ltx_Math" display="inline" id="S2.SS2.p2.5.m5.1"><semantics id="S2.SS2.p2.5.m5.1a"><msub id="S2.SS2.p2.5.m5.1.1" xref="S2.SS2.p2.5.m5.1.1.cmml"><mi id="S2.SS2.p2.5.m5.1.1.2" xref="S2.SS2.p2.5.m5.1.1.2.cmml">f</mi><mn id="S2.SS2.p2.5.m5.1.1.3" xref="S2.SS2.p2.5.m5.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.5.m5.1b"><apply id="S2.SS2.p2.5.m5.1.1.cmml" xref="S2.SS2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.5.m5.1.1.1.cmml" xref="S2.SS2.p2.5.m5.1.1">subscript</csymbol><ci id="S2.SS2.p2.5.m5.1.1.2.cmml" xref="S2.SS2.p2.5.m5.1.1.2">𝑓</ci><cn id="S2.SS2.p2.5.m5.1.1.3.cmml" type="integer" xref="S2.SS2.p2.5.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.5.m5.1c">f_{1}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.5.m5.1d">italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="f_{2}" class="ltx_Math" display="inline" id="S2.SS2.p2.6.m6.1"><semantics id="S2.SS2.p2.6.m6.1a"><msub id="S2.SS2.p2.6.m6.1.1" xref="S2.SS2.p2.6.m6.1.1.cmml"><mi id="S2.SS2.p2.6.m6.1.1.2" xref="S2.SS2.p2.6.m6.1.1.2.cmml">f</mi><mn id="S2.SS2.p2.6.m6.1.1.3" xref="S2.SS2.p2.6.m6.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.6.m6.1b"><apply id="S2.SS2.p2.6.m6.1.1.cmml" xref="S2.SS2.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.6.m6.1.1.1.cmml" xref="S2.SS2.p2.6.m6.1.1">subscript</csymbol><ci id="S2.SS2.p2.6.m6.1.1.2.cmml" xref="S2.SS2.p2.6.m6.1.1.2">𝑓</ci><cn id="S2.SS2.p2.6.m6.1.1.3.cmml" type="integer" xref="S2.SS2.p2.6.m6.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.6.m6.1c">f_{2}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.6.m6.1d">italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> within each channel separately.
We vary the complexity of the task by varying the number of independent input and/or output channels <math alttext="N" class="ltx_Math" display="inline" id="S2.SS2.p2.7.m7.1"><semantics id="S2.SS2.p2.7.m7.1a"><mi id="S2.SS2.p2.7.m7.1.1" xref="S2.SS2.p2.7.m7.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.7.m7.1b"><ci id="S2.SS2.p2.7.m7.1.1.cmml" xref="S2.SS2.p2.7.m7.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.7.m7.1c">N</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.7.m7.1d">italic_N</annotation></semantics></math>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.8"><span class="ltx_text ltx_font_bold" id="S2.SS2.p3.8.1">Sine Wave Generation</span>
RNNs receive a static input indicating a target frequency <math alttext="f\in[1,30]" class="ltx_Math" display="inline" id="S2.SS2.p3.1.m1.2"><semantics id="S2.SS2.p3.1.m1.2a"><mrow id="S2.SS2.p3.1.m1.2.3" xref="S2.SS2.p3.1.m1.2.3.cmml"><mi id="S2.SS2.p3.1.m1.2.3.2" xref="S2.SS2.p3.1.m1.2.3.2.cmml">f</mi><mo id="S2.SS2.p3.1.m1.2.3.1" xref="S2.SS2.p3.1.m1.2.3.1.cmml">∈</mo><mrow id="S2.SS2.p3.1.m1.2.3.3.2" xref="S2.SS2.p3.1.m1.2.3.3.1.cmml"><mo id="S2.SS2.p3.1.m1.2.3.3.2.1" stretchy="false" xref="S2.SS2.p3.1.m1.2.3.3.1.cmml">[</mo><mn id="S2.SS2.p3.1.m1.1.1" xref="S2.SS2.p3.1.m1.1.1.cmml">1</mn><mo id="S2.SS2.p3.1.m1.2.3.3.2.2" xref="S2.SS2.p3.1.m1.2.3.3.1.cmml">,</mo><mn id="S2.SS2.p3.1.m1.2.2" xref="S2.SS2.p3.1.m1.2.2.cmml">30</mn><mo id="S2.SS2.p3.1.m1.2.3.3.2.3" stretchy="false" xref="S2.SS2.p3.1.m1.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.1.m1.2b"><apply id="S2.SS2.p3.1.m1.2.3.cmml" xref="S2.SS2.p3.1.m1.2.3"><in id="S2.SS2.p3.1.m1.2.3.1.cmml" xref="S2.SS2.p3.1.m1.2.3.1"></in><ci id="S2.SS2.p3.1.m1.2.3.2.cmml" xref="S2.SS2.p3.1.m1.2.3.2">𝑓</ci><interval closure="closed" id="S2.SS2.p3.1.m1.2.3.3.1.cmml" xref="S2.SS2.p3.1.m1.2.3.3.2"><cn id="S2.SS2.p3.1.m1.1.1.cmml" type="integer" xref="S2.SS2.p3.1.m1.1.1">1</cn><cn id="S2.SS2.p3.1.m1.2.2.cmml" type="integer" xref="S2.SS2.p3.1.m1.2.2">30</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.1.m1.2c">f\in[1,30]</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p3.1.m1.2d">italic_f ∈ [ 1 , 30 ]</annotation></semantics></math> and are required to generate a sine wave of that frequency over time, effectively converting the input <math alttext="f" class="ltx_Math" display="inline" id="S2.SS2.p3.2.m2.1"><semantics id="S2.SS2.p3.2.m2.1a"><mi id="S2.SS2.p3.2.m2.1.1" xref="S2.SS2.p3.2.m2.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.2.m2.1b"><ci id="S2.SS2.p3.2.m2.1.1.cmml" xref="S2.SS2.p3.2.m2.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.2.m2.1c">f</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p3.2.m2.1d">italic_f</annotation></semantics></math> into the output <math alttext="\sin(2\pi ft)" class="ltx_Math" display="inline" id="S2.SS2.p3.3.m3.2"><semantics id="S2.SS2.p3.3.m3.2a"><mrow id="S2.SS2.p3.3.m3.2.2.1" xref="S2.SS2.p3.3.m3.2.2.2.cmml"><mi id="S2.SS2.p3.3.m3.1.1" xref="S2.SS2.p3.3.m3.1.1.cmml">sin</mi><mo id="S2.SS2.p3.3.m3.2.2.1a" xref="S2.SS2.p3.3.m3.2.2.2.cmml">⁡</mo><mrow id="S2.SS2.p3.3.m3.2.2.1.1" xref="S2.SS2.p3.3.m3.2.2.2.cmml"><mo id="S2.SS2.p3.3.m3.2.2.1.1.2" stretchy="false" xref="S2.SS2.p3.3.m3.2.2.2.cmml">(</mo><mrow id="S2.SS2.p3.3.m3.2.2.1.1.1" xref="S2.SS2.p3.3.m3.2.2.1.1.1.cmml"><mn id="S2.SS2.p3.3.m3.2.2.1.1.1.2" xref="S2.SS2.p3.3.m3.2.2.1.1.1.2.cmml">2</mn><mo id="S2.SS2.p3.3.m3.2.2.1.1.1.1" xref="S2.SS2.p3.3.m3.2.2.1.1.1.1.cmml">⁢</mo><mi id="S2.SS2.p3.3.m3.2.2.1.1.1.3" xref="S2.SS2.p3.3.m3.2.2.1.1.1.3.cmml">π</mi><mo id="S2.SS2.p3.3.m3.2.2.1.1.1.1a" xref="S2.SS2.p3.3.m3.2.2.1.1.1.1.cmml">⁢</mo><mi id="S2.SS2.p3.3.m3.2.2.1.1.1.4" xref="S2.SS2.p3.3.m3.2.2.1.1.1.4.cmml">f</mi><mo id="S2.SS2.p3.3.m3.2.2.1.1.1.1b" xref="S2.SS2.p3.3.m3.2.2.1.1.1.1.cmml">⁢</mo><mi id="S2.SS2.p3.3.m3.2.2.1.1.1.5" xref="S2.SS2.p3.3.m3.2.2.1.1.1.5.cmml">t</mi></mrow><mo id="S2.SS2.p3.3.m3.2.2.1.1.3" stretchy="false" xref="S2.SS2.p3.3.m3.2.2.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.3.m3.2b"><apply id="S2.SS2.p3.3.m3.2.2.2.cmml" xref="S2.SS2.p3.3.m3.2.2.1"><sin id="S2.SS2.p3.3.m3.1.1.cmml" xref="S2.SS2.p3.3.m3.1.1"></sin><apply id="S2.SS2.p3.3.m3.2.2.1.1.1.cmml" xref="S2.SS2.p3.3.m3.2.2.1.1.1"><times id="S2.SS2.p3.3.m3.2.2.1.1.1.1.cmml" xref="S2.SS2.p3.3.m3.2.2.1.1.1.1"></times><cn id="S2.SS2.p3.3.m3.2.2.1.1.1.2.cmml" type="integer" xref="S2.SS2.p3.3.m3.2.2.1.1.1.2">2</cn><ci id="S2.SS2.p3.3.m3.2.2.1.1.1.3.cmml" xref="S2.SS2.p3.3.m3.2.2.1.1.1.3">𝜋</ci><ci id="S2.SS2.p3.3.m3.2.2.1.1.1.4.cmml" xref="S2.SS2.p3.3.m3.2.2.1.1.1.4">𝑓</ci><ci id="S2.SS2.p3.3.m3.2.2.1.1.1.5.cmml" xref="S2.SS2.p3.3.m3.2.2.1.1.1.5">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.3.m3.2c">\sin(2\pi ft)</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p3.3.m3.2d">roman_sin ( 2 italic_π italic_f italic_t )</annotation></semantics></math>.
We define <math alttext="N_{\text{freq}}" class="ltx_Math" display="inline" id="S2.SS2.p3.4.m4.1"><semantics id="S2.SS2.p3.4.m4.1a"><msub id="S2.SS2.p3.4.m4.1.1" xref="S2.SS2.p3.4.m4.1.1.cmml"><mi id="S2.SS2.p3.4.m4.1.1.2" xref="S2.SS2.p3.4.m4.1.1.2.cmml">N</mi><mtext id="S2.SS2.p3.4.m4.1.1.3" xref="S2.SS2.p3.4.m4.1.1.3a.cmml">freq</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.4.m4.1b"><apply id="S2.SS2.p3.4.m4.1.1.cmml" xref="S2.SS2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS2.p3.4.m4.1.1.1.cmml" xref="S2.SS2.p3.4.m4.1.1">subscript</csymbol><ci id="S2.SS2.p3.4.m4.1.1.2.cmml" xref="S2.SS2.p3.4.m4.1.1.2">𝑁</ci><ci id="S2.SS2.p3.4.m4.1.1.3a.cmml" xref="S2.SS2.p3.4.m4.1.1.3"><mtext id="S2.SS2.p3.4.m4.1.1.3.cmml" mathsize="70%" xref="S2.SS2.p3.4.m4.1.1.3">freq</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.4.m4.1c">N_{\text{freq}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p3.4.m4.1d">italic_N start_POSTSUBSCRIPT freq end_POSTSUBSCRIPT</annotation></semantics></math> as the number of possible target frequencies presented in the training set, equally spaced within the range <math alttext="[1,30]" class="ltx_Math" display="inline" id="S2.SS2.p3.5.m5.2"><semantics id="S2.SS2.p3.5.m5.2a"><mrow id="S2.SS2.p3.5.m5.2.3.2" xref="S2.SS2.p3.5.m5.2.3.1.cmml"><mo id="S2.SS2.p3.5.m5.2.3.2.1" stretchy="false" xref="S2.SS2.p3.5.m5.2.3.1.cmml">[</mo><mn id="S2.SS2.p3.5.m5.1.1" xref="S2.SS2.p3.5.m5.1.1.cmml">1</mn><mo id="S2.SS2.p3.5.m5.2.3.2.2" xref="S2.SS2.p3.5.m5.2.3.1.cmml">,</mo><mn id="S2.SS2.p3.5.m5.2.2" xref="S2.SS2.p3.5.m5.2.2.cmml">30</mn><mo id="S2.SS2.p3.5.m5.2.3.2.3" stretchy="false" xref="S2.SS2.p3.5.m5.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.5.m5.2b"><interval closure="closed" id="S2.SS2.p3.5.m5.2.3.1.cmml" xref="S2.SS2.p3.5.m5.2.3.2"><cn id="S2.SS2.p3.5.m5.1.1.cmml" type="integer" xref="S2.SS2.p3.5.m5.1.1">1</cn><cn id="S2.SS2.p3.5.m5.2.2.cmml" type="integer" xref="S2.SS2.p3.5.m5.2.2">30</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.5.m5.2c">[1,30]</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p3.5.m5.2d">[ 1 , 30 ]</annotation></semantics></math>.
This task can involve <math alttext="N" class="ltx_Math" display="inline" id="S2.SS2.p3.6.m6.1"><semantics id="S2.SS2.p3.6.m6.1a"><mi id="S2.SS2.p3.6.m6.1.1" xref="S2.SS2.p3.6.m6.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.6.m6.1b"><ci id="S2.SS2.p3.6.m6.1.1.cmml" xref="S2.SS2.p3.6.m6.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.6.m6.1c">N</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p3.6.m6.1d">italic_N</annotation></semantics></math> independent input and output channels, where each channel outputs a sine wave with the frequency specified by its respective input channel.
We vary the complexity of the task by changing <math alttext="N_{\text{freq}}" class="ltx_Math" display="inline" id="S2.SS2.p3.7.m7.1"><semantics id="S2.SS2.p3.7.m7.1a"><msub id="S2.SS2.p3.7.m7.1.1" xref="S2.SS2.p3.7.m7.1.1.cmml"><mi id="S2.SS2.p3.7.m7.1.1.2" xref="S2.SS2.p3.7.m7.1.1.2.cmml">N</mi><mtext id="S2.SS2.p3.7.m7.1.1.3" xref="S2.SS2.p3.7.m7.1.1.3a.cmml">freq</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.7.m7.1b"><apply id="S2.SS2.p3.7.m7.1.1.cmml" xref="S2.SS2.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS2.p3.7.m7.1.1.1.cmml" xref="S2.SS2.p3.7.m7.1.1">subscript</csymbol><ci id="S2.SS2.p3.7.m7.1.1.2.cmml" xref="S2.SS2.p3.7.m7.1.1.2">𝑁</ci><ci id="S2.SS2.p3.7.m7.1.1.3a.cmml" xref="S2.SS2.p3.7.m7.1.1.3"><mtext id="S2.SS2.p3.7.m7.1.1.3.cmml" mathsize="70%" xref="S2.SS2.p3.7.m7.1.1.3">freq</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.7.m7.1c">N_{\text{freq}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p3.7.m7.1d">italic_N start_POSTSUBSCRIPT freq end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="N" class="ltx_Math" display="inline" id="S2.SS2.p3.8.m8.1"><semantics id="S2.SS2.p3.8.m8.1a"><mi id="S2.SS2.p3.8.m8.1.1" xref="S2.SS2.p3.8.m8.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.8.m8.1b"><ci id="S2.SS2.p3.8.m8.1.1.cmml" xref="S2.SS2.p3.8.m8.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.8.m8.1c">N</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p3.8.m8.1d">italic_N</annotation></semantics></math>, the number of independent input and/or output channels.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.7"><span class="ltx_text ltx_font_bold" id="S2.SS2.p4.7.1">Path Integration Task</span>
Each network is initialized at a random initial location within a bounded 2D environment. At each time step, the network receives inputs representing the angular direction <math alttext="\theta" class="ltx_Math" display="inline" id="S2.SS2.p4.1.m1.1"><semantics id="S2.SS2.p4.1.m1.1a"><mi id="S2.SS2.p4.1.m1.1.1" xref="S2.SS2.p4.1.m1.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.1.m1.1b"><ci id="S2.SS2.p4.1.m1.1.1.cmml" xref="S2.SS2.p4.1.m1.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.1.m1.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p4.1.m1.1d">italic_θ</annotation></semantics></math> and speed <math alttext="v" class="ltx_Math" display="inline" id="S2.SS2.p4.2.m2.1"><semantics id="S2.SS2.p4.2.m2.1a"><mi id="S2.SS2.p4.2.m2.1.1" xref="S2.SS2.p4.2.m2.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.2.m2.1b"><ci id="S2.SS2.p4.2.m2.1.1.cmml" xref="S2.SS2.p4.2.m2.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.2.m2.1c">v</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p4.2.m2.1d">italic_v</annotation></semantics></math>, and must integrate this information over time to output the updated <math alttext="(x,y)" class="ltx_Math" display="inline" id="S2.SS2.p4.3.m3.2"><semantics id="S2.SS2.p4.3.m3.2a"><mrow id="S2.SS2.p4.3.m3.2.3.2" xref="S2.SS2.p4.3.m3.2.3.1.cmml"><mo id="S2.SS2.p4.3.m3.2.3.2.1" stretchy="false" xref="S2.SS2.p4.3.m3.2.3.1.cmml">(</mo><mi id="S2.SS2.p4.3.m3.1.1" xref="S2.SS2.p4.3.m3.1.1.cmml">x</mi><mo id="S2.SS2.p4.3.m3.2.3.2.2" xref="S2.SS2.p4.3.m3.2.3.1.cmml">,</mo><mi id="S2.SS2.p4.3.m3.2.2" xref="S2.SS2.p4.3.m3.2.2.cmml">y</mi><mo id="S2.SS2.p4.3.m3.2.3.2.3" stretchy="false" xref="S2.SS2.p4.3.m3.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.3.m3.2b"><interval closure="open" id="S2.SS2.p4.3.m3.2.3.1.cmml" xref="S2.SS2.p4.3.m3.2.3.2"><ci id="S2.SS2.p4.3.m3.1.1.cmml" xref="S2.SS2.p4.3.m3.1.1">𝑥</ci><ci id="S2.SS2.p4.3.m3.2.2.cmml" xref="S2.SS2.p4.3.m3.2.2">𝑦</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.3.m3.2c">(x,y)</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p4.3.m3.2d">( italic_x , italic_y )</annotation></semantics></math> location. To vary the complexity of the task, we introduce a 3D version of the task, where the network receives inputs <math alttext="\theta" class="ltx_Math" display="inline" id="S2.SS2.p4.4.m4.1"><semantics id="S2.SS2.p4.4.m4.1a"><mi id="S2.SS2.p4.4.m4.1.1" xref="S2.SS2.p4.4.m4.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.4.m4.1b"><ci id="S2.SS2.p4.4.m4.1.1.cmml" xref="S2.SS2.p4.4.m4.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.4.m4.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p4.4.m4.1d">italic_θ</annotation></semantics></math> (azimuth angle), <math alttext="\phi" class="ltx_Math" display="inline" id="S2.SS2.p4.5.m5.1"><semantics id="S2.SS2.p4.5.m5.1a"><mi id="S2.SS2.p4.5.m5.1.1" xref="S2.SS2.p4.5.m5.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.5.m5.1b"><ci id="S2.SS2.p4.5.m5.1.1.cmml" xref="S2.SS2.p4.5.m5.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.5.m5.1c">\phi</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p4.5.m5.1d">italic_ϕ</annotation></semantics></math> (elevation angle), and <math alttext="v" class="ltx_Math" display="inline" id="S2.SS2.p4.6.m6.1"><semantics id="S2.SS2.p4.6.m6.1a"><mi id="S2.SS2.p4.6.m6.1.1" xref="S2.SS2.p4.6.m6.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.6.m6.1b"><ci id="S2.SS2.p4.6.m6.1.1.cmml" xref="S2.SS2.p4.6.m6.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.6.m6.1c">v</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p4.6.m6.1d">italic_v</annotation></semantics></math> (speed), and must output the updated <math alttext="(x,y,z)" class="ltx_Math" display="inline" id="S2.SS2.p4.7.m7.3"><semantics id="S2.SS2.p4.7.m7.3a"><mrow id="S2.SS2.p4.7.m7.3.4.2" xref="S2.SS2.p4.7.m7.3.4.1.cmml"><mo id="S2.SS2.p4.7.m7.3.4.2.1" stretchy="false" xref="S2.SS2.p4.7.m7.3.4.1.cmml">(</mo><mi id="S2.SS2.p4.7.m7.1.1" xref="S2.SS2.p4.7.m7.1.1.cmml">x</mi><mo id="S2.SS2.p4.7.m7.3.4.2.2" xref="S2.SS2.p4.7.m7.3.4.1.cmml">,</mo><mi id="S2.SS2.p4.7.m7.2.2" xref="S2.SS2.p4.7.m7.2.2.cmml">y</mi><mo id="S2.SS2.p4.7.m7.3.4.2.3" xref="S2.SS2.p4.7.m7.3.4.1.cmml">,</mo><mi id="S2.SS2.p4.7.m7.3.3" xref="S2.SS2.p4.7.m7.3.3.cmml">z</mi><mo id="S2.SS2.p4.7.m7.3.4.2.4" stretchy="false" xref="S2.SS2.p4.7.m7.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.7.m7.3b"><vector id="S2.SS2.p4.7.m7.3.4.1.cmml" xref="S2.SS2.p4.7.m7.3.4.2"><ci id="S2.SS2.p4.7.m7.1.1.cmml" xref="S2.SS2.p4.7.m7.1.1">𝑥</ci><ci id="S2.SS2.p4.7.m7.2.2.cmml" xref="S2.SS2.p4.7.m7.2.2">𝑦</ci><ci id="S2.SS2.p4.7.m7.3.3.cmml" xref="S2.SS2.p4.7.m7.3.3">𝑧</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.7.m7.3c">(x,y,z)</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p4.7.m7.3d">( italic_x , italic_y , italic_z )</annotation></semantics></math> location.
The network effectively performs path integration by accumulating movement vectors based on the input directions and speeds.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Characterizing tasks complexity and neural dynamics (RNN hidden states)</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We quantified the task complexity by calculating the Shannon entropy of its input and target (output) time-series <cite class="ltx_cite ltx_citemacro_citep">(Shannon, <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib51" title="">1948</a>; Bialek et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib4" title="">2001</a>; Crutchfield &amp; Young, <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib10" title="">1989</a>)</cite>.
We chose this measure because it directly reflects the amount of information present in the task’s inputs and outputs, which the network must process and represent through its hidden state <cite class="ltx_cite ltx_citemacro_citep">(Tishby et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib55" title="">2000</a>)</cite>.
By quantifying the statistical complexity of the input and output time series, we capture the information-processing demand imposed on the network.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="388" id="S3.F2.g1" src="https://arxiv.org/html/2410.03972v1/extracted/5902981/figures/ICLR-02.png" width="598">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>
<span class="ltx_text ltx_font_bold" id="S3.F2.2.1">Characterizing tasks and RNN hidden states (neural dynamics):</span>
We analyze task complexity and its effects on RNN hidden states using multiple measures:
(A) Input complexity, measured as the entropy of the time-averaged input signal. We use the integer in the task name to indicate the number of input-output channels for each task except in “sineF”, where “F” stands for frequency, and the integer indicates the number of frequencies the network learns to produce.
(B) Output complexity, measured as the entropy of the time-averaged output signal
(C) Our tasks span three quadrants of the Input and Output Complexity space (excluding low input and low output complexity).
(D) The number of leading Principal Components (PCs) explaining 95% of the variance, which we use as a measure of the dimensionality of the task, increases consistently across tasks as difficulty increases.
(E) RNN hidden state (neural dynamics) trajectories projected onto the leading two PCs for Path Integration and three PCs for the other tasks.

</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.5">Without loss of generality, given a one-dimensional output time series <math alttext="(y_{1},y_{2},\dots,y_{T})" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.4"><semantics id="S3.SS1.p2.1.m1.4a"><mrow id="S3.SS1.p2.1.m1.4.4.3" xref="S3.SS1.p2.1.m1.4.4.4.cmml"><mo id="S3.SS1.p2.1.m1.4.4.3.4" stretchy="false" xref="S3.SS1.p2.1.m1.4.4.4.cmml">(</mo><msub id="S3.SS1.p2.1.m1.2.2.1.1" xref="S3.SS1.p2.1.m1.2.2.1.1.cmml"><mi id="S3.SS1.p2.1.m1.2.2.1.1.2" xref="S3.SS1.p2.1.m1.2.2.1.1.2.cmml">y</mi><mn id="S3.SS1.p2.1.m1.2.2.1.1.3" xref="S3.SS1.p2.1.m1.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p2.1.m1.4.4.3.5" xref="S3.SS1.p2.1.m1.4.4.4.cmml">,</mo><msub id="S3.SS1.p2.1.m1.3.3.2.2" xref="S3.SS1.p2.1.m1.3.3.2.2.cmml"><mi id="S3.SS1.p2.1.m1.3.3.2.2.2" xref="S3.SS1.p2.1.m1.3.3.2.2.2.cmml">y</mi><mn id="S3.SS1.p2.1.m1.3.3.2.2.3" xref="S3.SS1.p2.1.m1.3.3.2.2.3.cmml">2</mn></msub><mo id="S3.SS1.p2.1.m1.4.4.3.6" xref="S3.SS1.p2.1.m1.4.4.4.cmml">,</mo><mi id="S3.SS1.p2.1.m1.1.1" mathvariant="normal" xref="S3.SS1.p2.1.m1.1.1.cmml">…</mi><mo id="S3.SS1.p2.1.m1.4.4.3.7" xref="S3.SS1.p2.1.m1.4.4.4.cmml">,</mo><msub id="S3.SS1.p2.1.m1.4.4.3.3" xref="S3.SS1.p2.1.m1.4.4.3.3.cmml"><mi id="S3.SS1.p2.1.m1.4.4.3.3.2" xref="S3.SS1.p2.1.m1.4.4.3.3.2.cmml">y</mi><mi id="S3.SS1.p2.1.m1.4.4.3.3.3" xref="S3.SS1.p2.1.m1.4.4.3.3.3.cmml">T</mi></msub><mo id="S3.SS1.p2.1.m1.4.4.3.8" stretchy="false" xref="S3.SS1.p2.1.m1.4.4.4.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.4b"><vector id="S3.SS1.p2.1.m1.4.4.4.cmml" xref="S3.SS1.p2.1.m1.4.4.3"><apply id="S3.SS1.p2.1.m1.2.2.1.1.cmml" xref="S3.SS1.p2.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.2.2.1.1.1.cmml" xref="S3.SS1.p2.1.m1.2.2.1.1">subscript</csymbol><ci id="S3.SS1.p2.1.m1.2.2.1.1.2.cmml" xref="S3.SS1.p2.1.m1.2.2.1.1.2">𝑦</ci><cn id="S3.SS1.p2.1.m1.2.2.1.1.3.cmml" type="integer" xref="S3.SS1.p2.1.m1.2.2.1.1.3">1</cn></apply><apply id="S3.SS1.p2.1.m1.3.3.2.2.cmml" xref="S3.SS1.p2.1.m1.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.3.3.2.2.1.cmml" xref="S3.SS1.p2.1.m1.3.3.2.2">subscript</csymbol><ci id="S3.SS1.p2.1.m1.3.3.2.2.2.cmml" xref="S3.SS1.p2.1.m1.3.3.2.2.2">𝑦</ci><cn id="S3.SS1.p2.1.m1.3.3.2.2.3.cmml" type="integer" xref="S3.SS1.p2.1.m1.3.3.2.2.3">2</cn></apply><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">…</ci><apply id="S3.SS1.p2.1.m1.4.4.3.3.cmml" xref="S3.SS1.p2.1.m1.4.4.3.3"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.4.4.3.3.1.cmml" xref="S3.SS1.p2.1.m1.4.4.3.3">subscript</csymbol><ci id="S3.SS1.p2.1.m1.4.4.3.3.2.cmml" xref="S3.SS1.p2.1.m1.4.4.3.3.2">𝑦</ci><ci id="S3.SS1.p2.1.m1.4.4.3.3.3.cmml" xref="S3.SS1.p2.1.m1.4.4.3.3.3">𝑇</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.4c">(y_{1},y_{2},\dots,y_{T})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.4d">( italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_y start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT )</annotation></semantics></math>, the entropy of this time series is given by:
<math alttext="\displaystyle H(Y)=-\sum_{k=1}^{n}p(y_{k})\log_{2}p(y_{k})," class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.2"><semantics id="S3.SS1.p2.2.m2.2a"><mrow id="S3.SS1.p2.2.m2.2.2.1" xref="S3.SS1.p2.2.m2.2.2.1.1.cmml"><mrow id="S3.SS1.p2.2.m2.2.2.1.1" xref="S3.SS1.p2.2.m2.2.2.1.1.cmml"><mrow id="S3.SS1.p2.2.m2.2.2.1.1.4" xref="S3.SS1.p2.2.m2.2.2.1.1.4.cmml"><mi id="S3.SS1.p2.2.m2.2.2.1.1.4.2" xref="S3.SS1.p2.2.m2.2.2.1.1.4.2.cmml">H</mi><mo id="S3.SS1.p2.2.m2.2.2.1.1.4.1" xref="S3.SS1.p2.2.m2.2.2.1.1.4.1.cmml">⁢</mo><mrow id="S3.SS1.p2.2.m2.2.2.1.1.4.3.2" xref="S3.SS1.p2.2.m2.2.2.1.1.4.cmml"><mo id="S3.SS1.p2.2.m2.2.2.1.1.4.3.2.1" stretchy="false" xref="S3.SS1.p2.2.m2.2.2.1.1.4.cmml">(</mo><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">Y</mi><mo id="S3.SS1.p2.2.m2.2.2.1.1.4.3.2.2" stretchy="false" xref="S3.SS1.p2.2.m2.2.2.1.1.4.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p2.2.m2.2.2.1.1.3" xref="S3.SS1.p2.2.m2.2.2.1.1.3.cmml">=</mo><mrow id="S3.SS1.p2.2.m2.2.2.1.1.2" xref="S3.SS1.p2.2.m2.2.2.1.1.2.cmml"><mo id="S3.SS1.p2.2.m2.2.2.1.1.2a" xref="S3.SS1.p2.2.m2.2.2.1.1.2.cmml">−</mo><mrow id="S3.SS1.p2.2.m2.2.2.1.1.2.2" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.cmml"><mstyle displaystyle="true" id="S3.SS1.p2.2.m2.2.2.1.1.2.2.3" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.3.cmml"><munderover id="S3.SS1.p2.2.m2.2.2.1.1.2.2.3a" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.3.cmml"><mo id="S3.SS1.p2.2.m2.2.2.1.1.2.2.3.2.2" movablelimits="false" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.3.2.2.cmml">∑</mo><mrow id="S3.SS1.p2.2.m2.2.2.1.1.2.2.3.2.3" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.3.2.3.cmml"><mi id="S3.SS1.p2.2.m2.2.2.1.1.2.2.3.2.3.2" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.3.2.3.2.cmml">k</mi><mo id="S3.SS1.p2.2.m2.2.2.1.1.2.2.3.2.3.1" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.3.2.3.1.cmml">=</mo><mn id="S3.SS1.p2.2.m2.2.2.1.1.2.2.3.2.3.3" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.3.2.3.3.cmml">1</mn></mrow><mi id="S3.SS1.p2.2.m2.2.2.1.1.2.2.3.3" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.3.3.cmml">n</mi></munderover></mstyle><mrow id="S3.SS1.p2.2.m2.2.2.1.1.2.2.2" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.cmml"><mi id="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.4" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.4.cmml">p</mi><mo id="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.3" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.3.cmml">⁢</mo><mrow id="S3.SS1.p2.2.m2.2.2.1.1.1.1.1.1.1" xref="S3.SS1.p2.2.m2.2.2.1.1.1.1.1.1.1.1.cmml"><mo id="S3.SS1.p2.2.m2.2.2.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.SS1.p2.2.m2.2.2.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS1.p2.2.m2.2.2.1.1.1.1.1.1.1.1" xref="S3.SS1.p2.2.m2.2.2.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.2.m2.2.2.1.1.1.1.1.1.1.1.2" xref="S3.SS1.p2.2.m2.2.2.1.1.1.1.1.1.1.1.2.cmml">y</mi><mi id="S3.SS1.p2.2.m2.2.2.1.1.1.1.1.1.1.1.3" xref="S3.SS1.p2.2.m2.2.2.1.1.1.1.1.1.1.1.3.cmml">k</mi></msub><mo id="S3.SS1.p2.2.m2.2.2.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.SS1.p2.2.m2.2.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.3a" lspace="0.167em" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.3.cmml">⁢</mo><mrow id="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.5" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.5.cmml"><msub id="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.5.1" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.5.1.cmml"><mi id="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.5.1.2" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.5.1.2.cmml">log</mi><mn id="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.5.1.3" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.5.1.3.cmml">2</mn></msub><mo id="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.5a" lspace="0.167em" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.5.cmml">⁡</mo><mi id="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.5.2" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.5.2.cmml">p</mi></mrow><mo id="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.3b" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.3.cmml">⁢</mo><mrow id="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.2.1" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.2.1.1.cmml"><mo id="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.2.1.2" stretchy="false" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.2.1.1.cmml">(</mo><msub id="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.2.1.1" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.2.1.1.cmml"><mi id="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.2.1.1.2" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.2.1.1.2.cmml">y</mi><mi id="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.2.1.1.3" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.2.1.1.3.cmml">k</mi></msub><mo id="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.2.1.3" stretchy="false" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.2.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S3.SS1.p2.2.m2.2.2.1.2" xref="S3.SS1.p2.2.m2.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.2b"><apply id="S3.SS1.p2.2.m2.2.2.1.1.cmml" xref="S3.SS1.p2.2.m2.2.2.1"><eq id="S3.SS1.p2.2.m2.2.2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.3"></eq><apply id="S3.SS1.p2.2.m2.2.2.1.1.4.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.4"><times id="S3.SS1.p2.2.m2.2.2.1.1.4.1.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.4.1"></times><ci id="S3.SS1.p2.2.m2.2.2.1.1.4.2.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.4.2">𝐻</ci><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">𝑌</ci></apply><apply id="S3.SS1.p2.2.m2.2.2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.2"><minus id="S3.SS1.p2.2.m2.2.2.1.1.2.3.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.2"></minus><apply id="S3.SS1.p2.2.m2.2.2.1.1.2.2.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2"><apply id="S3.SS1.p2.2.m2.2.2.1.1.2.2.3.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.2.2.1.1.2.2.3.1.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.3">superscript</csymbol><apply id="S3.SS1.p2.2.m2.2.2.1.1.2.2.3.2.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.2.2.1.1.2.2.3.2.1.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.3">subscript</csymbol><sum id="S3.SS1.p2.2.m2.2.2.1.1.2.2.3.2.2.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.3.2.2"></sum><apply id="S3.SS1.p2.2.m2.2.2.1.1.2.2.3.2.3.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.3.2.3"><eq id="S3.SS1.p2.2.m2.2.2.1.1.2.2.3.2.3.1.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.3.2.3.1"></eq><ci id="S3.SS1.p2.2.m2.2.2.1.1.2.2.3.2.3.2.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.3.2.3.2">𝑘</ci><cn id="S3.SS1.p2.2.m2.2.2.1.1.2.2.3.2.3.3.cmml" type="integer" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.3.2.3.3">1</cn></apply></apply><ci id="S3.SS1.p2.2.m2.2.2.1.1.2.2.3.3.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.3.3">𝑛</ci></apply><apply id="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.2"><times id="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.3.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.3"></times><ci id="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.4.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.4">𝑝</ci><apply id="S3.SS1.p2.2.m2.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p2.2.m2.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.1.1.1.1.1.1.2">𝑦</ci><ci id="S3.SS1.p2.2.m2.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.1.1.1.1.1.1.3">𝑘</ci></apply><apply id="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.5.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.5"><apply id="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.5.1.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.5.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.5.1.1.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.5.1">subscript</csymbol><log id="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.5.1.2.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.5.1.2"></log><cn id="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.5.1.3.cmml" type="integer" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.5.1.3">2</cn></apply><ci id="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.5.2.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.5.2">𝑝</ci></apply><apply id="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.2.1.1.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.2.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.2.1">subscript</csymbol><ci id="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.2.1.1.2">𝑦</ci><ci id="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.2.2.1.1.2.2.2.2.1.1.3">𝑘</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.2c">\displaystyle H(Y)=-\sum_{k=1}^{n}p(y_{k})\log_{2}p(y_{k}),</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.2d">italic_H ( italic_Y ) = - ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_p ( italic_y start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_p ( italic_y start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ,</annotation></semantics></math>
where <math alttext="n" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.1"><semantics id="S3.SS1.p2.3.m3.1a"><mi id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m3.1d">italic_n</annotation></semantics></math> is the number of unique values in the time series, and <math alttext="p(y_{k})" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m4.1"><semantics id="S3.SS1.p2.4.m4.1a"><mrow id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><mi id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml">p</mi><mo id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml">⁢</mo><mrow id="S3.SS1.p2.4.m4.1.1.1.1" xref="S3.SS1.p2.4.m4.1.1.1.1.1.cmml"><mo id="S3.SS1.p2.4.m4.1.1.1.1.2" stretchy="false" xref="S3.SS1.p2.4.m4.1.1.1.1.1.cmml">(</mo><msub id="S3.SS1.p2.4.m4.1.1.1.1.1" xref="S3.SS1.p2.4.m4.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.4.m4.1.1.1.1.1.2" xref="S3.SS1.p2.4.m4.1.1.1.1.1.2.cmml">y</mi><mi id="S3.SS1.p2.4.m4.1.1.1.1.1.3" xref="S3.SS1.p2.4.m4.1.1.1.1.1.3.cmml">k</mi></msub><mo id="S3.SS1.p2.4.m4.1.1.1.1.3" stretchy="false" xref="S3.SS1.p2.4.m4.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><times id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2"></times><ci id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3">𝑝</ci><apply id="S3.SS1.p2.4.m4.1.1.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.1.1.1.2">𝑦</ci><ci id="S3.SS1.p2.4.m4.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.1.1.1.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">p(y_{k})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.4.m4.1d">italic_p ( italic_y start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT )</annotation></semantics></math> is the probability of observing the value <math alttext="y_{k}" class="ltx_Math" display="inline" id="S3.SS1.p2.5.m5.1"><semantics id="S3.SS1.p2.5.m5.1a"><msub id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml"><mi id="S3.SS1.p2.5.m5.1.1.2" xref="S3.SS1.p2.5.m5.1.1.2.cmml">y</mi><mi id="S3.SS1.p2.5.m5.1.1.3" xref="S3.SS1.p2.5.m5.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><apply id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.1.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p2.5.m5.1.1.2.cmml" xref="S3.SS1.p2.5.m5.1.1.2">𝑦</ci><ci id="S3.SS1.p2.5.m5.1.1.3.cmml" xref="S3.SS1.p2.5.m5.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">y_{k}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.5.m5.1d">italic_y start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> in the time series.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.3">For Sine Wave Generation and Path Integration tasks, the outputs are continuous values rather than discrete ones.
Therefore, we binned the output values into discrete intervals before calculating the entropy.
For task variants with multiple independent channels, we multiplied the single-channel entropy by the number of channels.
Since each channel operates independently and has identical statistical properties, multiplying the single-channel entropy by <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.p3.1.m1.1"><semantics id="S3.SS1.p3.1.m1.1a"><mi id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><ci id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.1.m1.1d">italic_N</annotation></semantics></math> provides an accurate measure of the total entropy across all channels. For a training set with <math alttext="M" class="ltx_Math" display="inline" id="S3.SS1.p3.2.m2.1"><semantics id="S3.SS1.p3.2.m2.1a"><mi id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><ci id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">M</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.2.m2.1d">italic_M</annotation></semantics></math> trials, we averaged the entropy across all trials presented in the training set to obtain a representative measure of task complexity:
<math alttext="\displaystyle H_{\text{task}}=N\times\left(\frac{1}{M}\sum_{i=1}^{M}H(Y^{(i)})%
\right)." class="ltx_Math" display="inline" id="S3.SS1.p3.3.m3.2"><semantics id="S3.SS1.p3.3.m3.2a"><mrow id="S3.SS1.p3.3.m3.2.2.1" xref="S3.SS1.p3.3.m3.2.2.1.1.cmml"><mrow id="S3.SS1.p3.3.m3.2.2.1.1" xref="S3.SS1.p3.3.m3.2.2.1.1.cmml"><msub id="S3.SS1.p3.3.m3.2.2.1.1.3" xref="S3.SS1.p3.3.m3.2.2.1.1.3.cmml"><mi id="S3.SS1.p3.3.m3.2.2.1.1.3.2" xref="S3.SS1.p3.3.m3.2.2.1.1.3.2.cmml">H</mi><mtext id="S3.SS1.p3.3.m3.2.2.1.1.3.3" xref="S3.SS1.p3.3.m3.2.2.1.1.3.3a.cmml">task</mtext></msub><mo id="S3.SS1.p3.3.m3.2.2.1.1.2" xref="S3.SS1.p3.3.m3.2.2.1.1.2.cmml">=</mo><mrow id="S3.SS1.p3.3.m3.2.2.1.1.1" xref="S3.SS1.p3.3.m3.2.2.1.1.1.cmml"><mi id="S3.SS1.p3.3.m3.2.2.1.1.1.3" xref="S3.SS1.p3.3.m3.2.2.1.1.1.3.cmml">N</mi><mo id="S3.SS1.p3.3.m3.2.2.1.1.1.2" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p3.3.m3.2.2.1.1.1.2.cmml">×</mo><mrow id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.cmml"><mo id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.2" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.3" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.3.cmml"><mfrac id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.3a" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.3.cmml"><mn id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.3.2" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.3.2.cmml">1</mn><mi id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.3.3" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.3.3.cmml">M</mi></mfrac></mstyle><mo id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.2" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2.cmml"><munderover id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2a" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2.cmml"><mo id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2.2.2" movablelimits="false" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2.2.3" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2.2.3.cmml"><mi id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2.2.3.2" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2.2.3.1" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2.2.3.3" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2.3" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2.3.cmml">M</mi></munderover></mstyle><mrow id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.1" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.1.3" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.1.3.cmml">H</mi><mo id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.1.2" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.1.1.1" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msup id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.1.1.1.1" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml">Y</mi><mrow id="S3.SS1.p3.3.m3.1.1.1.3" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.SS1.p3.3.m3.1.1.1.3.1" stretchy="false" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mi id="S3.SS1.p3.3.m3.1.1.1.1" xref="S3.SS1.p3.3.m3.1.1.1.1.cmml">i</mi><mo id="S3.SS1.p3.3.m3.1.1.1.3.2" stretchy="false" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></msup><mo id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.3" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.SS1.p3.3.m3.2.2.1.2" lspace="0em" xref="S3.SS1.p3.3.m3.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.2b"><apply id="S3.SS1.p3.3.m3.2.2.1.1.cmml" xref="S3.SS1.p3.3.m3.2.2.1"><eq id="S3.SS1.p3.3.m3.2.2.1.1.2.cmml" xref="S3.SS1.p3.3.m3.2.2.1.1.2"></eq><apply id="S3.SS1.p3.3.m3.2.2.1.1.3.cmml" xref="S3.SS1.p3.3.m3.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p3.3.m3.2.2.1.1.3.1.cmml" xref="S3.SS1.p3.3.m3.2.2.1.1.3">subscript</csymbol><ci id="S3.SS1.p3.3.m3.2.2.1.1.3.2.cmml" xref="S3.SS1.p3.3.m3.2.2.1.1.3.2">𝐻</ci><ci id="S3.SS1.p3.3.m3.2.2.1.1.3.3a.cmml" xref="S3.SS1.p3.3.m3.2.2.1.1.3.3"><mtext id="S3.SS1.p3.3.m3.2.2.1.1.3.3.cmml" mathsize="70%" xref="S3.SS1.p3.3.m3.2.2.1.1.3.3">task</mtext></ci></apply><apply id="S3.SS1.p3.3.m3.2.2.1.1.1.cmml" xref="S3.SS1.p3.3.m3.2.2.1.1.1"><times id="S3.SS1.p3.3.m3.2.2.1.1.1.2.cmml" xref="S3.SS1.p3.3.m3.2.2.1.1.1.2"></times><ci id="S3.SS1.p3.3.m3.2.2.1.1.1.3.cmml" xref="S3.SS1.p3.3.m3.2.2.1.1.1.3">𝑁</ci><apply id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1"><times id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.2"></times><apply id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.3"><divide id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.3.1.cmml" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.3"></divide><cn id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.3.2.cmml" type="integer" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.3.2">1</cn><ci id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.3.3.cmml" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.3.3">𝑀</ci></apply><apply id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1"><apply id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2.1.cmml" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2">subscript</csymbol><sum id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2.2.2"></sum><apply id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2.2.3"><eq id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2.2.3.1.cmml" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2.2.3.1"></eq><ci id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2.2.3.2.cmml" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2.2.3.2">𝑖</ci><cn id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2.3.cmml" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.2.3">𝑀</ci></apply><apply id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.1"><times id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.1.2"></times><ci id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.1.3">𝐻</ci><apply id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p3.3.m3.2.2.1.1.1.1.1.1.1.1.1.1.1.2">𝑌</ci><ci id="S3.SS1.p3.3.m3.1.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1.1.1">𝑖</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.2c">\displaystyle H_{\text{task}}=N\times\left(\frac{1}{M}\sum_{i=1}^{M}H(Y^{(i)})%
\right).</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.3.m3.2d">italic_H start_POSTSUBSCRIPT task end_POSTSUBSCRIPT = italic_N × ( divide start_ARG 1 end_ARG start_ARG italic_M end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT italic_H ( italic_Y start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) ) .</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">Different tasks are characterized by their specific input and output complexity profiles (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S3.F2" title="Figure 2 ‣ 3.1 Characterizing tasks complexity and neural dynamics (RNN hidden states) ‣ 3 Results ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">2</span></a>A and <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S3.F2" title="Figure 2 ‣ 3.1 Characterizing tasks complexity and neural dynamics (RNN hidden states) ‣ 3 Results ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">2</span></a>B).
Within each task, increasing the number of input and output channels consistently increases both the input and output complexities.
We selected our tasks to span three quadrants of the input-output complexity space (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S3.F2" title="Figure 2 ‣ 3.1 Characterizing tasks complexity and neural dynamics (RNN hidden states) ‣ 3 Results ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">2</span></a>C):</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">High-input, high-output complexity: N-Bit Flip-Flop and Path Integration tasks</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">High-input, low-output complexity: Delayed Discrimination task</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">Low-input, high-output complexity: Sine Wave Generation task</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
<p class="ltx_p" id="S3.SS1.p4.2">Among them, the N-Bit Flip-Flop task exhibits the highest input and output complexities among all tasks, suggesting that it imposes the greatest information-processing demands on the networks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1">We then provide a link between the task complexity and the dimensionality of the representation demanded by the tasks.
We found that the number of leading Principal Components (PCs) explaining 95% of the variance increases consistently across tasks as difficulty increases, indicating that harder tasks demand higher-dimensional representations and fuller utilization of the networks’ representational capacity (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S3.F2" title="Figure 2 ‣ 3.1 Characterizing tasks complexity and neural dynamics (RNN hidden states) ‣ 3 Results ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">2</span></a>D) <cite class="ltx_cite ltx_citemacro_citep">(Gao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib22" title="">2017a</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.3">Typical solutions found by converged networks across the different tasks are as follows (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S3.F2" title="Figure 2 ‣ 3.1 Characterizing tasks complexity and neural dynamics (RNN hidden states) ‣ 3 Results ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">2</span></a>E):
In <span class="ltx_text ltx_font_bold" id="S3.SS1.p6.3.1">N-Bit flip-flop</span> tasks, networks learn two fixed points corresponding to the output value of <math alttext="\{-1,+1\}" class="ltx_Math" display="inline" id="S3.SS1.p6.1.m1.2"><semantics id="S3.SS1.p6.1.m1.2a"><mrow id="S3.SS1.p6.1.m1.2.2.2" xref="S3.SS1.p6.1.m1.2.2.3.cmml"><mo id="S3.SS1.p6.1.m1.2.2.2.3" stretchy="false" xref="S3.SS1.p6.1.m1.2.2.3.cmml">{</mo><mrow id="S3.SS1.p6.1.m1.1.1.1.1" xref="S3.SS1.p6.1.m1.1.1.1.1.cmml"><mo id="S3.SS1.p6.1.m1.1.1.1.1a" xref="S3.SS1.p6.1.m1.1.1.1.1.cmml">−</mo><mn id="S3.SS1.p6.1.m1.1.1.1.1.2" xref="S3.SS1.p6.1.m1.1.1.1.1.2.cmml">1</mn></mrow><mo id="S3.SS1.p6.1.m1.2.2.2.4" xref="S3.SS1.p6.1.m1.2.2.3.cmml">,</mo><mrow id="S3.SS1.p6.1.m1.2.2.2.2" xref="S3.SS1.p6.1.m1.2.2.2.2.cmml"><mo id="S3.SS1.p6.1.m1.2.2.2.2a" xref="S3.SS1.p6.1.m1.2.2.2.2.cmml">+</mo><mn id="S3.SS1.p6.1.m1.2.2.2.2.2" xref="S3.SS1.p6.1.m1.2.2.2.2.2.cmml">1</mn></mrow><mo id="S3.SS1.p6.1.m1.2.2.2.5" stretchy="false" xref="S3.SS1.p6.1.m1.2.2.3.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.1.m1.2b"><set id="S3.SS1.p6.1.m1.2.2.3.cmml" xref="S3.SS1.p6.1.m1.2.2.2"><apply id="S3.SS1.p6.1.m1.1.1.1.1.cmml" xref="S3.SS1.p6.1.m1.1.1.1.1"><minus id="S3.SS1.p6.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.p6.1.m1.1.1.1.1"></minus><cn id="S3.SS1.p6.1.m1.1.1.1.1.2.cmml" type="integer" xref="S3.SS1.p6.1.m1.1.1.1.1.2">1</cn></apply><apply id="S3.SS1.p6.1.m1.2.2.2.2.cmml" xref="S3.SS1.p6.1.m1.2.2.2.2"><plus id="S3.SS1.p6.1.m1.2.2.2.2.1.cmml" xref="S3.SS1.p6.1.m1.2.2.2.2"></plus><cn id="S3.SS1.p6.1.m1.2.2.2.2.2.cmml" type="integer" xref="S3.SS1.p6.1.m1.2.2.2.2.2">1</cn></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.1.m1.2c">\{-1,+1\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p6.1.m1.2d">{ - 1 , + 1 }</annotation></semantics></math> for each channel.
Networks further factorize the fixed points corresponding to different output channels along different orthogonal dimensions.
Specifically , when <math alttext="N=3" class="ltx_Math" display="inline" id="S3.SS1.p6.2.m2.1"><semantics id="S3.SS1.p6.2.m2.1a"><mrow id="S3.SS1.p6.2.m2.1.1" xref="S3.SS1.p6.2.m2.1.1.cmml"><mi id="S3.SS1.p6.2.m2.1.1.2" xref="S3.SS1.p6.2.m2.1.1.2.cmml">N</mi><mo id="S3.SS1.p6.2.m2.1.1.1" xref="S3.SS1.p6.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS1.p6.2.m2.1.1.3" xref="S3.SS1.p6.2.m2.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.2.m2.1b"><apply id="S3.SS1.p6.2.m2.1.1.cmml" xref="S3.SS1.p6.2.m2.1.1"><eq id="S3.SS1.p6.2.m2.1.1.1.cmml" xref="S3.SS1.p6.2.m2.1.1.1"></eq><ci id="S3.SS1.p6.2.m2.1.1.2.cmml" xref="S3.SS1.p6.2.m2.1.1.2">𝑁</ci><cn id="S3.SS1.p6.2.m2.1.1.3.cmml" type="integer" xref="S3.SS1.p6.2.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.2.m2.1c">N=3</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p6.2.m2.1d">italic_N = 3</annotation></semantics></math>, networks learn <math alttext="2^{3}=8" class="ltx_Math" display="inline" id="S3.SS1.p6.3.m3.1"><semantics id="S3.SS1.p6.3.m3.1a"><mrow id="S3.SS1.p6.3.m3.1.1" xref="S3.SS1.p6.3.m3.1.1.cmml"><msup id="S3.SS1.p6.3.m3.1.1.2" xref="S3.SS1.p6.3.m3.1.1.2.cmml"><mn id="S3.SS1.p6.3.m3.1.1.2.2" xref="S3.SS1.p6.3.m3.1.1.2.2.cmml">2</mn><mn id="S3.SS1.p6.3.m3.1.1.2.3" xref="S3.SS1.p6.3.m3.1.1.2.3.cmml">3</mn></msup><mo id="S3.SS1.p6.3.m3.1.1.1" xref="S3.SS1.p6.3.m3.1.1.1.cmml">=</mo><mn id="S3.SS1.p6.3.m3.1.1.3" xref="S3.SS1.p6.3.m3.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.3.m3.1b"><apply id="S3.SS1.p6.3.m3.1.1.cmml" xref="S3.SS1.p6.3.m3.1.1"><eq id="S3.SS1.p6.3.m3.1.1.1.cmml" xref="S3.SS1.p6.3.m3.1.1.1"></eq><apply id="S3.SS1.p6.3.m3.1.1.2.cmml" xref="S3.SS1.p6.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p6.3.m3.1.1.2.1.cmml" xref="S3.SS1.p6.3.m3.1.1.2">superscript</csymbol><cn id="S3.SS1.p6.3.m3.1.1.2.2.cmml" type="integer" xref="S3.SS1.p6.3.m3.1.1.2.2">2</cn><cn id="S3.SS1.p6.3.m3.1.1.2.3.cmml" type="integer" xref="S3.SS1.p6.3.m3.1.1.2.3">3</cn></apply><cn id="S3.SS1.p6.3.m3.1.1.3.cmml" type="integer" xref="S3.SS1.p6.3.m3.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.3.m3.1c">2^{3}=8</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p6.3.m3.1d">2 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT = 8</annotation></semantics></math> fixed points on the vertices of a 3D cube.
In <span class="ltx_text ltx_font_bold" id="S3.SS1.p6.3.2">Delayed Discrimination</span> tasks, networks memorize
the first input
value by storing each input value as a separate fixed point during the delay period, when the inputs to the network are no longer on and working memory is required <cite class="ltx_cite ltx_citemacro_citep">(Hopfield, <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib32" title="">1982</a>; Sussillo &amp; Barak, <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib54" title="">2013</a>; Driscoll et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib14" title="">2024</a>)</cite>. In <span class="ltx_text ltx_font_bold" id="S3.SS1.p6.3.3">Sine Wave Generation</span> tasks, networks develop limit cycles corresponding to different target frequencies indicated by inputs, and traverse different limit cycles to produce different sine wave outputs at the appropriate frequencies. In <span class="ltx_text ltx_font_bold" id="S3.SS1.p6.3.4">2D Path Integration</span> tasks, without any external input, networks develop 2D maps of their environments using a 2D plane of attractors. Without any inputs, a network’s hidden state (neural dynamics) stabilizes on fixed points corresponding to their current location.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Characterizing the degeneracy of task-trained RNNs</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We now quantify the degeneracy of task-trained RNNs at three levels of analysis: (out of distribution) behavior, neural dynamics, and weight space.
We also relate the measured degeneracy back to the task complexity measure introduced in the last section.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Degeneracy in dynamics decreases with task complexity</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">We use a recently published measure for comparing dynamical systems, called Dynamical Similarity Analysis (DSA) <cite class="ltx_cite ltx_citemacro_citep">(Ostrow et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib46" title="">2023</a>)</cite>, to perform pairwise comparisons of the neural dynamics in our task-trained networks.
DSA measures the temporal and topological structure of the system’s dynamics and ignores geometric configurations that alter the spatial representation of system trajectories without changing the underlying dynamics.
It provides a quantitative framework to assess how different networks may arrive at similar or distinct dynamical implementations of a task
(See Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#A3" title="Appendix C Dynamical Similarity Analysis (DSA) ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">C</span></a> for technical details).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1">To quantify the relationship between task complexity and dynamical degeneracy, we increased the complexity of the tasks by increasing the number of independent input and output channels.
This choice is grounded in information theory, as an entropy measure of task complexity is additive for statistically independent variables.
We found that within each task, increasing the number of input and output channels consistently reduced the dynamical degeneracy across the converged networks (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S3.F3" title="Figure 3 ‣ 3.2.1 Degeneracy in dynamics decreases with task complexity ‣ 3.2 Characterizing the degeneracy of task-trained RNNs ‣ 3 Results ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">3</span></a>A).
Strikingly, this relationship (“covariant” degeneracy with task complexity) holds across all the tasks we consider (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S3.F3" title="Figure 3 ‣ 3.2.1 Degeneracy in dynamics decreases with task complexity ‣ 3.2 Characterizing the degeneracy of task-trained RNNs ‣ 3 Results ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">3</span></a>C).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="223" id="S3.F3.g1" src="https://arxiv.org/html/2410.03972v1/extracted/5902981/figures/ICLR-03.png" width="598">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>
<span class="ltx_text ltx_font_bold" id="S3.F3.5.1">Dynamical degeneracy decreases with increased task complexity:</span>
<span class="ltx_text ltx_font_bold" id="S3.F3.6.2">(A)</span>
Pairwise Dynamical Similarity Analysis (DSA) scores decrease consistently across all tasks as we increase task complexity, suggesting more similar (less degenerate) dynamical solutions for harder tasks.
In Sine Wave Frequency, we change the frequency content of the task and in Sine Wave N, we change the number of input-output channels (N) of the task.
<span class="ltx_text ltx_font_bold" id="S3.F3.7.3">(B)</span>
2D embedding of pairwise DSA distances using Multi-Dimensional Scaling (MDS) on the N-bit flip-flop task shows how the spread of solutions decreases as task complexity is increased. Each dot represents a network with a different initialization.
<span class="ltx_text ltx_font_bold" id="S3.F3.8.4">(C)</span>
Output complexity versus the average dynamical degeneracy on a task shows how higher output complexity correlates with lower dynamical degeneracy both within and across tasks.
Within each task, larger marker size and less opacity indicate task variant with higher complexity.
Smaller DSA score implies dynamics are more similar.
</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Degeneracy in weights increases with task complexity</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="166" id="S3.F4.g1" src="https://arxiv.org/html/2410.03972v1/extracted/5902981/figures/ICLR-04.png" width="598">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>
<span class="ltx_text ltx_font_bold" id="S3.F4.4.1">Degeneracy in weight space (measured using permutation-invariant metrics) increases with increased task complexity</span>:
<span class="ltx_text ltx_font_bold" id="S3.F4.5.2">(A)</span> Pairwise distances between weights, measured by the normalized Frobenius norm of the recurrent connectivity matrix
, increase consistently with task complexity, across all tasks.
<span class="ltx_text ltx_font_bold" id="S3.F4.6.3">(B)</span> Distances from initialized weights, quantified by the Frobenius Norm, also increases with task complexity, indicating greater divergence in weight space for harder tasks. In other words, we observe greater variation/variability across trained individuals (individual differences) at the connectivity level, even with the opposite trend observed at the dynamical and behavioral levels. Each line represents the distance from the initialized weights for a given network. </figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.6">We quantify degeneracy across task-trained RNNs at the level of post-training weights by using a permutation invariant modification of the Frobenius Norm.
For a pair of RNNs with recurrent weight matrices given by <math alttext="\mathbf{W}_{1}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p1.1.m1.1"><semantics id="S3.SS2.SSS2.p1.1.m1.1a"><msub id="S3.SS2.SSS2.p1.1.m1.1.1" xref="S3.SS2.SSS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS2.p1.1.m1.1.1.2" xref="S3.SS2.SSS2.p1.1.m1.1.1.2.cmml">𝐖</mi><mn id="S3.SS2.SSS2.p1.1.m1.1.1.3" xref="S3.SS2.SSS2.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.1.m1.1b"><apply id="S3.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.2">𝐖</ci><cn id="S3.SS2.SSS2.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS2.SSS2.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.1.m1.1c">\mathbf{W}_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p1.1.m1.1d">bold_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\mathbf{W}_{2}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p1.2.m2.1"><semantics id="S3.SS2.SSS2.p1.2.m2.1a"><msub id="S3.SS2.SSS2.p1.2.m2.1.1" xref="S3.SS2.SSS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.SSS2.p1.2.m2.1.1.2" xref="S3.SS2.SSS2.p1.2.m2.1.1.2.cmml">𝐖</mi><mn id="S3.SS2.SSS2.p1.2.m2.1.1.3" xref="S3.SS2.SSS2.p1.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.2.m2.1b"><apply id="S3.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.2">𝐖</ci><cn id="S3.SS2.SSS2.p1.2.m2.1.1.3.cmml" type="integer" xref="S3.SS2.SSS2.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.2.m2.1c">\mathbf{W}_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p1.2.m2.1d">bold_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>, we define:
<math alttext="\displaystyle d_{\text{PIF}}(\mathbf{W}_{1},\mathbf{W}_{2})=\min_{\mathbf{P}_{%
1},\mathbf{P}_{2}\in\mathcal{P}(n)}\|\mathbf{W}_{1}-\mathbf{P}_{1}\mathbf{W}_{%
2}\mathbf{P}_{2}\|_{F}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p1.3.m3.6"><semantics id="S3.SS2.SSS2.p1.3.m3.6a"><mrow id="S3.SS2.SSS2.p1.3.m3.6.6" xref="S3.SS2.SSS2.p1.3.m3.6.6.cmml"><mrow id="S3.SS2.SSS2.p1.3.m3.5.5.2" xref="S3.SS2.SSS2.p1.3.m3.5.5.2.cmml"><msub id="S3.SS2.SSS2.p1.3.m3.5.5.2.4" xref="S3.SS2.SSS2.p1.3.m3.5.5.2.4.cmml"><mi id="S3.SS2.SSS2.p1.3.m3.5.5.2.4.2" xref="S3.SS2.SSS2.p1.3.m3.5.5.2.4.2.cmml">d</mi><mtext id="S3.SS2.SSS2.p1.3.m3.5.5.2.4.3" xref="S3.SS2.SSS2.p1.3.m3.5.5.2.4.3a.cmml">PIF</mtext></msub><mo id="S3.SS2.SSS2.p1.3.m3.5.5.2.3" xref="S3.SS2.SSS2.p1.3.m3.5.5.2.3.cmml">⁢</mo><mrow id="S3.SS2.SSS2.p1.3.m3.5.5.2.2.2" xref="S3.SS2.SSS2.p1.3.m3.5.5.2.2.3.cmml"><mo id="S3.SS2.SSS2.p1.3.m3.5.5.2.2.2.3" stretchy="false" xref="S3.SS2.SSS2.p1.3.m3.5.5.2.2.3.cmml">(</mo><msub id="S3.SS2.SSS2.p1.3.m3.4.4.1.1.1.1" xref="S3.SS2.SSS2.p1.3.m3.4.4.1.1.1.1.cmml"><mi id="S3.SS2.SSS2.p1.3.m3.4.4.1.1.1.1.2" xref="S3.SS2.SSS2.p1.3.m3.4.4.1.1.1.1.2.cmml">𝐖</mi><mn id="S3.SS2.SSS2.p1.3.m3.4.4.1.1.1.1.3" xref="S3.SS2.SSS2.p1.3.m3.4.4.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS2.SSS2.p1.3.m3.5.5.2.2.2.4" xref="S3.SS2.SSS2.p1.3.m3.5.5.2.2.3.cmml">,</mo><msub id="S3.SS2.SSS2.p1.3.m3.5.5.2.2.2.2" xref="S3.SS2.SSS2.p1.3.m3.5.5.2.2.2.2.cmml"><mi id="S3.SS2.SSS2.p1.3.m3.5.5.2.2.2.2.2" xref="S3.SS2.SSS2.p1.3.m3.5.5.2.2.2.2.2.cmml">𝐖</mi><mn id="S3.SS2.SSS2.p1.3.m3.5.5.2.2.2.2.3" xref="S3.SS2.SSS2.p1.3.m3.5.5.2.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS2.SSS2.p1.3.m3.5.5.2.2.2.5" stretchy="false" xref="S3.SS2.SSS2.p1.3.m3.5.5.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.SS2.SSS2.p1.3.m3.6.6.4" xref="S3.SS2.SSS2.p1.3.m3.6.6.4.cmml">=</mo><mrow id="S3.SS2.SSS2.p1.3.m3.6.6.3" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.cmml"><munder id="S3.SS2.SSS2.p1.3.m3.6.6.3.2" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.2.cmml"><mi id="S3.SS2.SSS2.p1.3.m3.6.6.3.2.2" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.2.2.cmml">min</mi><mrow id="S3.SS2.SSS2.p1.3.m3.3.3.3" xref="S3.SS2.SSS2.p1.3.m3.3.3.3.cmml"><mrow id="S3.SS2.SSS2.p1.3.m3.3.3.3.3.2" xref="S3.SS2.SSS2.p1.3.m3.3.3.3.3.3.cmml"><msub id="S3.SS2.SSS2.p1.3.m3.2.2.2.2.1.1" xref="S3.SS2.SSS2.p1.3.m3.2.2.2.2.1.1.cmml"><mi id="S3.SS2.SSS2.p1.3.m3.2.2.2.2.1.1.2" xref="S3.SS2.SSS2.p1.3.m3.2.2.2.2.1.1.2.cmml">𝐏</mi><mn id="S3.SS2.SSS2.p1.3.m3.2.2.2.2.1.1.3" xref="S3.SS2.SSS2.p1.3.m3.2.2.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS2.SSS2.p1.3.m3.3.3.3.3.2.3" xref="S3.SS2.SSS2.p1.3.m3.3.3.3.3.3.cmml">,</mo><msub id="S3.SS2.SSS2.p1.3.m3.3.3.3.3.2.2" xref="S3.SS2.SSS2.p1.3.m3.3.3.3.3.2.2.cmml"><mi id="S3.SS2.SSS2.p1.3.m3.3.3.3.3.2.2.2" xref="S3.SS2.SSS2.p1.3.m3.3.3.3.3.2.2.2.cmml">𝐏</mi><mn id="S3.SS2.SSS2.p1.3.m3.3.3.3.3.2.2.3" xref="S3.SS2.SSS2.p1.3.m3.3.3.3.3.2.2.3.cmml">2</mn></msub></mrow><mo id="S3.SS2.SSS2.p1.3.m3.3.3.3.4" xref="S3.SS2.SSS2.p1.3.m3.3.3.3.4.cmml">∈</mo><mrow id="S3.SS2.SSS2.p1.3.m3.3.3.3.5" xref="S3.SS2.SSS2.p1.3.m3.3.3.3.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.p1.3.m3.3.3.3.5.2" xref="S3.SS2.SSS2.p1.3.m3.3.3.3.5.2.cmml">𝒫</mi><mo id="S3.SS2.SSS2.p1.3.m3.3.3.3.5.1" xref="S3.SS2.SSS2.p1.3.m3.3.3.3.5.1.cmml">⁢</mo><mrow id="S3.SS2.SSS2.p1.3.m3.3.3.3.5.3.2" xref="S3.SS2.SSS2.p1.3.m3.3.3.3.5.cmml"><mo id="S3.SS2.SSS2.p1.3.m3.3.3.3.5.3.2.1" stretchy="false" xref="S3.SS2.SSS2.p1.3.m3.3.3.3.5.cmml">(</mo><mi id="S3.SS2.SSS2.p1.3.m3.1.1.1.1" xref="S3.SS2.SSS2.p1.3.m3.1.1.1.1.cmml">n</mi><mo id="S3.SS2.SSS2.p1.3.m3.3.3.3.5.3.2.2" stretchy="false" xref="S3.SS2.SSS2.p1.3.m3.3.3.3.5.cmml">)</mo></mrow></mrow></mrow></munder><mo id="S3.SS2.SSS2.p1.3.m3.6.6.3a" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.cmml">⁡</mo><msub id="S3.SS2.SSS2.p1.3.m3.6.6.3.1" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.cmml"><mrow id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.2.cmml"><mo id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.2" stretchy="false" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.2.1.cmml">‖</mo><mrow id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.cmml"><msub id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.2" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.2.cmml"><mi id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.2.2" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.2.2.cmml">𝐖</mi><mn id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.2.3" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.2.3.cmml">1</mn></msub><mo id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.1" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.1.cmml">−</mo><mrow id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.cmml"><msub id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.2" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.2.cmml"><mi id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.2.2" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.2.2.cmml">𝐏</mi><mn id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.2.3" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.2.3.cmml">1</mn></msub><mo id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.1" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.1.cmml">⁢</mo><msub id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.3" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.3.cmml"><mi id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.3.2" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.3.2.cmml">𝐖</mi><mn id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.3.3" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.3.3.cmml">2</mn></msub><mo id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.1a" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.1.cmml">⁢</mo><msub id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.4" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.4.cmml"><mi id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.4.2" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.4.2.cmml">𝐏</mi><mn id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.4.3" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.4.3.cmml">2</mn></msub></mrow></mrow><mo id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.3" stretchy="false" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.2.1.cmml">‖</mo></mrow><mi id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.3" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.3.cmml">F</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.3.m3.6b"><apply id="S3.SS2.SSS2.p1.3.m3.6.6.cmml" xref="S3.SS2.SSS2.p1.3.m3.6.6"><eq id="S3.SS2.SSS2.p1.3.m3.6.6.4.cmml" xref="S3.SS2.SSS2.p1.3.m3.6.6.4"></eq><apply id="S3.SS2.SSS2.p1.3.m3.5.5.2.cmml" xref="S3.SS2.SSS2.p1.3.m3.5.5.2"><times id="S3.SS2.SSS2.p1.3.m3.5.5.2.3.cmml" xref="S3.SS2.SSS2.p1.3.m3.5.5.2.3"></times><apply id="S3.SS2.SSS2.p1.3.m3.5.5.2.4.cmml" xref="S3.SS2.SSS2.p1.3.m3.5.5.2.4"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.3.m3.5.5.2.4.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.5.5.2.4">subscript</csymbol><ci id="S3.SS2.SSS2.p1.3.m3.5.5.2.4.2.cmml" xref="S3.SS2.SSS2.p1.3.m3.5.5.2.4.2">𝑑</ci><ci id="S3.SS2.SSS2.p1.3.m3.5.5.2.4.3a.cmml" xref="S3.SS2.SSS2.p1.3.m3.5.5.2.4.3"><mtext id="S3.SS2.SSS2.p1.3.m3.5.5.2.4.3.cmml" mathsize="70%" xref="S3.SS2.SSS2.p1.3.m3.5.5.2.4.3">PIF</mtext></ci></apply><interval closure="open" id="S3.SS2.SSS2.p1.3.m3.5.5.2.2.3.cmml" xref="S3.SS2.SSS2.p1.3.m3.5.5.2.2.2"><apply id="S3.SS2.SSS2.p1.3.m3.4.4.1.1.1.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.4.4.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.3.m3.4.4.1.1.1.1.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.4.4.1.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p1.3.m3.4.4.1.1.1.1.2.cmml" xref="S3.SS2.SSS2.p1.3.m3.4.4.1.1.1.1.2">𝐖</ci><cn id="S3.SS2.SSS2.p1.3.m3.4.4.1.1.1.1.3.cmml" type="integer" xref="S3.SS2.SSS2.p1.3.m3.4.4.1.1.1.1.3">1</cn></apply><apply id="S3.SS2.SSS2.p1.3.m3.5.5.2.2.2.2.cmml" xref="S3.SS2.SSS2.p1.3.m3.5.5.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.3.m3.5.5.2.2.2.2.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.5.5.2.2.2.2">subscript</csymbol><ci id="S3.SS2.SSS2.p1.3.m3.5.5.2.2.2.2.2.cmml" xref="S3.SS2.SSS2.p1.3.m3.5.5.2.2.2.2.2">𝐖</ci><cn id="S3.SS2.SSS2.p1.3.m3.5.5.2.2.2.2.3.cmml" type="integer" xref="S3.SS2.SSS2.p1.3.m3.5.5.2.2.2.2.3">2</cn></apply></interval></apply><apply id="S3.SS2.SSS2.p1.3.m3.6.6.3.cmml" xref="S3.SS2.SSS2.p1.3.m3.6.6.3"><apply id="S3.SS2.SSS2.p1.3.m3.6.6.3.2.cmml" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.3.m3.6.6.3.2.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.2">subscript</csymbol><min id="S3.SS2.SSS2.p1.3.m3.6.6.3.2.2.cmml" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.2.2"></min><apply id="S3.SS2.SSS2.p1.3.m3.3.3.3.cmml" xref="S3.SS2.SSS2.p1.3.m3.3.3.3"><in id="S3.SS2.SSS2.p1.3.m3.3.3.3.4.cmml" xref="S3.SS2.SSS2.p1.3.m3.3.3.3.4"></in><list id="S3.SS2.SSS2.p1.3.m3.3.3.3.3.3.cmml" xref="S3.SS2.SSS2.p1.3.m3.3.3.3.3.2"><apply id="S3.SS2.SSS2.p1.3.m3.2.2.2.2.1.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.3.m3.2.2.2.2.1.1.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.2.2.2.2.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p1.3.m3.2.2.2.2.1.1.2.cmml" xref="S3.SS2.SSS2.p1.3.m3.2.2.2.2.1.1.2">𝐏</ci><cn id="S3.SS2.SSS2.p1.3.m3.2.2.2.2.1.1.3.cmml" type="integer" xref="S3.SS2.SSS2.p1.3.m3.2.2.2.2.1.1.3">1</cn></apply><apply id="S3.SS2.SSS2.p1.3.m3.3.3.3.3.2.2.cmml" xref="S3.SS2.SSS2.p1.3.m3.3.3.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.3.m3.3.3.3.3.2.2.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.3.3.3.3.2.2">subscript</csymbol><ci id="S3.SS2.SSS2.p1.3.m3.3.3.3.3.2.2.2.cmml" xref="S3.SS2.SSS2.p1.3.m3.3.3.3.3.2.2.2">𝐏</ci><cn id="S3.SS2.SSS2.p1.3.m3.3.3.3.3.2.2.3.cmml" type="integer" xref="S3.SS2.SSS2.p1.3.m3.3.3.3.3.2.2.3">2</cn></apply></list><apply id="S3.SS2.SSS2.p1.3.m3.3.3.3.5.cmml" xref="S3.SS2.SSS2.p1.3.m3.3.3.3.5"><times id="S3.SS2.SSS2.p1.3.m3.3.3.3.5.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.3.3.3.5.1"></times><ci id="S3.SS2.SSS2.p1.3.m3.3.3.3.5.2.cmml" xref="S3.SS2.SSS2.p1.3.m3.3.3.3.5.2">𝒫</ci><ci id="S3.SS2.SSS2.p1.3.m3.1.1.1.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.1.1">𝑛</ci></apply></apply></apply><apply id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.2.cmml" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1">subscript</csymbol><apply id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.2.cmml" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1"><csymbol cd="latexml" id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.2.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.2">norm</csymbol><apply id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1"><minus id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.1"></minus><apply id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.2.cmml" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.2.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.2.2.cmml" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.2.2">𝐖</ci><cn id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.2.3.cmml" type="integer" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.2.3">1</cn></apply><apply id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.cmml" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3"><times id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.1"></times><apply id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.2.cmml" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.2.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.2">subscript</csymbol><ci id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.2.2.cmml" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.2.2">𝐏</ci><cn id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.2.3.cmml" type="integer" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.2.3">1</cn></apply><apply id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.3.cmml" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.3.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.3">subscript</csymbol><ci id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.3.2.cmml" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.3.2">𝐖</ci><cn id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.3.3.cmml" type="integer" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.3.3">2</cn></apply><apply id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.4.cmml" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.4"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.4.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.4">subscript</csymbol><ci id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.4.2.cmml" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.4.2">𝐏</ci><cn id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.4.3.cmml" type="integer" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.1.1.1.3.4.3">2</cn></apply></apply></apply></apply><ci id="S3.SS2.SSS2.p1.3.m3.6.6.3.1.3.cmml" xref="S3.SS2.SSS2.p1.3.m3.6.6.3.1.3">𝐹</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.3.m3.6c">\displaystyle d_{\text{PIF}}(\mathbf{W}_{1},\mathbf{W}_{2})=\min_{\mathbf{P}_{%
1},\mathbf{P}_{2}\in\mathcal{P}(n)}\|\mathbf{W}_{1}-\mathbf{P}_{1}\mathbf{W}_{%
2}\mathbf{P}_{2}\|_{F}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p1.3.m3.6d">italic_d start_POSTSUBSCRIPT PIF end_POSTSUBSCRIPT ( bold_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) = roman_min start_POSTSUBSCRIPT bold_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_P start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ∈ caligraphic_P ( italic_n ) end_POSTSUBSCRIPT ∥ bold_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - bold_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT bold_P start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT</annotation></semantics></math>,
where <math alttext="\mathcal{P}(n)" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p1.4.m4.1"><semantics id="S3.SS2.SSS2.p1.4.m4.1a"><mrow id="S3.SS2.SSS2.p1.4.m4.1.2" xref="S3.SS2.SSS2.p1.4.m4.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.p1.4.m4.1.2.2" xref="S3.SS2.SSS2.p1.4.m4.1.2.2.cmml">𝒫</mi><mo id="S3.SS2.SSS2.p1.4.m4.1.2.1" xref="S3.SS2.SSS2.p1.4.m4.1.2.1.cmml">⁢</mo><mrow id="S3.SS2.SSS2.p1.4.m4.1.2.3.2" xref="S3.SS2.SSS2.p1.4.m4.1.2.cmml"><mo id="S3.SS2.SSS2.p1.4.m4.1.2.3.2.1" stretchy="false" xref="S3.SS2.SSS2.p1.4.m4.1.2.cmml">(</mo><mi id="S3.SS2.SSS2.p1.4.m4.1.1" xref="S3.SS2.SSS2.p1.4.m4.1.1.cmml">n</mi><mo id="S3.SS2.SSS2.p1.4.m4.1.2.3.2.2" stretchy="false" xref="S3.SS2.SSS2.p1.4.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.4.m4.1b"><apply id="S3.SS2.SSS2.p1.4.m4.1.2.cmml" xref="S3.SS2.SSS2.p1.4.m4.1.2"><times id="S3.SS2.SSS2.p1.4.m4.1.2.1.cmml" xref="S3.SS2.SSS2.p1.4.m4.1.2.1"></times><ci id="S3.SS2.SSS2.p1.4.m4.1.2.2.cmml" xref="S3.SS2.SSS2.p1.4.m4.1.2.2">𝒫</ci><ci id="S3.SS2.SSS2.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS2.p1.4.m4.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.4.m4.1c">\mathcal{P}(n)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p1.4.m4.1d">caligraphic_P ( italic_n )</annotation></semantics></math> is the set of permutation matrices of size <math alttext="n\times n" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p1.5.m5.1"><semantics id="S3.SS2.SSS2.p1.5.m5.1a"><mrow id="S3.SS2.SSS2.p1.5.m5.1.1" xref="S3.SS2.SSS2.p1.5.m5.1.1.cmml"><mi id="S3.SS2.SSS2.p1.5.m5.1.1.2" xref="S3.SS2.SSS2.p1.5.m5.1.1.2.cmml">n</mi><mo id="S3.SS2.SSS2.p1.5.m5.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.SSS2.p1.5.m5.1.1.1.cmml">×</mo><mi id="S3.SS2.SSS2.p1.5.m5.1.1.3" xref="S3.SS2.SSS2.p1.5.m5.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.5.m5.1b"><apply id="S3.SS2.SSS2.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS2.p1.5.m5.1.1"><times id="S3.SS2.SSS2.p1.5.m5.1.1.1.cmml" xref="S3.SS2.SSS2.p1.5.m5.1.1.1"></times><ci id="S3.SS2.SSS2.p1.5.m5.1.1.2.cmml" xref="S3.SS2.SSS2.p1.5.m5.1.1.2">𝑛</ci><ci id="S3.SS2.SSS2.p1.5.m5.1.1.3.cmml" xref="S3.SS2.SSS2.p1.5.m5.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.5.m5.1c">n\times n</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p1.5.m5.1d">italic_n × italic_n</annotation></semantics></math>, and <math alttext="\|\cdot\|_{F}" class="ltx_math_unparsed" display="inline" id="S3.SS2.SSS2.p1.6.m6.1"><semantics id="S3.SS2.SSS2.p1.6.m6.1a"><mrow id="S3.SS2.SSS2.p1.6.m6.1b"><mo id="S3.SS2.SSS2.p1.6.m6.1.1" rspace="0em">∥</mo><mo id="S3.SS2.SSS2.p1.6.m6.1.2" lspace="0em" rspace="0em">⋅</mo><msub id="S3.SS2.SSS2.p1.6.m6.1.3"><mo id="S3.SS2.SSS2.p1.6.m6.1.3.2" lspace="0em">∥</mo><mi id="S3.SS2.SSS2.p1.6.m6.1.3.3">F</mi></msub></mrow><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.6.m6.1c">\|\cdot\|_{F}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p1.6.m6.1d">∥ ⋅ ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT</annotation></semantics></math> denotes the Frobenius norm.
(See Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#A4" title="Appendix D Permutation-independent distance between weights ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">D</span></a> for additional details)
For comparing networks of different sizes, we normalize the above norm by the number of parameters in the weight matrix. We found that pairwise distances between weight matrices from converged RNNs increases consistently as task complexity increases across all tasks (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S3.F4" title="Figure 4 ‣ 3.2.2 Degeneracy in weights increases with task complexity ‣ 3.2 Characterizing the degeneracy of task-trained RNNs ‣ 3 Results ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">4</span></a>A).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.p2.3">Inspired by research linking statistical mechanics of random Gaussian landscapes to deep learning theory <cite class="ltx_cite ltx_citemacro_citep">(Bahri et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib2" title="">2020</a>; Fyodorov &amp; Williams, <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib20" title="">2007</a>; Bray &amp; Dean, <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib6" title="">2007</a>)</cite>, we hypothesized that increasing task complexity while holding the network size constant could make possible solutions ’rarer’ at the level of recurrent weights.
This implies that the optimization process would have to, on average, search for solutions further away from the initial recurrent weights to converge.
We found that the distance between the converged weight matrices and the initialized weight matrices indeed increases consistently with task complexity, and does so across all the tasks (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S3.F4" title="Figure 4 ‣ 3.2.2 Degeneracy in weights increases with task complexity ‣ 3.2 Characterizing the degeneracy of task-trained RNNs ‣ 3 Results ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">4</span></a>B and Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S3.T1" title="Table 1 ‣ Figure 6 ‣ 3.3 Controlling degeneracy of task-trained RNNs ‣ 3 Results ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">1</span></a>). Formally, distance from initial weights is <math alttext="\|\mathbf{W}_{T}-\mathbf{W}_{0}\|_{F}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p2.1.m1.1"><semantics id="S3.SS2.SSS2.p2.1.m1.1a"><msub id="S3.SS2.SSS2.p2.1.m1.1.1" xref="S3.SS2.SSS2.p2.1.m1.1.1.cmml"><mrow id="S3.SS2.SSS2.p2.1.m1.1.1.1.1" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.2.cmml"><mo id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.2" stretchy="false" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.cmml"><msub id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.cmml"><mi id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.2" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.2.cmml">𝐖</mi><mi id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.3" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.3.cmml">T</mi></msub><mo id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.1" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.1.cmml">−</mo><msub id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.3" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.3.cmml"><mi id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.3.2" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.3.2.cmml">𝐖</mi><mn id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.3.3" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.3.3.cmml">0</mn></msub></mrow><mo id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.3" stretchy="false" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.2.1.cmml">‖</mo></mrow><mi id="S3.SS2.SSS2.p2.1.m1.1.1.3" xref="S3.SS2.SSS2.p2.1.m1.1.1.3.cmml">F</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.1.m1.1b"><apply id="S3.SS2.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1">subscript</csymbol><apply id="S3.SS2.SSS2.p2.1.m1.1.1.1.2.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1"><csymbol cd="latexml" id="S3.SS2.SSS2.p2.1.m1.1.1.1.2.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.2">norm</csymbol><apply id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1"><minus id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.1"></minus><apply id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.2.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.2">𝐖</ci><ci id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.3.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.2.3">𝑇</ci></apply><apply id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.3.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.3.2.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.3.2">𝐖</ci><cn id="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.3.3.cmml" type="integer" xref="S3.SS2.SSS2.p2.1.m1.1.1.1.1.1.3.3">0</cn></apply></apply></apply><ci id="S3.SS2.SSS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1.3">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.1.m1.1c">\|\mathbf{W}_{T}-\mathbf{W}_{0}\|_{F}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p2.1.m1.1d">∥ bold_W start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT - bold_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT</annotation></semantics></math>,
where <math alttext="\mathbf{W}_{T}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p2.2.m2.1"><semantics id="S3.SS2.SSS2.p2.2.m2.1a"><msub id="S3.SS2.SSS2.p2.2.m2.1.1" xref="S3.SS2.SSS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.SSS2.p2.2.m2.1.1.2" xref="S3.SS2.SSS2.p2.2.m2.1.1.2.cmml">𝐖</mi><mi id="S3.SS2.SSS2.p2.2.m2.1.1.3" xref="S3.SS2.SSS2.p2.2.m2.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.2.m2.1b"><apply id="S3.SS2.SSS2.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1.2">𝐖</ci><ci id="S3.SS2.SSS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.2.m2.1c">\mathbf{W}_{T}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p2.2.m2.1d">bold_W start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT</annotation></semantics></math> is the weight matrix after training and <math alttext="\mathbf{W}_{0}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p2.3.m3.1"><semantics id="S3.SS2.SSS2.p2.3.m3.1a"><msub id="S3.SS2.SSS2.p2.3.m3.1.1" xref="S3.SS2.SSS2.p2.3.m3.1.1.cmml"><mi id="S3.SS2.SSS2.p2.3.m3.1.1.2" xref="S3.SS2.SSS2.p2.3.m3.1.1.2.cmml">𝐖</mi><mn id="S3.SS2.SSS2.p2.3.m3.1.1.3" xref="S3.SS2.SSS2.p2.3.m3.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.3.m3.1b"><apply id="S3.SS2.SSS2.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.SSS2.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.SSS2.p2.3.m3.1.1.2">𝐖</ci><cn id="S3.SS2.SSS2.p2.3.m3.1.1.3.cmml" type="integer" xref="S3.SS2.SSS2.p2.3.m3.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.3.m3.1c">\mathbf{W}_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p2.3.m3.1d">bold_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> is the initial weight matrix.
We normalize this measure by the number of parameters in the weight matrix across tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Degeneracy in out-of-distribution generalization behaviors decreases with task complexity</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure ltx_align_floatright" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="445" id="S3.F5.g1" src="https://arxiv.org/html/2410.03972v1/extracted/5902981/figures/OOD_cv.png" width="598">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span class="ltx_text ltx_font_bold" id="S3.F5.2.1">Behavioral degeneracy, in terms of variability of the OOD generalization performance, decreases with increased task complexity:</span> Degeneracy of the out of distribution (OOD) generalization behavior, as measured by coefficient of variation of the OOD performance (mean squared error), decreases consistently with increasing task complexity across all tasks.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS3.p1">
<p class="ltx_p" id="S3.SS2.SSS3.p1.1">Does degeneracy in Out-of-Distribution generalization behaviors change concomitantly with degeneracy in neural dynamics (hidden state) with task complexity?
We hypothesize that lower dynamical degeneracy implies that networks respond to input stimuli with more similar dynamics. Therefore, when these networks are probed with out-of-distribution (OOD) inputs, they should produce more similar outputs, leading to lower dispersion in their OOD performance across networks (i.e., lower degeneracy).
In other words, as task difficulty increases and dynamical degeneracy decreases, the degeneracy in OOD performance should also decrease.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS3.p2">
<p class="ltx_p" id="S3.SS2.SSS3.p2.3">To test this hypothesis, we measured the OOD performance (mean squared error) of converged networks that all achieved near-asymptotic training loss under the following <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS3.p2.3.1">length generalization</em> conditions.
For Delayed Discrimination tasks, we doubled the length of the delay period; for all other tasks, we doubled the length of the entire trial.
We then calculated the coefficient of variation (CV) of the OOD performance across networks, defined as <math alttext="CV=\sigma/\mu" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.1.m1.1"><semantics id="S3.SS2.SSS3.p2.1.m1.1a"><mrow id="S3.SS2.SSS3.p2.1.m1.1.1" xref="S3.SS2.SSS3.p2.1.m1.1.1.cmml"><mrow id="S3.SS2.SSS3.p2.1.m1.1.1.2" xref="S3.SS2.SSS3.p2.1.m1.1.1.2.cmml"><mi id="S3.SS2.SSS3.p2.1.m1.1.1.2.2" xref="S3.SS2.SSS3.p2.1.m1.1.1.2.2.cmml">C</mi><mo id="S3.SS2.SSS3.p2.1.m1.1.1.2.1" xref="S3.SS2.SSS3.p2.1.m1.1.1.2.1.cmml">⁢</mo><mi id="S3.SS2.SSS3.p2.1.m1.1.1.2.3" xref="S3.SS2.SSS3.p2.1.m1.1.1.2.3.cmml">V</mi></mrow><mo id="S3.SS2.SSS3.p2.1.m1.1.1.1" xref="S3.SS2.SSS3.p2.1.m1.1.1.1.cmml">=</mo><mrow id="S3.SS2.SSS3.p2.1.m1.1.1.3" xref="S3.SS2.SSS3.p2.1.m1.1.1.3.cmml"><mi id="S3.SS2.SSS3.p2.1.m1.1.1.3.2" xref="S3.SS2.SSS3.p2.1.m1.1.1.3.2.cmml">σ</mi><mo id="S3.SS2.SSS3.p2.1.m1.1.1.3.1" xref="S3.SS2.SSS3.p2.1.m1.1.1.3.1.cmml">/</mo><mi id="S3.SS2.SSS3.p2.1.m1.1.1.3.3" xref="S3.SS2.SSS3.p2.1.m1.1.1.3.3.cmml">μ</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.1.m1.1b"><apply id="S3.SS2.SSS3.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p2.1.m1.1.1"><eq id="S3.SS2.SSS3.p2.1.m1.1.1.1.cmml" xref="S3.SS2.SSS3.p2.1.m1.1.1.1"></eq><apply id="S3.SS2.SSS3.p2.1.m1.1.1.2.cmml" xref="S3.SS2.SSS3.p2.1.m1.1.1.2"><times id="S3.SS2.SSS3.p2.1.m1.1.1.2.1.cmml" xref="S3.SS2.SSS3.p2.1.m1.1.1.2.1"></times><ci id="S3.SS2.SSS3.p2.1.m1.1.1.2.2.cmml" xref="S3.SS2.SSS3.p2.1.m1.1.1.2.2">𝐶</ci><ci id="S3.SS2.SSS3.p2.1.m1.1.1.2.3.cmml" xref="S3.SS2.SSS3.p2.1.m1.1.1.2.3">𝑉</ci></apply><apply id="S3.SS2.SSS3.p2.1.m1.1.1.3.cmml" xref="S3.SS2.SSS3.p2.1.m1.1.1.3"><divide id="S3.SS2.SSS3.p2.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS3.p2.1.m1.1.1.3.1"></divide><ci id="S3.SS2.SSS3.p2.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS3.p2.1.m1.1.1.3.2">𝜎</ci><ci id="S3.SS2.SSS3.p2.1.m1.1.1.3.3.cmml" xref="S3.SS2.SSS3.p2.1.m1.1.1.3.3">𝜇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.1.m1.1c">CV=\sigma/\mu</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.1.m1.1d">italic_C italic_V = italic_σ / italic_μ</annotation></semantics></math>,
where <math alttext="\sigma" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.2.m2.1"><semantics id="S3.SS2.SSS3.p2.2.m2.1a"><mi id="S3.SS2.SSS3.p2.2.m2.1.1" xref="S3.SS2.SSS3.p2.2.m2.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.2.m2.1b"><ci id="S3.SS2.SSS3.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS3.p2.2.m2.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.2.m2.1c">\sigma</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.2.m2.1d">italic_σ</annotation></semantics></math> is the standard deviation of the OOD performance across networks, and <math alttext="\mu" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p2.3.m3.1"><semantics id="S3.SS2.SSS3.p2.3.m3.1a"><mi id="S3.SS2.SSS3.p2.3.m3.1.1" xref="S3.SS2.SSS3.p2.3.m3.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p2.3.m3.1b"><ci id="S3.SS2.SSS3.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS3.p2.3.m3.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p2.3.m3.1c">\mu</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p2.3.m3.1d">italic_μ</annotation></semantics></math> is the mean OOD performance.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS3.p3">
<p class="ltx_p" id="S3.SS2.SSS3.p3.1">Our results show that across all tasks, networks with lower dynamical degeneracy indeed exhibited lower CV in their OOD performance <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S3.F5" title="Figure 5 ‣ 3.2.3 Degeneracy in out-of-distribution generalization behaviors decreases with task complexity ‣ 3.2 Characterizing the degeneracy of task-trained RNNs ‣ 3 Results ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">5</span></a>. This finding supports our hypothesis that increased task complexity, which reduces dynamical degeneracy, also leads to more consistent – less degenerate – OOD generalization behaviors across networks. The prediction this result makes is that when trained on harder tasks, “expert” animals in neuroscience labs or task-trained networks/agents in ML should show less individual variation in generalization performance when tested.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Controlling degeneracy of task-trained RNNs</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S3.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="353" id="S3.F6.g1" src="https://arxiv.org/html/2410.03972v1/extracted/5902981/figures/ICLR-05.png" width="598"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>
<span class="ltx_text ltx_font_bold" id="S3.F6.11.1">Differential <span class="ltx_text ltx_font_italic" id="S3.F6.11.1.1">control</span> of weight and dynamics degeneracy:</span>
Using Delayed Discrimination (DD) tasks as an example, we demonstrate various methods to control the degeneracy of solution spaces found by task-trained RNNs:
<span class="ltx_text ltx_font_bold" id="S3.F6.12.2">(A)</span>
<span class="ltx_text ltx_font_bold" id="S3.F6.13.3">Adding an auxiliary loss</span>.
Adding an additional loss term to calculate the signed difference (not just direction) between the two consecutive inputs adds additional structure to the learned dynamics, resulting in reduced dynamical degeneracy and increased weight degeneracy. <span class="ltx_text ltx_font_bold" id="S3.F6.14.4">(B)</span>
<span class="ltx_text ltx_font_bold" id="S3.F6.15.5">Changing network size</span>:
By increasing the networks’ size, we decrease the load on their representational capacity, which in turn, results in more degeneracy in their neural dynamics or hidden states.
Simultaneously, we see greater degeneracy in their weight space (i.e., the trend in degeneracy is in the same direction for both dynamics and weights).
<span class="ltx_text ltx_font_bold" id="S3.F6.16.6"> (C)</span>
<span class="ltx_text ltx_font_bold" id="S3.F6.17.7">Regularizing weights to be low rank</span>:
We see a decrease in <span class="ltx_text ltx_font_italic" id="S3.F6.18.8">both</span> dynamical and weight-space degeneracy with increasing low-rank regularization on the recurrent weights and reduced distance from initialized weights, likely due to smaller-magnitude weight updates.
<span class="ltx_text ltx_font_bold" id="S3.F6.19.9"> (D)</span>
<span class="ltx_text ltx_font_bold" id="S3.F6.20.10">Regularizing weights to be sparse</span>:
Similar to above, we see a decrease in both dynamical and weight degeneracy with increasing sparsity of the recurrent weight matrix.
</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_table ltx_figure_panel ltx_align_center" id="S3.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Impact of Various Factors on Degeneracy</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.10">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.10.11.1">
<td class="ltx_td ltx_border_tt" id="S3.T1.10.11.1.1"></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.10.11.1.2">Dynamical Degeneracy</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.10.11.1.3">Weight Degeneracy</th>
</tr>
<tr class="ltx_tr" id="S3.T1.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.2.3">Increase Task Complexity</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1"><math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T1.1.1.1.m1.1"><semantics id="S3.T1.1.1.1.m1.1a"><mo id="S3.T1.1.1.1.m1.1.1" stretchy="false" xref="S3.T1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.m1.1b"><ci id="S3.T1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.1.1.1.m1.1d">↓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.2.2"><math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T1.2.2.2.m1.1"><semantics id="S3.T1.2.2.2.m1.1a"><mo id="S3.T1.2.2.2.m1.1.1" stretchy="false" xref="S3.T1.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.m1.1b"><ci id="S3.T1.2.2.2.m1.1.1.cmml" xref="S3.T1.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.2.2.2.m1.1d">↑</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.4">
<td class="ltx_td ltx_align_left" id="S3.T1.4.4.3">Auxiliary Loss</td>
<td class="ltx_td ltx_align_center" id="S3.T1.3.3.1"><math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T1.3.3.1.m1.1"><semantics id="S3.T1.3.3.1.m1.1a"><mo id="S3.T1.3.3.1.m1.1.1" stretchy="false" xref="S3.T1.3.3.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.1.m1.1b"><ci id="S3.T1.3.3.1.m1.1.1.cmml" xref="S3.T1.3.3.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.3.3.1.m1.1d">↓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T1.4.4.2"><math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T1.4.4.2.m1.1"><semantics id="S3.T1.4.4.2.m1.1a"><mo id="S3.T1.4.4.2.m1.1.1" stretchy="false" xref="S3.T1.4.4.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.2.m1.1b"><ci id="S3.T1.4.4.2.m1.1.1.cmml" xref="S3.T1.4.4.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.4.4.2.m1.1d">↑</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.6">
<td class="ltx_td ltx_align_left" id="S3.T1.6.6.3">Representational Load</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.5.1"><math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T1.5.5.1.m1.1"><semantics id="S3.T1.5.5.1.m1.1a"><mo id="S3.T1.5.5.1.m1.1.1" stretchy="false" xref="S3.T1.5.5.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.1.m1.1b"><ci id="S3.T1.5.5.1.m1.1.1.cmml" xref="S3.T1.5.5.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.5.5.1.m1.1d">↓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.6.2"><math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T1.6.6.2.m1.1"><semantics id="S3.T1.6.6.2.m1.1a"><mo id="S3.T1.6.6.2.m1.1.1" stretchy="false" xref="S3.T1.6.6.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.2.m1.1b"><ci id="S3.T1.6.6.2.m1.1.1.cmml" xref="S3.T1.6.6.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.6.6.2.m1.1d">↑</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T1.8.8">
<td class="ltx_td ltx_align_left" id="S3.T1.8.8.3">Low-Rank Regularization</td>
<td class="ltx_td ltx_align_center" id="S3.T1.7.7.1"><math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T1.7.7.1.m1.1"><semantics id="S3.T1.7.7.1.m1.1a"><mo id="S3.T1.7.7.1.m1.1.1" stretchy="false" xref="S3.T1.7.7.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.7.7.1.m1.1b"><ci id="S3.T1.7.7.1.m1.1.1.cmml" xref="S3.T1.7.7.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.7.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.7.7.1.m1.1d">↓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T1.8.8.2"><math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T1.8.8.2.m1.1"><semantics id="S3.T1.8.8.2.m1.1a"><mo id="S3.T1.8.8.2.m1.1.1" stretchy="false" xref="S3.T1.8.8.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.8.8.2.m1.1b"><ci id="S3.T1.8.8.2.m1.1.1.cmml" xref="S3.T1.8.8.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.8.8.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.8.8.2.m1.1d">↓</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T1.10.10">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.10.10.3">Sparsity Regularization</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.9.9.1"><math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T1.9.9.1.m1.1"><semantics id="S3.T1.9.9.1.m1.1a"><mo id="S3.T1.9.9.1.m1.1.1" stretchy="false" xref="S3.T1.9.9.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.9.9.1.m1.1b"><ci id="S3.T1.9.9.1.m1.1.1.cmml" xref="S3.T1.9.9.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.9.9.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.9.9.1.m1.1d">↓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.10.10.2"><math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T1.10.10.2.m1.1"><semantics id="S3.T1.10.10.2.m1.1a"><mo id="S3.T1.10.10.2.m1.1.1" stretchy="false" xref="S3.T1.10.10.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.10.10.2.m1.1b"><ci id="S3.T1.10.10.2.m1.1.1.cmml" xref="S3.T1.10.10.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.10.10.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.10.10.2.m1.1d">↓</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Based on our characterization of task complexity and quantification of degeneracy at multiple levels of analysis, we propose several methods to control degeneracy across task-trained RNNs, making predictions for ML and neuroscience studies:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p2.1.1">Increase task complexity:</span>
As discussed in the previous section, increasing task complexity reduces dynamical degeneracy and increases weight degeneracy in a consistent and measurable way in a suite of tasks of broad interest to ML and neuroscience.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.3"><span class="ltx_text ltx_font_bold" id="S3.SS3.p3.3.1">Adding auxiliary loss:</span>
We can directly change the degeneracy of the solution spaces found by task-trained RNNs by adding additional terms to the objective/loss function that are consistent with but distinct from the primary task objective.
This introduces additional structure to the learned dynamics, which in turn, decreases dynamical degeneracy and increases weight degeneracy, as networks must satisfy multiple constraints simultaneously.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S3.F6" title="Figure 6 ‣ 3.3 Controlling degeneracy of task-trained RNNs ‣ 3 Results ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">6</span></a>A shows an example implementation of this idea on a Delayed Discrimination task. We demand that the network output <math alttext="f_{1}-f_{2}" class="ltx_Math" display="inline" id="S3.SS3.p3.1.m1.1"><semantics id="S3.SS3.p3.1.m1.1a"><mrow id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml"><msub id="S3.SS3.p3.1.m1.1.1.2" xref="S3.SS3.p3.1.m1.1.1.2.cmml"><mi id="S3.SS3.p3.1.m1.1.1.2.2" xref="S3.SS3.p3.1.m1.1.1.2.2.cmml">f</mi><mn id="S3.SS3.p3.1.m1.1.1.2.3" xref="S3.SS3.p3.1.m1.1.1.2.3.cmml">1</mn></msub><mo id="S3.SS3.p3.1.m1.1.1.1" xref="S3.SS3.p3.1.m1.1.1.1.cmml">−</mo><msub id="S3.SS3.p3.1.m1.1.1.3" xref="S3.SS3.p3.1.m1.1.1.3.cmml"><mi id="S3.SS3.p3.1.m1.1.1.3.2" xref="S3.SS3.p3.1.m1.1.1.3.2.cmml">f</mi><mn id="S3.SS3.p3.1.m1.1.1.3.3" xref="S3.SS3.p3.1.m1.1.1.3.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><apply id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1"><minus id="S3.SS3.p3.1.m1.1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1.1"></minus><apply id="S3.SS3.p3.1.m1.1.1.2.cmml" xref="S3.SS3.p3.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.1.1.2.1.cmml" xref="S3.SS3.p3.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS3.p3.1.m1.1.1.2.2.cmml" xref="S3.SS3.p3.1.m1.1.1.2.2">𝑓</ci><cn id="S3.SS3.p3.1.m1.1.1.2.3.cmml" type="integer" xref="S3.SS3.p3.1.m1.1.1.2.3">1</cn></apply><apply id="S3.SS3.p3.1.m1.1.1.3.cmml" xref="S3.SS3.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.1.1.3.1.cmml" xref="S3.SS3.p3.1.m1.1.1.3">subscript</csymbol><ci id="S3.SS3.p3.1.m1.1.1.3.2.cmml" xref="S3.SS3.p3.1.m1.1.1.3.2">𝑓</ci><cn id="S3.SS3.p3.1.m1.1.1.3.3.cmml" type="integer" xref="S3.SS3.p3.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">f_{1}-f_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.1.m1.1d">italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>, instead of only <math alttext="sign(f_{1}-f_{2})" class="ltx_Math" display="inline" id="S3.SS3.p3.2.m2.1"><semantics id="S3.SS3.p3.2.m2.1a"><mrow id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml"><mi id="S3.SS3.p3.2.m2.1.1.3" xref="S3.SS3.p3.2.m2.1.1.3.cmml">s</mi><mo id="S3.SS3.p3.2.m2.1.1.2" xref="S3.SS3.p3.2.m2.1.1.2.cmml">⁢</mo><mi id="S3.SS3.p3.2.m2.1.1.4" xref="S3.SS3.p3.2.m2.1.1.4.cmml">i</mi><mo id="S3.SS3.p3.2.m2.1.1.2a" xref="S3.SS3.p3.2.m2.1.1.2.cmml">⁢</mo><mi id="S3.SS3.p3.2.m2.1.1.5" xref="S3.SS3.p3.2.m2.1.1.5.cmml">g</mi><mo id="S3.SS3.p3.2.m2.1.1.2b" xref="S3.SS3.p3.2.m2.1.1.2.cmml">⁢</mo><mi id="S3.SS3.p3.2.m2.1.1.6" xref="S3.SS3.p3.2.m2.1.1.6.cmml">n</mi><mo id="S3.SS3.p3.2.m2.1.1.2c" xref="S3.SS3.p3.2.m2.1.1.2.cmml">⁢</mo><mrow id="S3.SS3.p3.2.m2.1.1.1.1" xref="S3.SS3.p3.2.m2.1.1.1.1.1.cmml"><mo id="S3.SS3.p3.2.m2.1.1.1.1.2" stretchy="false" xref="S3.SS3.p3.2.m2.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS3.p3.2.m2.1.1.1.1.1" xref="S3.SS3.p3.2.m2.1.1.1.1.1.cmml"><msub id="S3.SS3.p3.2.m2.1.1.1.1.1.2" xref="S3.SS3.p3.2.m2.1.1.1.1.1.2.cmml"><mi id="S3.SS3.p3.2.m2.1.1.1.1.1.2.2" xref="S3.SS3.p3.2.m2.1.1.1.1.1.2.2.cmml">f</mi><mn id="S3.SS3.p3.2.m2.1.1.1.1.1.2.3" xref="S3.SS3.p3.2.m2.1.1.1.1.1.2.3.cmml">1</mn></msub><mo id="S3.SS3.p3.2.m2.1.1.1.1.1.1" xref="S3.SS3.p3.2.m2.1.1.1.1.1.1.cmml">−</mo><msub id="S3.SS3.p3.2.m2.1.1.1.1.1.3" xref="S3.SS3.p3.2.m2.1.1.1.1.1.3.cmml"><mi id="S3.SS3.p3.2.m2.1.1.1.1.1.3.2" xref="S3.SS3.p3.2.m2.1.1.1.1.1.3.2.cmml">f</mi><mn id="S3.SS3.p3.2.m2.1.1.1.1.1.3.3" xref="S3.SS3.p3.2.m2.1.1.1.1.1.3.3.cmml">2</mn></msub></mrow><mo id="S3.SS3.p3.2.m2.1.1.1.1.3" stretchy="false" xref="S3.SS3.p3.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><apply id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1"><times id="S3.SS3.p3.2.m2.1.1.2.cmml" xref="S3.SS3.p3.2.m2.1.1.2"></times><ci id="S3.SS3.p3.2.m2.1.1.3.cmml" xref="S3.SS3.p3.2.m2.1.1.3">𝑠</ci><ci id="S3.SS3.p3.2.m2.1.1.4.cmml" xref="S3.SS3.p3.2.m2.1.1.4">𝑖</ci><ci id="S3.SS3.p3.2.m2.1.1.5.cmml" xref="S3.SS3.p3.2.m2.1.1.5">𝑔</ci><ci id="S3.SS3.p3.2.m2.1.1.6.cmml" xref="S3.SS3.p3.2.m2.1.1.6">𝑛</ci><apply id="S3.SS3.p3.2.m2.1.1.1.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1.1.1"><minus id="S3.SS3.p3.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1.1.1.1.1"></minus><apply id="S3.SS3.p3.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS3.p3.2.m2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p3.2.m2.1.1.1.1.1.2.1.cmml" xref="S3.SS3.p3.2.m2.1.1.1.1.1.2">subscript</csymbol><ci id="S3.SS3.p3.2.m2.1.1.1.1.1.2.2.cmml" xref="S3.SS3.p3.2.m2.1.1.1.1.1.2.2">𝑓</ci><cn id="S3.SS3.p3.2.m2.1.1.1.1.1.2.3.cmml" type="integer" xref="S3.SS3.p3.2.m2.1.1.1.1.1.2.3">1</cn></apply><apply id="S3.SS3.p3.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS3.p3.2.m2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p3.2.m2.1.1.1.1.1.3.1.cmml" xref="S3.SS3.p3.2.m2.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS3.p3.2.m2.1.1.1.1.1.3.2.cmml" xref="S3.SS3.p3.2.m2.1.1.1.1.1.3.2">𝑓</ci><cn id="S3.SS3.p3.2.m2.1.1.1.1.1.3.3.cmml" type="integer" xref="S3.SS3.p3.2.m2.1.1.1.1.1.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">sign(f_{1}-f_{2})</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.2.m2.1d">italic_s italic_i italic_g italic_n ( italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT )</annotation></semantics></math>.
By analyzing the RNN’s neural dynamics or hidden state after both input stimuli have been presented, , we find that the additional objective forces the network to learn “line attractors” <cite class="ltx_cite ltx_citemacro_citep">(Strogatz, <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib52" title="">2000</a>)</cite>
with the same sign as network without the additional loss, but with variable magnitudes reflecting different values of <math alttext="f_{1}-f_{2}" class="ltx_Math" display="inline" id="S3.SS3.p3.3.m3.1"><semantics id="S3.SS3.p3.3.m3.1a"><mrow id="S3.SS3.p3.3.m3.1.1" xref="S3.SS3.p3.3.m3.1.1.cmml"><msub id="S3.SS3.p3.3.m3.1.1.2" xref="S3.SS3.p3.3.m3.1.1.2.cmml"><mi id="S3.SS3.p3.3.m3.1.1.2.2" xref="S3.SS3.p3.3.m3.1.1.2.2.cmml">f</mi><mn id="S3.SS3.p3.3.m3.1.1.2.3" xref="S3.SS3.p3.3.m3.1.1.2.3.cmml">1</mn></msub><mo id="S3.SS3.p3.3.m3.1.1.1" xref="S3.SS3.p3.3.m3.1.1.1.cmml">−</mo><msub id="S3.SS3.p3.3.m3.1.1.3" xref="S3.SS3.p3.3.m3.1.1.3.cmml"><mi id="S3.SS3.p3.3.m3.1.1.3.2" xref="S3.SS3.p3.3.m3.1.1.3.2.cmml">f</mi><mn id="S3.SS3.p3.3.m3.1.1.3.3" xref="S3.SS3.p3.3.m3.1.1.3.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.1b"><apply id="S3.SS3.p3.3.m3.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1"><minus id="S3.SS3.p3.3.m3.1.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1.1"></minus><apply id="S3.SS3.p3.3.m3.1.1.2.cmml" xref="S3.SS3.p3.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p3.3.m3.1.1.2.1.cmml" xref="S3.SS3.p3.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS3.p3.3.m3.1.1.2.2.cmml" xref="S3.SS3.p3.3.m3.1.1.2.2">𝑓</ci><cn id="S3.SS3.p3.3.m3.1.1.2.3.cmml" type="integer" xref="S3.SS3.p3.3.m3.1.1.2.3">1</cn></apply><apply id="S3.SS3.p3.3.m3.1.1.3.cmml" xref="S3.SS3.p3.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p3.3.m3.1.1.3.1.cmml" xref="S3.SS3.p3.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS3.p3.3.m3.1.1.3.2.cmml" xref="S3.SS3.p3.3.m3.1.1.3.2">𝑓</ci><cn id="S3.SS3.p3.3.m3.1.1.3.3.cmml" type="integer" xref="S3.SS3.p3.3.m3.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.1c">f_{1}-f_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.3.m3.1d">italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S3.F7" title="Figure 7 ‣ 3.3 Controlling degeneracy of task-trained RNNs ‣ 3 Results ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">7</span></a>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p4.1.1">Modifying representational load (or network capacity):</span>
Changing the network size (or the number of units in the RNN) affects its representational capacity.
Increasing network size typically leads to increased dynamical degeneracy due to more degrees of freedom.
Simultaneously degeneracy increases across weight space because of more parameters.
(Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S3.F6" title="Figure 6 ‣ 3.3 Controlling degeneracy of task-trained RNNs ‣ 3 Results ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">6</span></a> B).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p5">
<p class="ltx_p" id="S3.SS3.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p5.1.1">Imposing structural constraints on weight spaces(or regularization of recurrent weight matrices):</span> Applying regularization techniques such as low-rank (nuclear norm) (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S3.F6" title="Figure 6 ‣ 3.3 Controlling degeneracy of task-trained RNNs ‣ 3 Results ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">6</span></a> C) or sparsity (L1) (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S3.F6" title="Figure 6 ‣ 3.3 Controlling degeneracy of task-trained RNNs ‣ 3 Results ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">6</span></a> D) regularization to the recurrent weight matrix generally decreases the degeneracy across neural dynamical and weight spaces simultaneously <cite class="ltx_cite ltx_citemacro_citep">(Mastrogiuseppe &amp; Ostojic, <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib41" title="">2018</a>; Narang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib45" title="">2017</a>; Herbert &amp; Ostojic, <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib31" title="">2022</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure ltx_align_floatright" id="S3.F7"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="276" id="S3.F7.g1" src="https://arxiv.org/html/2410.03972v1/extracted/5902981/figures/ICLR-06.png" width="628">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>
<span class="ltx_text ltx_font_bold" id="S3.F7.6.1">RNNs learn more structured dynamics with auxiliary losses:</span>
In Delayed Discrimination tasks trained with auxiliary losses (e.g., an auxiliary output <math alttext="f_{1}-f_{2}" class="ltx_Math" display="inline" id="S3.F7.3.m1.1"><semantics id="S3.F7.3.m1.1b"><mrow id="S3.F7.3.m1.1.1" xref="S3.F7.3.m1.1.1.cmml"><msub id="S3.F7.3.m1.1.1.2" xref="S3.F7.3.m1.1.1.2.cmml"><mi id="S3.F7.3.m1.1.1.2.2" xref="S3.F7.3.m1.1.1.2.2.cmml">f</mi><mn id="S3.F7.3.m1.1.1.2.3" xref="S3.F7.3.m1.1.1.2.3.cmml">1</mn></msub><mo id="S3.F7.3.m1.1.1.1" xref="S3.F7.3.m1.1.1.1.cmml">−</mo><msub id="S3.F7.3.m1.1.1.3" xref="S3.F7.3.m1.1.1.3.cmml"><mi id="S3.F7.3.m1.1.1.3.2" xref="S3.F7.3.m1.1.1.3.2.cmml">f</mi><mn id="S3.F7.3.m1.1.1.3.3" xref="S3.F7.3.m1.1.1.3.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.F7.3.m1.1c"><apply id="S3.F7.3.m1.1.1.cmml" xref="S3.F7.3.m1.1.1"><minus id="S3.F7.3.m1.1.1.1.cmml" xref="S3.F7.3.m1.1.1.1"></minus><apply id="S3.F7.3.m1.1.1.2.cmml" xref="S3.F7.3.m1.1.1.2"><csymbol cd="ambiguous" id="S3.F7.3.m1.1.1.2.1.cmml" xref="S3.F7.3.m1.1.1.2">subscript</csymbol><ci id="S3.F7.3.m1.1.1.2.2.cmml" xref="S3.F7.3.m1.1.1.2.2">𝑓</ci><cn id="S3.F7.3.m1.1.1.2.3.cmml" type="integer" xref="S3.F7.3.m1.1.1.2.3">1</cn></apply><apply id="S3.F7.3.m1.1.1.3.cmml" xref="S3.F7.3.m1.1.1.3"><csymbol cd="ambiguous" id="S3.F7.3.m1.1.1.3.1.cmml" xref="S3.F7.3.m1.1.1.3">subscript</csymbol><ci id="S3.F7.3.m1.1.1.3.2.cmml" xref="S3.F7.3.m1.1.1.3.2">𝑓</ci><cn id="S3.F7.3.m1.1.1.3.3.cmml" type="integer" xref="S3.F7.3.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F7.3.m1.1d">f_{1}-f_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.F7.3.m1.1e">italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> in addition to main output <math alttext="sign(f_{1}-f_{2})" class="ltx_Math" display="inline" id="S3.F7.4.m2.1"><semantics id="S3.F7.4.m2.1b"><mrow id="S3.F7.4.m2.1.1" xref="S3.F7.4.m2.1.1.cmml"><mi id="S3.F7.4.m2.1.1.3" xref="S3.F7.4.m2.1.1.3.cmml">s</mi><mo id="S3.F7.4.m2.1.1.2" xref="S3.F7.4.m2.1.1.2.cmml">⁢</mo><mi id="S3.F7.4.m2.1.1.4" xref="S3.F7.4.m2.1.1.4.cmml">i</mi><mo id="S3.F7.4.m2.1.1.2b" xref="S3.F7.4.m2.1.1.2.cmml">⁢</mo><mi id="S3.F7.4.m2.1.1.5" xref="S3.F7.4.m2.1.1.5.cmml">g</mi><mo id="S3.F7.4.m2.1.1.2c" xref="S3.F7.4.m2.1.1.2.cmml">⁢</mo><mi id="S3.F7.4.m2.1.1.6" xref="S3.F7.4.m2.1.1.6.cmml">n</mi><mo id="S3.F7.4.m2.1.1.2d" xref="S3.F7.4.m2.1.1.2.cmml">⁢</mo><mrow id="S3.F7.4.m2.1.1.1.1" xref="S3.F7.4.m2.1.1.1.1.1.cmml"><mo id="S3.F7.4.m2.1.1.1.1.2" stretchy="false" xref="S3.F7.4.m2.1.1.1.1.1.cmml">(</mo><mrow id="S3.F7.4.m2.1.1.1.1.1" xref="S3.F7.4.m2.1.1.1.1.1.cmml"><msub id="S3.F7.4.m2.1.1.1.1.1.2" xref="S3.F7.4.m2.1.1.1.1.1.2.cmml"><mi id="S3.F7.4.m2.1.1.1.1.1.2.2" xref="S3.F7.4.m2.1.1.1.1.1.2.2.cmml">f</mi><mn id="S3.F7.4.m2.1.1.1.1.1.2.3" xref="S3.F7.4.m2.1.1.1.1.1.2.3.cmml">1</mn></msub><mo id="S3.F7.4.m2.1.1.1.1.1.1" xref="S3.F7.4.m2.1.1.1.1.1.1.cmml">−</mo><msub id="S3.F7.4.m2.1.1.1.1.1.3" xref="S3.F7.4.m2.1.1.1.1.1.3.cmml"><mi id="S3.F7.4.m2.1.1.1.1.1.3.2" xref="S3.F7.4.m2.1.1.1.1.1.3.2.cmml">f</mi><mn id="S3.F7.4.m2.1.1.1.1.1.3.3" xref="S3.F7.4.m2.1.1.1.1.1.3.3.cmml">2</mn></msub></mrow><mo id="S3.F7.4.m2.1.1.1.1.3" stretchy="false" xref="S3.F7.4.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.F7.4.m2.1c"><apply id="S3.F7.4.m2.1.1.cmml" xref="S3.F7.4.m2.1.1"><times id="S3.F7.4.m2.1.1.2.cmml" xref="S3.F7.4.m2.1.1.2"></times><ci id="S3.F7.4.m2.1.1.3.cmml" xref="S3.F7.4.m2.1.1.3">𝑠</ci><ci id="S3.F7.4.m2.1.1.4.cmml" xref="S3.F7.4.m2.1.1.4">𝑖</ci><ci id="S3.F7.4.m2.1.1.5.cmml" xref="S3.F7.4.m2.1.1.5">𝑔</ci><ci id="S3.F7.4.m2.1.1.6.cmml" xref="S3.F7.4.m2.1.1.6">𝑛</ci><apply id="S3.F7.4.m2.1.1.1.1.1.cmml" xref="S3.F7.4.m2.1.1.1.1"><minus id="S3.F7.4.m2.1.1.1.1.1.1.cmml" xref="S3.F7.4.m2.1.1.1.1.1.1"></minus><apply id="S3.F7.4.m2.1.1.1.1.1.2.cmml" xref="S3.F7.4.m2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.F7.4.m2.1.1.1.1.1.2.1.cmml" xref="S3.F7.4.m2.1.1.1.1.1.2">subscript</csymbol><ci id="S3.F7.4.m2.1.1.1.1.1.2.2.cmml" xref="S3.F7.4.m2.1.1.1.1.1.2.2">𝑓</ci><cn id="S3.F7.4.m2.1.1.1.1.1.2.3.cmml" type="integer" xref="S3.F7.4.m2.1.1.1.1.1.2.3">1</cn></apply><apply id="S3.F7.4.m2.1.1.1.1.1.3.cmml" xref="S3.F7.4.m2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.F7.4.m2.1.1.1.1.1.3.1.cmml" xref="S3.F7.4.m2.1.1.1.1.1.3">subscript</csymbol><ci id="S3.F7.4.m2.1.1.1.1.1.3.2.cmml" xref="S3.F7.4.m2.1.1.1.1.1.3.2">𝑓</ci><cn id="S3.F7.4.m2.1.1.1.1.1.3.3.cmml" type="integer" xref="S3.F7.4.m2.1.1.1.1.1.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F7.4.m2.1d">sign(f_{1}-f_{2})</annotation><annotation encoding="application/x-llamapun" id="S3.F7.4.m2.1e">italic_s italic_i italic_g italic_n ( italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT )</annotation></semantics></math>), RNNs neural dynamics or hidden states develop more sophisticated representations to track task-variables with 1-D manifolds (right) rather than just point-attractors (left).</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS3.p6">
<p class="ltx_p" id="S3.SS3.p6.1">These methods provide a toolkit for controlling degeneracy that naturally follows from our earlier analyses.
Importantly, we report that some interventions (increasing task complexity and modifying network capacity) induce an inverse or “contravariant” relationship between weight-space and dynamical degeneracy, while others (structural constraints) tend to affect both dynamical and weight-space degeneracy in the same direction or “covariantly” (Summarized in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S3.T1" title="Table 1 ‣ Figure 6 ‣ 3.3 Controlling degeneracy of task-trained RNNs ‣ 3 Results ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">1</span></a>).
Overall, the degeneracy of the solution space is primarily controlled by two key factors: the task complexity and the representational capacity of the networks.
Among the manipulations to control degeneracy in RNNs, increasing task complexity and adding auxiliary losses directly alter the task complexity
while representational load and adding structural constraints directly impact the representational capacity of the networks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Related work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Our approach extends a previously proposed measures of task complexity <cite class="ltx_cite ltx_citemacro_citep">(Meister, <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib43" title="">2022</a>)</cite>, which defined a task as a mapping from states to actions, and quantified task complexity as the entropy of all possible state-action mappings.
Here, we regard tasks as predefined time-series of inputs and outputs rather than in terms of state-action mappings.
By taking into account the temporal dynamics explicitly, we quantify the statistical complexity of the input and output time-series using Shannon entropy.
<cite class="ltx_cite ltx_citemacro_cite">Meister (<a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib43" title="">2022</a>)</cite> focuses on evaluating task-complexity from a learning-theory perspective: specifically, the amount of information an animal needs to acquire a perfect mapping between states and actions.
In contrast, we focus on the representational capacity and the constraints imposed by the processing demands of a task.
Our measure directly captures the amount of information that must be “carried” through the neural dynamics to produce the desired output.
Our measure of task complexity is also consistent with the notion of <span class="ltx_text ltx_font_italic" id="S4.p1.1.1">Neural Task Complexity</span> proposed by <cite class="ltx_cite ltx_citemacro_cite">Gao et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib23" title="">2017b</a>)</cite>, which provides an upper bound on the dimensionality that neural trajectories can explore during a task.
We have demonstrated that within each type of task, variants with higher output complexity require higher dimensional representations (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S3.F2" title="Figure 2 ‣ 3.1 Characterizing tasks complexity and neural dynamics (RNN hidden states) ‣ 3 Results ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">2</span></a>D).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">We further show that the output complexity of a task directly shapes the degeneracy of solutions found by RNNs. While previous work speculated that the degree to which neural dynamics are attractor-heavy may influence degeneracy <cite class="ltx_cite ltx_citemacro_citep">(Turner et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib57" title="">2021</a>)</cite>, we have quantified the “attractor-ness” of our tasks and found that it alone does not account for the different levels of dynamical degeneracy across tasks (see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#A2" title="Appendix B Characterizing the Attractorness of the Tasks ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">B</span></a>).
Moreover, as we modified the task characteristics
to study its effect on degeneracy, we observed that degeneracy remains invariant under certain transformations of the task, e.g., changing the delay duration in Delayed Discrimination or altering the environment size in Path Integration tasks.
In our framework, these transformations do not affect the output complexity of the task and thus leave the degeneracy unchanged.
In the language of complex systems theory, these factors (e.g., delays) are “sloppy dimensions” of the task with respect to degeneracy.
In contrast, factors that directly modify output complexity (e.g., adding channels, changing network size) are “stiff dimensions” that significantly control and modulate degeneracy of solutions <cite class="ltx_cite ltx_citemacro_citep">(Gutenkunst et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib30" title="">2007b</a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib29" title="">a</a>; Daniels et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib12" title="">2008</a>; Kepple et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib36" title="">2022</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">Our findings are also consistent with the <span class="ltx_text ltx_font_italic" id="S4.p3.1.1">Contravariance Principle</span> from <cite class="ltx_cite ltx_citemacro_cite">Cao &amp; Yamins (<a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib7" title="">2024</a>)</cite>: the harder a task, the more constrained the system that solves it will need to be.
Another recent study by <cite class="ltx_cite ltx_citemacro_cite">Huh et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib34" title="">2024</a>)</cite> hypothesizes that AI models seem to be converging on the same representation spaces independent of modality, because more constraints from the massive-scale data make degenerate solutions less likely.
Notably, the above related findings are based on feedforward networks for visual object categorization and Transformer networks for language modeling, respectively. Here, our study demonstrates that these ideas also generalize to recurrent networks/RNNs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Our paper presents a unified approach to quantifying and controlling the degeneracy of solutions found by task-trained RNNs across behavioral, neural dynamical (or hidden states), and weight-space levels of analysis.
We simulated and analyzed a task suite of broad interest to ML and neuroscience, yielding several novel insights.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">First, information content in inputs and outputs of different tasks, which we use to quantify task complexity, emerges as a crucial determinant of degeneracy.
This perspective helps reconcile previously conflicting observations on degeneracy across a range of tasks and measures.
Second, across different tasks, as task complexity increases, we observe a robust inverse relationship between neural dynamical and weight degeneracy.
As a corollary, while harder tasks lead to more consistent neural dynamics across task-trained networks, their underlying recurrent weight matrices are more degenerate/variable.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">Third, our study exemplifies several strategies for controlling solution degeneracy, including manipulating task complexity, adding auxiliary loss functions, adjusting network capacity, and structural constraints on recurrent weights <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#S3.T1" title="Table 1 ‣ Figure 6 ‣ 3.3 Controlling degeneracy of task-trained RNNs ‣ 3 Results ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">1</span></a>. These prescriptions will allow researchers to tailor the level of degeneracy of networks to suit specific research questions (e.g., probing individual variability across animals in neuroscience experiments) or application needs <cite class="ltx_cite ltx_citemacro_citep">(Bouthillier et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib5" title="">2021</a>; Morik, <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib44" title="">2005</a>; Yang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib60" title="">2022</a>)</cite>.
Importantly, we observed that certain strategies–increasing task complexity or modifying network capacity—induce an inverse (contravariant) relationship between weight and dynamical degeneracy, while others—imposing structural constraints—tend to affect both types of degeneracy in the same or covariant direction.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">While our study is based on artificial neural network models, some of the above strategies could apply to neuroscience experiments, <cite class="ltx_cite ltx_citemacro_citep">(Howard, <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib33" title="">2002</a>)</cite> e.g., introducing an auxiliary sub-task during behavioral shaping of lab animals to learn different tasks may constrain the degeneracy of the solutions learned by the biological brain and manifest in a different representational load than in an animal trained using a “curriculum” without an auxiliary sub-task.
We note that degeneracy is also ubiquitous in other biological systems <cite class="ltx_cite ltx_citemacro_citep">(Golub et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib27" title="">2018</a>; Prinz et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib48" title="">2004</a>; Edelman &amp; Gally, <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib16" title="">2001</a>)</cite>; thus, we motivate future theoretical studies based on them.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S5.p5">
<p class="ltx_p" id="S5.p5.1">In summary, our work provides insights into the factors that shape the solution landscape of task-trained RNNs.
The framework we bridges differing perspectives in prior work, reconciling observations of both universality and individuality in trained networks in ML and neuroscience.
Future research could delve deeper into the theoretical underpinnings of the observed relationships in biological and artificial agents, including neural networks, between task complexity, representational capacity, and degeneracy.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This work was supported by the NIH (RF1DA056403), James S. McDonnell Foundation (220020466), Simons Foundation (Pilot Extension-00003332-02), McKnight Endowment Fund, CIFAR Azrieli Global Scholar Program, NSF (2046583), and Harvard Medical School Dean’s Innovation Award. A.H. is supported by the Kempner Graduate Fellowship. We would like to thank Arna Ghosh, Andy Keller, Naomi Saphra, Yu Duan, Hanson Lu, and all members of the Rajan Lab for helpful discussions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ainsworth et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Samuel&nbsp;K Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa.

</span>
<span class="ltx_bibblock">Git re-basin: Merging models modulo permutation symmetries.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2209.04836</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bahri et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yasaman Bahri, Jonathan Kadmon, Jeffrey Pennington, Sam&nbsp;S Schoenholz, Jascha
Sohl-Dickstein, and Surya Ganguli.

</span>
<span class="ltx_bibblock">Statistical mechanics of deep learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Annual Review of Condensed Matter Physics</em>, 11(1):501–528, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barak (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Omri Barak.

</span>
<span class="ltx_bibblock">Recurrent neural networks as versatile tools of neuroscience
research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Current Opinion in Neurobiology</em>, 46:1–6, 2017.

</span>
<span class="ltx_bibblock">ISSN 09594388.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1016/j.conb.2017.06.003</span>.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://linkinghub.elsevier.com/retrieve/pii/S0959438817300429" title="">https://linkinghub.elsevier.com/retrieve/pii/S0959438817300429</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bialek et&nbsp;al. (2001)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
William Bialek, Ilya Nemenman, and Naftali Tishby.

</span>
<span class="ltx_bibblock">Predictability, complexity and learning, 2001.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/physics/0007070" title="">https://arxiv.org/abs/physics/0007070</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bouthillier et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xavier Bouthillier, Pierre Delaunay, Mirko Bronzi, Assya Trofimov, Brennan
Nichyporuk, Justin Szeto, Nazanin Mohammadi&nbsp;Sepahvand, Edward Raff, Kanika
Madan, Vikram Voleti, et&nbsp;al.

</span>
<span class="ltx_bibblock">Accounting for variance in machine learning benchmarks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of Machine Learning and Systems</em>, 3:747–769, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bray &amp; Dean (2007)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alan&nbsp;J Bray and David&nbsp;S Dean.

</span>
<span class="ltx_bibblock">Statistics of critical points of gaussian fields on large-dimensional
spaces.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Physical review letters</em>, 98(15):150201,
2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao &amp; Yamins (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rosa Cao and Daniel Yamins.

</span>
<span class="ltx_bibblock">Explanatory models in neuroscience, part 2: Functional
intelligibility and the contravariance principle.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Cognitive Systems Research</em>, 85:101200, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chaudhari et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi,
Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina.

</span>
<span class="ltx_bibblock">Entropy-sgd: Biasing gradient descent into wide valleys, 2017.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1611.01838" title="">https://arxiv.org/abs/1611.01838</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Collins et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jasmine Collins, Jascha Sohl-Dickstein, and David Sussillo.

</span>
<span class="ltx_bibblock">Capacity and trainability in recurrent neural networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">International Conference on Learning Representations</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Crutchfield &amp; Young (1989)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
James&nbsp;P. Crutchfield and Karl Young.

</span>
<span class="ltx_bibblock">Inferring statistical complexity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Phys. Rev. Lett.</em>, 63:105–108, Jul 1989.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1103/PhysRevLett.63.105</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://link.aps.org/doi/10.1103/PhysRevLett.63.105" title="">https://link.aps.org/doi/10.1103/PhysRevLett.63.105</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">D’Amour et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alexander D’Amour, Katherine Heller, Dan Moldovan, Ben Adlam, Babak Alipanahi,
Alex Beutel, Christina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew&nbsp;D.
Hoffman, Farhad Hormozdiari, Neil Houlsby, Shaobo Hou, Ghassen Jerfel, Alan
Karthikesalingam, Mario Lucic, Yian Ma, Cory McLean, Diana Mincu, Akinori
Mitani, Andrea Montanari, Zachary Nado, Vivek Natarajan, Christopher Nielson,
Thomas&nbsp;F. Osborne, Rajiv Raman, Kim Ramasamy, Rory Sayres, Jessica Schrouff,
Martin Seneviratne, Shannon Sequeira, Harini Suresh, Victor Veitch, Max
Vladymyrov, Xuezhi Wang, Kellie Webster, Steve Yadlowsky, Taedong Yun,
Xiaohua Zhai, and D.&nbsp;Sculley.

</span>
<span class="ltx_bibblock">Underspecification presents challenges for credibility in modern
machine learning, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2011.03395" title="">https://arxiv.org/abs/2011.03395</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Daniels et&nbsp;al. (2008)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Bryan&nbsp;C Daniels, Yan-Jiun Chen, James&nbsp;P Sethna, Ryan&nbsp;N Gutenkunst, and
Christopher&nbsp;R Myers.

</span>
<span class="ltx_bibblock">Sloppiness, robustness, and evolvability in systems biology.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Current opinion in biotechnology</em>, 19(4):389–395, 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Das &amp; Fiete (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Abhranil Das and Ila&nbsp;R Fiete.

</span>
<span class="ltx_bibblock">Systematic errors in connectivity inferred from activity in strongly
recurrent networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Nature Neuroscience</em>, 23(10):1286–1296,
2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Driscoll et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Laura&nbsp;N. Driscoll, Krishna Shenoy, and David Sussillo.

</span>
<span class="ltx_bibblock">Flexible multitask computation in recurrent networks utilizes shared
dynamical motifs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Nature Neuroscience</em>, 27(7):1349–1363,
July 2024.

</span>
<span class="ltx_bibblock">ISSN 1097-6256, 1546-1726.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1038/s41593-024-01668-6</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nature.com/articles/s41593-024-01668-6" title="">https://www.nature.com/articles/s41593-024-01668-6</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Durstewitz et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Daniel Durstewitz, Georgia Koppe, and Max&nbsp;Ingo Thurm.

</span>
<span class="ltx_bibblock">Reconstructing computational system dynamics from neural data with
recurrent neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Nature Reviews Neuroscience</em>, 24(11):693–710, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Edelman &amp; Gally (2001)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gerald&nbsp;M. Edelman and Joseph&nbsp;A. Gally.

</span>
<span class="ltx_bibblock">Degeneracy and complexity in biological systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the National Academy of Sciences</em>, 98(24):13763–13768, November 2001.

</span>
<span class="ltx_bibblock">ISSN 0027-8424, 1091-6490.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1073/pnas.231499798</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pnas.org/doi/full/10.1073/pnas.231499798" title="">https://pnas.org/doi/full/10.1073/pnas.231499798</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fascianelli et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Valeria Fascianelli, Aldo Battista, Fabio Stefanini, Satoshi Tsujimoto, Aldo
Genovesio, and Stefano Fusi.

</span>
<span class="ltx_bibblock">Neural representational geometries reflect behavioral differences in
monkeys and recurrent neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Nature Communications</em>, 15(1):6479, August
2024.

</span>
<span class="ltx_bibblock">ISSN 2041-1723.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1038/s41467-024-50503-w</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nature.com/articles/s41467-024-50503-w" title="">https://www.nature.com/articles/s41467-024-50503-w</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fort et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Stanislav Fort, Huiyi Hu, and Balaji Lakshminarayanan.

</span>
<span class="ltx_bibblock">Deep ensembles: A loss landscape perspective.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:1912.02757</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frankle &amp; Carbin (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jonathan Frankle and Michael Carbin.

</span>
<span class="ltx_bibblock">The lottery ticket hypothesis: Finding sparse, trainable neural
networks, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1803.03635" title="">https://arxiv.org/abs/1803.03635</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fyodorov &amp; Williams (2007)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yan&nbsp;V Fyodorov and Ian Williams.

</span>
<span class="ltx_bibblock">Replica symmetry breaking condition exposed by random matrix
calculation of landscape complexity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Journal of Statistical Physics</em>, 129:1081–1116,
2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Galgali et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Aniruddh&nbsp;R Galgali, Maneesh Sahani, and Valerio Mante.

</span>
<span class="ltx_bibblock">Residual dynamics resolves recurrent contributions to neural
computation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Nature Neuroscience</em>, 26(2):326–338, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2017a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Peiran Gao, Eric Trautmann, Byron Yu, Gopal Santhanam, Stephen Ryu, Krishna
Shenoy, and Surya Ganguli.

</span>
<span class="ltx_bibblock">A theory of multineuronal dimensionality, dynamics and measurement,
2017a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://biorxiv.org/lookup/doi/10.1101/214262" title="">http://biorxiv.org/lookup/doi/10.1101/214262</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2017b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Peiran Gao, Eric Trautmann, Byron Yu, Gopal Santhanam, Stephen Ryu, Krishna
Shenoy, and Surya Ganguli.

</span>
<span class="ltx_bibblock">A theory of multineuronal dimensionality, dynamics and measurement,
November 2017b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://biorxiv.org/lookup/doi/10.1101/214262" title="">http://biorxiv.org/lookup/doi/10.1101/214262</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gholamrezaei &amp; Whishaw (2009)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gita Gholamrezaei and Ian&nbsp;Q. Whishaw.

</span>
<span class="ltx_bibblock">Individual differences in skilled reaching for food related to
increased number of gestures: Evidence for goal and habit learning of skilled
reaching.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Behavioral Neuroscience</em>, 123(4):863–874,
2009.

</span>
<span class="ltx_bibblock">ISSN 1939-0084, 0735-7044.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1037/a0016369</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://doi.apa.org/getdoi.cfm?doi=10.1037/a0016369" title="">http://doi.apa.org/getdoi.cfm?doi=10.1037/a0016369</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gilpin (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
William Gilpin.

</span>
<span class="ltx_bibblock">Generative learning for nonlinear dynamics.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Nature Reviews Physics</em>, 6(3):194–206,
2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Glorot &amp; Bengio (2010)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xavier Glorot and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Understanding the difficulty of training deep feedforward neural
networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the thirteenth international conference on
artificial intelligence and statistics</em>, pp.&nbsp; 249–256. JMLR Workshop and
Conference Proceedings, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Golub et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Matthew&nbsp;D. Golub, Patrick&nbsp;T. Sadtler, Emily&nbsp;R. Oby, Kristin&nbsp;M. Quick,
Stephen&nbsp;I. Ryu, Elizabeth&nbsp;C. Tyler-Kabara, Aaron&nbsp;P. Batista, Steven&nbsp;M. Chase,
and Byron&nbsp;M. Yu.

</span>
<span class="ltx_bibblock">Learning by neural reassociation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Nature Neuroscience</em>, 21(4):607–616, April
2018.

</span>
<span class="ltx_bibblock">ISSN 1097-6256, 1546-1726.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1038/s41593-018-0095-3</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nature.com/articles/s41593-018-0095-3" title="">https://www.nature.com/articles/s41593-018-0095-3</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goodfellow et&nbsp;al. (2015)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ian&nbsp;J. Goodfellow, Oriol Vinyals, and Andrew&nbsp;M. Saxe.

</span>
<span class="ltx_bibblock">Qualitatively characterizing neural network optimization problems,
2015.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1412.6544" title="">https://arxiv.org/abs/1412.6544</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gutenkunst et&nbsp;al. (2007a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ryan&nbsp;N Gutenkunst, Fergal&nbsp;P Casey, Joshua&nbsp;J Waterfall, Christopher&nbsp;R Myers, and
James&nbsp;P Sethna.

</span>
<span class="ltx_bibblock">Extracting falsifiable predictions from sloppy models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Annals of the New York Academy of Sciences</em>, 1115(1):203–211, 2007a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gutenkunst et&nbsp;al. (2007b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ryan&nbsp;N Gutenkunst, Joshua&nbsp;J Waterfall, Fergal&nbsp;P Casey, Kevin&nbsp;S Brown,
Christopher&nbsp;R Myers, and James&nbsp;P Sethna.

</span>
<span class="ltx_bibblock">Universally sloppy parameter sensitivities in systems biology models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">PLoS computational biology</em>, 3(10):e189,
2007b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Herbert &amp; Ostojic (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Elizabeth Herbert and Srdjan Ostojic.

</span>
<span class="ltx_bibblock">The impact of sparsity in low-rank recurrent neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">PLOS Computational Biology</em>, 18(8):e1010426, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hopfield (1982)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J&nbsp;J Hopfield.

</span>
<span class="ltx_bibblock">Neural networks and physical systems with emergent collective
computational abilities.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Proceedings of the National Academy of Sciences</em>, 79(8):2554–2558, 1982.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1073/pnas.79.8.2554</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.pnas.org/doi/abs/10.1073/pnas.79.8.2554" title="">https://www.pnas.org/doi/abs/10.1073/pnas.79.8.2554</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Howard (2002)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
BR&nbsp;Howard.

</span>
<span class="ltx_bibblock">Control of variability.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">ILAR journal</em>, 43(4):194–201, 2002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huh et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Minyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip Isola.

</span>
<span class="ltx_bibblock">The platonic representation hypothesis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:2405.07987</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jastrzebski et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Stanisław Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja
Fischer, Yoshua Bengio, and Amos Storkey.

</span>
<span class="ltx_bibblock">Three factors influencing minima in sgd, 2018.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1711.04623" title="">https://arxiv.org/abs/1711.04623</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kepple et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D&nbsp;Kepple, Rainer Engelken, and Kanaka Rajan.

</span>
<span class="ltx_bibblock">Curriculum learning as a tool to uncover learning principles in the
brain.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">International Conference on Learning Representations</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kornblith et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton.

</span>
<span class="ltx_bibblock">Similarity of neural network representations revisited, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1905.00414" title="">https://arxiv.org/abs/1905.00414</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein.

</span>
<span class="ltx_bibblock">Visualizing the loss landscape of neural nets, 2018.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1712.09913" title="">https://arxiv.org/abs/1712.09913</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liebana&nbsp;Garcia et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Samuel Liebana&nbsp;Garcia, Aeron Laffere, Chiara Toschi, Louisa Schilling, Jacek
Podlaski, Matthias Fritsche, Peter Zatka-Haas, Yulong Li, Rafal Bogacz,
Andrew Saxe, and Armin Lak.

</span>
<span class="ltx_bibblock">Striatal dopamine reflects individual long-term learning
trajectories, December 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://biorxiv.org/lookup/doi/10.1101/2023.12.14.571653" title="">http://biorxiv.org/lookup/doi/10.1101/2023.12.14.571653</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maheswaranathan et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Niru Maheswaranathan, Alex Williams, Matthew Golub, Surya Ganguli, and David
Sussillo.

</span>
<span class="ltx_bibblock">Universality and individuality in neural dynamics across large
populations of recurrent networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Advances in neural information processing systems</em>, pp.&nbsp;15629–15641, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mastrogiuseppe &amp; Ostojic (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Francesca Mastrogiuseppe and Srdjan Ostojic.

</span>
<span class="ltx_bibblock">Linking connectivity, dynamics, and computations in low-rank
recurrent neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Neuron</em>, 99(3):609–623, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mehrer et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Johannes Mehrer, Courtney&nbsp;J. Spoerer, Nikolaus Kriegeskorte, and Tim&nbsp;C.
Kietzmann.

</span>
<span class="ltx_bibblock">Individual differences among deep neural network models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Nature Communications</em>, 11(1):5725, 2020.

</span>
<span class="ltx_bibblock">ISSN 2041-1723.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1038/s41467-020-19632-w</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.nature.com/articles/s41467-020-19632-w" title="">http://www.nature.com/articles/s41467-020-19632-w</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meister (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Markus Meister.

</span>
<span class="ltx_bibblock">Learning, fast and slow, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2205.02075" title="">https://arxiv.org/abs/2205.02075</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Morik (2005)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Katharina Morik.

</span>
<span class="ltx_bibblock">Sloppy modeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Knowledge representation and organization in machine learning</em>,
pp.&nbsp; 107–134, 2005.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Narang et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sharan Narang, Erich Elsen, Gregory Diamos, and Shubho Sengupta.

</span>
<span class="ltx_bibblock">Exploring sparsity in recurrent neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">arXiv preprint arXiv:1704.05119</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ostrow et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mitchell Ostrow, Adam Eisen, Leo Kozachkov, and Ila Fiete.

</span>
<span class="ltx_bibblock">Beyond Geometry: Comparing the Temporal Structure of
Computation in Neural Circuits with Dynamical Similarity
Analysis, October 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2306.10168" title="">http://arxiv.org/abs/2306.10168</a>.

</span>
<span class="ltx_bibblock">arXiv:2306.10168 [cs, q-bio].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan-Vazquez et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A&nbsp;Pan-Vazquez, Y&nbsp;Sanchez&nbsp;Araujo, B&nbsp;McMannon, M&nbsp;Louka, A&nbsp;Bandi, L&nbsp;Haetzel,
International Brain Laboratory, JW&nbsp;Pillow, ND&nbsp;Daw, and IB&nbsp;Witten.

</span>
<span class="ltx_bibblock">Pre-existing visual responses in a projection-defined dopamine
population explain individual learning trajectories, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://europepmc.org/article/PPR/PPR811803" title="">https://europepmc.org/article/PPR/PPR811803</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Prinz et&nbsp;al. (2004)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Astrid&nbsp;A Prinz, Dirk Bucher, and Eve Marder.

</span>
<span class="ltx_bibblock">Similar network activity from disparate circuit parameters.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Nature Neuroscience</em>, 7(12):1345–1352,
December 2004.

</span>
<span class="ltx_bibblock">ISSN 1097-6256, 1546-1726.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1038/nn1352</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nature.com/articles/nn1352" title="">https://www.nature.com/articles/nn1352</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajan et&nbsp;al. (2016)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kanaka Rajan, Christopher&nbsp;D Harvey, and David&nbsp;W Tank.

</span>
<span class="ltx_bibblock">Recurrent network models of sequence generation and memory.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Neuron</em>, 90(1):128–142, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schmid (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Peter&nbsp;J Schmid.

</span>
<span class="ltx_bibblock">Dynamic mode decomposition and its variants.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Annual Review of Fluid Mechanics</em>, 54(1):225–254, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shannon (1948)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Claude&nbsp;Elwood Shannon.

</span>
<span class="ltx_bibblock">A mathematical theory of communication.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">The Bell System Technical Journal</em>, 27:379–423,
1948.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://plan9.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf" title="">http://plan9.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Strogatz (2000)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Steven&nbsp;H. Strogatz.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">Nonlinear Dynamics and Chaos: With Applications to Physics,
Biology, Chemistry and Engineering</em>.

</span>
<span class="ltx_bibblock">Westview Press, 2000.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sussillo (2014)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
David Sussillo.

</span>
<span class="ltx_bibblock">Neural circuits as computational dynamical systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">Current opinion in neurobiology</em>, 25:156–163, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sussillo &amp; Barak (2013)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
David Sussillo and Omri Barak.

</span>
<span class="ltx_bibblock">Opening the black box: low-dimensional dynamics in high-dimensional
recurrent neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Neural computation</em>, 25(3):626–649, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tishby et&nbsp;al. (2000)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Naftali Tishby, Fernando&nbsp;C. Pereira, and William Bialek.

</span>
<span class="ltx_bibblock">The information bottleneck method, 2000.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/physics/0004057" title="">https://arxiv.org/abs/physics/0004057</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Turner &amp; Barak (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Elia Turner and Omri Barak.

</span>
<span class="ltx_bibblock">The simplicity bias in multi-task rnns: shared attractors, reuse of
dynamics, and geometric representation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Turner et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Elia Turner, Kabir&nbsp;V Dabholkar, and Omri Barak.

</span>
<span class="ltx_bibblock">Charting and navigating the space of solutions for recurrent neural
networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">Advances in Neural Information Processing Systems</em>,
34:25320–25333, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vyas et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Saurabh Vyas, Matthew&nbsp;D Golub, David Sussillo, and Krishna&nbsp;V Shenoy.

</span>
<span class="ltx_bibblock">Computation through neural population dynamics.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Annual Review of Neuroscience</em>, 43:249–275, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Werbos (1990)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Paul&nbsp;J Werbos.

</span>
<span class="ltx_bibblock">Backpropagation through time: what it does and how to do it.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Proceedings of the IEEE</em>, 78(10):1550–1560, 1990.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rubing Yang, Jialin Mao, and Pratik Chaudhari.

</span>
<span class="ltx_bibblock">Does the data induce capacity control in deep learning?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">International Conference on Machine Learning</em>, pp.&nbsp;25166–25197. PMLR, 2022.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Task and training details</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>N-Bit Flip Flip</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A1.SS1.tab1">
<table class="ltx_tabular ltx_align_middle" id="A1.SS1.tab1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.SS1.tab1.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.SS1.tab1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.tab1.1.1.1.1.1">Hyperparameter</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.SS1.tab1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.SS1.tab1.1.1.1.2.1">Value</span></td>
</tr>
<tr class="ltx_tr" id="A1.SS1.tab1.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.tab1.1.2.2.1">Optimizer</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.SS1.tab1.1.2.2.2">Adam</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.tab1.1.3.3">
<td class="ltx_td ltx_align_left" id="A1.SS1.tab1.1.3.3.1">Learning rate</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.tab1.1.3.3.2">0.001</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.tab1.1.4.4">
<td class="ltx_td ltx_align_left" id="A1.SS1.tab1.1.4.4.1">Learning rate scheduler</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.tab1.1.4.4.2">None</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.tab1.1.5.5">
<td class="ltx_td ltx_align_left" id="A1.SS1.tab1.1.5.5.1">Max epochs</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.tab1.1.5.5.2">300</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.tab1.1.6.6">
<td class="ltx_td ltx_align_left" id="A1.SS1.tab1.1.6.6.1">Steps per epoch</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.tab1.1.6.6.2">128</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.tab1.1.7.7">
<td class="ltx_td ltx_align_left" id="A1.SS1.tab1.1.7.7.1">Batch size</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.tab1.1.7.7.2">256</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.tab1.1.8.8">
<td class="ltx_td ltx_align_left" id="A1.SS1.tab1.1.8.8.1">Early stopping threshold</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.tab1.1.8.8.2">0.0005</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.tab1.1.9.9">
<td class="ltx_td ltx_align_left" id="A1.SS1.tab1.1.9.9.1">Patience</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.tab1.1.9.9.2">3</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.tab1.1.10.10">
<td class="ltx_td ltx_align_left" id="A1.SS1.tab1.1.10.10.1">Probability of flip</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.tab1.1.10.10.2">0.3</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.tab1.1.11.11">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.SS1.tab1.1.11.11.1">Number of time steps</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.SS1.tab1.1.11.11.2">100</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Delayed Discrimination</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A1.SS2.tab1">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.SS2.tab1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.SS2.tab1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="A1.SS2.tab1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.SS2.tab1.1.1.1.1.1">Hyperparameter</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.SS2.tab1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.SS2.tab1.1.1.1.2.1">Value</span></td>
</tr>
<tr class="ltx_tr" id="A1.SS2.tab1.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.SS2.tab1.1.2.2.1">Optimizer</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.SS2.tab1.1.2.2.2">Adam</td>
</tr>
<tr class="ltx_tr" id="A1.SS2.tab1.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.SS2.tab1.1.3.3.1">Learning rate</th>
<td class="ltx_td ltx_align_center" id="A1.SS2.tab1.1.3.3.2">0.001</td>
</tr>
<tr class="ltx_tr" id="A1.SS2.tab1.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.SS2.tab1.1.4.4.1">Learning rate scheduler</th>
<td class="ltx_td ltx_align_center" id="A1.SS2.tab1.1.4.4.2">CosineAnnealingWarmRestarts</td>
</tr>
<tr class="ltx_tr" id="A1.SS2.tab1.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.SS2.tab1.1.5.5.1">Max epochs</th>
<td class="ltx_td ltx_align_center" id="A1.SS2.tab1.1.5.5.2">500</td>
</tr>
<tr class="ltx_tr" id="A1.SS2.tab1.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.SS2.tab1.1.6.6.1">Steps per epoch</th>
<td class="ltx_td ltx_align_center" id="A1.SS2.tab1.1.6.6.2">128</td>
</tr>
<tr class="ltx_tr" id="A1.SS2.tab1.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.SS2.tab1.1.7.7.1">Batch size</th>
<td class="ltx_td ltx_align_center" id="A1.SS2.tab1.1.7.7.2">256</td>
</tr>
<tr class="ltx_tr" id="A1.SS2.tab1.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.SS2.tab1.1.8.8.1">Early stopping threshold</th>
<td class="ltx_td ltx_align_center" id="A1.SS2.tab1.1.8.8.2">0.05</td>
</tr>
<tr class="ltx_tr" id="A1.SS2.tab1.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.SS2.tab1.1.9.9.1">Patience</th>
<td class="ltx_td ltx_align_center" id="A1.SS2.tab1.1.9.9.2">3</td>
</tr>
<tr class="ltx_tr" id="A1.SS2.tab1.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.SS2.tab1.1.10.10.1">Number of time steps</th>
<td class="ltx_td ltx_align_center" id="A1.SS2.tab1.1.10.10.2">60</td>
</tr>
<tr class="ltx_tr" id="A1.SS2.tab1.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.SS2.tab1.1.11.11.1">Max delay</th>
<td class="ltx_td ltx_align_center" id="A1.SS2.tab1.1.11.11.2">20</td>
</tr>
<tr class="ltx_tr" id="A1.SS2.tab1.1.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.SS2.tab1.1.12.12.1">Lowest stimulus value</th>
<td class="ltx_td ltx_align_center" id="A1.SS2.tab1.1.12.12.2">2</td>
</tr>
<tr class="ltx_tr" id="A1.SS2.tab1.1.13.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A1.SS2.tab1.1.13.13.1">Highest stimulus value</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.SS2.tab1.1.13.13.2">10</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Sine Wave Generation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A1.SS3.tab1">
<table class="ltx_tabular ltx_align_middle" id="A1.SS3.tab1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.SS3.tab1.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.SS3.tab1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.SS3.tab1.1.1.1.1.1">Hyperparameter</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.SS3.tab1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.SS3.tab1.1.1.1.2.1">Value</span></td>
</tr>
<tr class="ltx_tr" id="A1.SS3.tab1.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS3.tab1.1.2.2.1">Optimizer</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.SS3.tab1.1.2.2.2">Adam</td>
</tr>
<tr class="ltx_tr" id="A1.SS3.tab1.1.3.3">
<td class="ltx_td ltx_align_left" id="A1.SS3.tab1.1.3.3.1">Learning rate</td>
<td class="ltx_td ltx_align_center" id="A1.SS3.tab1.1.3.3.2">0.0005</td>
</tr>
<tr class="ltx_tr" id="A1.SS3.tab1.1.4.4">
<td class="ltx_td ltx_align_left" id="A1.SS3.tab1.1.4.4.1">Learning rate scheduler</td>
<td class="ltx_td ltx_align_center" id="A1.SS3.tab1.1.4.4.2">None</td>
</tr>
<tr class="ltx_tr" id="A1.SS3.tab1.1.5.5">
<td class="ltx_td ltx_align_left" id="A1.SS3.tab1.1.5.5.1">Max epochs</td>
<td class="ltx_td ltx_align_center" id="A1.SS3.tab1.1.5.5.2">500</td>
</tr>
<tr class="ltx_tr" id="A1.SS3.tab1.1.6.6">
<td class="ltx_td ltx_align_left" id="A1.SS3.tab1.1.6.6.1">Steps per epoch</td>
<td class="ltx_td ltx_align_center" id="A1.SS3.tab1.1.6.6.2">128</td>
</tr>
<tr class="ltx_tr" id="A1.SS3.tab1.1.7.7">
<td class="ltx_td ltx_align_left" id="A1.SS3.tab1.1.7.7.1">Batch size</td>
<td class="ltx_td ltx_align_center" id="A1.SS3.tab1.1.7.7.2">32</td>
</tr>
<tr class="ltx_tr" id="A1.SS3.tab1.1.8.8">
<td class="ltx_td ltx_align_left" id="A1.SS3.tab1.1.8.8.1">Early stopping threshold</td>
<td class="ltx_td ltx_align_center" id="A1.SS3.tab1.1.8.8.2">0.05</td>
</tr>
<tr class="ltx_tr" id="A1.SS3.tab1.1.9.9">
<td class="ltx_td ltx_align_left" id="A1.SS3.tab1.1.9.9.1">Patience</td>
<td class="ltx_td ltx_align_center" id="A1.SS3.tab1.1.9.9.2">3</td>
</tr>
<tr class="ltx_tr" id="A1.SS3.tab1.1.10.10">
<td class="ltx_td ltx_align_left" id="A1.SS3.tab1.1.10.10.1">Number of time steps</td>
<td class="ltx_td ltx_align_center" id="A1.SS3.tab1.1.10.10.2">100</td>
</tr>
<tr class="ltx_tr" id="A1.SS3.tab1.1.11.11">
<td class="ltx_td ltx_align_left" id="A1.SS3.tab1.1.11.11.1">Lowest frequency</td>
<td class="ltx_td ltx_align_center" id="A1.SS3.tab1.1.11.11.2">1</td>
</tr>
<tr class="ltx_tr" id="A1.SS3.tab1.1.12.12">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.SS3.tab1.1.12.12.1">Highest frequency</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.SS3.tab1.1.12.12.2">30</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="A1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>Path Integration</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A1.SS4.tab1">
<table class="ltx_tabular ltx_align_middle" id="A1.SS4.tab1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.SS4.tab1.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.SS4.tab1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.SS4.tab1.1.1.1.1.1">Hyperparameter</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.SS4.tab1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.SS4.tab1.1.1.1.2.1">Value</span></td>
</tr>
<tr class="ltx_tr" id="A1.SS4.tab1.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS4.tab1.1.2.2.1">Optimizer</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.SS4.tab1.1.2.2.2">Adam</td>
</tr>
<tr class="ltx_tr" id="A1.SS4.tab1.1.3.3">
<td class="ltx_td ltx_align_left" id="A1.SS4.tab1.1.3.3.1">Learning rate</td>
<td class="ltx_td ltx_align_center" id="A1.SS4.tab1.1.3.3.2">0.001</td>
</tr>
<tr class="ltx_tr" id="A1.SS4.tab1.1.4.4">
<td class="ltx_td ltx_align_left" id="A1.SS4.tab1.1.4.4.1">Learning rate scheduler</td>
<td class="ltx_td ltx_align_center" id="A1.SS4.tab1.1.4.4.2">ReduceLROnPlateau</td>
</tr>
<tr class="ltx_tr" id="A1.SS4.tab1.1.5.5">
<td class="ltx_td ltx_align_left" id="A1.SS4.tab1.1.5.5.1">Learning rate decay factor</td>
<td class="ltx_td ltx_align_center" id="A1.SS4.tab1.1.5.5.2">0.5</td>
</tr>
<tr class="ltx_tr" id="A1.SS4.tab1.1.6.6">
<td class="ltx_td ltx_align_left" id="A1.SS4.tab1.1.6.6.1">Learning rate decay patience</td>
<td class="ltx_td ltx_align_center" id="A1.SS4.tab1.1.6.6.2">40</td>
</tr>
<tr class="ltx_tr" id="A1.SS4.tab1.1.7.7">
<td class="ltx_td ltx_align_left" id="A1.SS4.tab1.1.7.7.1">Max epochs</td>
<td class="ltx_td ltx_align_center" id="A1.SS4.tab1.1.7.7.2">1000</td>
</tr>
<tr class="ltx_tr" id="A1.SS4.tab1.1.8.8">
<td class="ltx_td ltx_align_left" id="A1.SS4.tab1.1.8.8.1">Steps per epoch</td>
<td class="ltx_td ltx_align_center" id="A1.SS4.tab1.1.8.8.2">128</td>
</tr>
<tr class="ltx_tr" id="A1.SS4.tab1.1.9.9">
<td class="ltx_td ltx_align_left" id="A1.SS4.tab1.1.9.9.1">Batch size</td>
<td class="ltx_td ltx_align_center" id="A1.SS4.tab1.1.9.9.2">64</td>
</tr>
<tr class="ltx_tr" id="A1.SS4.tab1.1.10.10">
<td class="ltx_td ltx_align_left" id="A1.SS4.tab1.1.10.10.1">Early stopping threshold (2D)</td>
<td class="ltx_td ltx_align_center" id="A1.SS4.tab1.1.10.10.2">0.1</td>
</tr>
<tr class="ltx_tr" id="A1.SS4.tab1.1.11.11">
<td class="ltx_td ltx_align_left" id="A1.SS4.tab1.1.11.11.1">Early stopping threshold (3D)</td>
<td class="ltx_td ltx_align_center" id="A1.SS4.tab1.1.11.11.2">0.3</td>
</tr>
<tr class="ltx_tr" id="A1.SS4.tab1.1.12.12">
<td class="ltx_td ltx_align_left" id="A1.SS4.tab1.1.12.12.1">Patience</td>
<td class="ltx_td ltx_align_center" id="A1.SS4.tab1.1.12.12.2">3</td>
</tr>
<tr class="ltx_tr" id="A1.SS4.tab1.1.13.13">
<td class="ltx_td ltx_align_left" id="A1.SS4.tab1.1.13.13.1">Number of time steps</td>
<td class="ltx_td ltx_align_center" id="A1.SS4.tab1.1.13.13.2">100</td>
</tr>
<tr class="ltx_tr" id="A1.SS4.tab1.1.14.14">
<td class="ltx_td ltx_align_left" id="A1.SS4.tab1.1.14.14.1">Lowest frequency</td>
<td class="ltx_td ltx_align_center" id="A1.SS4.tab1.1.14.14.2">1</td>
</tr>
<tr class="ltx_tr" id="A1.SS4.tab1.1.15.15">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.SS4.tab1.1.15.15.1">Highest frequency</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.SS4.tab1.1.15.15.2">30</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Characterizing the Attractorness of the Tasks</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">Attractors are associated with stable states in a network’s dynamics. To quantify how attractor-heavy a task is, we measure the <span class="ltx_text ltx_font_bold" id="A2.p1.1.1">speed of change</span> in the RNN’s hidden activities. Specifically, we calculate the average normalized difference between the hidden states at two consecutive time steps. This measure reflects the stability of the network’s internal states over time, where slower changes indicate the presence of attractor-like behavior.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="A2.p2">
<p class="ltx_p" id="A2.p2.1">For each trial, given a time series of hidden state activities, we calculate the speed of change in the hidden activities as:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="A2.p3">
<table class="ltx_equation ltx_eqn_table" id="A2.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\Delta h=\frac{\lVert h_{t+1}-h_{t}\rVert}{\frac{1}{T}\sum_{t=1}^{T}\lVert h_{%
t}\rVert}" class="ltx_Math" display="block" id="A2.Ex1.m1.2"><semantics id="A2.Ex1.m1.2a"><mrow id="A2.Ex1.m1.2.3" xref="A2.Ex1.m1.2.3.cmml"><mrow id="A2.Ex1.m1.2.3.2" xref="A2.Ex1.m1.2.3.2.cmml"><mi id="A2.Ex1.m1.2.3.2.2" mathvariant="normal" xref="A2.Ex1.m1.2.3.2.2.cmml">Δ</mi><mo id="A2.Ex1.m1.2.3.2.1" xref="A2.Ex1.m1.2.3.2.1.cmml">⁢</mo><mi id="A2.Ex1.m1.2.3.2.3" xref="A2.Ex1.m1.2.3.2.3.cmml">h</mi></mrow><mo id="A2.Ex1.m1.2.3.1" xref="A2.Ex1.m1.2.3.1.cmml">=</mo><mfrac id="A2.Ex1.m1.2.2" xref="A2.Ex1.m1.2.2.cmml"><mrow id="A2.Ex1.m1.1.1.1.1" xref="A2.Ex1.m1.1.1.1.2.cmml"><mo fence="true" id="A2.Ex1.m1.1.1.1.1.2" rspace="0em" xref="A2.Ex1.m1.1.1.1.2.1.cmml">∥</mo><mrow id="A2.Ex1.m1.1.1.1.1.1" xref="A2.Ex1.m1.1.1.1.1.1.cmml"><msub id="A2.Ex1.m1.1.1.1.1.1.2" xref="A2.Ex1.m1.1.1.1.1.1.2.cmml"><mi id="A2.Ex1.m1.1.1.1.1.1.2.2" xref="A2.Ex1.m1.1.1.1.1.1.2.2.cmml">h</mi><mrow id="A2.Ex1.m1.1.1.1.1.1.2.3" xref="A2.Ex1.m1.1.1.1.1.1.2.3.cmml"><mi id="A2.Ex1.m1.1.1.1.1.1.2.3.2" xref="A2.Ex1.m1.1.1.1.1.1.2.3.2.cmml">t</mi><mo id="A2.Ex1.m1.1.1.1.1.1.2.3.1" xref="A2.Ex1.m1.1.1.1.1.1.2.3.1.cmml">+</mo><mn id="A2.Ex1.m1.1.1.1.1.1.2.3.3" xref="A2.Ex1.m1.1.1.1.1.1.2.3.3.cmml">1</mn></mrow></msub><mo id="A2.Ex1.m1.1.1.1.1.1.1" xref="A2.Ex1.m1.1.1.1.1.1.1.cmml">−</mo><msub id="A2.Ex1.m1.1.1.1.1.1.3" xref="A2.Ex1.m1.1.1.1.1.1.3.cmml"><mi id="A2.Ex1.m1.1.1.1.1.1.3.2" xref="A2.Ex1.m1.1.1.1.1.1.3.2.cmml">h</mi><mi id="A2.Ex1.m1.1.1.1.1.1.3.3" xref="A2.Ex1.m1.1.1.1.1.1.3.3.cmml">t</mi></msub></mrow><mo fence="true" id="A2.Ex1.m1.1.1.1.1.3" lspace="0em" xref="A2.Ex1.m1.1.1.1.2.1.cmml">∥</mo></mrow><mrow id="A2.Ex1.m1.2.2.2" xref="A2.Ex1.m1.2.2.2.cmml"><mfrac id="A2.Ex1.m1.2.2.2.3" xref="A2.Ex1.m1.2.2.2.3.cmml"><mn id="A2.Ex1.m1.2.2.2.3.2" xref="A2.Ex1.m1.2.2.2.3.2.cmml">1</mn><mi id="A2.Ex1.m1.2.2.2.3.3" xref="A2.Ex1.m1.2.2.2.3.3.cmml">T</mi></mfrac><mo id="A2.Ex1.m1.2.2.2.2" xref="A2.Ex1.m1.2.2.2.2.cmml">⁢</mo><mrow id="A2.Ex1.m1.2.2.2.1" xref="A2.Ex1.m1.2.2.2.1.cmml"><msubsup id="A2.Ex1.m1.2.2.2.1.2" xref="A2.Ex1.m1.2.2.2.1.2.cmml"><mo id="A2.Ex1.m1.2.2.2.1.2.2.2" rspace="0em" xref="A2.Ex1.m1.2.2.2.1.2.2.2.cmml">∑</mo><mrow id="A2.Ex1.m1.2.2.2.1.2.2.3" xref="A2.Ex1.m1.2.2.2.1.2.2.3.cmml"><mi id="A2.Ex1.m1.2.2.2.1.2.2.3.2" xref="A2.Ex1.m1.2.2.2.1.2.2.3.2.cmml">t</mi><mo id="A2.Ex1.m1.2.2.2.1.2.2.3.1" xref="A2.Ex1.m1.2.2.2.1.2.2.3.1.cmml">=</mo><mn id="A2.Ex1.m1.2.2.2.1.2.2.3.3" xref="A2.Ex1.m1.2.2.2.1.2.2.3.3.cmml">1</mn></mrow><mi id="A2.Ex1.m1.2.2.2.1.2.3" xref="A2.Ex1.m1.2.2.2.1.2.3.cmml">T</mi></msubsup><mrow id="A2.Ex1.m1.2.2.2.1.1.1" xref="A2.Ex1.m1.2.2.2.1.1.2.cmml"><mo fence="true" id="A2.Ex1.m1.2.2.2.1.1.1.2" lspace="0em" rspace="0em" xref="A2.Ex1.m1.2.2.2.1.1.2.1.cmml">∥</mo><msub id="A2.Ex1.m1.2.2.2.1.1.1.1" xref="A2.Ex1.m1.2.2.2.1.1.1.1.cmml"><mi id="A2.Ex1.m1.2.2.2.1.1.1.1.2" xref="A2.Ex1.m1.2.2.2.1.1.1.1.2.cmml">h</mi><mi id="A2.Ex1.m1.2.2.2.1.1.1.1.3" xref="A2.Ex1.m1.2.2.2.1.1.1.1.3.cmml">t</mi></msub><mo fence="true" id="A2.Ex1.m1.2.2.2.1.1.1.3" lspace="0em" xref="A2.Ex1.m1.2.2.2.1.1.2.1.cmml">∥</mo></mrow></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="A2.Ex1.m1.2b"><apply id="A2.Ex1.m1.2.3.cmml" xref="A2.Ex1.m1.2.3"><eq id="A2.Ex1.m1.2.3.1.cmml" xref="A2.Ex1.m1.2.3.1"></eq><apply id="A2.Ex1.m1.2.3.2.cmml" xref="A2.Ex1.m1.2.3.2"><times id="A2.Ex1.m1.2.3.2.1.cmml" xref="A2.Ex1.m1.2.3.2.1"></times><ci id="A2.Ex1.m1.2.3.2.2.cmml" xref="A2.Ex1.m1.2.3.2.2">Δ</ci><ci id="A2.Ex1.m1.2.3.2.3.cmml" xref="A2.Ex1.m1.2.3.2.3">ℎ</ci></apply><apply id="A2.Ex1.m1.2.2.cmml" xref="A2.Ex1.m1.2.2"><divide id="A2.Ex1.m1.2.2.3.cmml" xref="A2.Ex1.m1.2.2"></divide><apply id="A2.Ex1.m1.1.1.1.2.cmml" xref="A2.Ex1.m1.1.1.1.1"><csymbol cd="latexml" id="A2.Ex1.m1.1.1.1.2.1.cmml" xref="A2.Ex1.m1.1.1.1.1.2">delimited-∥∥</csymbol><apply id="A2.Ex1.m1.1.1.1.1.1.cmml" xref="A2.Ex1.m1.1.1.1.1.1"><minus id="A2.Ex1.m1.1.1.1.1.1.1.cmml" xref="A2.Ex1.m1.1.1.1.1.1.1"></minus><apply id="A2.Ex1.m1.1.1.1.1.1.2.cmml" xref="A2.Ex1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="A2.Ex1.m1.1.1.1.1.1.2.1.cmml" xref="A2.Ex1.m1.1.1.1.1.1.2">subscript</csymbol><ci id="A2.Ex1.m1.1.1.1.1.1.2.2.cmml" xref="A2.Ex1.m1.1.1.1.1.1.2.2">ℎ</ci><apply id="A2.Ex1.m1.1.1.1.1.1.2.3.cmml" xref="A2.Ex1.m1.1.1.1.1.1.2.3"><plus id="A2.Ex1.m1.1.1.1.1.1.2.3.1.cmml" xref="A2.Ex1.m1.1.1.1.1.1.2.3.1"></plus><ci id="A2.Ex1.m1.1.1.1.1.1.2.3.2.cmml" xref="A2.Ex1.m1.1.1.1.1.1.2.3.2">𝑡</ci><cn id="A2.Ex1.m1.1.1.1.1.1.2.3.3.cmml" type="integer" xref="A2.Ex1.m1.1.1.1.1.1.2.3.3">1</cn></apply></apply><apply id="A2.Ex1.m1.1.1.1.1.1.3.cmml" xref="A2.Ex1.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="A2.Ex1.m1.1.1.1.1.1.3.1.cmml" xref="A2.Ex1.m1.1.1.1.1.1.3">subscript</csymbol><ci id="A2.Ex1.m1.1.1.1.1.1.3.2.cmml" xref="A2.Ex1.m1.1.1.1.1.1.3.2">ℎ</ci><ci id="A2.Ex1.m1.1.1.1.1.1.3.3.cmml" xref="A2.Ex1.m1.1.1.1.1.1.3.3">𝑡</ci></apply></apply></apply><apply id="A2.Ex1.m1.2.2.2.cmml" xref="A2.Ex1.m1.2.2.2"><times id="A2.Ex1.m1.2.2.2.2.cmml" xref="A2.Ex1.m1.2.2.2.2"></times><apply id="A2.Ex1.m1.2.2.2.3.cmml" xref="A2.Ex1.m1.2.2.2.3"><divide id="A2.Ex1.m1.2.2.2.3.1.cmml" xref="A2.Ex1.m1.2.2.2.3"></divide><cn id="A2.Ex1.m1.2.2.2.3.2.cmml" type="integer" xref="A2.Ex1.m1.2.2.2.3.2">1</cn><ci id="A2.Ex1.m1.2.2.2.3.3.cmml" xref="A2.Ex1.m1.2.2.2.3.3">𝑇</ci></apply><apply id="A2.Ex1.m1.2.2.2.1.cmml" xref="A2.Ex1.m1.2.2.2.1"><apply id="A2.Ex1.m1.2.2.2.1.2.cmml" xref="A2.Ex1.m1.2.2.2.1.2"><csymbol cd="ambiguous" id="A2.Ex1.m1.2.2.2.1.2.1.cmml" xref="A2.Ex1.m1.2.2.2.1.2">superscript</csymbol><apply id="A2.Ex1.m1.2.2.2.1.2.2.cmml" xref="A2.Ex1.m1.2.2.2.1.2"><csymbol cd="ambiguous" id="A2.Ex1.m1.2.2.2.1.2.2.1.cmml" xref="A2.Ex1.m1.2.2.2.1.2">subscript</csymbol><sum id="A2.Ex1.m1.2.2.2.1.2.2.2.cmml" xref="A2.Ex1.m1.2.2.2.1.2.2.2"></sum><apply id="A2.Ex1.m1.2.2.2.1.2.2.3.cmml" xref="A2.Ex1.m1.2.2.2.1.2.2.3"><eq id="A2.Ex1.m1.2.2.2.1.2.2.3.1.cmml" xref="A2.Ex1.m1.2.2.2.1.2.2.3.1"></eq><ci id="A2.Ex1.m1.2.2.2.1.2.2.3.2.cmml" xref="A2.Ex1.m1.2.2.2.1.2.2.3.2">𝑡</ci><cn id="A2.Ex1.m1.2.2.2.1.2.2.3.3.cmml" type="integer" xref="A2.Ex1.m1.2.2.2.1.2.2.3.3">1</cn></apply></apply><ci id="A2.Ex1.m1.2.2.2.1.2.3.cmml" xref="A2.Ex1.m1.2.2.2.1.2.3">𝑇</ci></apply><apply id="A2.Ex1.m1.2.2.2.1.1.2.cmml" xref="A2.Ex1.m1.2.2.2.1.1.1"><csymbol cd="latexml" id="A2.Ex1.m1.2.2.2.1.1.2.1.cmml" xref="A2.Ex1.m1.2.2.2.1.1.1.2">delimited-∥∥</csymbol><apply id="A2.Ex1.m1.2.2.2.1.1.1.1.cmml" xref="A2.Ex1.m1.2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="A2.Ex1.m1.2.2.2.1.1.1.1.1.cmml" xref="A2.Ex1.m1.2.2.2.1.1.1.1">subscript</csymbol><ci id="A2.Ex1.m1.2.2.2.1.1.1.1.2.cmml" xref="A2.Ex1.m1.2.2.2.1.1.1.1.2">ℎ</ci><ci id="A2.Ex1.m1.2.2.2.1.1.1.1.3.cmml" xref="A2.Ex1.m1.2.2.2.1.1.1.1.3">𝑡</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.Ex1.m1.2c">\Delta h=\frac{\lVert h_{t+1}-h_{t}\rVert}{\frac{1}{T}\sum_{t=1}^{T}\lVert h_{%
t}\rVert}</annotation><annotation encoding="application/x-llamapun" id="A2.Ex1.m1.2d">roman_Δ italic_h = divide start_ARG ∥ italic_h start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT - italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ end_ARG start_ARG divide start_ARG 1 end_ARG start_ARG italic_T end_ARG ∑ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ∥ italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∥ end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="A2.p4">
<p class="ltx_p" id="A2.p4.4">where <math alttext="T" class="ltx_Math" display="inline" id="A2.p4.1.m1.1"><semantics id="A2.p4.1.m1.1a"><mi id="A2.p4.1.m1.1.1" xref="A2.p4.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="A2.p4.1.m1.1b"><ci id="A2.p4.1.m1.1.1.cmml" xref="A2.p4.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p4.1.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="A2.p4.1.m1.1d">italic_T</annotation></semantics></math> is the total number of time steps in the trial. We then average <math alttext="\Delta h" class="ltx_Math" display="inline" id="A2.p4.2.m2.1"><semantics id="A2.p4.2.m2.1a"><mrow id="A2.p4.2.m2.1.1" xref="A2.p4.2.m2.1.1.cmml"><mi id="A2.p4.2.m2.1.1.2" mathvariant="normal" xref="A2.p4.2.m2.1.1.2.cmml">Δ</mi><mo id="A2.p4.2.m2.1.1.1" xref="A2.p4.2.m2.1.1.1.cmml">⁢</mo><mi id="A2.p4.2.m2.1.1.3" xref="A2.p4.2.m2.1.1.3.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.p4.2.m2.1b"><apply id="A2.p4.2.m2.1.1.cmml" xref="A2.p4.2.m2.1.1"><times id="A2.p4.2.m2.1.1.1.cmml" xref="A2.p4.2.m2.1.1.1"></times><ci id="A2.p4.2.m2.1.1.2.cmml" xref="A2.p4.2.m2.1.1.2">Δ</ci><ci id="A2.p4.2.m2.1.1.3.cmml" xref="A2.p4.2.m2.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p4.2.m2.1c">\Delta h</annotation><annotation encoding="application/x-llamapun" id="A2.p4.2.m2.1d">roman_Δ italic_h</annotation></semantics></math> across all trials in the training set to obtain a representative measure of the average speed of change in hidden activations for the task. A low <math alttext="\Delta h" class="ltx_Math" display="inline" id="A2.p4.3.m3.1"><semantics id="A2.p4.3.m3.1a"><mrow id="A2.p4.3.m3.1.1" xref="A2.p4.3.m3.1.1.cmml"><mi id="A2.p4.3.m3.1.1.2" mathvariant="normal" xref="A2.p4.3.m3.1.1.2.cmml">Δ</mi><mo id="A2.p4.3.m3.1.1.1" xref="A2.p4.3.m3.1.1.1.cmml">⁢</mo><mi id="A2.p4.3.m3.1.1.3" xref="A2.p4.3.m3.1.1.3.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.p4.3.m3.1b"><apply id="A2.p4.3.m3.1.1.cmml" xref="A2.p4.3.m3.1.1"><times id="A2.p4.3.m3.1.1.1.cmml" xref="A2.p4.3.m3.1.1.1"></times><ci id="A2.p4.3.m3.1.1.2.cmml" xref="A2.p4.3.m3.1.1.2">Δ</ci><ci id="A2.p4.3.m3.1.1.3.cmml" xref="A2.p4.3.m3.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p4.3.m3.1c">\Delta h</annotation><annotation encoding="application/x-llamapun" id="A2.p4.3.m3.1d">roman_Δ italic_h</annotation></semantics></math> indicates that the hidden state is changing slowly over time, suggesting that the network is in or near an attractor state where the hidden activations are relatively stable. Conversely, a high <math alttext="\Delta h" class="ltx_Math" display="inline" id="A2.p4.4.m4.1"><semantics id="A2.p4.4.m4.1a"><mrow id="A2.p4.4.m4.1.1" xref="A2.p4.4.m4.1.1.cmml"><mi id="A2.p4.4.m4.1.1.2" mathvariant="normal" xref="A2.p4.4.m4.1.1.2.cmml">Δ</mi><mo id="A2.p4.4.m4.1.1.1" xref="A2.p4.4.m4.1.1.1.cmml">⁢</mo><mi id="A2.p4.4.m4.1.1.3" xref="A2.p4.4.m4.1.1.3.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.p4.4.m4.1b"><apply id="A2.p4.4.m4.1.1.cmml" xref="A2.p4.4.m4.1.1"><times id="A2.p4.4.m4.1.1.1.cmml" xref="A2.p4.4.m4.1.1.1"></times><ci id="A2.p4.4.m4.1.1.2.cmml" xref="A2.p4.4.m4.1.1.2">Δ</ci><ci id="A2.p4.4.m4.1.1.3.cmml" xref="A2.p4.4.m4.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p4.4.m4.1c">\Delta h</annotation><annotation encoding="application/x-llamapun" id="A2.p4.4.m4.1d">roman_Δ italic_h</annotation></semantics></math> indicates more rapid changes in hidden activity, implying less attractor-heavy dynamics.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="A2.p5">
<p class="ltx_p" id="A2.p5.1">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#A2.F1" title="Figure A1 ‣ Appendix B Characterizing the Attractorness of the Tasks ‣ Measuring and controlling solution degeneracy across task-trained Recurrent Neural Networks"><span class="ltx_text ltx_ref_tag">A1</span></a>, tasks like N-Bits Flip-Flop and Path Integration, which have the highest task complexity and lowest dynamical degeneracy, span opposite ends of the ”attractorness” spectrum. Interestingly, Delayed Discrimination, which exhibits the highest dynamical degeneracy, shows an intermediate attractorness score. This suggests that the attractorness score, as measured by the speed of change in hidden activity, does not fully account for the dynamical degeneracy observed in the networks. Therefore, while attractor-like behavior influences network dynamics, it is not the sole factor determining dynamical degeneracy across tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="A2.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="176" id="A2.F1.g1" src="https://arxiv.org/html/2410.03972v1/extracted/5902981/figures/appendix_attractor_0.png" width="598"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="176" id="A2.F1.g2" src="https://arxiv.org/html/2410.03972v1/extracted/5902981/figures/appendix_attractor_1.png" width="598"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure A1: </span><span class="ltx_text ltx_font_bold" id="A2.F1.8.1">Normalized speed of change in hidden state activities for each task:</span> (A) Speed of change in hidden state activity (<math alttext="\Delta h)" class="ltx_math_unparsed" display="inline" id="A2.F1.4.m1.1"><semantics id="A2.F1.4.m1.1b"><mrow id="A2.F1.4.m1.1c"><mi id="A2.F1.4.m1.1.1" mathvariant="normal">Δ</mi><mi id="A2.F1.4.m1.1.2">h</mi><mo id="A2.F1.4.m1.1.3" stretchy="false">)</mo></mrow><annotation encoding="application/x-tex" id="A2.F1.4.m1.1d">\Delta h)</annotation><annotation encoding="application/x-llamapun" id="A2.F1.4.m1.1e">roman_Δ italic_h )</annotation></semantics></math>for all trials in the training set plotted in violin plot for each task (B) Overlaid histogram of (<math alttext="\Delta h)" class="ltx_math_unparsed" display="inline" id="A2.F1.5.m2.1"><semantics id="A2.F1.5.m2.1b"><mrow id="A2.F1.5.m2.1c"><mi id="A2.F1.5.m2.1.1" mathvariant="normal">Δ</mi><mi id="A2.F1.5.m2.1.2">h</mi><mo id="A2.F1.5.m2.1.3" stretchy="false">)</mo></mrow><annotation encoding="application/x-tex" id="A2.F1.5.m2.1d">\Delta h)</annotation><annotation encoding="application/x-llamapun" id="A2.F1.5.m2.1e">roman_Δ italic_h )</annotation></semantics></math> for all tasks (C) Individual (<math alttext="\Delta h)" class="ltx_math_unparsed" display="inline" id="A2.F1.6.m3.1"><semantics id="A2.F1.6.m3.1b"><mrow id="A2.F1.6.m3.1c"><mi id="A2.F1.6.m3.1.1" mathvariant="normal">Δ</mi><mi id="A2.F1.6.m3.1.2">h</mi><mo id="A2.F1.6.m3.1.3" stretchy="false">)</mo></mrow><annotation encoding="application/x-tex" id="A2.F1.6.m3.1d">\Delta h)</annotation><annotation encoding="application/x-llamapun" id="A2.F1.6.m3.1e">roman_Δ italic_h )</annotation></semantics></math> for each task. </figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Dynamical Similarity Analysis (DSA)</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A3.p1">
<p class="ltx_p" id="A3.p1.12">Briefly, DSA proceeds as follows:
Given two RNNs with hidden states <math alttext="\mathbf{h}_{1}(t)\in\mathbb{R}^{n}" class="ltx_Math" display="inline" id="A3.p1.1.m1.1"><semantics id="A3.p1.1.m1.1a"><mrow id="A3.p1.1.m1.1.2" xref="A3.p1.1.m1.1.2.cmml"><mrow id="A3.p1.1.m1.1.2.2" xref="A3.p1.1.m1.1.2.2.cmml"><msub id="A3.p1.1.m1.1.2.2.2" xref="A3.p1.1.m1.1.2.2.2.cmml"><mi id="A3.p1.1.m1.1.2.2.2.2" xref="A3.p1.1.m1.1.2.2.2.2.cmml">𝐡</mi><mn id="A3.p1.1.m1.1.2.2.2.3" xref="A3.p1.1.m1.1.2.2.2.3.cmml">1</mn></msub><mo id="A3.p1.1.m1.1.2.2.1" xref="A3.p1.1.m1.1.2.2.1.cmml">⁢</mo><mrow id="A3.p1.1.m1.1.2.2.3.2" xref="A3.p1.1.m1.1.2.2.cmml"><mo id="A3.p1.1.m1.1.2.2.3.2.1" stretchy="false" xref="A3.p1.1.m1.1.2.2.cmml">(</mo><mi id="A3.p1.1.m1.1.1" xref="A3.p1.1.m1.1.1.cmml">t</mi><mo id="A3.p1.1.m1.1.2.2.3.2.2" stretchy="false" xref="A3.p1.1.m1.1.2.2.cmml">)</mo></mrow></mrow><mo id="A3.p1.1.m1.1.2.1" xref="A3.p1.1.m1.1.2.1.cmml">∈</mo><msup id="A3.p1.1.m1.1.2.3" xref="A3.p1.1.m1.1.2.3.cmml"><mi id="A3.p1.1.m1.1.2.3.2" xref="A3.p1.1.m1.1.2.3.2.cmml">ℝ</mi><mi id="A3.p1.1.m1.1.2.3.3" xref="A3.p1.1.m1.1.2.3.3.cmml">n</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="A3.p1.1.m1.1b"><apply id="A3.p1.1.m1.1.2.cmml" xref="A3.p1.1.m1.1.2"><in id="A3.p1.1.m1.1.2.1.cmml" xref="A3.p1.1.m1.1.2.1"></in><apply id="A3.p1.1.m1.1.2.2.cmml" xref="A3.p1.1.m1.1.2.2"><times id="A3.p1.1.m1.1.2.2.1.cmml" xref="A3.p1.1.m1.1.2.2.1"></times><apply id="A3.p1.1.m1.1.2.2.2.cmml" xref="A3.p1.1.m1.1.2.2.2"><csymbol cd="ambiguous" id="A3.p1.1.m1.1.2.2.2.1.cmml" xref="A3.p1.1.m1.1.2.2.2">subscript</csymbol><ci id="A3.p1.1.m1.1.2.2.2.2.cmml" xref="A3.p1.1.m1.1.2.2.2.2">𝐡</ci><cn id="A3.p1.1.m1.1.2.2.2.3.cmml" type="integer" xref="A3.p1.1.m1.1.2.2.2.3">1</cn></apply><ci id="A3.p1.1.m1.1.1.cmml" xref="A3.p1.1.m1.1.1">𝑡</ci></apply><apply id="A3.p1.1.m1.1.2.3.cmml" xref="A3.p1.1.m1.1.2.3"><csymbol cd="ambiguous" id="A3.p1.1.m1.1.2.3.1.cmml" xref="A3.p1.1.m1.1.2.3">superscript</csymbol><ci id="A3.p1.1.m1.1.2.3.2.cmml" xref="A3.p1.1.m1.1.2.3.2">ℝ</ci><ci id="A3.p1.1.m1.1.2.3.3.cmml" xref="A3.p1.1.m1.1.2.3.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.1.m1.1c">\mathbf{h}_{1}(t)\in\mathbb{R}^{n}</annotation><annotation encoding="application/x-llamapun" id="A3.p1.1.m1.1d">bold_h start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_t ) ∈ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\mathbf{h}_{2}(t)\in\mathbb{R}^{n}" class="ltx_Math" display="inline" id="A3.p1.2.m2.1"><semantics id="A3.p1.2.m2.1a"><mrow id="A3.p1.2.m2.1.2" xref="A3.p1.2.m2.1.2.cmml"><mrow id="A3.p1.2.m2.1.2.2" xref="A3.p1.2.m2.1.2.2.cmml"><msub id="A3.p1.2.m2.1.2.2.2" xref="A3.p1.2.m2.1.2.2.2.cmml"><mi id="A3.p1.2.m2.1.2.2.2.2" xref="A3.p1.2.m2.1.2.2.2.2.cmml">𝐡</mi><mn id="A3.p1.2.m2.1.2.2.2.3" xref="A3.p1.2.m2.1.2.2.2.3.cmml">2</mn></msub><mo id="A3.p1.2.m2.1.2.2.1" xref="A3.p1.2.m2.1.2.2.1.cmml">⁢</mo><mrow id="A3.p1.2.m2.1.2.2.3.2" xref="A3.p1.2.m2.1.2.2.cmml"><mo id="A3.p1.2.m2.1.2.2.3.2.1" stretchy="false" xref="A3.p1.2.m2.1.2.2.cmml">(</mo><mi id="A3.p1.2.m2.1.1" xref="A3.p1.2.m2.1.1.cmml">t</mi><mo id="A3.p1.2.m2.1.2.2.3.2.2" stretchy="false" xref="A3.p1.2.m2.1.2.2.cmml">)</mo></mrow></mrow><mo id="A3.p1.2.m2.1.2.1" xref="A3.p1.2.m2.1.2.1.cmml">∈</mo><msup id="A3.p1.2.m2.1.2.3" xref="A3.p1.2.m2.1.2.3.cmml"><mi id="A3.p1.2.m2.1.2.3.2" xref="A3.p1.2.m2.1.2.3.2.cmml">ℝ</mi><mi id="A3.p1.2.m2.1.2.3.3" xref="A3.p1.2.m2.1.2.3.3.cmml">n</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="A3.p1.2.m2.1b"><apply id="A3.p1.2.m2.1.2.cmml" xref="A3.p1.2.m2.1.2"><in id="A3.p1.2.m2.1.2.1.cmml" xref="A3.p1.2.m2.1.2.1"></in><apply id="A3.p1.2.m2.1.2.2.cmml" xref="A3.p1.2.m2.1.2.2"><times id="A3.p1.2.m2.1.2.2.1.cmml" xref="A3.p1.2.m2.1.2.2.1"></times><apply id="A3.p1.2.m2.1.2.2.2.cmml" xref="A3.p1.2.m2.1.2.2.2"><csymbol cd="ambiguous" id="A3.p1.2.m2.1.2.2.2.1.cmml" xref="A3.p1.2.m2.1.2.2.2">subscript</csymbol><ci id="A3.p1.2.m2.1.2.2.2.2.cmml" xref="A3.p1.2.m2.1.2.2.2.2">𝐡</ci><cn id="A3.p1.2.m2.1.2.2.2.3.cmml" type="integer" xref="A3.p1.2.m2.1.2.2.2.3">2</cn></apply><ci id="A3.p1.2.m2.1.1.cmml" xref="A3.p1.2.m2.1.1">𝑡</ci></apply><apply id="A3.p1.2.m2.1.2.3.cmml" xref="A3.p1.2.m2.1.2.3"><csymbol cd="ambiguous" id="A3.p1.2.m2.1.2.3.1.cmml" xref="A3.p1.2.m2.1.2.3">superscript</csymbol><ci id="A3.p1.2.m2.1.2.3.2.cmml" xref="A3.p1.2.m2.1.2.3.2">ℝ</ci><ci id="A3.p1.2.m2.1.2.3.3.cmml" xref="A3.p1.2.m2.1.2.3.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.2.m2.1c">\mathbf{h}_{2}(t)\in\mathbb{R}^{n}</annotation><annotation encoding="application/x-llamapun" id="A3.p1.2.m2.1d">bold_h start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_t ) ∈ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT</annotation></semantics></math>, we first generate a delay-embedded matrix, <math alttext="\mathbf{H}_{1}" class="ltx_Math" display="inline" id="A3.p1.3.m3.1"><semantics id="A3.p1.3.m3.1a"><msub id="A3.p1.3.m3.1.1" xref="A3.p1.3.m3.1.1.cmml"><mi id="A3.p1.3.m3.1.1.2" xref="A3.p1.3.m3.1.1.2.cmml">𝐇</mi><mn id="A3.p1.3.m3.1.1.3" xref="A3.p1.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A3.p1.3.m3.1b"><apply id="A3.p1.3.m3.1.1.cmml" xref="A3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="A3.p1.3.m3.1.1.1.cmml" xref="A3.p1.3.m3.1.1">subscript</csymbol><ci id="A3.p1.3.m3.1.1.2.cmml" xref="A3.p1.3.m3.1.1.2">𝐇</ci><cn id="A3.p1.3.m3.1.1.3.cmml" type="integer" xref="A3.p1.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.3.m3.1c">\mathbf{H}_{1}</annotation><annotation encoding="application/x-llamapun" id="A3.p1.3.m3.1d">bold_H start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\mathbf{H}_{2}" class="ltx_Math" display="inline" id="A3.p1.4.m4.1"><semantics id="A3.p1.4.m4.1a"><msub id="A3.p1.4.m4.1.1" xref="A3.p1.4.m4.1.1.cmml"><mi id="A3.p1.4.m4.1.1.2" xref="A3.p1.4.m4.1.1.2.cmml">𝐇</mi><mn id="A3.p1.4.m4.1.1.3" xref="A3.p1.4.m4.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="A3.p1.4.m4.1b"><apply id="A3.p1.4.m4.1.1.cmml" xref="A3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="A3.p1.4.m4.1.1.1.cmml" xref="A3.p1.4.m4.1.1">subscript</csymbol><ci id="A3.p1.4.m4.1.1.2.cmml" xref="A3.p1.4.m4.1.1.2">𝐇</ci><cn id="A3.p1.4.m4.1.1.3.cmml" type="integer" xref="A3.p1.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.4.m4.1c">\mathbf{H}_{2}</annotation><annotation encoding="application/x-llamapun" id="A3.p1.4.m4.1d">bold_H start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>, by sampling several hidden-state trajectories from each RNN.
Next, for each delay-embedded matrix, we use Dynamic Mode Decomposition (DMD) <cite class="ltx_cite ltx_citemacro_citep">(Schmid, <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib50" title="">2022</a>)</cite> to extract linear forward operators <math alttext="\mathbf{A}_{1}" class="ltx_Math" display="inline" id="A3.p1.5.m5.1"><semantics id="A3.p1.5.m5.1a"><msub id="A3.p1.5.m5.1.1" xref="A3.p1.5.m5.1.1.cmml"><mi id="A3.p1.5.m5.1.1.2" xref="A3.p1.5.m5.1.1.2.cmml">𝐀</mi><mn id="A3.p1.5.m5.1.1.3" xref="A3.p1.5.m5.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A3.p1.5.m5.1b"><apply id="A3.p1.5.m5.1.1.cmml" xref="A3.p1.5.m5.1.1"><csymbol cd="ambiguous" id="A3.p1.5.m5.1.1.1.cmml" xref="A3.p1.5.m5.1.1">subscript</csymbol><ci id="A3.p1.5.m5.1.1.2.cmml" xref="A3.p1.5.m5.1.1.2">𝐀</ci><cn id="A3.p1.5.m5.1.1.3.cmml" type="integer" xref="A3.p1.5.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.5.m5.1c">\mathbf{A}_{1}</annotation><annotation encoding="application/x-llamapun" id="A3.p1.5.m5.1d">bold_A start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\mathbf{A}_{2}" class="ltx_Math" display="inline" id="A3.p1.6.m6.1"><semantics id="A3.p1.6.m6.1a"><msub id="A3.p1.6.m6.1.1" xref="A3.p1.6.m6.1.1.cmml"><mi id="A3.p1.6.m6.1.1.2" xref="A3.p1.6.m6.1.1.2.cmml">𝐀</mi><mn id="A3.p1.6.m6.1.1.3" xref="A3.p1.6.m6.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="A3.p1.6.m6.1b"><apply id="A3.p1.6.m6.1.1.cmml" xref="A3.p1.6.m6.1.1"><csymbol cd="ambiguous" id="A3.p1.6.m6.1.1.1.cmml" xref="A3.p1.6.m6.1.1">subscript</csymbol><ci id="A3.p1.6.m6.1.1.2.cmml" xref="A3.p1.6.m6.1.1.2">𝐀</ci><cn id="A3.p1.6.m6.1.1.3.cmml" type="integer" xref="A3.p1.6.m6.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.6.m6.1c">\mathbf{A}_{2}</annotation><annotation encoding="application/x-llamapun" id="A3.p1.6.m6.1d">bold_A start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> of the two systems’ dynamics.
Finally, a Procrustes distance between the two matrices <math alttext="\mathbf{A}_{1}" class="ltx_Math" display="inline" id="A3.p1.7.m7.1"><semantics id="A3.p1.7.m7.1a"><msub id="A3.p1.7.m7.1.1" xref="A3.p1.7.m7.1.1.cmml"><mi id="A3.p1.7.m7.1.1.2" xref="A3.p1.7.m7.1.1.2.cmml">𝐀</mi><mn id="A3.p1.7.m7.1.1.3" xref="A3.p1.7.m7.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A3.p1.7.m7.1b"><apply id="A3.p1.7.m7.1.1.cmml" xref="A3.p1.7.m7.1.1"><csymbol cd="ambiguous" id="A3.p1.7.m7.1.1.1.cmml" xref="A3.p1.7.m7.1.1">subscript</csymbol><ci id="A3.p1.7.m7.1.1.2.cmml" xref="A3.p1.7.m7.1.1.2">𝐀</ci><cn id="A3.p1.7.m7.1.1.3.cmml" type="integer" xref="A3.p1.7.m7.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.7.m7.1c">\mathbf{A}_{1}</annotation><annotation encoding="application/x-llamapun" id="A3.p1.7.m7.1d">bold_A start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\mathbf{A}_{2}" class="ltx_Math" display="inline" id="A3.p1.8.m8.1"><semantics id="A3.p1.8.m8.1a"><msub id="A3.p1.8.m8.1.1" xref="A3.p1.8.m8.1.1.cmml"><mi id="A3.p1.8.m8.1.1.2" xref="A3.p1.8.m8.1.1.2.cmml">𝐀</mi><mn id="A3.p1.8.m8.1.1.3" xref="A3.p1.8.m8.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="A3.p1.8.m8.1b"><apply id="A3.p1.8.m8.1.1.cmml" xref="A3.p1.8.m8.1.1"><csymbol cd="ambiguous" id="A3.p1.8.m8.1.1.1.cmml" xref="A3.p1.8.m8.1.1">subscript</csymbol><ci id="A3.p1.8.m8.1.1.2.cmml" xref="A3.p1.8.m8.1.1.2">𝐀</ci><cn id="A3.p1.8.m8.1.1.3.cmml" type="integer" xref="A3.p1.8.m8.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.8.m8.1c">\mathbf{A}_{2}</annotation><annotation encoding="application/x-llamapun" id="A3.p1.8.m8.1d">bold_A start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> is used to quantify the dissimilarity between the two dynamical systems and provide an overall DSA score, defined as:
<math alttext="\displaystyle d_{\text{Procrustes}}(\mathbf{A}_{1},\mathbf{A}_{2})=\min_{%
\mathbf{Q}\in O(n)}\|\mathbf{A}_{1}-\mathbf{Q}\mathbf{A}_{2}\mathbf{Q}^{-1}\|_%
{F}" class="ltx_Math" display="inline" id="A3.p1.9.m9.4"><semantics id="A3.p1.9.m9.4a"><mrow id="A3.p1.9.m9.4.4" xref="A3.p1.9.m9.4.4.cmml"><mrow id="A3.p1.9.m9.3.3.2" xref="A3.p1.9.m9.3.3.2.cmml"><msub id="A3.p1.9.m9.3.3.2.4" xref="A3.p1.9.m9.3.3.2.4.cmml"><mi id="A3.p1.9.m9.3.3.2.4.2" xref="A3.p1.9.m9.3.3.2.4.2.cmml">d</mi><mtext id="A3.p1.9.m9.3.3.2.4.3" xref="A3.p1.9.m9.3.3.2.4.3a.cmml">Procrustes</mtext></msub><mo id="A3.p1.9.m9.3.3.2.3" xref="A3.p1.9.m9.3.3.2.3.cmml">⁢</mo><mrow id="A3.p1.9.m9.3.3.2.2.2" xref="A3.p1.9.m9.3.3.2.2.3.cmml"><mo id="A3.p1.9.m9.3.3.2.2.2.3" stretchy="false" xref="A3.p1.9.m9.3.3.2.2.3.cmml">(</mo><msub id="A3.p1.9.m9.2.2.1.1.1.1" xref="A3.p1.9.m9.2.2.1.1.1.1.cmml"><mi id="A3.p1.9.m9.2.2.1.1.1.1.2" xref="A3.p1.9.m9.2.2.1.1.1.1.2.cmml">𝐀</mi><mn id="A3.p1.9.m9.2.2.1.1.1.1.3" xref="A3.p1.9.m9.2.2.1.1.1.1.3.cmml">1</mn></msub><mo id="A3.p1.9.m9.3.3.2.2.2.4" xref="A3.p1.9.m9.3.3.2.2.3.cmml">,</mo><msub id="A3.p1.9.m9.3.3.2.2.2.2" xref="A3.p1.9.m9.3.3.2.2.2.2.cmml"><mi id="A3.p1.9.m9.3.3.2.2.2.2.2" xref="A3.p1.9.m9.3.3.2.2.2.2.2.cmml">𝐀</mi><mn id="A3.p1.9.m9.3.3.2.2.2.2.3" xref="A3.p1.9.m9.3.3.2.2.2.2.3.cmml">2</mn></msub><mo id="A3.p1.9.m9.3.3.2.2.2.5" stretchy="false" xref="A3.p1.9.m9.3.3.2.2.3.cmml">)</mo></mrow></mrow><mo id="A3.p1.9.m9.4.4.4" xref="A3.p1.9.m9.4.4.4.cmml">=</mo><mrow id="A3.p1.9.m9.4.4.3" xref="A3.p1.9.m9.4.4.3.cmml"><munder id="A3.p1.9.m9.4.4.3.2" xref="A3.p1.9.m9.4.4.3.2.cmml"><mi id="A3.p1.9.m9.4.4.3.2.2" xref="A3.p1.9.m9.4.4.3.2.2.cmml">min</mi><mrow id="A3.p1.9.m9.1.1.1" xref="A3.p1.9.m9.1.1.1.cmml"><mi id="A3.p1.9.m9.1.1.1.3" xref="A3.p1.9.m9.1.1.1.3.cmml">𝐐</mi><mo id="A3.p1.9.m9.1.1.1.2" xref="A3.p1.9.m9.1.1.1.2.cmml">∈</mo><mrow id="A3.p1.9.m9.1.1.1.4" xref="A3.p1.9.m9.1.1.1.4.cmml"><mi id="A3.p1.9.m9.1.1.1.4.2" xref="A3.p1.9.m9.1.1.1.4.2.cmml">O</mi><mo id="A3.p1.9.m9.1.1.1.4.1" xref="A3.p1.9.m9.1.1.1.4.1.cmml">⁢</mo><mrow id="A3.p1.9.m9.1.1.1.4.3.2" xref="A3.p1.9.m9.1.1.1.4.cmml"><mo id="A3.p1.9.m9.1.1.1.4.3.2.1" stretchy="false" xref="A3.p1.9.m9.1.1.1.4.cmml">(</mo><mi id="A3.p1.9.m9.1.1.1.1" xref="A3.p1.9.m9.1.1.1.1.cmml">n</mi><mo id="A3.p1.9.m9.1.1.1.4.3.2.2" stretchy="false" xref="A3.p1.9.m9.1.1.1.4.cmml">)</mo></mrow></mrow></mrow></munder><mo id="A3.p1.9.m9.4.4.3a" xref="A3.p1.9.m9.4.4.3.cmml">⁡</mo><msub id="A3.p1.9.m9.4.4.3.1" xref="A3.p1.9.m9.4.4.3.1.cmml"><mrow id="A3.p1.9.m9.4.4.3.1.1.1" xref="A3.p1.9.m9.4.4.3.1.1.2.cmml"><mo id="A3.p1.9.m9.4.4.3.1.1.1.2" stretchy="false" xref="A3.p1.9.m9.4.4.3.1.1.2.1.cmml">‖</mo><mrow id="A3.p1.9.m9.4.4.3.1.1.1.1" xref="A3.p1.9.m9.4.4.3.1.1.1.1.cmml"><msub id="A3.p1.9.m9.4.4.3.1.1.1.1.2" xref="A3.p1.9.m9.4.4.3.1.1.1.1.2.cmml"><mi id="A3.p1.9.m9.4.4.3.1.1.1.1.2.2" xref="A3.p1.9.m9.4.4.3.1.1.1.1.2.2.cmml">𝐀</mi><mn id="A3.p1.9.m9.4.4.3.1.1.1.1.2.3" xref="A3.p1.9.m9.4.4.3.1.1.1.1.2.3.cmml">1</mn></msub><mo id="A3.p1.9.m9.4.4.3.1.1.1.1.1" xref="A3.p1.9.m9.4.4.3.1.1.1.1.1.cmml">−</mo><mrow id="A3.p1.9.m9.4.4.3.1.1.1.1.3" xref="A3.p1.9.m9.4.4.3.1.1.1.1.3.cmml"><msub id="A3.p1.9.m9.4.4.3.1.1.1.1.3.2" xref="A3.p1.9.m9.4.4.3.1.1.1.1.3.2.cmml"><mi id="A3.p1.9.m9.4.4.3.1.1.1.1.3.2.2" xref="A3.p1.9.m9.4.4.3.1.1.1.1.3.2.2.cmml">𝐐𝐀</mi><mn id="A3.p1.9.m9.4.4.3.1.1.1.1.3.2.3" xref="A3.p1.9.m9.4.4.3.1.1.1.1.3.2.3.cmml">2</mn></msub><mo id="A3.p1.9.m9.4.4.3.1.1.1.1.3.1" xref="A3.p1.9.m9.4.4.3.1.1.1.1.3.1.cmml">⁢</mo><msup id="A3.p1.9.m9.4.4.3.1.1.1.1.3.3" xref="A3.p1.9.m9.4.4.3.1.1.1.1.3.3.cmml"><mi id="A3.p1.9.m9.4.4.3.1.1.1.1.3.3.2" xref="A3.p1.9.m9.4.4.3.1.1.1.1.3.3.2.cmml">𝐐</mi><mrow id="A3.p1.9.m9.4.4.3.1.1.1.1.3.3.3" xref="A3.p1.9.m9.4.4.3.1.1.1.1.3.3.3.cmml"><mo id="A3.p1.9.m9.4.4.3.1.1.1.1.3.3.3a" xref="A3.p1.9.m9.4.4.3.1.1.1.1.3.3.3.cmml">−</mo><mn id="A3.p1.9.m9.4.4.3.1.1.1.1.3.3.3.2" xref="A3.p1.9.m9.4.4.3.1.1.1.1.3.3.3.2.cmml">1</mn></mrow></msup></mrow></mrow><mo id="A3.p1.9.m9.4.4.3.1.1.1.3" stretchy="false" xref="A3.p1.9.m9.4.4.3.1.1.2.1.cmml">‖</mo></mrow><mi id="A3.p1.9.m9.4.4.3.1.3" xref="A3.p1.9.m9.4.4.3.1.3.cmml">F</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="A3.p1.9.m9.4b"><apply id="A3.p1.9.m9.4.4.cmml" xref="A3.p1.9.m9.4.4"><eq id="A3.p1.9.m9.4.4.4.cmml" xref="A3.p1.9.m9.4.4.4"></eq><apply id="A3.p1.9.m9.3.3.2.cmml" xref="A3.p1.9.m9.3.3.2"><times id="A3.p1.9.m9.3.3.2.3.cmml" xref="A3.p1.9.m9.3.3.2.3"></times><apply id="A3.p1.9.m9.3.3.2.4.cmml" xref="A3.p1.9.m9.3.3.2.4"><csymbol cd="ambiguous" id="A3.p1.9.m9.3.3.2.4.1.cmml" xref="A3.p1.9.m9.3.3.2.4">subscript</csymbol><ci id="A3.p1.9.m9.3.3.2.4.2.cmml" xref="A3.p1.9.m9.3.3.2.4.2">𝑑</ci><ci id="A3.p1.9.m9.3.3.2.4.3a.cmml" xref="A3.p1.9.m9.3.3.2.4.3"><mtext id="A3.p1.9.m9.3.3.2.4.3.cmml" mathsize="70%" xref="A3.p1.9.m9.3.3.2.4.3">Procrustes</mtext></ci></apply><interval closure="open" id="A3.p1.9.m9.3.3.2.2.3.cmml" xref="A3.p1.9.m9.3.3.2.2.2"><apply id="A3.p1.9.m9.2.2.1.1.1.1.cmml" xref="A3.p1.9.m9.2.2.1.1.1.1"><csymbol cd="ambiguous" id="A3.p1.9.m9.2.2.1.1.1.1.1.cmml" xref="A3.p1.9.m9.2.2.1.1.1.1">subscript</csymbol><ci id="A3.p1.9.m9.2.2.1.1.1.1.2.cmml" xref="A3.p1.9.m9.2.2.1.1.1.1.2">𝐀</ci><cn id="A3.p1.9.m9.2.2.1.1.1.1.3.cmml" type="integer" xref="A3.p1.9.m9.2.2.1.1.1.1.3">1</cn></apply><apply id="A3.p1.9.m9.3.3.2.2.2.2.cmml" xref="A3.p1.9.m9.3.3.2.2.2.2"><csymbol cd="ambiguous" id="A3.p1.9.m9.3.3.2.2.2.2.1.cmml" xref="A3.p1.9.m9.3.3.2.2.2.2">subscript</csymbol><ci id="A3.p1.9.m9.3.3.2.2.2.2.2.cmml" xref="A3.p1.9.m9.3.3.2.2.2.2.2">𝐀</ci><cn id="A3.p1.9.m9.3.3.2.2.2.2.3.cmml" type="integer" xref="A3.p1.9.m9.3.3.2.2.2.2.3">2</cn></apply></interval></apply><apply id="A3.p1.9.m9.4.4.3.cmml" xref="A3.p1.9.m9.4.4.3"><apply id="A3.p1.9.m9.4.4.3.2.cmml" xref="A3.p1.9.m9.4.4.3.2"><csymbol cd="ambiguous" id="A3.p1.9.m9.4.4.3.2.1.cmml" xref="A3.p1.9.m9.4.4.3.2">subscript</csymbol><min id="A3.p1.9.m9.4.4.3.2.2.cmml" xref="A3.p1.9.m9.4.4.3.2.2"></min><apply id="A3.p1.9.m9.1.1.1.cmml" xref="A3.p1.9.m9.1.1.1"><in id="A3.p1.9.m9.1.1.1.2.cmml" xref="A3.p1.9.m9.1.1.1.2"></in><ci id="A3.p1.9.m9.1.1.1.3.cmml" xref="A3.p1.9.m9.1.1.1.3">𝐐</ci><apply id="A3.p1.9.m9.1.1.1.4.cmml" xref="A3.p1.9.m9.1.1.1.4"><times id="A3.p1.9.m9.1.1.1.4.1.cmml" xref="A3.p1.9.m9.1.1.1.4.1"></times><ci id="A3.p1.9.m9.1.1.1.4.2.cmml" xref="A3.p1.9.m9.1.1.1.4.2">𝑂</ci><ci id="A3.p1.9.m9.1.1.1.1.cmml" xref="A3.p1.9.m9.1.1.1.1">𝑛</ci></apply></apply></apply><apply id="A3.p1.9.m9.4.4.3.1.cmml" xref="A3.p1.9.m9.4.4.3.1"><csymbol cd="ambiguous" id="A3.p1.9.m9.4.4.3.1.2.cmml" xref="A3.p1.9.m9.4.4.3.1">subscript</csymbol><apply id="A3.p1.9.m9.4.4.3.1.1.2.cmml" xref="A3.p1.9.m9.4.4.3.1.1.1"><csymbol cd="latexml" id="A3.p1.9.m9.4.4.3.1.1.2.1.cmml" xref="A3.p1.9.m9.4.4.3.1.1.1.2">norm</csymbol><apply id="A3.p1.9.m9.4.4.3.1.1.1.1.cmml" xref="A3.p1.9.m9.4.4.3.1.1.1.1"><minus id="A3.p1.9.m9.4.4.3.1.1.1.1.1.cmml" xref="A3.p1.9.m9.4.4.3.1.1.1.1.1"></minus><apply id="A3.p1.9.m9.4.4.3.1.1.1.1.2.cmml" xref="A3.p1.9.m9.4.4.3.1.1.1.1.2"><csymbol cd="ambiguous" id="A3.p1.9.m9.4.4.3.1.1.1.1.2.1.cmml" xref="A3.p1.9.m9.4.4.3.1.1.1.1.2">subscript</csymbol><ci id="A3.p1.9.m9.4.4.3.1.1.1.1.2.2.cmml" xref="A3.p1.9.m9.4.4.3.1.1.1.1.2.2">𝐀</ci><cn id="A3.p1.9.m9.4.4.3.1.1.1.1.2.3.cmml" type="integer" xref="A3.p1.9.m9.4.4.3.1.1.1.1.2.3">1</cn></apply><apply id="A3.p1.9.m9.4.4.3.1.1.1.1.3.cmml" xref="A3.p1.9.m9.4.4.3.1.1.1.1.3"><times id="A3.p1.9.m9.4.4.3.1.1.1.1.3.1.cmml" xref="A3.p1.9.m9.4.4.3.1.1.1.1.3.1"></times><apply id="A3.p1.9.m9.4.4.3.1.1.1.1.3.2.cmml" xref="A3.p1.9.m9.4.4.3.1.1.1.1.3.2"><csymbol cd="ambiguous" id="A3.p1.9.m9.4.4.3.1.1.1.1.3.2.1.cmml" xref="A3.p1.9.m9.4.4.3.1.1.1.1.3.2">subscript</csymbol><ci id="A3.p1.9.m9.4.4.3.1.1.1.1.3.2.2.cmml" xref="A3.p1.9.m9.4.4.3.1.1.1.1.3.2.2">𝐐𝐀</ci><cn id="A3.p1.9.m9.4.4.3.1.1.1.1.3.2.3.cmml" type="integer" xref="A3.p1.9.m9.4.4.3.1.1.1.1.3.2.3">2</cn></apply><apply id="A3.p1.9.m9.4.4.3.1.1.1.1.3.3.cmml" xref="A3.p1.9.m9.4.4.3.1.1.1.1.3.3"><csymbol cd="ambiguous" id="A3.p1.9.m9.4.4.3.1.1.1.1.3.3.1.cmml" xref="A3.p1.9.m9.4.4.3.1.1.1.1.3.3">superscript</csymbol><ci id="A3.p1.9.m9.4.4.3.1.1.1.1.3.3.2.cmml" xref="A3.p1.9.m9.4.4.3.1.1.1.1.3.3.2">𝐐</ci><apply id="A3.p1.9.m9.4.4.3.1.1.1.1.3.3.3.cmml" xref="A3.p1.9.m9.4.4.3.1.1.1.1.3.3.3"><minus id="A3.p1.9.m9.4.4.3.1.1.1.1.3.3.3.1.cmml" xref="A3.p1.9.m9.4.4.3.1.1.1.1.3.3.3"></minus><cn id="A3.p1.9.m9.4.4.3.1.1.1.1.3.3.3.2.cmml" type="integer" xref="A3.p1.9.m9.4.4.3.1.1.1.1.3.3.3.2">1</cn></apply></apply></apply></apply></apply><ci id="A3.p1.9.m9.4.4.3.1.3.cmml" xref="A3.p1.9.m9.4.4.3.1.3">𝐹</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.9.m9.4c">\displaystyle d_{\text{Procrustes}}(\mathbf{A}_{1},\mathbf{A}_{2})=\min_{%
\mathbf{Q}\in O(n)}\|\mathbf{A}_{1}-\mathbf{Q}\mathbf{A}_{2}\mathbf{Q}^{-1}\|_%
{F}</annotation><annotation encoding="application/x-llamapun" id="A3.p1.9.m9.4d">italic_d start_POSTSUBSCRIPT Procrustes end_POSTSUBSCRIPT ( bold_A start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_A start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) = roman_min start_POSTSUBSCRIPT bold_Q ∈ italic_O ( italic_n ) end_POSTSUBSCRIPT ∥ bold_A start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - bold_QA start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT bold_Q start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT</annotation></semantics></math>
where <math alttext="\mathbf{Q}" class="ltx_Math" display="inline" id="A3.p1.10.m10.1"><semantics id="A3.p1.10.m10.1a"><mi id="A3.p1.10.m10.1.1" xref="A3.p1.10.m10.1.1.cmml">𝐐</mi><annotation-xml encoding="MathML-Content" id="A3.p1.10.m10.1b"><ci id="A3.p1.10.m10.1.1.cmml" xref="A3.p1.10.m10.1.1">𝐐</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.10.m10.1c">\mathbf{Q}</annotation><annotation encoding="application/x-llamapun" id="A3.p1.10.m10.1d">bold_Q</annotation></semantics></math> is a rotation matrix from the orthogonal group <math alttext="O(n)" class="ltx_Math" display="inline" id="A3.p1.11.m11.1"><semantics id="A3.p1.11.m11.1a"><mrow id="A3.p1.11.m11.1.2" xref="A3.p1.11.m11.1.2.cmml"><mi id="A3.p1.11.m11.1.2.2" xref="A3.p1.11.m11.1.2.2.cmml">O</mi><mo id="A3.p1.11.m11.1.2.1" xref="A3.p1.11.m11.1.2.1.cmml">⁢</mo><mrow id="A3.p1.11.m11.1.2.3.2" xref="A3.p1.11.m11.1.2.cmml"><mo id="A3.p1.11.m11.1.2.3.2.1" stretchy="false" xref="A3.p1.11.m11.1.2.cmml">(</mo><mi id="A3.p1.11.m11.1.1" xref="A3.p1.11.m11.1.1.cmml">n</mi><mo id="A3.p1.11.m11.1.2.3.2.2" stretchy="false" xref="A3.p1.11.m11.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A3.p1.11.m11.1b"><apply id="A3.p1.11.m11.1.2.cmml" xref="A3.p1.11.m11.1.2"><times id="A3.p1.11.m11.1.2.1.cmml" xref="A3.p1.11.m11.1.2.1"></times><ci id="A3.p1.11.m11.1.2.2.cmml" xref="A3.p1.11.m11.1.2.2">𝑂</ci><ci id="A3.p1.11.m11.1.1.cmml" xref="A3.p1.11.m11.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.11.m11.1c">O(n)</annotation><annotation encoding="application/x-llamapun" id="A3.p1.11.m11.1d">italic_O ( italic_n )</annotation></semantics></math> and <math alttext="\|\cdot\|_{F}" class="ltx_math_unparsed" display="inline" id="A3.p1.12.m12.1"><semantics id="A3.p1.12.m12.1a"><mrow id="A3.p1.12.m12.1b"><mo id="A3.p1.12.m12.1.1" rspace="0em">∥</mo><mo id="A3.p1.12.m12.1.2" lspace="0em" rspace="0em">⋅</mo><msub id="A3.p1.12.m12.1.3"><mo id="A3.p1.12.m12.1.3.2" lspace="0em">∥</mo><mi id="A3.p1.12.m12.1.3.3">F</mi></msub></mrow><annotation encoding="application/x-tex" id="A3.p1.12.m12.1c">\|\cdot\|_{F}</annotation><annotation encoding="application/x-llamapun" id="A3.p1.12.m12.1d">∥ ⋅ ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT</annotation></semantics></math> is the Frobenius norm.
This metric quantifies how dissimilar the dynamics of the two RNNs are after accounting for orthogonal transformations.
We quantify Dynamical Degeneracy across many RNNs as the average pairwise distance between pairs of RNN neural-dynamics/hidden-state trajectories</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="A3.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A3.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A3.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A3.T2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A3.T2.1.1.1.1.1">Task</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A3.T2.1.1.1.2.1">Number of delay</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A3.T2.1.1.1.3.1">Rank</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.T2.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T2.1.2.1.1">N-Bits Flip Flop</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T2.1.2.1.2">30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T2.1.2.1.3">1000</td>
</tr>
<tr class="ltx_tr" id="A3.T2.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T2.1.3.2.1">Delayed Discrimination</th>
<td class="ltx_td ltx_align_center" id="A3.T2.1.3.2.2">20</td>
<td class="ltx_td ltx_align_center" id="A3.T2.1.3.2.3">100</td>
</tr>
<tr class="ltx_tr" id="A3.T2.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T2.1.4.3.1">Sine Wave Generation</th>
<td class="ltx_td ltx_align_center" id="A3.T2.1.4.3.2">30</td>
<td class="ltx_td ltx_align_center" id="A3.T2.1.4.3.3">100</td>
</tr>
<tr class="ltx_tr" id="A3.T2.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A3.T2.1.5.4.1">Path Integration</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T2.1.5.4.2">30</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T2.1.5.4.3">100</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Hyperparameters for DSA</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Permutation-independent distance between weights</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A4.p1">
<p class="ltx_p" id="A4.p1.2">To quantify the dissimilarity between recurrent weight matrices in a permutation-independent manner, we use a variant of the Frobenius norm that allows for optimal row and column permutations.
Given two RNNs with weight matrices <math alttext="\mathbf{W}_{1}\in\mathbb{R}^{n\times n}" class="ltx_Math" display="inline" id="A4.p1.1.m1.1"><semantics id="A4.p1.1.m1.1a"><mrow id="A4.p1.1.m1.1.1" xref="A4.p1.1.m1.1.1.cmml"><msub id="A4.p1.1.m1.1.1.2" xref="A4.p1.1.m1.1.1.2.cmml"><mi id="A4.p1.1.m1.1.1.2.2" xref="A4.p1.1.m1.1.1.2.2.cmml">𝐖</mi><mn id="A4.p1.1.m1.1.1.2.3" xref="A4.p1.1.m1.1.1.2.3.cmml">1</mn></msub><mo id="A4.p1.1.m1.1.1.1" xref="A4.p1.1.m1.1.1.1.cmml">∈</mo><msup id="A4.p1.1.m1.1.1.3" xref="A4.p1.1.m1.1.1.3.cmml"><mi id="A4.p1.1.m1.1.1.3.2" xref="A4.p1.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="A4.p1.1.m1.1.1.3.3" xref="A4.p1.1.m1.1.1.3.3.cmml"><mi id="A4.p1.1.m1.1.1.3.3.2" xref="A4.p1.1.m1.1.1.3.3.2.cmml">n</mi><mo id="A4.p1.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="A4.p1.1.m1.1.1.3.3.1.cmml">×</mo><mi id="A4.p1.1.m1.1.1.3.3.3" xref="A4.p1.1.m1.1.1.3.3.3.cmml">n</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A4.p1.1.m1.1b"><apply id="A4.p1.1.m1.1.1.cmml" xref="A4.p1.1.m1.1.1"><in id="A4.p1.1.m1.1.1.1.cmml" xref="A4.p1.1.m1.1.1.1"></in><apply id="A4.p1.1.m1.1.1.2.cmml" xref="A4.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="A4.p1.1.m1.1.1.2.1.cmml" xref="A4.p1.1.m1.1.1.2">subscript</csymbol><ci id="A4.p1.1.m1.1.1.2.2.cmml" xref="A4.p1.1.m1.1.1.2.2">𝐖</ci><cn id="A4.p1.1.m1.1.1.2.3.cmml" type="integer" xref="A4.p1.1.m1.1.1.2.3">1</cn></apply><apply id="A4.p1.1.m1.1.1.3.cmml" xref="A4.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="A4.p1.1.m1.1.1.3.1.cmml" xref="A4.p1.1.m1.1.1.3">superscript</csymbol><ci id="A4.p1.1.m1.1.1.3.2.cmml" xref="A4.p1.1.m1.1.1.3.2">ℝ</ci><apply id="A4.p1.1.m1.1.1.3.3.cmml" xref="A4.p1.1.m1.1.1.3.3"><times id="A4.p1.1.m1.1.1.3.3.1.cmml" xref="A4.p1.1.m1.1.1.3.3.1"></times><ci id="A4.p1.1.m1.1.1.3.3.2.cmml" xref="A4.p1.1.m1.1.1.3.3.2">𝑛</ci><ci id="A4.p1.1.m1.1.1.3.3.3.cmml" xref="A4.p1.1.m1.1.1.3.3.3">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.1.m1.1c">\mathbf{W}_{1}\in\mathbb{R}^{n\times n}</annotation><annotation encoding="application/x-llamapun" id="A4.p1.1.m1.1d">bold_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_n end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\mathbf{W}_{2}\in\mathbb{R}^{n\times n}" class="ltx_Math" display="inline" id="A4.p1.2.m2.1"><semantics id="A4.p1.2.m2.1a"><mrow id="A4.p1.2.m2.1.1" xref="A4.p1.2.m2.1.1.cmml"><msub id="A4.p1.2.m2.1.1.2" xref="A4.p1.2.m2.1.1.2.cmml"><mi id="A4.p1.2.m2.1.1.2.2" xref="A4.p1.2.m2.1.1.2.2.cmml">𝐖</mi><mn id="A4.p1.2.m2.1.1.2.3" xref="A4.p1.2.m2.1.1.2.3.cmml">2</mn></msub><mo id="A4.p1.2.m2.1.1.1" xref="A4.p1.2.m2.1.1.1.cmml">∈</mo><msup id="A4.p1.2.m2.1.1.3" xref="A4.p1.2.m2.1.1.3.cmml"><mi id="A4.p1.2.m2.1.1.3.2" xref="A4.p1.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="A4.p1.2.m2.1.1.3.3" xref="A4.p1.2.m2.1.1.3.3.cmml"><mi id="A4.p1.2.m2.1.1.3.3.2" xref="A4.p1.2.m2.1.1.3.3.2.cmml">n</mi><mo id="A4.p1.2.m2.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="A4.p1.2.m2.1.1.3.3.1.cmml">×</mo><mi id="A4.p1.2.m2.1.1.3.3.3" xref="A4.p1.2.m2.1.1.3.3.3.cmml">n</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A4.p1.2.m2.1b"><apply id="A4.p1.2.m2.1.1.cmml" xref="A4.p1.2.m2.1.1"><in id="A4.p1.2.m2.1.1.1.cmml" xref="A4.p1.2.m2.1.1.1"></in><apply id="A4.p1.2.m2.1.1.2.cmml" xref="A4.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="A4.p1.2.m2.1.1.2.1.cmml" xref="A4.p1.2.m2.1.1.2">subscript</csymbol><ci id="A4.p1.2.m2.1.1.2.2.cmml" xref="A4.p1.2.m2.1.1.2.2">𝐖</ci><cn id="A4.p1.2.m2.1.1.2.3.cmml" type="integer" xref="A4.p1.2.m2.1.1.2.3">2</cn></apply><apply id="A4.p1.2.m2.1.1.3.cmml" xref="A4.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="A4.p1.2.m2.1.1.3.1.cmml" xref="A4.p1.2.m2.1.1.3">superscript</csymbol><ci id="A4.p1.2.m2.1.1.3.2.cmml" xref="A4.p1.2.m2.1.1.3.2">ℝ</ci><apply id="A4.p1.2.m2.1.1.3.3.cmml" xref="A4.p1.2.m2.1.1.3.3"><times id="A4.p1.2.m2.1.1.3.3.1.cmml" xref="A4.p1.2.m2.1.1.3.3.1"></times><ci id="A4.p1.2.m2.1.1.3.3.2.cmml" xref="A4.p1.2.m2.1.1.3.3.2">𝑛</ci><ci id="A4.p1.2.m2.1.1.3.3.3.cmml" xref="A4.p1.2.m2.1.1.3.3.3">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.2.m2.1c">\mathbf{W}_{2}\in\mathbb{R}^{n\times n}</annotation><annotation encoding="application/x-llamapun" id="A4.p1.2.m2.1d">bold_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_n end_POSTSUPERSCRIPT</annotation></semantics></math>, we define the Permutation-Independent Frobenius (PIF) distance as:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="A4.p2">
<table class="ltx_equation ltx_eqn_table" id="A4.Ex2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="d_{\text{PIF}}(\mathbf{W}_{1},\mathbf{W}_{2})=\min_{\mathbf{P}_{1},\mathbf{P}_%
{2}\in\mathcal{P}(n)}\|\mathbf{W}_{1}-\mathbf{P}_{1}\mathbf{W}_{2}\mathbf{P}_{%
2}\|_{F}" class="ltx_Math" display="block" id="A4.Ex2.m1.6"><semantics id="A4.Ex2.m1.6a"><mrow id="A4.Ex2.m1.6.6" xref="A4.Ex2.m1.6.6.cmml"><mrow id="A4.Ex2.m1.5.5.2" xref="A4.Ex2.m1.5.5.2.cmml"><msub id="A4.Ex2.m1.5.5.2.4" xref="A4.Ex2.m1.5.5.2.4.cmml"><mi id="A4.Ex2.m1.5.5.2.4.2" xref="A4.Ex2.m1.5.5.2.4.2.cmml">d</mi><mtext id="A4.Ex2.m1.5.5.2.4.3" xref="A4.Ex2.m1.5.5.2.4.3a.cmml">PIF</mtext></msub><mo id="A4.Ex2.m1.5.5.2.3" xref="A4.Ex2.m1.5.5.2.3.cmml">⁢</mo><mrow id="A4.Ex2.m1.5.5.2.2.2" xref="A4.Ex2.m1.5.5.2.2.3.cmml"><mo id="A4.Ex2.m1.5.5.2.2.2.3" stretchy="false" xref="A4.Ex2.m1.5.5.2.2.3.cmml">(</mo><msub id="A4.Ex2.m1.4.4.1.1.1.1" xref="A4.Ex2.m1.4.4.1.1.1.1.cmml"><mi id="A4.Ex2.m1.4.4.1.1.1.1.2" xref="A4.Ex2.m1.4.4.1.1.1.1.2.cmml">𝐖</mi><mn id="A4.Ex2.m1.4.4.1.1.1.1.3" xref="A4.Ex2.m1.4.4.1.1.1.1.3.cmml">1</mn></msub><mo id="A4.Ex2.m1.5.5.2.2.2.4" xref="A4.Ex2.m1.5.5.2.2.3.cmml">,</mo><msub id="A4.Ex2.m1.5.5.2.2.2.2" xref="A4.Ex2.m1.5.5.2.2.2.2.cmml"><mi id="A4.Ex2.m1.5.5.2.2.2.2.2" xref="A4.Ex2.m1.5.5.2.2.2.2.2.cmml">𝐖</mi><mn id="A4.Ex2.m1.5.5.2.2.2.2.3" xref="A4.Ex2.m1.5.5.2.2.2.2.3.cmml">2</mn></msub><mo id="A4.Ex2.m1.5.5.2.2.2.5" stretchy="false" xref="A4.Ex2.m1.5.5.2.2.3.cmml">)</mo></mrow></mrow><mo id="A4.Ex2.m1.6.6.4" xref="A4.Ex2.m1.6.6.4.cmml">=</mo><mrow id="A4.Ex2.m1.6.6.3" xref="A4.Ex2.m1.6.6.3.cmml"><munder id="A4.Ex2.m1.6.6.3.2" xref="A4.Ex2.m1.6.6.3.2.cmml"><mi id="A4.Ex2.m1.6.6.3.2.2" xref="A4.Ex2.m1.6.6.3.2.2.cmml">min</mi><mrow id="A4.Ex2.m1.3.3.3" xref="A4.Ex2.m1.3.3.3.cmml"><mrow id="A4.Ex2.m1.3.3.3.3.2" xref="A4.Ex2.m1.3.3.3.3.3.cmml"><msub id="A4.Ex2.m1.2.2.2.2.1.1" xref="A4.Ex2.m1.2.2.2.2.1.1.cmml"><mi id="A4.Ex2.m1.2.2.2.2.1.1.2" xref="A4.Ex2.m1.2.2.2.2.1.1.2.cmml">𝐏</mi><mn id="A4.Ex2.m1.2.2.2.2.1.1.3" xref="A4.Ex2.m1.2.2.2.2.1.1.3.cmml">1</mn></msub><mo id="A4.Ex2.m1.3.3.3.3.2.3" xref="A4.Ex2.m1.3.3.3.3.3.cmml">,</mo><msub id="A4.Ex2.m1.3.3.3.3.2.2" xref="A4.Ex2.m1.3.3.3.3.2.2.cmml"><mi id="A4.Ex2.m1.3.3.3.3.2.2.2" xref="A4.Ex2.m1.3.3.3.3.2.2.2.cmml">𝐏</mi><mn id="A4.Ex2.m1.3.3.3.3.2.2.3" xref="A4.Ex2.m1.3.3.3.3.2.2.3.cmml">2</mn></msub></mrow><mo id="A4.Ex2.m1.3.3.3.4" xref="A4.Ex2.m1.3.3.3.4.cmml">∈</mo><mrow id="A4.Ex2.m1.3.3.3.5" xref="A4.Ex2.m1.3.3.3.5.cmml"><mi class="ltx_font_mathcaligraphic" id="A4.Ex2.m1.3.3.3.5.2" xref="A4.Ex2.m1.3.3.3.5.2.cmml">𝒫</mi><mo id="A4.Ex2.m1.3.3.3.5.1" xref="A4.Ex2.m1.3.3.3.5.1.cmml">⁢</mo><mrow id="A4.Ex2.m1.3.3.3.5.3.2" xref="A4.Ex2.m1.3.3.3.5.cmml"><mo id="A4.Ex2.m1.3.3.3.5.3.2.1" stretchy="false" xref="A4.Ex2.m1.3.3.3.5.cmml">(</mo><mi id="A4.Ex2.m1.1.1.1.1" xref="A4.Ex2.m1.1.1.1.1.cmml">n</mi><mo id="A4.Ex2.m1.3.3.3.5.3.2.2" stretchy="false" xref="A4.Ex2.m1.3.3.3.5.cmml">)</mo></mrow></mrow></mrow></munder><mo id="A4.Ex2.m1.6.6.3a" xref="A4.Ex2.m1.6.6.3.cmml">⁡</mo><msub id="A4.Ex2.m1.6.6.3.1" xref="A4.Ex2.m1.6.6.3.1.cmml"><mrow id="A4.Ex2.m1.6.6.3.1.1.1" xref="A4.Ex2.m1.6.6.3.1.1.2.cmml"><mo id="A4.Ex2.m1.6.6.3.1.1.1.2" stretchy="false" xref="A4.Ex2.m1.6.6.3.1.1.2.1.cmml">‖</mo><mrow id="A4.Ex2.m1.6.6.3.1.1.1.1" xref="A4.Ex2.m1.6.6.3.1.1.1.1.cmml"><msub id="A4.Ex2.m1.6.6.3.1.1.1.1.2" xref="A4.Ex2.m1.6.6.3.1.1.1.1.2.cmml"><mi id="A4.Ex2.m1.6.6.3.1.1.1.1.2.2" xref="A4.Ex2.m1.6.6.3.1.1.1.1.2.2.cmml">𝐖</mi><mn id="A4.Ex2.m1.6.6.3.1.1.1.1.2.3" xref="A4.Ex2.m1.6.6.3.1.1.1.1.2.3.cmml">1</mn></msub><mo id="A4.Ex2.m1.6.6.3.1.1.1.1.1" xref="A4.Ex2.m1.6.6.3.1.1.1.1.1.cmml">−</mo><mrow id="A4.Ex2.m1.6.6.3.1.1.1.1.3" xref="A4.Ex2.m1.6.6.3.1.1.1.1.3.cmml"><msub id="A4.Ex2.m1.6.6.3.1.1.1.1.3.2" xref="A4.Ex2.m1.6.6.3.1.1.1.1.3.2.cmml"><mi id="A4.Ex2.m1.6.6.3.1.1.1.1.3.2.2" xref="A4.Ex2.m1.6.6.3.1.1.1.1.3.2.2.cmml">𝐏</mi><mn id="A4.Ex2.m1.6.6.3.1.1.1.1.3.2.3" xref="A4.Ex2.m1.6.6.3.1.1.1.1.3.2.3.cmml">1</mn></msub><mo id="A4.Ex2.m1.6.6.3.1.1.1.1.3.1" xref="A4.Ex2.m1.6.6.3.1.1.1.1.3.1.cmml">⁢</mo><msub id="A4.Ex2.m1.6.6.3.1.1.1.1.3.3" xref="A4.Ex2.m1.6.6.3.1.1.1.1.3.3.cmml"><mi id="A4.Ex2.m1.6.6.3.1.1.1.1.3.3.2" xref="A4.Ex2.m1.6.6.3.1.1.1.1.3.3.2.cmml">𝐖</mi><mn id="A4.Ex2.m1.6.6.3.1.1.1.1.3.3.3" xref="A4.Ex2.m1.6.6.3.1.1.1.1.3.3.3.cmml">2</mn></msub><mo id="A4.Ex2.m1.6.6.3.1.1.1.1.3.1a" xref="A4.Ex2.m1.6.6.3.1.1.1.1.3.1.cmml">⁢</mo><msub id="A4.Ex2.m1.6.6.3.1.1.1.1.3.4" xref="A4.Ex2.m1.6.6.3.1.1.1.1.3.4.cmml"><mi id="A4.Ex2.m1.6.6.3.1.1.1.1.3.4.2" xref="A4.Ex2.m1.6.6.3.1.1.1.1.3.4.2.cmml">𝐏</mi><mn id="A4.Ex2.m1.6.6.3.1.1.1.1.3.4.3" xref="A4.Ex2.m1.6.6.3.1.1.1.1.3.4.3.cmml">2</mn></msub></mrow></mrow><mo id="A4.Ex2.m1.6.6.3.1.1.1.3" stretchy="false" xref="A4.Ex2.m1.6.6.3.1.1.2.1.cmml">‖</mo></mrow><mi id="A4.Ex2.m1.6.6.3.1.3" xref="A4.Ex2.m1.6.6.3.1.3.cmml">F</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="A4.Ex2.m1.6b"><apply id="A4.Ex2.m1.6.6.cmml" xref="A4.Ex2.m1.6.6"><eq id="A4.Ex2.m1.6.6.4.cmml" xref="A4.Ex2.m1.6.6.4"></eq><apply id="A4.Ex2.m1.5.5.2.cmml" xref="A4.Ex2.m1.5.5.2"><times id="A4.Ex2.m1.5.5.2.3.cmml" xref="A4.Ex2.m1.5.5.2.3"></times><apply id="A4.Ex2.m1.5.5.2.4.cmml" xref="A4.Ex2.m1.5.5.2.4"><csymbol cd="ambiguous" id="A4.Ex2.m1.5.5.2.4.1.cmml" xref="A4.Ex2.m1.5.5.2.4">subscript</csymbol><ci id="A4.Ex2.m1.5.5.2.4.2.cmml" xref="A4.Ex2.m1.5.5.2.4.2">𝑑</ci><ci id="A4.Ex2.m1.5.5.2.4.3a.cmml" xref="A4.Ex2.m1.5.5.2.4.3"><mtext id="A4.Ex2.m1.5.5.2.4.3.cmml" mathsize="70%" xref="A4.Ex2.m1.5.5.2.4.3">PIF</mtext></ci></apply><interval closure="open" id="A4.Ex2.m1.5.5.2.2.3.cmml" xref="A4.Ex2.m1.5.5.2.2.2"><apply id="A4.Ex2.m1.4.4.1.1.1.1.cmml" xref="A4.Ex2.m1.4.4.1.1.1.1"><csymbol cd="ambiguous" id="A4.Ex2.m1.4.4.1.1.1.1.1.cmml" xref="A4.Ex2.m1.4.4.1.1.1.1">subscript</csymbol><ci id="A4.Ex2.m1.4.4.1.1.1.1.2.cmml" xref="A4.Ex2.m1.4.4.1.1.1.1.2">𝐖</ci><cn id="A4.Ex2.m1.4.4.1.1.1.1.3.cmml" type="integer" xref="A4.Ex2.m1.4.4.1.1.1.1.3">1</cn></apply><apply id="A4.Ex2.m1.5.5.2.2.2.2.cmml" xref="A4.Ex2.m1.5.5.2.2.2.2"><csymbol cd="ambiguous" id="A4.Ex2.m1.5.5.2.2.2.2.1.cmml" xref="A4.Ex2.m1.5.5.2.2.2.2">subscript</csymbol><ci id="A4.Ex2.m1.5.5.2.2.2.2.2.cmml" xref="A4.Ex2.m1.5.5.2.2.2.2.2">𝐖</ci><cn id="A4.Ex2.m1.5.5.2.2.2.2.3.cmml" type="integer" xref="A4.Ex2.m1.5.5.2.2.2.2.3">2</cn></apply></interval></apply><apply id="A4.Ex2.m1.6.6.3.cmml" xref="A4.Ex2.m1.6.6.3"><apply id="A4.Ex2.m1.6.6.3.2.cmml" xref="A4.Ex2.m1.6.6.3.2"><csymbol cd="ambiguous" id="A4.Ex2.m1.6.6.3.2.1.cmml" xref="A4.Ex2.m1.6.6.3.2">subscript</csymbol><min id="A4.Ex2.m1.6.6.3.2.2.cmml" xref="A4.Ex2.m1.6.6.3.2.2"></min><apply id="A4.Ex2.m1.3.3.3.cmml" xref="A4.Ex2.m1.3.3.3"><in id="A4.Ex2.m1.3.3.3.4.cmml" xref="A4.Ex2.m1.3.3.3.4"></in><list id="A4.Ex2.m1.3.3.3.3.3.cmml" xref="A4.Ex2.m1.3.3.3.3.2"><apply id="A4.Ex2.m1.2.2.2.2.1.1.cmml" xref="A4.Ex2.m1.2.2.2.2.1.1"><csymbol cd="ambiguous" id="A4.Ex2.m1.2.2.2.2.1.1.1.cmml" xref="A4.Ex2.m1.2.2.2.2.1.1">subscript</csymbol><ci id="A4.Ex2.m1.2.2.2.2.1.1.2.cmml" xref="A4.Ex2.m1.2.2.2.2.1.1.2">𝐏</ci><cn id="A4.Ex2.m1.2.2.2.2.1.1.3.cmml" type="integer" xref="A4.Ex2.m1.2.2.2.2.1.1.3">1</cn></apply><apply id="A4.Ex2.m1.3.3.3.3.2.2.cmml" xref="A4.Ex2.m1.3.3.3.3.2.2"><csymbol cd="ambiguous" id="A4.Ex2.m1.3.3.3.3.2.2.1.cmml" xref="A4.Ex2.m1.3.3.3.3.2.2">subscript</csymbol><ci id="A4.Ex2.m1.3.3.3.3.2.2.2.cmml" xref="A4.Ex2.m1.3.3.3.3.2.2.2">𝐏</ci><cn id="A4.Ex2.m1.3.3.3.3.2.2.3.cmml" type="integer" xref="A4.Ex2.m1.3.3.3.3.2.2.3">2</cn></apply></list><apply id="A4.Ex2.m1.3.3.3.5.cmml" xref="A4.Ex2.m1.3.3.3.5"><times id="A4.Ex2.m1.3.3.3.5.1.cmml" xref="A4.Ex2.m1.3.3.3.5.1"></times><ci id="A4.Ex2.m1.3.3.3.5.2.cmml" xref="A4.Ex2.m1.3.3.3.5.2">𝒫</ci><ci id="A4.Ex2.m1.1.1.1.1.cmml" xref="A4.Ex2.m1.1.1.1.1">𝑛</ci></apply></apply></apply><apply id="A4.Ex2.m1.6.6.3.1.cmml" xref="A4.Ex2.m1.6.6.3.1"><csymbol cd="ambiguous" id="A4.Ex2.m1.6.6.3.1.2.cmml" xref="A4.Ex2.m1.6.6.3.1">subscript</csymbol><apply id="A4.Ex2.m1.6.6.3.1.1.2.cmml" xref="A4.Ex2.m1.6.6.3.1.1.1"><csymbol cd="latexml" id="A4.Ex2.m1.6.6.3.1.1.2.1.cmml" xref="A4.Ex2.m1.6.6.3.1.1.1.2">norm</csymbol><apply id="A4.Ex2.m1.6.6.3.1.1.1.1.cmml" xref="A4.Ex2.m1.6.6.3.1.1.1.1"><minus id="A4.Ex2.m1.6.6.3.1.1.1.1.1.cmml" xref="A4.Ex2.m1.6.6.3.1.1.1.1.1"></minus><apply id="A4.Ex2.m1.6.6.3.1.1.1.1.2.cmml" xref="A4.Ex2.m1.6.6.3.1.1.1.1.2"><csymbol cd="ambiguous" id="A4.Ex2.m1.6.6.3.1.1.1.1.2.1.cmml" xref="A4.Ex2.m1.6.6.3.1.1.1.1.2">subscript</csymbol><ci id="A4.Ex2.m1.6.6.3.1.1.1.1.2.2.cmml" xref="A4.Ex2.m1.6.6.3.1.1.1.1.2.2">𝐖</ci><cn id="A4.Ex2.m1.6.6.3.1.1.1.1.2.3.cmml" type="integer" xref="A4.Ex2.m1.6.6.3.1.1.1.1.2.3">1</cn></apply><apply id="A4.Ex2.m1.6.6.3.1.1.1.1.3.cmml" xref="A4.Ex2.m1.6.6.3.1.1.1.1.3"><times id="A4.Ex2.m1.6.6.3.1.1.1.1.3.1.cmml" xref="A4.Ex2.m1.6.6.3.1.1.1.1.3.1"></times><apply id="A4.Ex2.m1.6.6.3.1.1.1.1.3.2.cmml" xref="A4.Ex2.m1.6.6.3.1.1.1.1.3.2"><csymbol cd="ambiguous" id="A4.Ex2.m1.6.6.3.1.1.1.1.3.2.1.cmml" xref="A4.Ex2.m1.6.6.3.1.1.1.1.3.2">subscript</csymbol><ci id="A4.Ex2.m1.6.6.3.1.1.1.1.3.2.2.cmml" xref="A4.Ex2.m1.6.6.3.1.1.1.1.3.2.2">𝐏</ci><cn id="A4.Ex2.m1.6.6.3.1.1.1.1.3.2.3.cmml" type="integer" xref="A4.Ex2.m1.6.6.3.1.1.1.1.3.2.3">1</cn></apply><apply id="A4.Ex2.m1.6.6.3.1.1.1.1.3.3.cmml" xref="A4.Ex2.m1.6.6.3.1.1.1.1.3.3"><csymbol cd="ambiguous" id="A4.Ex2.m1.6.6.3.1.1.1.1.3.3.1.cmml" xref="A4.Ex2.m1.6.6.3.1.1.1.1.3.3">subscript</csymbol><ci id="A4.Ex2.m1.6.6.3.1.1.1.1.3.3.2.cmml" xref="A4.Ex2.m1.6.6.3.1.1.1.1.3.3.2">𝐖</ci><cn id="A4.Ex2.m1.6.6.3.1.1.1.1.3.3.3.cmml" type="integer" xref="A4.Ex2.m1.6.6.3.1.1.1.1.3.3.3">2</cn></apply><apply id="A4.Ex2.m1.6.6.3.1.1.1.1.3.4.cmml" xref="A4.Ex2.m1.6.6.3.1.1.1.1.3.4"><csymbol cd="ambiguous" id="A4.Ex2.m1.6.6.3.1.1.1.1.3.4.1.cmml" xref="A4.Ex2.m1.6.6.3.1.1.1.1.3.4">subscript</csymbol><ci id="A4.Ex2.m1.6.6.3.1.1.1.1.3.4.2.cmml" xref="A4.Ex2.m1.6.6.3.1.1.1.1.3.4.2">𝐏</ci><cn id="A4.Ex2.m1.6.6.3.1.1.1.1.3.4.3.cmml" type="integer" xref="A4.Ex2.m1.6.6.3.1.1.1.1.3.4.3">2</cn></apply></apply></apply></apply><ci id="A4.Ex2.m1.6.6.3.1.3.cmml" xref="A4.Ex2.m1.6.6.3.1.3">𝐹</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.Ex2.m1.6c">d_{\text{PIF}}(\mathbf{W}_{1},\mathbf{W}_{2})=\min_{\mathbf{P}_{1},\mathbf{P}_%
{2}\in\mathcal{P}(n)}\|\mathbf{W}_{1}-\mathbf{P}_{1}\mathbf{W}_{2}\mathbf{P}_{%
2}\|_{F}</annotation><annotation encoding="application/x-llamapun" id="A4.Ex2.m1.6d">italic_d start_POSTSUBSCRIPT PIF end_POSTSUBSCRIPT ( bold_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) = roman_min start_POSTSUBSCRIPT bold_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_P start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ∈ caligraphic_P ( italic_n ) end_POSTSUBSCRIPT ∥ bold_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - bold_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT bold_P start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="A4.p3">
<p class="ltx_p" id="A4.p3.3">where <math alttext="\mathcal{P}(n)" class="ltx_Math" display="inline" id="A4.p3.1.m1.1"><semantics id="A4.p3.1.m1.1a"><mrow id="A4.p3.1.m1.1.2" xref="A4.p3.1.m1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="A4.p3.1.m1.1.2.2" xref="A4.p3.1.m1.1.2.2.cmml">𝒫</mi><mo id="A4.p3.1.m1.1.2.1" xref="A4.p3.1.m1.1.2.1.cmml">⁢</mo><mrow id="A4.p3.1.m1.1.2.3.2" xref="A4.p3.1.m1.1.2.cmml"><mo id="A4.p3.1.m1.1.2.3.2.1" stretchy="false" xref="A4.p3.1.m1.1.2.cmml">(</mo><mi id="A4.p3.1.m1.1.1" xref="A4.p3.1.m1.1.1.cmml">n</mi><mo id="A4.p3.1.m1.1.2.3.2.2" stretchy="false" xref="A4.p3.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A4.p3.1.m1.1b"><apply id="A4.p3.1.m1.1.2.cmml" xref="A4.p3.1.m1.1.2"><times id="A4.p3.1.m1.1.2.1.cmml" xref="A4.p3.1.m1.1.2.1"></times><ci id="A4.p3.1.m1.1.2.2.cmml" xref="A4.p3.1.m1.1.2.2">𝒫</ci><ci id="A4.p3.1.m1.1.1.cmml" xref="A4.p3.1.m1.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p3.1.m1.1c">\mathcal{P}(n)</annotation><annotation encoding="application/x-llamapun" id="A4.p3.1.m1.1d">caligraphic_P ( italic_n )</annotation></semantics></math> is the set of permutation matrices of size <math alttext="n\times n" class="ltx_Math" display="inline" id="A4.p3.2.m2.1"><semantics id="A4.p3.2.m2.1a"><mrow id="A4.p3.2.m2.1.1" xref="A4.p3.2.m2.1.1.cmml"><mi id="A4.p3.2.m2.1.1.2" xref="A4.p3.2.m2.1.1.2.cmml">n</mi><mo id="A4.p3.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="A4.p3.2.m2.1.1.1.cmml">×</mo><mi id="A4.p3.2.m2.1.1.3" xref="A4.p3.2.m2.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="A4.p3.2.m2.1b"><apply id="A4.p3.2.m2.1.1.cmml" xref="A4.p3.2.m2.1.1"><times id="A4.p3.2.m2.1.1.1.cmml" xref="A4.p3.2.m2.1.1.1"></times><ci id="A4.p3.2.m2.1.1.2.cmml" xref="A4.p3.2.m2.1.1.2">𝑛</ci><ci id="A4.p3.2.m2.1.1.3.cmml" xref="A4.p3.2.m2.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p3.2.m2.1c">n\times n</annotation><annotation encoding="application/x-llamapun" id="A4.p3.2.m2.1d">italic_n × italic_n</annotation></semantics></math>, and <math alttext="\|\cdot\|_{F}" class="ltx_math_unparsed" display="inline" id="A4.p3.3.m3.1"><semantics id="A4.p3.3.m3.1a"><mrow id="A4.p3.3.m3.1b"><mo id="A4.p3.3.m3.1.1" rspace="0em">∥</mo><mo id="A4.p3.3.m3.1.2" lspace="0em" rspace="0em">⋅</mo><msub id="A4.p3.3.m3.1.3"><mo id="A4.p3.3.m3.1.3.2" lspace="0em">∥</mo><mi id="A4.p3.3.m3.1.3.3">F</mi></msub></mrow><annotation encoding="application/x-tex" id="A4.p3.3.m3.1c">\|\cdot\|_{F}</annotation><annotation encoding="application/x-llamapun" id="A4.p3.3.m3.1d">∥ ⋅ ∥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT</annotation></semantics></math> denotes the Frobenius norm.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="A4.p4">
<p class="ltx_p" id="A4.p4.2">This distance is calculated using an iterative coordinate-descent optimization strategy with multiple random restarts to avoid local minima.
At each iteration, both the row permutation <math alttext="\mathbf{P}_{1}" class="ltx_Math" display="inline" id="A4.p4.1.m1.1"><semantics id="A4.p4.1.m1.1a"><msub id="A4.p4.1.m1.1.1" xref="A4.p4.1.m1.1.1.cmml"><mi id="A4.p4.1.m1.1.1.2" xref="A4.p4.1.m1.1.1.2.cmml">𝐏</mi><mn id="A4.p4.1.m1.1.1.3" xref="A4.p4.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A4.p4.1.m1.1b"><apply id="A4.p4.1.m1.1.1.cmml" xref="A4.p4.1.m1.1.1"><csymbol cd="ambiguous" id="A4.p4.1.m1.1.1.1.cmml" xref="A4.p4.1.m1.1.1">subscript</csymbol><ci id="A4.p4.1.m1.1.1.2.cmml" xref="A4.p4.1.m1.1.1.2">𝐏</ci><cn id="A4.p4.1.m1.1.1.3.cmml" type="integer" xref="A4.p4.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.1.m1.1c">\mathbf{P}_{1}</annotation><annotation encoding="application/x-llamapun" id="A4.p4.1.m1.1d">bold_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> or the column permutation <math alttext="\mathbf{P}_{2}" class="ltx_Math" display="inline" id="A4.p4.2.m2.1"><semantics id="A4.p4.2.m2.1a"><msub id="A4.p4.2.m2.1.1" xref="A4.p4.2.m2.1.1.cmml"><mi id="A4.p4.2.m2.1.1.2" xref="A4.p4.2.m2.1.1.2.cmml">𝐏</mi><mn id="A4.p4.2.m2.1.1.3" xref="A4.p4.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="A4.p4.2.m2.1b"><apply id="A4.p4.2.m2.1.1.cmml" xref="A4.p4.2.m2.1.1"><csymbol cd="ambiguous" id="A4.p4.2.m2.1.1.1.cmml" xref="A4.p4.2.m2.1.1">subscript</csymbol><ci id="A4.p4.2.m2.1.1.2.cmml" xref="A4.p4.2.m2.1.1.2">𝐏</ci><cn id="A4.p4.2.m2.1.1.3.cmml" type="integer" xref="A4.p4.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.2.m2.1c">\mathbf{P}_{2}</annotation><annotation encoding="application/x-llamapun" id="A4.p4.2.m2.1d">bold_P start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> is optimized using a linear-sum-assignment approach as described in <cite class="ltx_cite ltx_citemacro_citep">(Ainsworth et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03972v1#bib.bib1" title="">2022</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</article>
</div>

</div>


<button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button><div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer></body></html>