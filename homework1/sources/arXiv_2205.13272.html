<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2205.13272] FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices</title><meta property="og:description" content="IoT devices are a technology recurrent in the contexts of Industry 4.0 and real-time applications. Nonetheless, they suffer from resource limitations, such as processor, RAM, and disc storage. These limitations become …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2205.13272">

<!--Generated on Mon Mar 11 13:34:05 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="en">
<h1 class="ltx_title ltx_title_document">FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Marrone Silvério Melo Dantas,
Iago Richard Rodrigues,
Assis Tiago Oliveira Filho,
Gibson Barbosa,
Daniel Bezerra,
Djamel F. H. Sadok,
Judith Kelner,
Maria Marquezini,
Ricardo Silva
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id9.9" class="ltx_p"><span id="id9.9.9" class="ltx_text">IoT devices are a technology recurrent in the contexts of Industry 4.0 and real-time applications. Nonetheless, they suffer from resource limitations, such as processor, RAM, and disc storage. These limitations become more evident when handling demanding applications, such as deep learning, well-known for their heavy computational requirements. A case in point is robot pose estimation, an application that predicts the critical points of the desired image object. One way to mitigate processing and storage problems is compressing that deep learning application. This paper proposes a new CNN for the pose estimation while applying the compression techniques of pruning and quantization to reduce his demands and improve the response time. While the pruning process reduces the total number of parameters required for inference, quantization decreases the precision of the floating-point. We run the approach using a pose estimation task for a robotic arm and compare the results in a high-end device and a constrained device. As metrics, we consider the number of Floating-point Operations Per Second(FLOPS), the total of mathematical computations, the calculation of parameters, the inference time, and the number of video frames processed per second. In addition, we undertake a qualitative evaluation where we compare the output image predicted for each pruned network with the corresponding original one. We reduce the originally proposed network to a <math id="id1.1.1.m1.1" class="ltx_Math" alttext="70\%" display="inline"><semantics id="id1.1.1.m1.1a"><mrow id="id1.1.1.m1.1.1" xref="id1.1.1.m1.1.1.cmml"><mn id="id1.1.1.m1.1.1.2" xref="id1.1.1.m1.1.1.2.cmml">70</mn><mo id="id1.1.1.m1.1.1.1" xref="id1.1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="id1.1.1.m1.1b"><apply id="id1.1.1.m1.1.1.cmml" xref="id1.1.1.m1.1.1"><csymbol cd="latexml" id="id1.1.1.m1.1.1.1.cmml" xref="id1.1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="id1.1.1.m1.1.1.2.cmml" xref="id1.1.1.m1.1.1.2">70</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.1.m1.1c">70\%</annotation></semantics></math> pruning rate, implying an <math id="id2.2.2.m2.1" class="ltx_Math" alttext="88.86\%" display="inline"><semantics id="id2.2.2.m2.1a"><mrow id="id2.2.2.m2.1.1" xref="id2.2.2.m2.1.1.cmml"><mn id="id2.2.2.m2.1.1.2" xref="id2.2.2.m2.1.1.2.cmml">88.86</mn><mo id="id2.2.2.m2.1.1.1" xref="id2.2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="id2.2.2.m2.1b"><apply id="id2.2.2.m2.1.1.cmml" xref="id2.2.2.m2.1.1"><csymbol cd="latexml" id="id2.2.2.m2.1.1.1.cmml" xref="id2.2.2.m2.1.1.1">percent</csymbol><cn type="float" id="id2.2.2.m2.1.1.2.cmml" xref="id2.2.2.m2.1.1.2">88.86</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.2.m2.1c">88.86\%</annotation></semantics></math> reduction in parameters, <math id="id3.3.3.m3.1" class="ltx_Math" alttext="94.45\%" display="inline"><semantics id="id3.3.3.m3.1a"><mrow id="id3.3.3.m3.1.1" xref="id3.3.3.m3.1.1.cmml"><mn id="id3.3.3.m3.1.1.2" xref="id3.3.3.m3.1.1.2.cmml">94.45</mn><mo id="id3.3.3.m3.1.1.1" xref="id3.3.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="id3.3.3.m3.1b"><apply id="id3.3.3.m3.1.1.cmml" xref="id3.3.3.m3.1.1"><csymbol cd="latexml" id="id3.3.3.m3.1.1.1.cmml" xref="id3.3.3.m3.1.1.1">percent</csymbol><cn type="float" id="id3.3.3.m3.1.1.2.cmml" xref="id3.3.3.m3.1.1.2">94.45</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id3.3.3.m3.1c">94.45\%</annotation></semantics></math> reduction in FLOPS, and for the disc storage, we reduced the requirement in <math id="id4.4.4.m4.1" class="ltx_Math" alttext="70\%" display="inline"><semantics id="id4.4.4.m4.1a"><mrow id="id4.4.4.m4.1.1" xref="id4.4.4.m4.1.1.cmml"><mn id="id4.4.4.m4.1.1.2" xref="id4.4.4.m4.1.1.2.cmml">70</mn><mo id="id4.4.4.m4.1.1.1" xref="id4.4.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="id4.4.4.m4.1b"><apply id="id4.4.4.m4.1.1.cmml" xref="id4.4.4.m4.1.1"><csymbol cd="latexml" id="id4.4.4.m4.1.1.1.cmml" xref="id4.4.4.m4.1.1.1">percent</csymbol><cn type="integer" id="id4.4.4.m4.1.1.2.cmml" xref="id4.4.4.m4.1.1.2">70</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id4.4.4.m4.1c">70\%</annotation></semantics></math> while increasing error by a mere <math id="id5.5.5.m5.1" class="ltx_Math" alttext="1\%" display="inline"><semantics id="id5.5.5.m5.1a"><mrow id="id5.5.5.m5.1.1" xref="id5.5.5.m5.1.1.cmml"><mn id="id5.5.5.m5.1.1.2" xref="id5.5.5.m5.1.1.2.cmml">1</mn><mo id="id5.5.5.m5.1.1.1" xref="id5.5.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="id5.5.5.m5.1b"><apply id="id5.5.5.m5.1.1.cmml" xref="id5.5.5.m5.1.1"><csymbol cd="latexml" id="id5.5.5.m5.1.1.1.cmml" xref="id5.5.5.m5.1.1.1">percent</csymbol><cn type="integer" id="id5.5.5.m5.1.1.2.cmml" xref="id5.5.5.m5.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id5.5.5.m5.1c">1\%</annotation></semantics></math>. With regard input image processing, this metric increases from <math id="id6.6.6.m6.1" class="ltx_Math" alttext="11.71" display="inline"><semantics id="id6.6.6.m6.1a"><mn id="id6.6.6.m6.1.1" xref="id6.6.6.m6.1.1.cmml">11.71</mn><annotation-xml encoding="MathML-Content" id="id6.6.6.m6.1b"><cn type="float" id="id6.6.6.m6.1.1.cmml" xref="id6.6.6.m6.1.1">11.71</cn></annotation-xml><annotation encoding="application/x-tex" id="id6.6.6.m6.1c">11.71</annotation></semantics></math> FPS to <math id="id7.7.7.m7.1" class="ltx_Math" alttext="41.9" display="inline"><semantics id="id7.7.7.m7.1a"><mn id="id7.7.7.m7.1.1" xref="id7.7.7.m7.1.1.cmml">41.9</mn><annotation-xml encoding="MathML-Content" id="id7.7.7.m7.1b"><cn type="float" id="id7.7.7.m7.1.1.cmml" xref="id7.7.7.m7.1.1">41.9</cn></annotation-xml><annotation encoding="application/x-tex" id="id7.7.7.m7.1c">41.9</annotation></semantics></math> FPS for the Desktop case. When using the constrained device, image processing augmented from <math id="id8.8.8.m8.1" class="ltx_Math" alttext="2.86" display="inline"><semantics id="id8.8.8.m8.1a"><mn id="id8.8.8.m8.1.1" xref="id8.8.8.m8.1.1.cmml">2.86</mn><annotation-xml encoding="MathML-Content" id="id8.8.8.m8.1b"><cn type="float" id="id8.8.8.m8.1.1.cmml" xref="id8.8.8.m8.1.1">2.86</cn></annotation-xml><annotation encoding="application/x-tex" id="id8.8.8.m8.1c">2.86</annotation></semantics></math> FPS to <math id="id9.9.9.m9.1" class="ltx_Math" alttext="10.04" display="inline"><semantics id="id9.9.9.m9.1a"><mn id="id9.9.9.m9.1.1" xref="id9.9.9.m9.1.1.cmml">10.04</mn><annotation-xml encoding="MathML-Content" id="id9.9.9.m9.1b"><cn type="float" id="id9.9.9.m9.1.1.cmml" xref="id9.9.9.m9.1.1">10.04</cn></annotation-xml><annotation encoding="application/x-tex" id="id9.9.9.m9.1c">10.04</annotation></semantics></math> FPS. The higher processing rate of image frames achieved by the proposed approach allows a much shorter response time.</span></p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
<span id="id10.id1" class="ltx_text">
COMPRESSION, PRUNING, HRI, SAFETY, POSE ESTIMATION
</span>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Robots had become a common choice when we tried to improve the throughput of many tasks while keeping the reliably <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. The robot’s evolution can lead us to the scenario where we have a complex or dangerous task on the human hand and insert a robot factor into the pipeline, or vice-versa. The systems where a robot and human interact have a set of requirements, and those requirements are the so-called Human-Robot Interaction (HRI) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">One of the main HRI refers to safety, where we need to ensure that the person interacting with the robot is risk-free, such as collision, burn, and other possible threats <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Many approaches were developed to ensure safety, notable many of them relaying in some level of intelligence. Those methods, with time, get more and more complex, sometimes requiring an extensive, robust system and response time.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The HRI places can be significant with many types of equipment or even remote locations where single robots perform risky tasks. Implementing those intelligent solutions can be challenging; first, due to the dimension of the space, we need to ensure a reliable connection between the devices and the processing unity with minimal delay. With the addition of devices, if the processing occurs on a server, the server needs to be robust, increasing the cost of the solutions considerably. One alternative to performing the HRI for safety is deploying it on constrained devices, reducing the network load, and completing the process on the device. Constrained devices are cheap and reliable components, a RaspberryPi, for instance. While they contain limited configurations, such as memory and storage, they can work as edge devices on the network, developing a self-contained solution.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">This work will focus on one of those safety methods, the pose estimation. The pose estimation is a method where we can detect the position of an object inserted in a scenario. Usually, this position is determined by detecting the keypoints that generate the skeleton of the desired object. Those points can return us feedback on the object’s position concerning an image o even about the world. For instance, the keypoints can be the joints in a human. With those keypoints, we can work on solutions for HRI methods, such as distance measure, collision detection, and prediction, and intention prediction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The pose estimation is still a current topic of study nowadays; most of the solutions proposed for pose-estimation rely on deep learning, with convolutions neural networks (CNN), for its capabilities and robustness, and is the current state of the art <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Despite the overwhelming contributions, a downside of CNN is the usual requirement of millions or even billions of parameters that need careful tuning, and this process is highly CPU/GPU demand. Another possible downside in some cases is the disk storage requirements. When CNN’s parameters are stored on the disk, they can occupy just a few kilobytes or even gigabytes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">For instance, one of its most straightforward architectures, namely LeNet-5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> with only five convolutional layers and two dense ones, contains around 60 thousand parameters, with 4.81 MB, and the VGG-16, with 138.4 million parameters, on the default setting, can occupy about 528 MB. Furthermore, we observe that even slight modifications to these networks culminate in an exponential increase in the number of parameters. This is the case of AlexNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, where the use of eight convolutional layers and three dense layers has this number of parameters reach as many as 60 million parameters, in other words exhibiting a thousandfold increase.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">To execute these robust and resource-demanding solutions in limited environments such as an edge device located or an embedded system is challenging to say the least. To deploy a pose estimation on a constrained, we need to generate an efficient CNN that can output the required keypoints with considerable response time and precision. Reducing the requirements of a CNN is an engaging topic and a highly discussed research area. One of the alternatives is compressing the CNN, where we try to cut the heavy requirements steps of the CNN, being those steps the weights itself, the precision, learning steps, or even another light version of the same CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. This paper focuses on two main processes, pruning and quantization. In contrast, the pruning process reduces the total of parameters on the CNN. The quantization reduces the disk storage requirement by lowering the precision of the parameters; for instance, a value that requires 16 bits can be represented as 8 bits. The compression process can increase the response time while keeping a minimal loss o precision.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">In this paper, we propose a solution for pose estimation, where we present a new CNN that can detect the keypoints of a robot, called FCN-Pose. We also tackle the constrained device requirements, applying our proposed model to a pruning technique and quantization.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">Our main contributions are:</p>
</div>
<div id="S1.p10" class="ltx_para">
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Generate a new convolutional neural network for robot pose-estimation (FCN-Pose) a light compressed version;</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Join CNN compression methods of pruning and quantization;</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Evaluate our proposed solution on an actual constrained device (Raspberry Pi3)</p>
</div>
</li>
</ol>
</div>
<div id="S1.p11" class="ltx_para">
<p id="S1.p11.1" class="ltx_p">The rest of this paper is organized as follows. Section <a href="#S2" title="II Related Works ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> shows an overview and literature scan on convolutional neural networks, how the interaction robot-human can occur, and how state of the art handles such subject. It also leverages some methods of reducing the computational demands of CNNs and their affinities with constrained devices. Section <a href="#S3" title="III Methodology ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> shows our image acquisition process and the creation of our scenario for the experiments. At the same time, it provides the details of the generation of the datasets used in our proposed method. Section <a href="#S4" title="IV Proposed Method ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> summarizes our proposed solution, first demonstrating our new model, FCN-Pose, the process of pruning and quantization selected, and final, the post-process applied. Section <a href="#S6" title="VI Experiments and Results ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a> shows our results and validation of the proposed method for our selected scenarios. In this scenario, we also include some discussions related to the results. Finally, Section <a href="#S7" title="VII Conclusion ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VII</span></a> lists our contributions and future works.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Works</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Even with the Convolutional Neural Networks (CNNs) being nowadays state of the art in many fields, such as object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, semantic segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, or image classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, they usually are highly resourcing demand (consumption of RAM, CPU, or GPU). Some works are emerging proposing a method for the demand reduction and thus get a fast response time, the so-called CNN compression methods.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Compressing a CNN can lead the scope of applications to be more extensive, thus embracing real-time applications or augmented reality applications running over embedded systems with limited resources. Two well-known techniques used for compression are pruning and weight quantization.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">The pruning process handles the smart removal of a subset of parameters evaluated as less critical for the task. About neural networks, such parameters are filters, layers, or weights. Pruning techniques have been proposed for a long time, with studies dating back to the late 80’s <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. But most recently, due to recent advances in CNN techniques, new pruning strategies may be contemplated. In order to define the parameters that must be pruned, a set of rules were suggested. Lee <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> proposes a global measure for scoring, where sparse layers were pruned after retraining on a spatial domain and a Winograd domain. The work in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> considers the filter or layer with the most sparse information as the pruning candidates. Usually, pruning is performed as an offline process, where we already have the trained model, and then we follow the punning operation. But there are works that accomplished the pruning process in parallel with the training. Xiaohan et al. propose an on-the-fly pruning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> using a Stochastic Gradient Descent (SGD) based momentum, zeroing the redundant parameters gradually.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Besides the punning approaches, other techniques rely on quantization, reducing the required number of bits to represent each weight. Wu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> used the k-means clustering technique to reduce the representation based on the codebook generated. Another work can be seen as being more straightforward, with the direct precision conversion from the weight to a different bit scale, such as 16 bits or 8bits <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. Choukroun et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> claim an aggressive precision change, targeting the linear quantization as a mean squared error problem, reaching a 4-bit precision with minimal loss of accuracy.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Pruning and quantization, in general, are orthogonal research areas, but combining them seems a natural path to follow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. Paupamah et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> compare both techniques and show that pruning can help with the overfitting problem despite observing a slight decrease in accuracy on the quantization. On the other hand, the research constructs a smaller and faster neural network. Thung <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> proposes an online process of quantization for pruned networks. Here the full-precise network maintains a form of signal for a possible quantization. Ding et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> propose an online process; after each epoch, the framework chooses the best technique to proceed with the training and decides whether the next step involves quantization or pruning. Our work aims at one of those weak spots, the shortage of actual experiments on combining pruning and weight quantization, also showing the improvement in the combination.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">Most of the works suggest the need for network compression in order to achieve real-time applications over resource-limited devices. Such devices can be part of the Internet of Things (IoT), sharing information, and processing images and an extensive range of signals <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. Despite the numerous efforts targeting compression, few of these effectively consider running their solution on devices with limited resources typically used in an IoT or Industry context. For instance, Yao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> proposes a framework for sensing applications from finding the minimal number of non-redundant hidden elements, and a designed model for mobile devices <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> while evaluating both on an Intel Edison computing platform. Bhattacharya <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> proposes an approach to increase the sparsity on CNNs, and evaluates it on four types of limited devices.</p>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p">Most of the works suggest that we can achieve desirable response time applications over resource-limited devices with CNN’s compression. Such devices can be part of the Internet of Things (IoT), sharing information and processing images and an extensive range of signals <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. For instance, Yao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> proposes a framework for sensing applications by finding the minimal number of non-redundant hidden elements and a designed model for mobile devices <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> while evaluating both on an Intel Edison computing platform. Bhattacharya <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> proposes an approach to increase the sparsity on CNNs, and evaluates it on four types of limited devices. Despite the numerous efforts targeting compression, few of these effectively consider running their solution on devices with limited resources typically used in an IoT or Industry context. We improve this field by adding our experiments and evaluating our proposed combination of pruning and vector quantization on an actual constrained device, a RaspBerryPi 3.</p>
</div>
<div id="S2.p8" class="ltx_para">
<p id="S2.p8.1" class="ltx_p">Our experiments are related to pose-estimation for a robot, in our case, a robot arm, and his relations in a human-robot collaboration (HRC) environment. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> proposes a random decision forest applied on depth images to offer the segments and further the pose estimation. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> proposes a framework called Dream (Deep Robot-to-camera Extrinsics for Articulated Manipulators) for pose estimation on robots. This work uses a convolutional neural network, joined with the kinematics of the robotic, to predict the pose estimation based on synthetic data. While having outstanding performance, it requires an expensive setup for real-time performance and does not add human interaction to the process. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> proposes a system for 3D pose estimation for a quadruped robot, mainly based on an IMU sensor, a set of gyro, and accelerometer sensors. In many of those works, we face some limitations. On one side, we can get a good performance but requires additional resources for the decision pipeline.</p>
</div>
<div id="S2.p9" class="ltx_para">
<p id="S2.p9.1" class="ltx_p">Unfourtanaly, just a few works tackle the HRC related to posing detection. For instance, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> proposes a safety collision detection but relies on robot and human segmentation for the task. While presenting good results, we can improve the precision of those tasks with pose estimation. Some of those are more just direct guidelines <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. Improving that area with some intelligence level can further reduce the automatization process’s risks and manual load.</p>
</div>
<div id="S2.p10" class="ltx_para">
<p id="S2.p10.1" class="ltx_p">In this study, we consider using a constrained device to perform pose-estimation with a pruned and quantized CNN to handle further problems related to HRI. To the best of our knowledge, no other work jointly combines those requirements, being this one of our main contributions.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Methodology</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we will explore the process of dataset generation and data augmentation. How the proposed CNN was proposed, he was trained, and his post-processing.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Datasets</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">This work tries to solve the problem of robot pose detection with segmentation data. Unfortunately, as far as our research goes, there is no public dataset available for robot pose or robot segmentation. We generate two new datasets to overcome this limitation: keypoints for pose detection and segmentation for data support. Our data is the same as used on <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, but since the data only has the full segmentation of the robotic arm, we need to make some additions.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS1.5.1.1" class="ltx_text">III-A</span>1 </span>Data Description</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">The scenario comprises a well-controlled environment simulation. It represents human-robot interaction. A UR5 robotic arm performs maintenance from Universal Robotics with five degrees of freedom. Regarding image capture, a web camera, Logitech C270, recording at 20 frames per second in 1080p. A volunteer assists the robotic arm in-loco. Figure <a href="#S3.F1" title="Figure 1 ‣ III-A1 Data Description ‣ III-A Datasets ‣ III Methodology ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the scenario´s overview.</p>
</div>
<figure id="S3.F1" class="ltx_figure">
<div id="S3.F1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:346.9pt;height:323.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-43.4pt,40.5pt) scale(0.800003026280785,0.800003026280785) ;"><img src="/html/2205.13272/assets/images/cenario.png" id="S3.F1.1.g1" class="ltx_graphics ltx_img_square" width="600" height="560" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Scenario overview (same as Silva et. al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>.</figcaption>
</figure>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.1" class="ltx_p">The dataset contains approximately 20,000 frames. For the data annotation, we chose to follow a filtering strategy. We select 1000 frames from all the video files, around one frame at each second. We consider that reducing the data also decreases the likelihood of overfitting. The frames of a video generate a strong interdependency; each frame has a slight change, being closely similar to its neighbors, so removing those samples can lead to a diverse dataset and a better generalization. Figure <a href="#S3.F3" title="Figure 3 ‣ III-A2 Keypoints Annotation ‣ III-A Datasets ‣ III Methodology ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, depicts a sample frame.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2205.13272/assets/images/HUMAN_ROBOT.png" id="S3.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="410" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Sample of the image acquired from the webcam. On the left, we have the robotic arm; on the right, the human.</figcaption>
</figure>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS2.5.1.1" class="ltx_text">III-A</span>2 </span>Keypoints Annotation</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">As shown in Figure <a href="#S3.F3" title="Figure 3 ‣ III-A2 Keypoints Annotation ‣ III-A Datasets ‣ III Methodology ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, one can see the robotic arm and our human volunteer, but this image is raw. We add a plain overlay for better visualization in the sample image, but we first need to generate some manually annotated data for the pose detection estimation training. This data comprises the set of keypoints that compose the robotic arm. We consider each elbow as a keypoint, obtaining a total of 8 keypoints. It is valid to notice that the total of keypoint is bonded to the model of a robotic arm, may in another type of robot, the total of keypoint changes. The VGG Image Annotator (VIA) performed the annotation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>. Fig. <a href="#S3.F3" title="Figure 3 ‣ III-A2 Keypoints Annotation ‣ III-A Datasets ‣ III Methodology ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows an example of how the data was annotated.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2205.13272/assets/images/example_gt.png" id="S3.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="538" height="536" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Example of robot keypoints annotation with the VIA software.</figcaption>
</figure>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.1" class="ltx_p">In cases where the robotic arm presented is most occluded, the positions concerning the camera or the volunteer body were removed. We consider that those cases could increase the noise scope on the dataset.</p>
</div>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS3.5.1.1" class="ltx_text">III-A</span>3 </span>Segmentation Annotation</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.1" class="ltx_p">Concerning our approach, our main target was the keypoints detection but based on the segmentation task. After keypoints annotation, we generated our segmentation dataset. We generated a segmentation mask for each point for each sample image with the eight keypoints annotated. We also generated a segmentation mask referent to the line segmentation that connects the robot skeleton’s keypoints.</p>
</div>
<div id="S3.SS1.SSS3.p2" class="ltx_para">
<p id="S3.SS1.SSS3.p2.1" class="ltx_p">Finally, we ended up with nine masks for each sample and a set of annotated keypoints. Eight are related to keypoints, and the last one is to the full robot skeleton. Figure <a href="#S3.F4" title="Figure 4 ‣ III-A3 Segmentation Annotation ‣ III-A Datasets ‣ III Methodology ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows a sample of the segmentation process.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2205.13272/assets/images/segmetation_process.png" id="S3.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="592" height="624" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Sample of segmentation masks generated. First we have a input image, and as output we generate the set of masks for the keypoints and skeleton.</figcaption>
</figure>
<div id="S3.SS1.SSS3.p3" class="ltx_para">
<p id="S3.SS1.SSS3.p3.1" class="ltx_p">As seen in Figure <a href="#S3.F4" title="Figure 4 ‣ III-A3 Segmentation Annotation ‣ III-A Datasets ‣ III Methodology ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we have an input image with the annotated keypoints, and we generated all the masks for each point. The points are the region of interest. We chose to select a circular region around the keypoint, expanding it by a radius of 6 pixels. For the robot skeleton, we follow a similar approach. We generate the connection lines between the keypoints and expand next to the thickness by 30. This thickness was enough to cover the whole robotic arm in our case.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Data Augmentation</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.2" class="ltx_p">As mentioned earlier, we removed some frames due to the correlation factor of the video stream. Still, after this filtering process, we ended up with few images for the training process. To avoid the problem of over-fitting, we chose to perform some data augmentation. The data augmentation process relies on the generation of new images based on the previous samples of the dataset. For instance, changes in rotation, brightness, perspective, and others, can be considered data augmentation techniques.
After the filtering process, removing the images with few keypoints available due to extreme occlusion, we obtained 508 images. We split the dataset into two sets to perform the data augmentation, a training and validation set. The training set contains <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="80\%" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mn id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">80</mn><mo id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="latexml" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">80\%</annotation></semantics></math> of the data or 406 images, and the validation set includes <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="20\%" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mn id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">20</mn><mo id="S3.SS2.p1.2.m2.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="latexml" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">20\%</annotation></semantics></math> of the data or, more specifically, 102 images. We generated two random data augmentations for each image on the training dataset, enlarging our dataset into 812 samples. After the union of our original dataset with the data augmented data, we got a total of 1218 samples for training.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">For the data augmentation, we chose two techniques: padding and rotation. Padding dislocates the center of the image for each direction, whereas rotation occurs randomly considering the center of the picture. Both methods are considered due to the simulation of the possible camera displacements in a deployment scenario, with some level of collusion for better generalization. During the process of generation, all the data augmentation was random so that we can have the following augmentations and their combinations:</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">only rotation;</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">only padding;</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">rotation and padding;</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">The images used to generate the data augmentation are also chosen randomly. Figure <a href="#S3.F5" title="Figure 5 ‣ III-B Data Augmentation ‣ III Methodology ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows a sample of the possible transformations carried out over an image.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2205.13272/assets/images/data_augmentation.png" id="S3.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="538" height="699" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Sample of data augmentation. Above we have the original image, and bellow we have the three generated images.</figcaption>
</figure>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">As illustrated in Figure<a href="#S3.F5" title="Figure 5 ‣ III-B Data Augmentation ‣ III Methodology ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we generate the images and have some spatial transformation. This leads us to the need to take care of some possible issues. One may lose the robotic arm on the scene with the spatial transformation due to some extreme padding or rotation. We only consider the augmented images with the robotic arm contained in the image. It is valid to notice that the data augmentation was performed with keypoints. Hence, after the augmentation, there is a new set of points. With the new set of points at hand, we also generate the new set of segmentation masks. Figure <a href="#S3.F6" title="Figure 6 ‣ III-B Data Augmentation ‣ III Methodology ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows a sample of the augmented artifacts.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2205.13272/assets/images/data_augmentation_sample_fix.png" id="S3.F6.1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="299" height="946" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Sample of keypoints and masks augmented. In (a) we have the keypoints augmented, (b) the segmented keypoints, here we unified the masks as a single image for better visualization, and finally (c), we have the robotic arm skeleton.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Proposed Method</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">This section describes how the Fully Connected Network Pose (FCN-Pose) architecture is built, how the training pipeline worked, and finally presents the additionally performed post-processing refinement.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">FCN-Pose</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Several well-known architectures for segmentation rely on complex underlying networks, such as U-Net<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>. Even when considering a lighter version with a smaller backbone, such as MobileNet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, one still needs to handle millions of parameters. The U-Net, and other proposed networks, are indeed powerful but may be considered excessively resource-demanding in the IoT context. Recall that our primary goal is present in two folds: it must first operate close to real-time and secondly execute over devices with limited processing and memory resources. The new model, named FCN-Pose, has been designed with these two requirements in mind.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">The FCN-Pose has its architecture heavily based on the work of Jonathan et. al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. It nonetheless introduces some major changes. Jonathan proposes an end-to-end semantic segmentation based on Fully Connected Networks; such architecture is an evolution of primitive convents. Previous works on FCN require the definition of a set of parameters. They include size region-based approaches that require the region parameters. They consequently were heavy to process since they needed to classify each region <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>. FCN tries to handle these issues, proposing a network that handles arbitrary input size images and producing a direct output segmentation map. Figure <a href="#S4.F8" title="Figure 8 ‣ IV-A FCN-Pose ‣ IV Proposed Method ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> illustrates the FCN model.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2205.13272/assets/images/fcnet.png" id="S4.F7.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="283" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Fully Convolutional Networks for Semantic Segmentation. Adapted from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>.</figcaption>
</figure>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">As depicted in Figure <a href="#S4.F8" title="Figure 8 ‣ IV-A FCN-Pose ‣ IV Proposed Method ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, an FCN consists of a stack of convolutional and pooling layers. The stack of layers of the FCN can lead to one major problem: due to the heavy decrease of the feature map, the resolution of the segmentation map is downsampled and can generate some artifacts, such as fuzzy boundaries. To avoid this problem, we extended the FCN with an auto-encoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, adding a decoder step before the stage of the segmentation map proposal. Figure <a href="#S4.F8" title="Figure 8 ‣ IV-A FCN-Pose ‣ IV Proposed Method ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows the new proposed overall architecture.</p>
</div>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2205.13272/assets/images/proposed_model.png" id="S4.F8.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="383" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>FCN-Pose. Structural architecture, with the input on the left, and the proposal with segmentation maps on the right.</figcaption>
</figure>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">An examination of Figure <a href="#S4.F8" title="Figure 8 ‣ IV-A FCN-Pose ‣ IV Proposed Method ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows that our model follows the same proposal for FCN. On the left side in blue, we find an encoding process based on a stack of convolutions and max-pooling layers. There is a decoding process based on a stack of upsampling and convolution layers in green on the right side. The encoding part tries to extract the features from the image and compress them in the latent space (purple part of the image). At the same time, the decoding process aims to improve the map feature reduction and the reconstruction of the segmentation map.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p">No additional types of layers were added, such as batch normalization. Besides improving model performance, we sought to keep the model as simple as possible. We believe for our application context and goals that the FCN-Pose model works adequately, despite undergoing a sizeable reduction in the number of its parameters, based on the following observations:</p>
</div>
<div id="S4.SS1.p6" class="ltx_para">
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">We generate a well-known scenario, with a single target being the robotic arm;</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">The possible classes to predict are limited;</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">All our possible predictions correlate. All the keypoints correlate with each other and with the skeleton.</p>
</div>
</li>
</ul>
</div>
<div id="S4.SS1.p7" class="ltx_para">
<p id="S4.SS1.p7.1" class="ltx_p">An additional segmentation map that was cited before is the skeleton one. We believe that the addition of the skeleton segmentation map, besides the segmentation itself, provides a better spacial generalization for our model. We are adding the skeleton to push the keypoint to follow a structured flow, providing better results, and causing fewer outliers.</p>
</div>
<div id="S4.SS1.p8" class="ltx_para">
<p id="S4.SS1.p8.1" class="ltx_p">The FCN-Pose model consists of 10 convolutional layers, five max-pooling layers, and four up-sampling layers. In all intermediary layers, we use the ReLu activation, whereas we apply the Sigmoid activation for the output layers. In our training, we used input images with 224 pixels of height and 224 pixels of length. Table <a href="#S4.T1" title="Table I ‣ IV-A FCN-Pose ‣ IV Proposed Method ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> details the proposed model.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table I: </span>FCN-Pose Structure</figcaption>
<table id="S4.T1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1" class="ltx_tr">
<td id="S4.T1.1.1.1" class="ltx_td ltx_align_left">Model: FCN-Pose</td>
<td id="S4.T1.1.1.2" class="ltx_td"></td>
<td id="S4.T1.1.1.3" class="ltx_td"></td>
</tr>
<tr id="S4.T1.1.2" class="ltx_tr">
<td id="S4.T1.1.2.1" class="ltx_td ltx_align_left ltx_border_t">Layer (type)</td>
<td id="S4.T1.1.2.2" class="ltx_td ltx_align_left ltx_border_t">Output Shape</td>
<td id="S4.T1.1.2.3" class="ltx_td ltx_align_left ltx_border_t">Parameters</td>
</tr>
<tr id="S4.T1.1.3" class="ltx_tr">
<td id="S4.T1.1.3.1" class="ltx_td ltx_align_left ltx_border_tt">input_5 (InputLayer)</td>
<td id="S4.T1.1.3.2" class="ltx_td ltx_align_left ltx_border_tt">(224, 224, 3)</td>
<td id="S4.T1.1.3.3" class="ltx_td ltx_align_left ltx_border_tt">0</td>
</tr>
<tr id="S4.T1.1.4" class="ltx_tr">
<td id="S4.T1.1.4.1" class="ltx_td ltx_align_left ltx_border_t">Convolutional Layer1</td>
<td id="S4.T1.1.4.2" class="ltx_td ltx_align_left ltx_border_t">(224, 224, 128)</td>
<td id="S4.T1.1.4.3" class="ltx_td ltx_align_left ltx_border_t">3584</td>
</tr>
<tr id="S4.T1.1.5" class="ltx_tr">
<td id="S4.T1.1.5.1" class="ltx_td ltx_align_left ltx_border_t">MaxPooling Layer 1</td>
<td id="S4.T1.1.5.2" class="ltx_td ltx_align_left ltx_border_t">(112, 112, 128)</td>
<td id="S4.T1.1.5.3" class="ltx_td ltx_align_left ltx_border_t">0</td>
</tr>
<tr id="S4.T1.1.6" class="ltx_tr">
<td id="S4.T1.1.6.1" class="ltx_td ltx_align_left ltx_border_t">Convolutional Layer 2</td>
<td id="S4.T1.1.6.2" class="ltx_td ltx_align_left ltx_border_t">(112, 112, 64)</td>
<td id="S4.T1.1.6.3" class="ltx_td ltx_align_left ltx_border_t">73792</td>
</tr>
<tr id="S4.T1.1.7" class="ltx_tr">
<td id="S4.T1.1.7.1" class="ltx_td ltx_align_left ltx_border_t">MaxPooling Layer 2</td>
<td id="S4.T1.1.7.2" class="ltx_td ltx_align_left ltx_border_t">(56, 56, 64)</td>
<td id="S4.T1.1.7.3" class="ltx_td ltx_align_left ltx_border_t">0</td>
</tr>
<tr id="S4.T1.1.8" class="ltx_tr">
<td id="S4.T1.1.8.1" class="ltx_td ltx_align_left ltx_border_t">Convolutional Layer 3</td>
<td id="S4.T1.1.8.2" class="ltx_td ltx_align_left ltx_border_t">(56, 56, 32)</td>
<td id="S4.T1.1.8.3" class="ltx_td ltx_align_left ltx_border_t">18464</td>
</tr>
<tr id="S4.T1.1.9" class="ltx_tr">
<td id="S4.T1.1.9.1" class="ltx_td ltx_align_left ltx_border_t">MaxPooling Layer 3</td>
<td id="S4.T1.1.9.2" class="ltx_td ltx_align_left ltx_border_t">(28, 28, 32)</td>
<td id="S4.T1.1.9.3" class="ltx_td ltx_align_left ltx_border_t">0</td>
</tr>
<tr id="S4.T1.1.10" class="ltx_tr">
<td id="S4.T1.1.10.1" class="ltx_td ltx_align_left ltx_border_t">Convolutional Layer 4</td>
<td id="S4.T1.1.10.2" class="ltx_td ltx_align_left ltx_border_t">(28, 28, 16)</td>
<td id="S4.T1.1.10.3" class="ltx_td ltx_align_left ltx_border_t">4624</td>
</tr>
<tr id="S4.T1.1.11" class="ltx_tr">
<td id="S4.T1.1.11.1" class="ltx_td ltx_align_left ltx_border_t">MaxPooling Layer 4</td>
<td id="S4.T1.1.11.2" class="ltx_td ltx_align_left ltx_border_t">(14, 14, 16)</td>
<td id="S4.T1.1.11.3" class="ltx_td ltx_align_left ltx_border_t">0</td>
</tr>
<tr id="S4.T1.1.12" class="ltx_tr">
<td id="S4.T1.1.12.1" class="ltx_td ltx_align_left ltx_border_t">Convolutional Layer 5</td>
<td id="S4.T1.1.12.2" class="ltx_td ltx_align_left ltx_border_t">(14, 14, 8)</td>
<td id="S4.T1.1.12.3" class="ltx_td ltx_align_left ltx_border_t">1160</td>
</tr>
<tr id="S4.T1.1.13" class="ltx_tr">
<td id="S4.T1.1.13.1" class="ltx_td ltx_align_left ltx_border_t">MaxPooling Layer 5</td>
<td id="S4.T1.1.13.2" class="ltx_td ltx_align_left ltx_border_t">(7, 7, 8)</td>
<td id="S4.T1.1.13.3" class="ltx_td ltx_align_left ltx_border_t">0</td>
</tr>
<tr id="S4.T1.1.14" class="ltx_tr">
<td id="S4.T1.1.14.1" class="ltx_td ltx_align_left ltx_border_t">Convolutional Layer 6</td>
<td id="S4.T1.1.14.2" class="ltx_td ltx_align_left ltx_border_t">(7, 7, 8)</td>
<td id="S4.T1.1.14.3" class="ltx_td ltx_align_left ltx_border_t">584</td>
</tr>
<tr id="S4.T1.1.15" class="ltx_tr">
<td id="S4.T1.1.15.1" class="ltx_td ltx_align_left ltx_border_t">UpSampling Layer 1</td>
<td id="S4.T1.1.15.2" class="ltx_td ltx_align_left ltx_border_t">(14, 14, 8)</td>
<td id="S4.T1.1.15.3" class="ltx_td ltx_align_left ltx_border_t">0</td>
</tr>
<tr id="S4.T1.1.16" class="ltx_tr">
<td id="S4.T1.1.16.1" class="ltx_td ltx_align_left ltx_border_t">Convolutional Layer 7</td>
<td id="S4.T1.1.16.2" class="ltx_td ltx_align_left ltx_border_t">(14, 14, 16)</td>
<td id="S4.T1.1.16.3" class="ltx_td ltx_align_left ltx_border_t">1168</td>
</tr>
<tr id="S4.T1.1.17" class="ltx_tr">
<td id="S4.T1.1.17.1" class="ltx_td ltx_align_left ltx_border_t">UpSampling Layer 2</td>
<td id="S4.T1.1.17.2" class="ltx_td ltx_align_left ltx_border_t">(28, 28, 16)</td>
<td id="S4.T1.1.17.3" class="ltx_td ltx_align_left ltx_border_t">0</td>
</tr>
<tr id="S4.T1.1.18" class="ltx_tr">
<td id="S4.T1.1.18.1" class="ltx_td ltx_align_left ltx_border_t">Convolutional Layer 8</td>
<td id="S4.T1.1.18.2" class="ltx_td ltx_align_left ltx_border_t">(28, 28, 32)</td>
<td id="S4.T1.1.18.3" class="ltx_td ltx_align_left ltx_border_t">4640</td>
</tr>
<tr id="S4.T1.1.19" class="ltx_tr">
<td id="S4.T1.1.19.1" class="ltx_td ltx_align_left ltx_border_t">UpSampling Layer 3</td>
<td id="S4.T1.1.19.2" class="ltx_td ltx_align_left ltx_border_t">(56, 56, 32)</td>
<td id="S4.T1.1.19.3" class="ltx_td ltx_align_left ltx_border_t">0</td>
</tr>
<tr id="S4.T1.1.20" class="ltx_tr">
<td id="S4.T1.1.20.1" class="ltx_td ltx_align_left ltx_border_t">Convolutional Layer 9</td>
<td id="S4.T1.1.20.2" class="ltx_td ltx_align_left ltx_border_t">(56, 56, 64)</td>
<td id="S4.T1.1.20.3" class="ltx_td ltx_align_left ltx_border_t">18496</td>
</tr>
<tr id="S4.T1.1.21" class="ltx_tr">
<td id="S4.T1.1.21.1" class="ltx_td ltx_align_left ltx_border_t">UpSampling Layer 4</td>
<td id="S4.T1.1.21.2" class="ltx_td ltx_align_left ltx_border_t">(224, 224, 64)</td>
<td id="S4.T1.1.21.3" class="ltx_td ltx_align_left ltx_border_t">0</td>
</tr>
<tr id="S4.T1.1.22" class="ltx_tr">
<td id="S4.T1.1.22.1" class="ltx_td ltx_align_left ltx_border_t">Convolutional Layer 10</td>
<td id="S4.T1.1.22.2" class="ltx_td ltx_align_left ltx_border_t">(224, 224, 9)</td>
<td id="S4.T1.1.22.3" class="ltx_td ltx_align_left ltx_border_t">5193</td>
</tr>
<tr id="S4.T1.1.23" class="ltx_tr">
<td id="S4.T1.1.23.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_tt">Total parameters: 131,705</td>
<td id="S4.T1.1.23.2" class="ltx_td ltx_border_b ltx_border_tt"></td>
<td id="S4.T1.1.23.3" class="ltx_td ltx_border_b ltx_border_tt"></td>
</tr>
</table>
</figure>
<div id="S4.SS1.p9" class="ltx_para">
<p id="S4.SS1.p9.1" class="ltx_p">As observed in Table <a href="#S4.T1" title="Table I ‣ IV-A FCN-Pose ‣ IV Proposed Method ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, we generate as small model with only 131,705 parameters. Note that, for instance, the FCN model, with the VGG16<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, with the same input dimensions, requires over 130 million parameters. Additionally, our model requires minimal disk storage space. One needs only 1.7 Megabytes to save the full model.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Post-Processing</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Once we obtain the activation map, we need to maintain only the relevant region from the keypoints. The generated segmentation can contain some noise levels, with active regions not required. To filter those regions out, we performed a clustering method heavily based on the K-Means clustering method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>. We first extract each relevant region from the image. Each region will be a cluster. Select the most relevant cluster, and use its centroid as the predicted keypoint.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.3" class="ltx_p">While the original algorithm requires setting the clusters (K) total, we do not hold this in information before execution. To overcome this limitation, we added a connectivity restriction. All the points inside a cluster need to be at least a distance <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mi id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><ci id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">M</annotation></semantics></math>, from any other point into the same cluster. Using this restriction, we forced that a region is fully connected within a distance <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mi id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><ci id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">M</annotation></semantics></math>. In our case, we force the <math id="S4.SS2.p2.3.m3.1" class="ltx_Math" alttext="M=1" display="inline"><semantics id="S4.SS2.p2.3.m3.1a"><mrow id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml"><mi id="S4.SS2.p2.3.m3.1.1.2" xref="S4.SS2.p2.3.m3.1.1.2.cmml">M</mi><mo id="S4.SS2.p2.3.m3.1.1.1" xref="S4.SS2.p2.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS2.p2.3.m3.1.1.3" xref="S4.SS2.p2.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><apply id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1"><eq id="S4.SS2.p2.3.m3.1.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1.1"></eq><ci id="S4.SS2.p2.3.m3.1.1.2.cmml" xref="S4.SS2.p2.3.m3.1.1.2">𝑀</ci><cn type="integer" id="S4.SS2.p2.3.m3.1.1.3.cmml" xref="S4.SS2.p2.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">M=1</annotation></semantics></math>; hence the region becomes as close as possible.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">We also added self-cluster generation. We generate the initial cluster from an initial point, then start searching for points at a distance <math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><mi id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><ci id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">M</annotation></semantics></math>, adding them to the cluster. We then repeat the process for each new point. Those points are removed from the initial set of points, then we proceed again to the generation of another cluster, continuing the process until the set is empty. It is valid to notice that, unlike the K-Means, our centroid is not used as an update rule but as a result. At the end of clustering, we generate the centroid, the average of all the points on the cluster. Cluster generation solves our limitation of the K parameters. The Algorithm <a href="#alg1" title="Algorithm 1 ‣ IV-B Post-Processing ‣ IV Proposed Method ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> lists the actions of the adopted clustering method.</p>
</div>
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg1.2.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span> Expansion Clustering</figcaption>
<div id="alg1.3" class="ltx_listing ltx_listing">
<div id="alg1.l1" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l1.1.1.1" class="ltx_text" style="font-size:80%;">1:</span></span><math id="alg1.l1.m1.1" class="ltx_Math" alttext="S" display="inline"><semantics id="alg1.l1.m1.1a"><mi id="alg1.l1.m1.1.1" xref="alg1.l1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m1.1b"><ci id="alg1.l1.m1.1.1.cmml" xref="alg1.l1.m1.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m1.1c">S</annotation></semantics></math> = set of points <math id="alg1.l1.m2.2" class="ltx_Math" alttext="(x,y)" display="inline"><semantics id="alg1.l1.m2.2a"><mrow id="alg1.l1.m2.2.3.2" xref="alg1.l1.m2.2.3.1.cmml"><mo stretchy="false" id="alg1.l1.m2.2.3.2.1" xref="alg1.l1.m2.2.3.1.cmml">(</mo><mi id="alg1.l1.m2.1.1" xref="alg1.l1.m2.1.1.cmml">x</mi><mo id="alg1.l1.m2.2.3.2.2" xref="alg1.l1.m2.2.3.1.cmml">,</mo><mi id="alg1.l1.m2.2.2" xref="alg1.l1.m2.2.2.cmml">y</mi><mo stretchy="false" id="alg1.l1.m2.2.3.2.3" xref="alg1.l1.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="alg1.l1.m2.2b"><interval closure="open" id="alg1.l1.m2.2.3.1.cmml" xref="alg1.l1.m2.2.3.2"><ci id="alg1.l1.m2.1.1.cmml" xref="alg1.l1.m2.1.1">𝑥</ci><ci id="alg1.l1.m2.2.2.cmml" xref="alg1.l1.m2.2.2">𝑦</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m2.2c">(x,y)</annotation></semantics></math>

</div>
<div id="alg1.l2" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l2.1.1.1" class="ltx_text" style="font-size:80%;">2:</span></span><math id="alg1.l2.m1.1" class="ltx_Math" alttext="Clusters" display="inline"><semantics id="alg1.l2.m1.1a"><mrow id="alg1.l2.m1.1.1" xref="alg1.l2.m1.1.1.cmml"><mi id="alg1.l2.m1.1.1.2" xref="alg1.l2.m1.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="alg1.l2.m1.1.1.1" xref="alg1.l2.m1.1.1.1.cmml">​</mo><mi id="alg1.l2.m1.1.1.3" xref="alg1.l2.m1.1.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="alg1.l2.m1.1.1.1a" xref="alg1.l2.m1.1.1.1.cmml">​</mo><mi id="alg1.l2.m1.1.1.4" xref="alg1.l2.m1.1.1.4.cmml">u</mi><mo lspace="0em" rspace="0em" id="alg1.l2.m1.1.1.1b" xref="alg1.l2.m1.1.1.1.cmml">​</mo><mi id="alg1.l2.m1.1.1.5" xref="alg1.l2.m1.1.1.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="alg1.l2.m1.1.1.1c" xref="alg1.l2.m1.1.1.1.cmml">​</mo><mi id="alg1.l2.m1.1.1.6" xref="alg1.l2.m1.1.1.6.cmml">t</mi><mo lspace="0em" rspace="0em" id="alg1.l2.m1.1.1.1d" xref="alg1.l2.m1.1.1.1.cmml">​</mo><mi id="alg1.l2.m1.1.1.7" xref="alg1.l2.m1.1.1.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="alg1.l2.m1.1.1.1e" xref="alg1.l2.m1.1.1.1.cmml">​</mo><mi id="alg1.l2.m1.1.1.8" xref="alg1.l2.m1.1.1.8.cmml">r</mi><mo lspace="0em" rspace="0em" id="alg1.l2.m1.1.1.1f" xref="alg1.l2.m1.1.1.1.cmml">​</mo><mi id="alg1.l2.m1.1.1.9" xref="alg1.l2.m1.1.1.9.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l2.m1.1b"><apply id="alg1.l2.m1.1.1.cmml" xref="alg1.l2.m1.1.1"><times id="alg1.l2.m1.1.1.1.cmml" xref="alg1.l2.m1.1.1.1"></times><ci id="alg1.l2.m1.1.1.2.cmml" xref="alg1.l2.m1.1.1.2">𝐶</ci><ci id="alg1.l2.m1.1.1.3.cmml" xref="alg1.l2.m1.1.1.3">𝑙</ci><ci id="alg1.l2.m1.1.1.4.cmml" xref="alg1.l2.m1.1.1.4">𝑢</ci><ci id="alg1.l2.m1.1.1.5.cmml" xref="alg1.l2.m1.1.1.5">𝑠</ci><ci id="alg1.l2.m1.1.1.6.cmml" xref="alg1.l2.m1.1.1.6">𝑡</ci><ci id="alg1.l2.m1.1.1.7.cmml" xref="alg1.l2.m1.1.1.7">𝑒</ci><ci id="alg1.l2.m1.1.1.8.cmml" xref="alg1.l2.m1.1.1.8">𝑟</ci><ci id="alg1.l2.m1.1.1.9.cmml" xref="alg1.l2.m1.1.1.9">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m1.1c">Clusters</annotation></semantics></math> = set of clusters

</div>
<div id="alg1.l3" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l3.1.1.1" class="ltx_text" style="font-size:80%;">3:</span></span><math id="alg1.l3.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="alg1.l3.m1.1a"><mi id="alg1.l3.m1.1.1" xref="alg1.l3.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="alg1.l3.m1.1b"><ci id="alg1.l3.m1.1.1.cmml" xref="alg1.l3.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m1.1c">M</annotation></semantics></math> = Minimum distance

</div>
<div id="alg1.l4" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l4.1.1.1" class="ltx_text" style="font-size:80%;">4:</span></span><span id="alg1.l4.2" class="ltx_text ltx_font_bold">while</span> <math id="alg1.l4.m1.1" class="ltx_Math" alttext="S" display="inline"><semantics id="alg1.l4.m1.1a"><mi id="alg1.l4.m1.1.1" xref="alg1.l4.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="alg1.l4.m1.1b"><ci id="alg1.l4.m1.1.1.cmml" xref="alg1.l4.m1.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m1.1c">S</annotation></semantics></math> is not empty <span id="alg1.l4.3" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l5" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l5.1.1.1" class="ltx_text" style="font-size:80%;">5:</span></span>     <span id="alg1.l5.2" class="ltx_text ltx_font_bold">for</span> Each point <math id="alg1.l5.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="alg1.l5.m1.1a"><mi id="alg1.l5.m1.1.1" xref="alg1.l5.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="alg1.l5.m1.1b"><ci id="alg1.l5.m1.1.1.cmml" xref="alg1.l5.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m1.1c">p</annotation></semantics></math> in <math id="alg1.l5.m2.1" class="ltx_Math" alttext="S" display="inline"><semantics id="alg1.l5.m2.1a"><mi id="alg1.l5.m2.1.1" xref="alg1.l5.m2.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="alg1.l5.m2.1b"><ci id="alg1.l5.m2.1.1.cmml" xref="alg1.l5.m2.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m2.1c">S</annotation></semantics></math> <span id="alg1.l5.3" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l6" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l6.1.1.1" class="ltx_text" style="font-size:80%;">6:</span></span>         Create a new cluster <math id="alg1.l6.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="alg1.l6.m1.1a"><mi id="alg1.l6.m1.1.1" xref="alg1.l6.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="alg1.l6.m1.1b"><ci id="alg1.l6.m1.1.1.cmml" xref="alg1.l6.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m1.1c">C</annotation></semantics></math>

</div>
<div id="alg1.l7" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l7.1.1.1" class="ltx_text" style="font-size:80%;">7:</span></span>         Add the point <math id="alg1.l7.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="alg1.l7.m1.1a"><mi id="alg1.l7.m1.1.1" xref="alg1.l7.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="alg1.l7.m1.1b"><ci id="alg1.l7.m1.1.1.cmml" xref="alg1.l7.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m1.1c">p</annotation></semantics></math> in the cluster <math id="alg1.l7.m2.1" class="ltx_Math" alttext="C" display="inline"><semantics id="alg1.l7.m2.1a"><mi id="alg1.l7.m2.1.1" xref="alg1.l7.m2.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="alg1.l7.m2.1b"><ci id="alg1.l7.m2.1.1.cmml" xref="alg1.l7.m2.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m2.1c">C</annotation></semantics></math>

</div>
<div id="alg1.l8" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l8.1.1.1" class="ltx_text" style="font-size:80%;">8:</span></span>         <span id="alg1.l8.2" class="ltx_text ltx_font_bold">while</span> New point is added on <math id="alg1.l8.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="alg1.l8.m1.1a"><mi id="alg1.l8.m1.1.1" xref="alg1.l8.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="alg1.l8.m1.1b"><ci id="alg1.l8.m1.1.1.cmml" xref="alg1.l8.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l8.m1.1c">C</annotation></semantics></math> <span id="alg1.l8.3" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l9" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l9.1.1.1" class="ltx_text" style="font-size:80%;">9:</span></span>              <span id="alg1.l9.2" class="ltx_text ltx_font_bold">for</span> Each point <math id="alg1.l9.m1.1" class="ltx_Math" alttext="p_{i}" display="inline"><semantics id="alg1.l9.m1.1a"><msub id="alg1.l9.m1.1.1" xref="alg1.l9.m1.1.1.cmml"><mi id="alg1.l9.m1.1.1.2" xref="alg1.l9.m1.1.1.2.cmml">p</mi><mi id="alg1.l9.m1.1.1.3" xref="alg1.l9.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l9.m1.1b"><apply id="alg1.l9.m1.1.1.cmml" xref="alg1.l9.m1.1.1"><csymbol cd="ambiguous" id="alg1.l9.m1.1.1.1.cmml" xref="alg1.l9.m1.1.1">subscript</csymbol><ci id="alg1.l9.m1.1.1.2.cmml" xref="alg1.l9.m1.1.1.2">𝑝</ci><ci id="alg1.l9.m1.1.1.3.cmml" xref="alg1.l9.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l9.m1.1c">p_{i}</annotation></semantics></math> in <math id="alg1.l9.m2.1" class="ltx_Math" alttext="C" display="inline"><semantics id="alg1.l9.m2.1a"><mi id="alg1.l9.m2.1.1" xref="alg1.l9.m2.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="alg1.l9.m2.1b"><ci id="alg1.l9.m2.1.1.cmml" xref="alg1.l9.m2.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l9.m2.1c">C</annotation></semantics></math> <span id="alg1.l9.3" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l10" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l10.1.1.1" class="ltx_text" style="font-size:80%;">10:</span></span>                  <span id="alg1.l10.2" class="ltx_text ltx_font_bold">for</span> Each point <math id="alg1.l10.m1.1" class="ltx_Math" alttext="p_{j}" display="inline"><semantics id="alg1.l10.m1.1a"><msub id="alg1.l10.m1.1.1" xref="alg1.l10.m1.1.1.cmml"><mi id="alg1.l10.m1.1.1.2" xref="alg1.l10.m1.1.1.2.cmml">p</mi><mi id="alg1.l10.m1.1.1.3" xref="alg1.l10.m1.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l10.m1.1b"><apply id="alg1.l10.m1.1.1.cmml" xref="alg1.l10.m1.1.1"><csymbol cd="ambiguous" id="alg1.l10.m1.1.1.1.cmml" xref="alg1.l10.m1.1.1">subscript</csymbol><ci id="alg1.l10.m1.1.1.2.cmml" xref="alg1.l10.m1.1.1.2">𝑝</ci><ci id="alg1.l10.m1.1.1.3.cmml" xref="alg1.l10.m1.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l10.m1.1c">p_{j}</annotation></semantics></math> in <math id="alg1.l10.m2.1" class="ltx_Math" alttext="S" display="inline"><semantics id="alg1.l10.m2.1a"><mi id="alg1.l10.m2.1.1" xref="alg1.l10.m2.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="alg1.l10.m2.1b"><ci id="alg1.l10.m2.1.1.cmml" xref="alg1.l10.m2.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l10.m2.1c">S</annotation></semantics></math> <span id="alg1.l10.3" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l11" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l11.1.1.1" class="ltx_text" style="font-size:80%;">11:</span></span>                       <span id="alg1.l11.2" class="ltx_text ltx_font_bold">if</span> <math id="alg1.l11.m1.1" class="ltx_Math" alttext="p_{i}" display="inline"><semantics id="alg1.l11.m1.1a"><msub id="alg1.l11.m1.1.1" xref="alg1.l11.m1.1.1.cmml"><mi id="alg1.l11.m1.1.1.2" xref="alg1.l11.m1.1.1.2.cmml">p</mi><mi id="alg1.l11.m1.1.1.3" xref="alg1.l11.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l11.m1.1b"><apply id="alg1.l11.m1.1.1.cmml" xref="alg1.l11.m1.1.1"><csymbol cd="ambiguous" id="alg1.l11.m1.1.1.1.cmml" xref="alg1.l11.m1.1.1">subscript</csymbol><ci id="alg1.l11.m1.1.1.2.cmml" xref="alg1.l11.m1.1.1.2">𝑝</ci><ci id="alg1.l11.m1.1.1.3.cmml" xref="alg1.l11.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l11.m1.1c">p_{i}</annotation></semantics></math> != <math id="alg1.l11.m2.1" class="ltx_Math" alttext="p_{j}" display="inline"><semantics id="alg1.l11.m2.1a"><msub id="alg1.l11.m2.1.1" xref="alg1.l11.m2.1.1.cmml"><mi id="alg1.l11.m2.1.1.2" xref="alg1.l11.m2.1.1.2.cmml">p</mi><mi id="alg1.l11.m2.1.1.3" xref="alg1.l11.m2.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l11.m2.1b"><apply id="alg1.l11.m2.1.1.cmml" xref="alg1.l11.m2.1.1"><csymbol cd="ambiguous" id="alg1.l11.m2.1.1.1.cmml" xref="alg1.l11.m2.1.1">subscript</csymbol><ci id="alg1.l11.m2.1.1.2.cmml" xref="alg1.l11.m2.1.1.2">𝑝</ci><ci id="alg1.l11.m2.1.1.3.cmml" xref="alg1.l11.m2.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l11.m2.1c">p_{j}</annotation></semantics></math> and EuclidianDistance(<math id="alg1.l11.m3.1" class="ltx_Math" alttext="p_{i}" display="inline"><semantics id="alg1.l11.m3.1a"><msub id="alg1.l11.m3.1.1" xref="alg1.l11.m3.1.1.cmml"><mi id="alg1.l11.m3.1.1.2" xref="alg1.l11.m3.1.1.2.cmml">p</mi><mi id="alg1.l11.m3.1.1.3" xref="alg1.l11.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l11.m3.1b"><apply id="alg1.l11.m3.1.1.cmml" xref="alg1.l11.m3.1.1"><csymbol cd="ambiguous" id="alg1.l11.m3.1.1.1.cmml" xref="alg1.l11.m3.1.1">subscript</csymbol><ci id="alg1.l11.m3.1.1.2.cmml" xref="alg1.l11.m3.1.1.2">𝑝</ci><ci id="alg1.l11.m3.1.1.3.cmml" xref="alg1.l11.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l11.m3.1c">p_{i}</annotation></semantics></math>,<math id="alg1.l11.m4.1" class="ltx_Math" alttext="p_{j}" display="inline"><semantics id="alg1.l11.m4.1a"><msub id="alg1.l11.m4.1.1" xref="alg1.l11.m4.1.1.cmml"><mi id="alg1.l11.m4.1.1.2" xref="alg1.l11.m4.1.1.2.cmml">p</mi><mi id="alg1.l11.m4.1.1.3" xref="alg1.l11.m4.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l11.m4.1b"><apply id="alg1.l11.m4.1.1.cmml" xref="alg1.l11.m4.1.1"><csymbol cd="ambiguous" id="alg1.l11.m4.1.1.1.cmml" xref="alg1.l11.m4.1.1">subscript</csymbol><ci id="alg1.l11.m4.1.1.2.cmml" xref="alg1.l11.m4.1.1.2">𝑝</ci><ci id="alg1.l11.m4.1.1.3.cmml" xref="alg1.l11.m4.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l11.m4.1c">p_{j}</annotation></semantics></math>)&lt;=M <span id="alg1.l11.3" class="ltx_text ltx_font_bold">then</span>

</div>
<div id="alg1.l12" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l12.1.1.1" class="ltx_text" style="font-size:80%;">12:</span></span>                           Add the point <math id="alg1.l12.m1.1" class="ltx_Math" alttext="p_{j}" display="inline"><semantics id="alg1.l12.m1.1a"><msub id="alg1.l12.m1.1.1" xref="alg1.l12.m1.1.1.cmml"><mi id="alg1.l12.m1.1.1.2" xref="alg1.l12.m1.1.1.2.cmml">p</mi><mi id="alg1.l12.m1.1.1.3" xref="alg1.l12.m1.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l12.m1.1b"><apply id="alg1.l12.m1.1.1.cmml" xref="alg1.l12.m1.1.1"><csymbol cd="ambiguous" id="alg1.l12.m1.1.1.1.cmml" xref="alg1.l12.m1.1.1">subscript</csymbol><ci id="alg1.l12.m1.1.1.2.cmml" xref="alg1.l12.m1.1.1.2">𝑝</ci><ci id="alg1.l12.m1.1.1.3.cmml" xref="alg1.l12.m1.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l12.m1.1c">p_{j}</annotation></semantics></math> to the cluster <math id="alg1.l12.m2.1" class="ltx_Math" alttext="C" display="inline"><semantics id="alg1.l12.m2.1a"><mi id="alg1.l12.m2.1.1" xref="alg1.l12.m2.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="alg1.l12.m2.1b"><ci id="alg1.l12.m2.1.1.cmml" xref="alg1.l12.m2.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l12.m2.1c">C</annotation></semantics></math>

</div>
<div id="alg1.l13" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l13.1.1.1" class="ltx_text" style="font-size:80%;">13:</span></span>                           Remove <math id="alg1.l13.m1.1" class="ltx_Math" alttext="p_{j}" display="inline"><semantics id="alg1.l13.m1.1a"><msub id="alg1.l13.m1.1.1" xref="alg1.l13.m1.1.1.cmml"><mi id="alg1.l13.m1.1.1.2" xref="alg1.l13.m1.1.1.2.cmml">p</mi><mi id="alg1.l13.m1.1.1.3" xref="alg1.l13.m1.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l13.m1.1b"><apply id="alg1.l13.m1.1.1.cmml" xref="alg1.l13.m1.1.1"><csymbol cd="ambiguous" id="alg1.l13.m1.1.1.1.cmml" xref="alg1.l13.m1.1.1">subscript</csymbol><ci id="alg1.l13.m1.1.1.2.cmml" xref="alg1.l13.m1.1.1.2">𝑝</ci><ci id="alg1.l13.m1.1.1.3.cmml" xref="alg1.l13.m1.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l13.m1.1c">p_{j}</annotation></semantics></math> from <math id="alg1.l13.m2.1" class="ltx_Math" alttext="S" display="inline"><semantics id="alg1.l13.m2.1a"><mi id="alg1.l13.m2.1.1" xref="alg1.l13.m2.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="alg1.l13.m2.1b"><ci id="alg1.l13.m2.1.1.cmml" xref="alg1.l13.m2.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l13.m2.1c">S</annotation></semantics></math>

</div>
<div id="alg1.l14" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l14.1.1.1" class="ltx_text" style="font-size:80%;">14:</span></span>                       <span id="alg1.l14.2" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l14.3" class="ltx_text ltx_font_bold">if</span>
</div>
<div id="alg1.l15" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l15.1.1.1" class="ltx_text" style="font-size:80%;">15:</span></span>                  <span id="alg1.l15.2" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l15.3" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg1.l16" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l16.1.1.1" class="ltx_text" style="font-size:80%;">16:</span></span>              <span id="alg1.l16.2" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l16.3" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg1.l17" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l17.1.1.1" class="ltx_text" style="font-size:80%;">17:</span></span>         <span id="alg1.l17.2" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l17.3" class="ltx_text ltx_font_bold">while</span>
</div>
<div id="alg1.l18" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l18.1.1.1" class="ltx_text" style="font-size:80%;">18:</span></span>         Add cluster <math id="alg1.l18.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="alg1.l18.m1.1a"><mi id="alg1.l18.m1.1.1" xref="alg1.l18.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="alg1.l18.m1.1b"><ci id="alg1.l18.m1.1.1.cmml" xref="alg1.l18.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l18.m1.1c">C</annotation></semantics></math> into <math id="alg1.l18.m2.1" class="ltx_Math" alttext="Clusters" display="inline"><semantics id="alg1.l18.m2.1a"><mrow id="alg1.l18.m2.1.1" xref="alg1.l18.m2.1.1.cmml"><mi id="alg1.l18.m2.1.1.2" xref="alg1.l18.m2.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="alg1.l18.m2.1.1.1" xref="alg1.l18.m2.1.1.1.cmml">​</mo><mi id="alg1.l18.m2.1.1.3" xref="alg1.l18.m2.1.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="alg1.l18.m2.1.1.1a" xref="alg1.l18.m2.1.1.1.cmml">​</mo><mi id="alg1.l18.m2.1.1.4" xref="alg1.l18.m2.1.1.4.cmml">u</mi><mo lspace="0em" rspace="0em" id="alg1.l18.m2.1.1.1b" xref="alg1.l18.m2.1.1.1.cmml">​</mo><mi id="alg1.l18.m2.1.1.5" xref="alg1.l18.m2.1.1.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="alg1.l18.m2.1.1.1c" xref="alg1.l18.m2.1.1.1.cmml">​</mo><mi id="alg1.l18.m2.1.1.6" xref="alg1.l18.m2.1.1.6.cmml">t</mi><mo lspace="0em" rspace="0em" id="alg1.l18.m2.1.1.1d" xref="alg1.l18.m2.1.1.1.cmml">​</mo><mi id="alg1.l18.m2.1.1.7" xref="alg1.l18.m2.1.1.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="alg1.l18.m2.1.1.1e" xref="alg1.l18.m2.1.1.1.cmml">​</mo><mi id="alg1.l18.m2.1.1.8" xref="alg1.l18.m2.1.1.8.cmml">r</mi><mo lspace="0em" rspace="0em" id="alg1.l18.m2.1.1.1f" xref="alg1.l18.m2.1.1.1.cmml">​</mo><mi id="alg1.l18.m2.1.1.9" xref="alg1.l18.m2.1.1.9.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l18.m2.1b"><apply id="alg1.l18.m2.1.1.cmml" xref="alg1.l18.m2.1.1"><times id="alg1.l18.m2.1.1.1.cmml" xref="alg1.l18.m2.1.1.1"></times><ci id="alg1.l18.m2.1.1.2.cmml" xref="alg1.l18.m2.1.1.2">𝐶</ci><ci id="alg1.l18.m2.1.1.3.cmml" xref="alg1.l18.m2.1.1.3">𝑙</ci><ci id="alg1.l18.m2.1.1.4.cmml" xref="alg1.l18.m2.1.1.4">𝑢</ci><ci id="alg1.l18.m2.1.1.5.cmml" xref="alg1.l18.m2.1.1.5">𝑠</ci><ci id="alg1.l18.m2.1.1.6.cmml" xref="alg1.l18.m2.1.1.6">𝑡</ci><ci id="alg1.l18.m2.1.1.7.cmml" xref="alg1.l18.m2.1.1.7">𝑒</ci><ci id="alg1.l18.m2.1.1.8.cmml" xref="alg1.l18.m2.1.1.8">𝑟</ci><ci id="alg1.l18.m2.1.1.9.cmml" xref="alg1.l18.m2.1.1.9">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l18.m2.1c">Clusters</annotation></semantics></math>

</div>
<div id="alg1.l19" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l19.1.1.1" class="ltx_text" style="font-size:80%;">19:</span></span>     <span id="alg1.l19.2" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l19.3" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg1.l20" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l20.1.1.1" class="ltx_text" style="font-size:80%;">20:</span></span><span id="alg1.l20.2" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l20.3" class="ltx_text ltx_font_bold">while</span>
</div>
<div id="alg1.l21" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l21.1.1.1" class="ltx_text" style="font-size:80%;">21:</span></span>Return the centroids of each cluster <math id="alg1.l21.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="alg1.l21.m1.1a"><mi id="alg1.l21.m1.1.1" xref="alg1.l21.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="alg1.l21.m1.1b"><ci id="alg1.l21.m1.1.1.cmml" xref="alg1.l21.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l21.m1.1c">C</annotation></semantics></math> in <math id="alg1.l21.m2.1" class="ltx_Math" alttext="Clusters" display="inline"><semantics id="alg1.l21.m2.1a"><mrow id="alg1.l21.m2.1.1" xref="alg1.l21.m2.1.1.cmml"><mi id="alg1.l21.m2.1.1.2" xref="alg1.l21.m2.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="alg1.l21.m2.1.1.1" xref="alg1.l21.m2.1.1.1.cmml">​</mo><mi id="alg1.l21.m2.1.1.3" xref="alg1.l21.m2.1.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="alg1.l21.m2.1.1.1a" xref="alg1.l21.m2.1.1.1.cmml">​</mo><mi id="alg1.l21.m2.1.1.4" xref="alg1.l21.m2.1.1.4.cmml">u</mi><mo lspace="0em" rspace="0em" id="alg1.l21.m2.1.1.1b" xref="alg1.l21.m2.1.1.1.cmml">​</mo><mi id="alg1.l21.m2.1.1.5" xref="alg1.l21.m2.1.1.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="alg1.l21.m2.1.1.1c" xref="alg1.l21.m2.1.1.1.cmml">​</mo><mi id="alg1.l21.m2.1.1.6" xref="alg1.l21.m2.1.1.6.cmml">t</mi><mo lspace="0em" rspace="0em" id="alg1.l21.m2.1.1.1d" xref="alg1.l21.m2.1.1.1.cmml">​</mo><mi id="alg1.l21.m2.1.1.7" xref="alg1.l21.m2.1.1.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="alg1.l21.m2.1.1.1e" xref="alg1.l21.m2.1.1.1.cmml">​</mo><mi id="alg1.l21.m2.1.1.8" xref="alg1.l21.m2.1.1.8.cmml">r</mi><mo lspace="0em" rspace="0em" id="alg1.l21.m2.1.1.1f" xref="alg1.l21.m2.1.1.1.cmml">​</mo><mi id="alg1.l21.m2.1.1.9" xref="alg1.l21.m2.1.1.9.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l21.m2.1b"><apply id="alg1.l21.m2.1.1.cmml" xref="alg1.l21.m2.1.1"><times id="alg1.l21.m2.1.1.1.cmml" xref="alg1.l21.m2.1.1.1"></times><ci id="alg1.l21.m2.1.1.2.cmml" xref="alg1.l21.m2.1.1.2">𝐶</ci><ci id="alg1.l21.m2.1.1.3.cmml" xref="alg1.l21.m2.1.1.3">𝑙</ci><ci id="alg1.l21.m2.1.1.4.cmml" xref="alg1.l21.m2.1.1.4">𝑢</ci><ci id="alg1.l21.m2.1.1.5.cmml" xref="alg1.l21.m2.1.1.5">𝑠</ci><ci id="alg1.l21.m2.1.1.6.cmml" xref="alg1.l21.m2.1.1.6">𝑡</ci><ci id="alg1.l21.m2.1.1.7.cmml" xref="alg1.l21.m2.1.1.7">𝑒</ci><ci id="alg1.l21.m2.1.1.8.cmml" xref="alg1.l21.m2.1.1.8">𝑟</ci><ci id="alg1.l21.m2.1.1.9.cmml" xref="alg1.l21.m2.1.1.9">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l21.m2.1c">Clusters</annotation></semantics></math>

</div>
</div>
</figure>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">The optimal result would be a single cluster with the interest region, but the CNN can produce some noise or artifacts. To solve this problem, we only consider the cluster with the highest number of components in our work and its centroid as our predicted keypoint. Figure <a href="#S4.F9" title="Figure 9 ‣ IV-B Post-Processing ‣ IV Proposed Method ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> describes a sample of the workflow for keypoint prediction.</p>
</div>
<figure id="S4.F9" class="ltx_figure"><img src="/html/2205.13272/assets/images/clustering.png" id="S4.F9.1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="240" height="675" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Clustering Process. </figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">CNN Compressing</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This section will present how the selected methods for pruning and quantization were applied on the FCN-Pose, and finally, how we proceed with the combination of pruning and quantization.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.5.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.6.2" class="ltx_text ltx_font_italic">Pruning</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.5" class="ltx_p">Our selected pruning approach was the filter ranking pruning technique proposed by Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>. The pruning filter for each layer is based on the sum of the absolute weights on the filters. Let <math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="n_{i}" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><msub id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml"><mi id="S5.SS1.p1.1.m1.1.1.2" xref="S5.SS1.p1.1.m1.1.1.2.cmml">n</mi><mi id="S5.SS1.p1.1.m1.1.1.3" xref="S5.SS1.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><apply id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.p1.1.m1.1.1.2.cmml" xref="S5.SS1.p1.1.m1.1.1.2">𝑛</ci><ci id="S5.SS1.p1.1.m1.1.1.3.cmml" xref="S5.SS1.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">n_{i}</annotation></semantics></math> denote the number of input channels for the <math id="S5.SS1.p1.2.m2.1" class="ltx_Math" alttext="i_{th}" display="inline"><semantics id="S5.SS1.p1.2.m2.1a"><msub id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml"><mi id="S5.SS1.p1.2.m2.1.1.2" xref="S5.SS1.p1.2.m2.1.1.2.cmml">i</mi><mrow id="S5.SS1.p1.2.m2.1.1.3" xref="S5.SS1.p1.2.m2.1.1.3.cmml"><mi id="S5.SS1.p1.2.m2.1.1.3.2" xref="S5.SS1.p1.2.m2.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.2.m2.1.1.3.1" xref="S5.SS1.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S5.SS1.p1.2.m2.1.1.3.3" xref="S5.SS1.p1.2.m2.1.1.3.3.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><apply id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.2.m2.1.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S5.SS1.p1.2.m2.1.1.2.cmml" xref="S5.SS1.p1.2.m2.1.1.2">𝑖</ci><apply id="S5.SS1.p1.2.m2.1.1.3.cmml" xref="S5.SS1.p1.2.m2.1.1.3"><times id="S5.SS1.p1.2.m2.1.1.3.1.cmml" xref="S5.SS1.p1.2.m2.1.1.3.1"></times><ci id="S5.SS1.p1.2.m2.1.1.3.2.cmml" xref="S5.SS1.p1.2.m2.1.1.3.2">𝑡</ci><ci id="S5.SS1.p1.2.m2.1.1.3.3.cmml" xref="S5.SS1.p1.2.m2.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">i_{th}</annotation></semantics></math> convolutional layer and <math id="S5.SS1.p1.3.m3.1" class="ltx_Math" alttext="h_{i}/w_{i}" display="inline"><semantics id="S5.SS1.p1.3.m3.1a"><mrow id="S5.SS1.p1.3.m3.1.1" xref="S5.SS1.p1.3.m3.1.1.cmml"><msub id="S5.SS1.p1.3.m3.1.1.2" xref="S5.SS1.p1.3.m3.1.1.2.cmml"><mi id="S5.SS1.p1.3.m3.1.1.2.2" xref="S5.SS1.p1.3.m3.1.1.2.2.cmml">h</mi><mi id="S5.SS1.p1.3.m3.1.1.2.3" xref="S5.SS1.p1.3.m3.1.1.2.3.cmml">i</mi></msub><mo id="S5.SS1.p1.3.m3.1.1.1" xref="S5.SS1.p1.3.m3.1.1.1.cmml">/</mo><msub id="S5.SS1.p1.3.m3.1.1.3" xref="S5.SS1.p1.3.m3.1.1.3.cmml"><mi id="S5.SS1.p1.3.m3.1.1.3.2" xref="S5.SS1.p1.3.m3.1.1.3.2.cmml">w</mi><mi id="S5.SS1.p1.3.m3.1.1.3.3" xref="S5.SS1.p1.3.m3.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.3.m3.1b"><apply id="S5.SS1.p1.3.m3.1.1.cmml" xref="S5.SS1.p1.3.m3.1.1"><divide id="S5.SS1.p1.3.m3.1.1.1.cmml" xref="S5.SS1.p1.3.m3.1.1.1"></divide><apply id="S5.SS1.p1.3.m3.1.1.2.cmml" xref="S5.SS1.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S5.SS1.p1.3.m3.1.1.2.1.cmml" xref="S5.SS1.p1.3.m3.1.1.2">subscript</csymbol><ci id="S5.SS1.p1.3.m3.1.1.2.2.cmml" xref="S5.SS1.p1.3.m3.1.1.2.2">ℎ</ci><ci id="S5.SS1.p1.3.m3.1.1.2.3.cmml" xref="S5.SS1.p1.3.m3.1.1.2.3">𝑖</ci></apply><apply id="S5.SS1.p1.3.m3.1.1.3.cmml" xref="S5.SS1.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p1.3.m3.1.1.3.1.cmml" xref="S5.SS1.p1.3.m3.1.1.3">subscript</csymbol><ci id="S5.SS1.p1.3.m3.1.1.3.2.cmml" xref="S5.SS1.p1.3.m3.1.1.3.2">𝑤</ci><ci id="S5.SS1.p1.3.m3.1.1.3.3.cmml" xref="S5.SS1.p1.3.m3.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.3.m3.1c">h_{i}/w_{i}</annotation></semantics></math> be the height/width of the input feature maps. The convolutional layer transforms the input feature maps <math id="S5.SS1.p1.4.m4.1" class="ltx_Math" alttext="x_{i}\in n_{i}×h_{i}×w_{i}" display="inline"><semantics id="S5.SS1.p1.4.m4.1a"><mrow id="S5.SS1.p1.4.m4.1.1" xref="S5.SS1.p1.4.m4.1.1.cmml"><msub id="S5.SS1.p1.4.m4.1.1.2" xref="S5.SS1.p1.4.m4.1.1.2.cmml"><mi id="S5.SS1.p1.4.m4.1.1.2.2" xref="S5.SS1.p1.4.m4.1.1.2.2.cmml">x</mi><mi id="S5.SS1.p1.4.m4.1.1.2.3" xref="S5.SS1.p1.4.m4.1.1.2.3.cmml">i</mi></msub><mo id="S5.SS1.p1.4.m4.1.1.1" xref="S5.SS1.p1.4.m4.1.1.1.cmml">∈</mo><mrow id="S5.SS1.p1.4.m4.1.1.3" xref="S5.SS1.p1.4.m4.1.1.3.cmml"><msub id="S5.SS1.p1.4.m4.1.1.3.2" xref="S5.SS1.p1.4.m4.1.1.3.2.cmml"><mi id="S5.SS1.p1.4.m4.1.1.3.2.2" xref="S5.SS1.p1.4.m4.1.1.3.2.2.cmml">n</mi><mi id="S5.SS1.p1.4.m4.1.1.3.2.3" xref="S5.SS1.p1.4.m4.1.1.3.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S5.SS1.p1.4.m4.1.1.3.1" xref="S5.SS1.p1.4.m4.1.1.3.1.cmml">​</mo><mi mathvariant="normal" id="S5.SS1.p1.4.m4.1.1.3.3" xref="S5.SS1.p1.4.m4.1.1.3.3.cmml">×</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.4.m4.1.1.3.1a" xref="S5.SS1.p1.4.m4.1.1.3.1.cmml">​</mo><msub id="S5.SS1.p1.4.m4.1.1.3.4" xref="S5.SS1.p1.4.m4.1.1.3.4.cmml"><mi id="S5.SS1.p1.4.m4.1.1.3.4.2" xref="S5.SS1.p1.4.m4.1.1.3.4.2.cmml">h</mi><mi id="S5.SS1.p1.4.m4.1.1.3.4.3" xref="S5.SS1.p1.4.m4.1.1.3.4.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S5.SS1.p1.4.m4.1.1.3.1b" xref="S5.SS1.p1.4.m4.1.1.3.1.cmml">​</mo><mi mathvariant="normal" id="S5.SS1.p1.4.m4.1.1.3.5" xref="S5.SS1.p1.4.m4.1.1.3.5.cmml">×</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.4.m4.1.1.3.1c" xref="S5.SS1.p1.4.m4.1.1.3.1.cmml">​</mo><msub id="S5.SS1.p1.4.m4.1.1.3.6" xref="S5.SS1.p1.4.m4.1.1.3.6.cmml"><mi id="S5.SS1.p1.4.m4.1.1.3.6.2" xref="S5.SS1.p1.4.m4.1.1.3.6.2.cmml">w</mi><mi id="S5.SS1.p1.4.m4.1.1.3.6.3" xref="S5.SS1.p1.4.m4.1.1.3.6.3.cmml">i</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.4.m4.1b"><apply id="S5.SS1.p1.4.m4.1.1.cmml" xref="S5.SS1.p1.4.m4.1.1"><in id="S5.SS1.p1.4.m4.1.1.1.cmml" xref="S5.SS1.p1.4.m4.1.1.1"></in><apply id="S5.SS1.p1.4.m4.1.1.2.cmml" xref="S5.SS1.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S5.SS1.p1.4.m4.1.1.2.1.cmml" xref="S5.SS1.p1.4.m4.1.1.2">subscript</csymbol><ci id="S5.SS1.p1.4.m4.1.1.2.2.cmml" xref="S5.SS1.p1.4.m4.1.1.2.2">𝑥</ci><ci id="S5.SS1.p1.4.m4.1.1.2.3.cmml" xref="S5.SS1.p1.4.m4.1.1.2.3">𝑖</ci></apply><apply id="S5.SS1.p1.4.m4.1.1.3.cmml" xref="S5.SS1.p1.4.m4.1.1.3"><times id="S5.SS1.p1.4.m4.1.1.3.1.cmml" xref="S5.SS1.p1.4.m4.1.1.3.1"></times><apply id="S5.SS1.p1.4.m4.1.1.3.2.cmml" xref="S5.SS1.p1.4.m4.1.1.3.2"><csymbol cd="ambiguous" id="S5.SS1.p1.4.m4.1.1.3.2.1.cmml" xref="S5.SS1.p1.4.m4.1.1.3.2">subscript</csymbol><ci id="S5.SS1.p1.4.m4.1.1.3.2.2.cmml" xref="S5.SS1.p1.4.m4.1.1.3.2.2">𝑛</ci><ci id="S5.SS1.p1.4.m4.1.1.3.2.3.cmml" xref="S5.SS1.p1.4.m4.1.1.3.2.3">𝑖</ci></apply><ci id="S5.SS1.p1.4.m4.1.1.3.3.cmml" xref="S5.SS1.p1.4.m4.1.1.3.3">×</ci><apply id="S5.SS1.p1.4.m4.1.1.3.4.cmml" xref="S5.SS1.p1.4.m4.1.1.3.4"><csymbol cd="ambiguous" id="S5.SS1.p1.4.m4.1.1.3.4.1.cmml" xref="S5.SS1.p1.4.m4.1.1.3.4">subscript</csymbol><ci id="S5.SS1.p1.4.m4.1.1.3.4.2.cmml" xref="S5.SS1.p1.4.m4.1.1.3.4.2">ℎ</ci><ci id="S5.SS1.p1.4.m4.1.1.3.4.3.cmml" xref="S5.SS1.p1.4.m4.1.1.3.4.3">𝑖</ci></apply><ci id="S5.SS1.p1.4.m4.1.1.3.5.cmml" xref="S5.SS1.p1.4.m4.1.1.3.5">×</ci><apply id="S5.SS1.p1.4.m4.1.1.3.6.cmml" xref="S5.SS1.p1.4.m4.1.1.3.6"><csymbol cd="ambiguous" id="S5.SS1.p1.4.m4.1.1.3.6.1.cmml" xref="S5.SS1.p1.4.m4.1.1.3.6">subscript</csymbol><ci id="S5.SS1.p1.4.m4.1.1.3.6.2.cmml" xref="S5.SS1.p1.4.m4.1.1.3.6.2">𝑤</ci><ci id="S5.SS1.p1.4.m4.1.1.3.6.3.cmml" xref="S5.SS1.p1.4.m4.1.1.3.6.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.4.m4.1c">x_{i}\in n_{i}×h_{i}×w_{i}</annotation></semantics></math> into the output feature maps <math id="S5.SS1.p1.5.m5.1" class="ltx_Math" alttext="x_{i}+1\in R^{n_{i}+1×h_{i}+1×w_{i}+1}" display="inline"><semantics id="S5.SS1.p1.5.m5.1a"><mrow id="S5.SS1.p1.5.m5.1.1" xref="S5.SS1.p1.5.m5.1.1.cmml"><mrow id="S5.SS1.p1.5.m5.1.1.2" xref="S5.SS1.p1.5.m5.1.1.2.cmml"><msub id="S5.SS1.p1.5.m5.1.1.2.2" xref="S5.SS1.p1.5.m5.1.1.2.2.cmml"><mi id="S5.SS1.p1.5.m5.1.1.2.2.2" xref="S5.SS1.p1.5.m5.1.1.2.2.2.cmml">x</mi><mi id="S5.SS1.p1.5.m5.1.1.2.2.3" xref="S5.SS1.p1.5.m5.1.1.2.2.3.cmml">i</mi></msub><mo id="S5.SS1.p1.5.m5.1.1.2.1" xref="S5.SS1.p1.5.m5.1.1.2.1.cmml">+</mo><mn id="S5.SS1.p1.5.m5.1.1.2.3" xref="S5.SS1.p1.5.m5.1.1.2.3.cmml">1</mn></mrow><mo id="S5.SS1.p1.5.m5.1.1.1" xref="S5.SS1.p1.5.m5.1.1.1.cmml">∈</mo><msup id="S5.SS1.p1.5.m5.1.1.3" xref="S5.SS1.p1.5.m5.1.1.3.cmml"><mi id="S5.SS1.p1.5.m5.1.1.3.2" xref="S5.SS1.p1.5.m5.1.1.3.2.cmml">R</mi><mrow id="S5.SS1.p1.5.m5.1.1.3.3" xref="S5.SS1.p1.5.m5.1.1.3.3.cmml"><msub id="S5.SS1.p1.5.m5.1.1.3.3.2" xref="S5.SS1.p1.5.m5.1.1.3.3.2.cmml"><mi id="S5.SS1.p1.5.m5.1.1.3.3.2.2" xref="S5.SS1.p1.5.m5.1.1.3.3.2.2.cmml">n</mi><mi id="S5.SS1.p1.5.m5.1.1.3.3.2.3" xref="S5.SS1.p1.5.m5.1.1.3.3.2.3.cmml">i</mi></msub><mo id="S5.SS1.p1.5.m5.1.1.3.3.1" xref="S5.SS1.p1.5.m5.1.1.3.3.1.cmml">+</mo><mrow id="S5.SS1.p1.5.m5.1.1.3.3.3" xref="S5.SS1.p1.5.m5.1.1.3.3.3.cmml"><mn id="S5.SS1.p1.5.m5.1.1.3.3.3.2" xref="S5.SS1.p1.5.m5.1.1.3.3.3.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S5.SS1.p1.5.m5.1.1.3.3.3.1" xref="S5.SS1.p1.5.m5.1.1.3.3.3.1.cmml">​</mo><mi mathvariant="normal" id="S5.SS1.p1.5.m5.1.1.3.3.3.3" xref="S5.SS1.p1.5.m5.1.1.3.3.3.3.cmml">×</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.5.m5.1.1.3.3.3.1a" xref="S5.SS1.p1.5.m5.1.1.3.3.3.1.cmml">​</mo><msub id="S5.SS1.p1.5.m5.1.1.3.3.3.4" xref="S5.SS1.p1.5.m5.1.1.3.3.3.4.cmml"><mi id="S5.SS1.p1.5.m5.1.1.3.3.3.4.2" xref="S5.SS1.p1.5.m5.1.1.3.3.3.4.2.cmml">h</mi><mi id="S5.SS1.p1.5.m5.1.1.3.3.3.4.3" xref="S5.SS1.p1.5.m5.1.1.3.3.3.4.3.cmml">i</mi></msub></mrow><mo id="S5.SS1.p1.5.m5.1.1.3.3.1a" xref="S5.SS1.p1.5.m5.1.1.3.3.1.cmml">+</mo><mrow id="S5.SS1.p1.5.m5.1.1.3.3.4" xref="S5.SS1.p1.5.m5.1.1.3.3.4.cmml"><mn id="S5.SS1.p1.5.m5.1.1.3.3.4.2" xref="S5.SS1.p1.5.m5.1.1.3.3.4.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S5.SS1.p1.5.m5.1.1.3.3.4.1" xref="S5.SS1.p1.5.m5.1.1.3.3.4.1.cmml">​</mo><mi mathvariant="normal" id="S5.SS1.p1.5.m5.1.1.3.3.4.3" xref="S5.SS1.p1.5.m5.1.1.3.3.4.3.cmml">×</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p1.5.m5.1.1.3.3.4.1a" xref="S5.SS1.p1.5.m5.1.1.3.3.4.1.cmml">​</mo><msub id="S5.SS1.p1.5.m5.1.1.3.3.4.4" xref="S5.SS1.p1.5.m5.1.1.3.3.4.4.cmml"><mi id="S5.SS1.p1.5.m5.1.1.3.3.4.4.2" xref="S5.SS1.p1.5.m5.1.1.3.3.4.4.2.cmml">w</mi><mi id="S5.SS1.p1.5.m5.1.1.3.3.4.4.3" xref="S5.SS1.p1.5.m5.1.1.3.3.4.4.3.cmml">i</mi></msub></mrow><mo id="S5.SS1.p1.5.m5.1.1.3.3.1b" xref="S5.SS1.p1.5.m5.1.1.3.3.1.cmml">+</mo><mn id="S5.SS1.p1.5.m5.1.1.3.3.5" xref="S5.SS1.p1.5.m5.1.1.3.3.5.cmml">1</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.5.m5.1b"><apply id="S5.SS1.p1.5.m5.1.1.cmml" xref="S5.SS1.p1.5.m5.1.1"><in id="S5.SS1.p1.5.m5.1.1.1.cmml" xref="S5.SS1.p1.5.m5.1.1.1"></in><apply id="S5.SS1.p1.5.m5.1.1.2.cmml" xref="S5.SS1.p1.5.m5.1.1.2"><plus id="S5.SS1.p1.5.m5.1.1.2.1.cmml" xref="S5.SS1.p1.5.m5.1.1.2.1"></plus><apply id="S5.SS1.p1.5.m5.1.1.2.2.cmml" xref="S5.SS1.p1.5.m5.1.1.2.2"><csymbol cd="ambiguous" id="S5.SS1.p1.5.m5.1.1.2.2.1.cmml" xref="S5.SS1.p1.5.m5.1.1.2.2">subscript</csymbol><ci id="S5.SS1.p1.5.m5.1.1.2.2.2.cmml" xref="S5.SS1.p1.5.m5.1.1.2.2.2">𝑥</ci><ci id="S5.SS1.p1.5.m5.1.1.2.2.3.cmml" xref="S5.SS1.p1.5.m5.1.1.2.2.3">𝑖</ci></apply><cn type="integer" id="S5.SS1.p1.5.m5.1.1.2.3.cmml" xref="S5.SS1.p1.5.m5.1.1.2.3">1</cn></apply><apply id="S5.SS1.p1.5.m5.1.1.3.cmml" xref="S5.SS1.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p1.5.m5.1.1.3.1.cmml" xref="S5.SS1.p1.5.m5.1.1.3">superscript</csymbol><ci id="S5.SS1.p1.5.m5.1.1.3.2.cmml" xref="S5.SS1.p1.5.m5.1.1.3.2">𝑅</ci><apply id="S5.SS1.p1.5.m5.1.1.3.3.cmml" xref="S5.SS1.p1.5.m5.1.1.3.3"><plus id="S5.SS1.p1.5.m5.1.1.3.3.1.cmml" xref="S5.SS1.p1.5.m5.1.1.3.3.1"></plus><apply id="S5.SS1.p1.5.m5.1.1.3.3.2.cmml" xref="S5.SS1.p1.5.m5.1.1.3.3.2"><csymbol cd="ambiguous" id="S5.SS1.p1.5.m5.1.1.3.3.2.1.cmml" xref="S5.SS1.p1.5.m5.1.1.3.3.2">subscript</csymbol><ci id="S5.SS1.p1.5.m5.1.1.3.3.2.2.cmml" xref="S5.SS1.p1.5.m5.1.1.3.3.2.2">𝑛</ci><ci id="S5.SS1.p1.5.m5.1.1.3.3.2.3.cmml" xref="S5.SS1.p1.5.m5.1.1.3.3.2.3">𝑖</ci></apply><apply id="S5.SS1.p1.5.m5.1.1.3.3.3.cmml" xref="S5.SS1.p1.5.m5.1.1.3.3.3"><times id="S5.SS1.p1.5.m5.1.1.3.3.3.1.cmml" xref="S5.SS1.p1.5.m5.1.1.3.3.3.1"></times><cn type="integer" id="S5.SS1.p1.5.m5.1.1.3.3.3.2.cmml" xref="S5.SS1.p1.5.m5.1.1.3.3.3.2">1</cn><ci id="S5.SS1.p1.5.m5.1.1.3.3.3.3.cmml" xref="S5.SS1.p1.5.m5.1.1.3.3.3.3">×</ci><apply id="S5.SS1.p1.5.m5.1.1.3.3.3.4.cmml" xref="S5.SS1.p1.5.m5.1.1.3.3.3.4"><csymbol cd="ambiguous" id="S5.SS1.p1.5.m5.1.1.3.3.3.4.1.cmml" xref="S5.SS1.p1.5.m5.1.1.3.3.3.4">subscript</csymbol><ci id="S5.SS1.p1.5.m5.1.1.3.3.3.4.2.cmml" xref="S5.SS1.p1.5.m5.1.1.3.3.3.4.2">ℎ</ci><ci id="S5.SS1.p1.5.m5.1.1.3.3.3.4.3.cmml" xref="S5.SS1.p1.5.m5.1.1.3.3.3.4.3">𝑖</ci></apply></apply><apply id="S5.SS1.p1.5.m5.1.1.3.3.4.cmml" xref="S5.SS1.p1.5.m5.1.1.3.3.4"><times id="S5.SS1.p1.5.m5.1.1.3.3.4.1.cmml" xref="S5.SS1.p1.5.m5.1.1.3.3.4.1"></times><cn type="integer" id="S5.SS1.p1.5.m5.1.1.3.3.4.2.cmml" xref="S5.SS1.p1.5.m5.1.1.3.3.4.2">1</cn><ci id="S5.SS1.p1.5.m5.1.1.3.3.4.3.cmml" xref="S5.SS1.p1.5.m5.1.1.3.3.4.3">×</ci><apply id="S5.SS1.p1.5.m5.1.1.3.3.4.4.cmml" xref="S5.SS1.p1.5.m5.1.1.3.3.4.4"><csymbol cd="ambiguous" id="S5.SS1.p1.5.m5.1.1.3.3.4.4.1.cmml" xref="S5.SS1.p1.5.m5.1.1.3.3.4.4">subscript</csymbol><ci id="S5.SS1.p1.5.m5.1.1.3.3.4.4.2.cmml" xref="S5.SS1.p1.5.m5.1.1.3.3.4.4.2">𝑤</ci><ci id="S5.SS1.p1.5.m5.1.1.3.3.4.4.3.cmml" xref="S5.SS1.p1.5.m5.1.1.3.3.4.4.3">𝑖</ci></apply></apply><cn type="integer" id="S5.SS1.p1.5.m5.1.1.3.3.5.cmml" xref="S5.SS1.p1.5.m5.1.1.3.3.5">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.5.m5.1c">x_{i}+1\in R^{n_{i}+1×h_{i}+1×w_{i}+1}</annotation></semantics></math>. These are then used as input feature maps for the next convolutional layer. Figure <a href="#S5.F10" title="Figure 10 ‣ V-A Pruning ‣ V CNN Compressing ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> presents the overall convolution process.</p>
</div>
<figure id="S5.F10" class="ltx_figure"><img src="/html/2205.13272/assets/x1.png" id="S5.F10.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="657" height="317" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Convolution Process. Adapted from Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite></figcaption>
</figure>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.3" class="ltx_p">As seen in Figure <a href="#S5.F10" title="Figure 10 ‣ V-A Pruning ‣ V CNN Compressing ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, the convolution on a CNN is an attached process, with information shared between the layers and filter, revealing the necessity to propagate the pruning, removing subsequent filters and feature maps relative to the pruned filter. With this sequential pruning, we can reduce the computational cost, getting an <math id="S5.SS1.p2.1.m1.1" class="ltx_Math" alttext="m/n_{i}+1" display="inline"><semantics id="S5.SS1.p2.1.m1.1a"><mrow id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml"><mrow id="S5.SS1.p2.1.m1.1.1.2" xref="S5.SS1.p2.1.m1.1.1.2.cmml"><mi id="S5.SS1.p2.1.m1.1.1.2.2" xref="S5.SS1.p2.1.m1.1.1.2.2.cmml">m</mi><mo id="S5.SS1.p2.1.m1.1.1.2.1" xref="S5.SS1.p2.1.m1.1.1.2.1.cmml">/</mo><msub id="S5.SS1.p2.1.m1.1.1.2.3" xref="S5.SS1.p2.1.m1.1.1.2.3.cmml"><mi id="S5.SS1.p2.1.m1.1.1.2.3.2" xref="S5.SS1.p2.1.m1.1.1.2.3.2.cmml">n</mi><mi id="S5.SS1.p2.1.m1.1.1.2.3.3" xref="S5.SS1.p2.1.m1.1.1.2.3.3.cmml">i</mi></msub></mrow><mo id="S5.SS1.p2.1.m1.1.1.1" xref="S5.SS1.p2.1.m1.1.1.1.cmml">+</mo><mn id="S5.SS1.p2.1.m1.1.1.3" xref="S5.SS1.p2.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><apply id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1"><plus id="S5.SS1.p2.1.m1.1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1.1"></plus><apply id="S5.SS1.p2.1.m1.1.1.2.cmml" xref="S5.SS1.p2.1.m1.1.1.2"><divide id="S5.SS1.p2.1.m1.1.1.2.1.cmml" xref="S5.SS1.p2.1.m1.1.1.2.1"></divide><ci id="S5.SS1.p2.1.m1.1.1.2.2.cmml" xref="S5.SS1.p2.1.m1.1.1.2.2">𝑚</ci><apply id="S5.SS1.p2.1.m1.1.1.2.3.cmml" xref="S5.SS1.p2.1.m1.1.1.2.3"><csymbol cd="ambiguous" id="S5.SS1.p2.1.m1.1.1.2.3.1.cmml" xref="S5.SS1.p2.1.m1.1.1.2.3">subscript</csymbol><ci id="S5.SS1.p2.1.m1.1.1.2.3.2.cmml" xref="S5.SS1.p2.1.m1.1.1.2.3.2">𝑛</ci><ci id="S5.SS1.p2.1.m1.1.1.2.3.3.cmml" xref="S5.SS1.p2.1.m1.1.1.2.3.3">𝑖</ci></apply></apply><cn type="integer" id="S5.SS1.p2.1.m1.1.1.3.cmml" xref="S5.SS1.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">m/n_{i}+1</annotation></semantics></math> of the original computational cost from <math id="S5.SS1.p2.2.m2.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S5.SS1.p2.2.m2.1a"><mi id="S5.SS1.p2.2.m2.1.1" xref="S5.SS1.p2.2.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.2.m2.1b"><ci id="S5.SS1.p2.2.m2.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.2.m2.1c">m</annotation></semantics></math> pruned filters on a layer <math id="S5.SS1.p2.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S5.SS1.p2.3.m3.1a"><mi id="S5.SS1.p2.3.m3.1.1" xref="S5.SS1.p2.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.3.m3.1b"><ci id="S5.SS1.p2.3.m3.1.1.cmml" xref="S5.SS1.p2.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.3.m3.1c">i</annotation></semantics></math>.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.2" class="ltx_p">For the pruning phase, we need to take an indicator of which parameter will be reduced or removed. In our case, we evaluate each filter’s importance based on the absolute sum of each filter <math id="S5.SS1.p3.1.m1.3" class="ltx_Math" alttext="\sum\left|F_{i,j}\right|" display="inline"><semantics id="S5.SS1.p3.1.m1.3a"><mrow id="S5.SS1.p3.1.m1.3.3" xref="S5.SS1.p3.1.m1.3.3.cmml"><mo rspace="0em" id="S5.SS1.p3.1.m1.3.3.2" xref="S5.SS1.p3.1.m1.3.3.2.cmml">∑</mo><mrow id="S5.SS1.p3.1.m1.3.3.1.1" xref="S5.SS1.p3.1.m1.3.3.1.2.cmml"><mo id="S5.SS1.p3.1.m1.3.3.1.1.2" xref="S5.SS1.p3.1.m1.3.3.1.2.1.cmml">|</mo><msub id="S5.SS1.p3.1.m1.3.3.1.1.1" xref="S5.SS1.p3.1.m1.3.3.1.1.1.cmml"><mi id="S5.SS1.p3.1.m1.3.3.1.1.1.2" xref="S5.SS1.p3.1.m1.3.3.1.1.1.2.cmml">F</mi><mrow id="S5.SS1.p3.1.m1.2.2.2.4" xref="S5.SS1.p3.1.m1.2.2.2.3.cmml"><mi id="S5.SS1.p3.1.m1.1.1.1.1" xref="S5.SS1.p3.1.m1.1.1.1.1.cmml">i</mi><mo id="S5.SS1.p3.1.m1.2.2.2.4.1" xref="S5.SS1.p3.1.m1.2.2.2.3.cmml">,</mo><mi id="S5.SS1.p3.1.m1.2.2.2.2" xref="S5.SS1.p3.1.m1.2.2.2.2.cmml">j</mi></mrow></msub><mo id="S5.SS1.p3.1.m1.3.3.1.1.3" xref="S5.SS1.p3.1.m1.3.3.1.2.1.cmml">|</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.3b"><apply id="S5.SS1.p3.1.m1.3.3.cmml" xref="S5.SS1.p3.1.m1.3.3"><sum id="S5.SS1.p3.1.m1.3.3.2.cmml" xref="S5.SS1.p3.1.m1.3.3.2"></sum><apply id="S5.SS1.p3.1.m1.3.3.1.2.cmml" xref="S5.SS1.p3.1.m1.3.3.1.1"><abs id="S5.SS1.p3.1.m1.3.3.1.2.1.cmml" xref="S5.SS1.p3.1.m1.3.3.1.1.2"></abs><apply id="S5.SS1.p3.1.m1.3.3.1.1.1.cmml" xref="S5.SS1.p3.1.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S5.SS1.p3.1.m1.3.3.1.1.1.1.cmml" xref="S5.SS1.p3.1.m1.3.3.1.1.1">subscript</csymbol><ci id="S5.SS1.p3.1.m1.3.3.1.1.1.2.cmml" xref="S5.SS1.p3.1.m1.3.3.1.1.1.2">𝐹</ci><list id="S5.SS1.p3.1.m1.2.2.2.3.cmml" xref="S5.SS1.p3.1.m1.2.2.2.4"><ci id="S5.SS1.p3.1.m1.1.1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1.1.1">𝑖</ci><ci id="S5.SS1.p3.1.m1.2.2.2.2.cmml" xref="S5.SS1.p3.1.m1.2.2.2.2">𝑗</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.3c">\sum\left|F_{i,j}\right|</annotation></semantics></math>. As a criterion for data-free selection and as a feature for unbiased selection, <math id="S5.SS1.p3.2.m2.1" class="ltx_Math" alttext="l1-norm" display="inline"><semantics id="S5.SS1.p3.2.m2.1a"><mrow id="S5.SS1.p3.2.m2.1.1" xref="S5.SS1.p3.2.m2.1.1.cmml"><mrow id="S5.SS1.p3.2.m2.1.1.2" xref="S5.SS1.p3.2.m2.1.1.2.cmml"><mi id="S5.SS1.p3.2.m2.1.1.2.2" xref="S5.SS1.p3.2.m2.1.1.2.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p3.2.m2.1.1.2.1" xref="S5.SS1.p3.2.m2.1.1.2.1.cmml">​</mo><mn id="S5.SS1.p3.2.m2.1.1.2.3" xref="S5.SS1.p3.2.m2.1.1.2.3.cmml">1</mn></mrow><mo id="S5.SS1.p3.2.m2.1.1.1" xref="S5.SS1.p3.2.m2.1.1.1.cmml">−</mo><mrow id="S5.SS1.p3.2.m2.1.1.3" xref="S5.SS1.p3.2.m2.1.1.3.cmml"><mi id="S5.SS1.p3.2.m2.1.1.3.2" xref="S5.SS1.p3.2.m2.1.1.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p3.2.m2.1.1.3.1" xref="S5.SS1.p3.2.m2.1.1.3.1.cmml">​</mo><mi id="S5.SS1.p3.2.m2.1.1.3.3" xref="S5.SS1.p3.2.m2.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p3.2.m2.1.1.3.1a" xref="S5.SS1.p3.2.m2.1.1.3.1.cmml">​</mo><mi id="S5.SS1.p3.2.m2.1.1.3.4" xref="S5.SS1.p3.2.m2.1.1.3.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p3.2.m2.1.1.3.1b" xref="S5.SS1.p3.2.m2.1.1.3.1.cmml">​</mo><mi id="S5.SS1.p3.2.m2.1.1.3.5" xref="S5.SS1.p3.2.m2.1.1.3.5.cmml">m</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.2.m2.1b"><apply id="S5.SS1.p3.2.m2.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1"><minus id="S5.SS1.p3.2.m2.1.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1.1"></minus><apply id="S5.SS1.p3.2.m2.1.1.2.cmml" xref="S5.SS1.p3.2.m2.1.1.2"><times id="S5.SS1.p3.2.m2.1.1.2.1.cmml" xref="S5.SS1.p3.2.m2.1.1.2.1"></times><ci id="S5.SS1.p3.2.m2.1.1.2.2.cmml" xref="S5.SS1.p3.2.m2.1.1.2.2">𝑙</ci><cn type="integer" id="S5.SS1.p3.2.m2.1.1.2.3.cmml" xref="S5.SS1.p3.2.m2.1.1.2.3">1</cn></apply><apply id="S5.SS1.p3.2.m2.1.1.3.cmml" xref="S5.SS1.p3.2.m2.1.1.3"><times id="S5.SS1.p3.2.m2.1.1.3.1.cmml" xref="S5.SS1.p3.2.m2.1.1.3.1"></times><ci id="S5.SS1.p3.2.m2.1.1.3.2.cmml" xref="S5.SS1.p3.2.m2.1.1.3.2">𝑛</ci><ci id="S5.SS1.p3.2.m2.1.1.3.3.cmml" xref="S5.SS1.p3.2.m2.1.1.3.3">𝑜</ci><ci id="S5.SS1.p3.2.m2.1.1.3.4.cmml" xref="S5.SS1.p3.2.m2.1.1.3.4">𝑟</ci><ci id="S5.SS1.p3.2.m2.1.1.3.5.cmml" xref="S5.SS1.p3.2.m2.1.1.3.5">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.2.m2.1c">l1-norm</annotation></semantics></math> was chosen. The normalization was required since we have different kernel sizes. Some feature maps naturally can have fewer activation but may represent a relevant feature map. Figure <a href="#S5.F11" title="Figure 11 ‣ V-A Pruning ‣ V CNN Compressing ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> shows the pruning pipeline.</p>
</div>
<figure id="S5.F11" class="ltx_figure"><img src="/html/2205.13272/assets/images/pruning_process.png" id="S5.F11.1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="210" height="583" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Pruning process.</figcaption>
</figure>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">This type of pruning worked just fine for us. Since we have a ConvNet, the pruning is straightforward, requiring minimal effort, with no need for feature map projection, and requiring minimal retraining.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.5.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.6.2" class="ltx_text ltx_font_italic">Quantization</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We based our quantization on the precision change approaches, converting the single-precision float-point to a half-precision float-point <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>. At the same time, usually, the weights are stored as 32 bits (FP32) values. We switch to using a 16 bits values (FP16) standard. Concerning the FP16 representation, there are some slight changes. While the FP32 uses one bit for the sign, 8 bits for the exponent, and finally 23 bits for the significant store, the FP16 standard keeps the one bit for a sign but reduces it to 5 bits for the exponent and 10 bits for the significant precision part. By reducing the float points size from the usual 32 to 16 bits, one can cut almost by half the dimension of the weight while maintaining nearly the same precision <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.5.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.6.2" class="ltx_text ltx_font_italic">Combining Pruning with Quantization</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">The final step of our compression approach consists of using the pruning process, followed by quantization. The pruning process is the first one applied. Once a pruning rate is selected, the pruning is performed. The second step is related to retraining. During the retraining process, the pruned network is trained again to reduce the degradation caused by the pruning phase. As a final step, we perform post-training quantization, which reduces parameter precision from FP32 to FP16. Figure <a href="#S5.F12" title="Figure 12 ‣ V-C Combining Pruning with Quantization ‣ V CNN Compressing ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> depicts the entire process.</p>
</div>
<figure id="S5.F12" class="ltx_figure"><img src="/html/2205.13272/assets/images/sample.png" id="S5.F12.1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="240" height="918" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Overall compression process.</figcaption>
</figure>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">As seen in Figure <a href="#S5.F12" title="Figure 12 ‣ V-C Combining Pruning with Quantization ‣ V CNN Compressing ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>, we used the final CNN without any additional training. The post-quantization phase reduces the time and effort of the process while keeping considerable efficiency and precision.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Experiments and Results</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This section demonstrates a set of experiments aimed to evaluate if the proposed set o techniques can produce a reliable output in our scenario. By reliability, we mean a model that, even compressed, can result in a slight to no decrease in performance. We also aim to evaluate our model in an actual constrained device to demonstrate the viability of our proposed approach to the scenario for HRI and IoT.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS1.5.1.1" class="ltx_text">VI-A</span> </span><span id="S6.SS1.6.2" class="ltx_text ltx_font_italic">Experiment Description</span>
</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.3" class="ltx_p">Concerning the conducted experiments, we first need to set some parameters. The first one is the desired pruning rate. We select a range of values with a slight variation; more specifically, pruning rate variation (or variation step) is set to <math id="S6.SS1.p1.1.m1.1" class="ltx_Math" alttext="10\%" display="inline"><semantics id="S6.SS1.p1.1.m1.1a"><mrow id="S6.SS1.p1.1.m1.1.1" xref="S6.SS1.p1.1.m1.1.1.cmml"><mn id="S6.SS1.p1.1.m1.1.1.2" xref="S6.SS1.p1.1.m1.1.1.2.cmml">10</mn><mo id="S6.SS1.p1.1.m1.1.1.1" xref="S6.SS1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.1.m1.1b"><apply id="S6.SS1.p1.1.m1.1.1.cmml" xref="S6.SS1.p1.1.m1.1.1"><csymbol cd="latexml" id="S6.SS1.p1.1.m1.1.1.1.cmml" xref="S6.SS1.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S6.SS1.p1.1.m1.1.1.2.cmml" xref="S6.SS1.p1.1.m1.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.1.m1.1c">10\%</annotation></semantics></math>. The <math id="S6.SS1.p1.2.m2.1" class="ltx_Math" alttext="10\%" display="inline"><semantics id="S6.SS1.p1.2.m2.1a"><mrow id="S6.SS1.p1.2.m2.1.1" xref="S6.SS1.p1.2.m2.1.1.cmml"><mn id="S6.SS1.p1.2.m2.1.1.2" xref="S6.SS1.p1.2.m2.1.1.2.cmml">10</mn><mo id="S6.SS1.p1.2.m2.1.1.1" xref="S6.SS1.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.2.m2.1b"><apply id="S6.SS1.p1.2.m2.1.1.cmml" xref="S6.SS1.p1.2.m2.1.1"><csymbol cd="latexml" id="S6.SS1.p1.2.m2.1.1.1.cmml" xref="S6.SS1.p1.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S6.SS1.p1.2.m2.1.1.2.cmml" xref="S6.SS1.p1.2.m2.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.2.m2.1c">10\%</annotation></semantics></math> steps were chosen based on previous evaluations, we noticed that smaller steps could be repetitive and laborious, representing the tiniest changes, and <math id="S6.SS1.p1.3.m3.1" class="ltx_Math" alttext="10\%" display="inline"><semantics id="S6.SS1.p1.3.m3.1a"><mrow id="S6.SS1.p1.3.m3.1.1" xref="S6.SS1.p1.3.m3.1.1.cmml"><mn id="S6.SS1.p1.3.m3.1.1.2" xref="S6.SS1.p1.3.m3.1.1.2.cmml">10</mn><mo id="S6.SS1.p1.3.m3.1.1.1" xref="S6.SS1.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.3.m3.1b"><apply id="S6.SS1.p1.3.m3.1.1.cmml" xref="S6.SS1.p1.3.m3.1.1"><csymbol cd="latexml" id="S6.SS1.p1.3.m3.1.1.1.cmml" xref="S6.SS1.p1.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S6.SS1.p1.3.m3.1.1.2.cmml" xref="S6.SS1.p1.3.m3.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.3.m3.1c">10\%</annotation></semantics></math> showed a notable performance in our scenario. As metrics, we collect and observe the
percentage of correct keypoints (PCK) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>, inference time, the floating-point operation (FLOP), the total number of parameters, the required storage size, and the achieved processed frames per second (FPS).</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p">PCK is an important metric that compares the distance between the predicted and the keypoint. A keypoint is considered correct if the detected keypoint is contained within a distance below a certain threshold value. The distance is calculated pixel-wise. In our case, the distance selected was the euclidian. A <math id="S6.SS1.p2.1.m1.1" class="ltx_Math" alttext="50\%" display="inline"><semantics id="S6.SS1.p2.1.m1.1a"><mrow id="S6.SS1.p2.1.m1.1.1" xref="S6.SS1.p2.1.m1.1.1.cmml"><mn id="S6.SS1.p2.1.m1.1.1.2" xref="S6.SS1.p2.1.m1.1.1.2.cmml">50</mn><mo id="S6.SS1.p2.1.m1.1.1.1" xref="S6.SS1.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.1.m1.1b"><apply id="S6.SS1.p2.1.m1.1.1.cmml" xref="S6.SS1.p2.1.m1.1.1"><csymbol cd="latexml" id="S6.SS1.p2.1.m1.1.1.1.cmml" xref="S6.SS1.p2.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S6.SS1.p2.1.m1.1.1.2.cmml" xref="S6.SS1.p2.1.m1.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.1.m1.1c">50\%</annotation></semantics></math> distance threshold value is adopted as suggested by many works such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>. Intuitively, the higher the achieved PCK value, the better our CNN results are.</p>
</div>
<div id="S6.SS1.p3" class="ltx_para">
<p id="S6.SS1.p3.1" class="ltx_p">The total of parameters represents the learnable values from the CNN. FLOP reflects the computational calculations needed by CNN. Reducing the total number of parameters and FLOP also indicates faster processing and reduced response time. FPS measures the rate of frames we can process per second while still compressing the inference time and the post-processing. Last but not least, we also evaluate the disc storage size needed by the network. It is a critical metric, especially when considering constrained devices that may have limited disc storage space.</p>
</div>
<div id="S6.SS1.p4" class="ltx_para">
<p id="S6.SS1.p4.1" class="ltx_p">Since our work mainly targets limited resource devices, we performed our experiment in two Setups, a Desktop setup, and a constrained device, using a Raspberry Pi 3, the Constrained Device setup. The details of these two setups are listed in Table <a href="#S6.T2" title="Table II ‣ VI-A Experiment Description ‣ VI Experiments and Results ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>.</p>
</div>
<figure id="S6.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table II: </span>Setup</figcaption>
<table id="S6.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S6.T2.1.1" class="ltx_tr">
<td id="S6.T2.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T2.1.1.1.1" class="ltx_text ltx_font_bold">Setup</span></td>
<td id="S6.T2.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T2.1.1.2.1" class="ltx_text ltx_font_bold">Processor</span></td>
<td id="S6.T2.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T2.1.1.3.1" class="ltx_text ltx_font_bold">RAM</span></td>
<td id="S6.T2.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T2.1.1.4.1" class="ltx_text ltx_font_bold">S.O.</span></td>
<td id="S6.T2.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S6.T2.1.1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S6.T2.1.1.5.1.1" class="ltx_tr">
<td id="S6.T2.1.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T2.1.1.5.1.1.1.1" class="ltx_text ltx_font_bold">Disc</span></td>
</tr>
<tr id="S6.T2.1.1.5.1.2" class="ltx_tr">
<td id="S6.T2.1.1.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T2.1.1.5.1.2.1.1" class="ltx_text ltx_font_bold">Storage</span></td>
</tr>
<tr id="S6.T2.1.1.5.1.3" class="ltx_tr">
<td id="S6.T2.1.1.5.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T2.1.1.5.1.3.1.1" class="ltx_text ltx_font_bold">Size</span></td>
</tr>
</table>
</td>
<td id="S6.T2.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T2.1.1.6.1" class="ltx_text ltx_font_bold">GPU</span></td>
</tr>
<tr id="S6.T2.1.2" class="ltx_tr">
<td id="S6.T2.1.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Desktop</td>
<td id="S6.T2.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S6.T2.1.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S6.T2.1.2.2.1.1" class="ltx_tr">
<td id="S6.T2.1.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Intel Core</td>
</tr>
<tr id="S6.T2.1.2.2.1.2" class="ltx_tr">
<td id="S6.T2.1.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">i7-3770</td>
</tr>
<tr id="S6.T2.1.2.2.1.3" class="ltx_tr">
<td id="S6.T2.1.2.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center">3.40GHz</td>
</tr>
</table>
</td>
<td id="S6.T2.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">16 Gb</td>
<td id="S6.T2.1.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S6.T2.1.2.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S6.T2.1.2.4.1.1" class="ltx_tr">
<td id="S6.T2.1.2.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Ubuntu</td>
</tr>
<tr id="S6.T2.1.2.4.1.2" class="ltx_tr">
<td id="S6.T2.1.2.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">18.04</td>
</tr>
</table>
</td>
<td id="S6.T2.1.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1 TB</td>
<td id="S6.T2.1.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S6.T2.1.2.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S6.T2.1.2.6.1.1" class="ltx_tr">
<td id="S6.T2.1.2.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">NVIDIA</td>
</tr>
<tr id="S6.T2.1.2.6.1.2" class="ltx_tr">
<td id="S6.T2.1.2.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">GeForce</td>
</tr>
<tr id="S6.T2.1.2.6.1.3" class="ltx_tr">
<td id="S6.T2.1.2.6.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center">GTX Titan</td>
</tr>
<tr id="S6.T2.1.2.6.1.4" class="ltx_tr">
<td id="S6.T2.1.2.6.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center">6 Gb</td>
</tr>
</table>
</td>
</tr>
<tr id="S6.T2.1.3" class="ltx_tr">
<td id="S6.T2.1.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<table id="S6.T2.1.3.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S6.T2.1.3.1.1.1" class="ltx_tr">
<td id="S6.T2.1.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Constrained</td>
</tr>
<tr id="S6.T2.1.3.1.1.2" class="ltx_tr">
<td id="S6.T2.1.3.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Device</td>
</tr>
</table>
</td>
<td id="S6.T2.1.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
<table id="S6.T2.1.3.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S6.T2.1.3.2.1.1" class="ltx_tr">
<td id="S6.T2.1.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Quad Core</td>
</tr>
<tr id="S6.T2.1.3.2.1.2" class="ltx_tr">
<td id="S6.T2.1.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">1.2GHz</td>
</tr>
<tr id="S6.T2.1.3.2.1.3" class="ltx_tr">
<td id="S6.T2.1.3.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center">Broadcom</td>
</tr>
<tr id="S6.T2.1.3.2.1.4" class="ltx_tr">
<td id="S6.T2.1.3.2.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center">BCM2837</td>
</tr>
</table>
</td>
<td id="S6.T2.1.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">1 Gb</td>
<td id="S6.T2.1.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Raspian</td>
<td id="S6.T2.1.3.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">8 GB</td>
<td id="S6.T2.1.3.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">-</td>
</tr>
</table>
</figure>
<div id="S6.SS1.p5" class="ltx_para">
<p id="S6.SS1.p5.1" class="ltx_p">The GPU was only used for the original training and retraining processes. In the following sections, the predictions running on the Desktop were performed on its CPU. To validate the adopted process, we use K-Fold cross <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> validation. Data is splinted into K folders. We select a folder for testing at each run, and the remaining data is used for the training. We give K the value 5. For each metric, we obtain its average standard deviation.</p>
</div>
<section id="S6.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S6.SS1.SSS1.5.1.1" class="ltx_text">VI-A</span>1 </span>Desktop Setup</h4>

<div id="S6.SS1.SSS1.p1" class="ltx_para">
<p id="S6.SS1.SSS1.p1.1" class="ltx_p">The desktop setup was used in two stages: the training of the base network and the pruning process. The base network represents the full model, in other words, without pruning or quantization. The pruning process also includes the retraining step. For the base training, we followed the pipeline previously mentioned. We trained the FCN-Pose for a maximum of 500 epochs, or until convergence. We used the binary cross-entropy measure to represent a loss in the training phase. Figure <a href="#S6.F13" title="Figure 13 ‣ VI-A1 Desktop Setup ‣ VI-A Experiment Description ‣ VI Experiments and Results ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> shows the average plot of the training models with cross-validation.</p>
</div>
<figure id="S6.F13" class="ltx_figure"><img src="/html/2205.13272/assets/images/training_loss.png" id="S6.F13.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="388" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Loss plotting in relation from training and validation set.</figcaption>
</figure>
<div id="S6.SS1.SSS1.p2" class="ltx_para">
<p id="S6.SS1.SSS1.p2.1" class="ltx_p">Figure <a href="#S6.F13" title="Figure 13 ‣ VI-A1 Desktop Setup ‣ VI-A Experiment Description ‣ VI Experiments and Results ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> shows a considerable decrease at around 20 epochs and stops to learn, converging about 60 epochs. For each folder on the cross-validation, we gather the metrics PCK, inference time, and the estimated FPS. Table <a href="#S6.T3" title="Table III ‣ VI-A1 Desktop Setup ‣ VI-A Experiment Description ‣ VI Experiments and Results ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> offers details of the initial conducted training.</p>
</div>
<figure id="S6.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table III: </span>5-Folds Desktop</figcaption>
<table id="S6.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S6.T3.1.1" class="ltx_tr">
<td id="S6.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T3.1.1.1.1" class="ltx_text ltx_font_bold">Folder ID</span></td>
<td id="S6.T3.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T3.1.1.2.1" class="ltx_text ltx_font_bold">PCK@0.5</span></td>
<td id="S6.T3.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T3.1.1.3.1" class="ltx_text ltx_font_bold">Inference Time</span></td>
<td id="S6.T3.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T3.1.1.4.1" class="ltx_text ltx_font_bold">FPS - CPU</span></td>
</tr>
<tr id="S6.T3.1.2" class="ltx_tr">
<td id="S6.T3.1.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T3.1.2.1.1" class="ltx_text ltx_font_bold">0</span></td>
<td id="S6.T3.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.997</td>
<td id="S6.T3.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.088</td>
<td id="S6.T3.1.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.346</td>
</tr>
<tr id="S6.T3.1.3" class="ltx_tr">
<td id="S6.T3.1.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T3.1.3.1.1" class="ltx_text ltx_font_bold">1</span></td>
<td id="S6.T3.1.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.997</td>
<td id="S6.T3.1.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.085</td>
<td id="S6.T3.1.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.731</td>
</tr>
<tr id="S6.T3.1.4" class="ltx_tr">
<td id="S6.T3.1.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T3.1.4.1.1" class="ltx_text ltx_font_bold">2</span></td>
<td id="S6.T3.1.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.999</td>
<td id="S6.T3.1.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.084</td>
<td id="S6.T3.1.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.825</td>
</tr>
<tr id="S6.T3.1.5" class="ltx_tr">
<td id="S6.T3.1.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T3.1.5.1.1" class="ltx_text ltx_font_bold">3</span></td>
<td id="S6.T3.1.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.998</td>
<td id="S6.T3.1.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.085</td>
<td id="S6.T3.1.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.754</td>
</tr>
<tr id="S6.T3.1.6" class="ltx_tr">
<td id="S6.T3.1.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T3.1.6.1.1" class="ltx_text ltx_font_bold">4</span></td>
<td id="S6.T3.1.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.996</td>
<td id="S6.T3.1.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.084</td>
<td id="S6.T3.1.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.899</td>
</tr>
<tr id="S6.T3.1.7" class="ltx_tr">
<td id="S6.T3.1.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T3.1.7.1.1" class="ltx_text ltx_font_bold">Average</span></td>
<td id="S6.T3.1.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.997</td>
<td id="S6.T3.1.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.085</td>
<td id="S6.T3.1.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.711</td>
</tr>
<tr id="S6.T3.1.8" class="ltx_tr">
<td id="S6.T3.1.8.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T3.1.8.1.1" class="ltx_text ltx_font_bold">STD</span></td>
<td id="S6.T3.1.8.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.001</td>
<td id="S6.T3.1.8.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.001</td>
<td id="S6.T3.1.8.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.214</td>
</tr>
</table>
</figure>
<div id="S6.SS1.SSS1.p3" class="ltx_para">
<p id="S6.SS1.SSS1.p3.5" class="ltx_p">Table <a href="#S6.T3" title="Table III ‣ VI-A1 Desktop Setup ‣ VI-A Experiment Description ‣ VI Experiments and Results ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> indicates some good results with an average <math id="S6.SS1.SSS1.p3.1.m1.1" class="ltx_Math" alttext="PCK@0.5" display="inline"><semantics id="S6.SS1.SSS1.p3.1.m1.1a"><mrow id="S6.SS1.SSS1.p3.1.m1.1.1" xref="S6.SS1.SSS1.p3.1.m1.1.1.cmml"><mi id="S6.SS1.SSS1.p3.1.m1.1.1.2" xref="S6.SS1.SSS1.p3.1.m1.1.1.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S6.SS1.SSS1.p3.1.m1.1.1.1" xref="S6.SS1.SSS1.p3.1.m1.1.1.1.cmml">​</mo><mi id="S6.SS1.SSS1.p3.1.m1.1.1.3" xref="S6.SS1.SSS1.p3.1.m1.1.1.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="S6.SS1.SSS1.p3.1.m1.1.1.1a" xref="S6.SS1.SSS1.p3.1.m1.1.1.1.cmml">​</mo><mi id="S6.SS1.SSS1.p3.1.m1.1.1.4" xref="S6.SS1.SSS1.p3.1.m1.1.1.4.cmml">K</mi><mo lspace="0em" rspace="0em" id="S6.SS1.SSS1.p3.1.m1.1.1.1b" xref="S6.SS1.SSS1.p3.1.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S6.SS1.SSS1.p3.1.m1.1.1.5" xref="S6.SS1.SSS1.p3.1.m1.1.1.5.cmml">@</mi><mo lspace="0em" rspace="0em" id="S6.SS1.SSS1.p3.1.m1.1.1.1c" xref="S6.SS1.SSS1.p3.1.m1.1.1.1.cmml">​</mo><mn id="S6.SS1.SSS1.p3.1.m1.1.1.6" xref="S6.SS1.SSS1.p3.1.m1.1.1.6.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p3.1.m1.1b"><apply id="S6.SS1.SSS1.p3.1.m1.1.1.cmml" xref="S6.SS1.SSS1.p3.1.m1.1.1"><times id="S6.SS1.SSS1.p3.1.m1.1.1.1.cmml" xref="S6.SS1.SSS1.p3.1.m1.1.1.1"></times><ci id="S6.SS1.SSS1.p3.1.m1.1.1.2.cmml" xref="S6.SS1.SSS1.p3.1.m1.1.1.2">𝑃</ci><ci id="S6.SS1.SSS1.p3.1.m1.1.1.3.cmml" xref="S6.SS1.SSS1.p3.1.m1.1.1.3">𝐶</ci><ci id="S6.SS1.SSS1.p3.1.m1.1.1.4.cmml" xref="S6.SS1.SSS1.p3.1.m1.1.1.4">𝐾</ci><ci id="S6.SS1.SSS1.p3.1.m1.1.1.5.cmml" xref="S6.SS1.SSS1.p3.1.m1.1.1.5">@</ci><cn type="float" id="S6.SS1.SSS1.p3.1.m1.1.1.6.cmml" xref="S6.SS1.SSS1.p3.1.m1.1.1.6">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p3.1.m1.1c">PCK@0.5</annotation></semantics></math> of <math id="S6.SS1.SSS1.p3.2.m2.1" class="ltx_Math" alttext="0.997" display="inline"><semantics id="S6.SS1.SSS1.p3.2.m2.1a"><mn id="S6.SS1.SSS1.p3.2.m2.1.1" xref="S6.SS1.SSS1.p3.2.m2.1.1.cmml">0.997</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p3.2.m2.1b"><cn type="float" id="S6.SS1.SSS1.p3.2.m2.1.1.cmml" xref="S6.SS1.SSS1.p3.2.m2.1.1">0.997</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p3.2.m2.1c">0.997</annotation></semantics></math>. FPS was estimated based on the inference time, being [<math id="S6.SS1.SSS1.p3.3.m3.1" class="ltx_Math" alttext="1/inference\ time" display="inline"><semantics id="S6.SS1.SSS1.p3.3.m3.1a"><mrow id="S6.SS1.SSS1.p3.3.m3.1.1" xref="S6.SS1.SSS1.p3.3.m3.1.1.cmml"><mrow id="S6.SS1.SSS1.p3.3.m3.1.1.2" xref="S6.SS1.SSS1.p3.3.m3.1.1.2.cmml"><mn id="S6.SS1.SSS1.p3.3.m3.1.1.2.2" xref="S6.SS1.SSS1.p3.3.m3.1.1.2.2.cmml">1</mn><mo id="S6.SS1.SSS1.p3.3.m3.1.1.2.1" xref="S6.SS1.SSS1.p3.3.m3.1.1.2.1.cmml">/</mo><mi id="S6.SS1.SSS1.p3.3.m3.1.1.2.3" xref="S6.SS1.SSS1.p3.3.m3.1.1.2.3.cmml">i</mi></mrow><mo lspace="0em" rspace="0em" id="S6.SS1.SSS1.p3.3.m3.1.1.1" xref="S6.SS1.SSS1.p3.3.m3.1.1.1.cmml">​</mo><mi id="S6.SS1.SSS1.p3.3.m3.1.1.3" xref="S6.SS1.SSS1.p3.3.m3.1.1.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S6.SS1.SSS1.p3.3.m3.1.1.1a" xref="S6.SS1.SSS1.p3.3.m3.1.1.1.cmml">​</mo><mi id="S6.SS1.SSS1.p3.3.m3.1.1.4" xref="S6.SS1.SSS1.p3.3.m3.1.1.4.cmml">f</mi><mo lspace="0em" rspace="0em" id="S6.SS1.SSS1.p3.3.m3.1.1.1b" xref="S6.SS1.SSS1.p3.3.m3.1.1.1.cmml">​</mo><mi id="S6.SS1.SSS1.p3.3.m3.1.1.5" xref="S6.SS1.SSS1.p3.3.m3.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S6.SS1.SSS1.p3.3.m3.1.1.1c" xref="S6.SS1.SSS1.p3.3.m3.1.1.1.cmml">​</mo><mi id="S6.SS1.SSS1.p3.3.m3.1.1.6" xref="S6.SS1.SSS1.p3.3.m3.1.1.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S6.SS1.SSS1.p3.3.m3.1.1.1d" xref="S6.SS1.SSS1.p3.3.m3.1.1.1.cmml">​</mo><mi id="S6.SS1.SSS1.p3.3.m3.1.1.7" xref="S6.SS1.SSS1.p3.3.m3.1.1.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S6.SS1.SSS1.p3.3.m3.1.1.1e" xref="S6.SS1.SSS1.p3.3.m3.1.1.1.cmml">​</mo><mi id="S6.SS1.SSS1.p3.3.m3.1.1.8" xref="S6.SS1.SSS1.p3.3.m3.1.1.8.cmml">n</mi><mo lspace="0em" rspace="0em" id="S6.SS1.SSS1.p3.3.m3.1.1.1f" xref="S6.SS1.SSS1.p3.3.m3.1.1.1.cmml">​</mo><mi id="S6.SS1.SSS1.p3.3.m3.1.1.9" xref="S6.SS1.SSS1.p3.3.m3.1.1.9.cmml">c</mi><mo lspace="0em" rspace="0em" id="S6.SS1.SSS1.p3.3.m3.1.1.1g" xref="S6.SS1.SSS1.p3.3.m3.1.1.1.cmml">​</mo><mi id="S6.SS1.SSS1.p3.3.m3.1.1.10" xref="S6.SS1.SSS1.p3.3.m3.1.1.10.cmml">e</mi><mo lspace="0.500em" rspace="0em" id="S6.SS1.SSS1.p3.3.m3.1.1.1h" xref="S6.SS1.SSS1.p3.3.m3.1.1.1.cmml">​</mo><mi id="S6.SS1.SSS1.p3.3.m3.1.1.11" xref="S6.SS1.SSS1.p3.3.m3.1.1.11.cmml">t</mi><mo lspace="0em" rspace="0em" id="S6.SS1.SSS1.p3.3.m3.1.1.1i" xref="S6.SS1.SSS1.p3.3.m3.1.1.1.cmml">​</mo><mi id="S6.SS1.SSS1.p3.3.m3.1.1.12" xref="S6.SS1.SSS1.p3.3.m3.1.1.12.cmml">i</mi><mo lspace="0em" rspace="0em" id="S6.SS1.SSS1.p3.3.m3.1.1.1j" xref="S6.SS1.SSS1.p3.3.m3.1.1.1.cmml">​</mo><mi id="S6.SS1.SSS1.p3.3.m3.1.1.13" xref="S6.SS1.SSS1.p3.3.m3.1.1.13.cmml">m</mi><mo lspace="0em" rspace="0em" id="S6.SS1.SSS1.p3.3.m3.1.1.1k" xref="S6.SS1.SSS1.p3.3.m3.1.1.1.cmml">​</mo><mi id="S6.SS1.SSS1.p3.3.m3.1.1.14" xref="S6.SS1.SSS1.p3.3.m3.1.1.14.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p3.3.m3.1b"><apply id="S6.SS1.SSS1.p3.3.m3.1.1.cmml" xref="S6.SS1.SSS1.p3.3.m3.1.1"><times id="S6.SS1.SSS1.p3.3.m3.1.1.1.cmml" xref="S6.SS1.SSS1.p3.3.m3.1.1.1"></times><apply id="S6.SS1.SSS1.p3.3.m3.1.1.2.cmml" xref="S6.SS1.SSS1.p3.3.m3.1.1.2"><divide id="S6.SS1.SSS1.p3.3.m3.1.1.2.1.cmml" xref="S6.SS1.SSS1.p3.3.m3.1.1.2.1"></divide><cn type="integer" id="S6.SS1.SSS1.p3.3.m3.1.1.2.2.cmml" xref="S6.SS1.SSS1.p3.3.m3.1.1.2.2">1</cn><ci id="S6.SS1.SSS1.p3.3.m3.1.1.2.3.cmml" xref="S6.SS1.SSS1.p3.3.m3.1.1.2.3">𝑖</ci></apply><ci id="S6.SS1.SSS1.p3.3.m3.1.1.3.cmml" xref="S6.SS1.SSS1.p3.3.m3.1.1.3">𝑛</ci><ci id="S6.SS1.SSS1.p3.3.m3.1.1.4.cmml" xref="S6.SS1.SSS1.p3.3.m3.1.1.4">𝑓</ci><ci id="S6.SS1.SSS1.p3.3.m3.1.1.5.cmml" xref="S6.SS1.SSS1.p3.3.m3.1.1.5">𝑒</ci><ci id="S6.SS1.SSS1.p3.3.m3.1.1.6.cmml" xref="S6.SS1.SSS1.p3.3.m3.1.1.6">𝑟</ci><ci id="S6.SS1.SSS1.p3.3.m3.1.1.7.cmml" xref="S6.SS1.SSS1.p3.3.m3.1.1.7">𝑒</ci><ci id="S6.SS1.SSS1.p3.3.m3.1.1.8.cmml" xref="S6.SS1.SSS1.p3.3.m3.1.1.8">𝑛</ci><ci id="S6.SS1.SSS1.p3.3.m3.1.1.9.cmml" xref="S6.SS1.SSS1.p3.3.m3.1.1.9">𝑐</ci><ci id="S6.SS1.SSS1.p3.3.m3.1.1.10.cmml" xref="S6.SS1.SSS1.p3.3.m3.1.1.10">𝑒</ci><ci id="S6.SS1.SSS1.p3.3.m3.1.1.11.cmml" xref="S6.SS1.SSS1.p3.3.m3.1.1.11">𝑡</ci><ci id="S6.SS1.SSS1.p3.3.m3.1.1.12.cmml" xref="S6.SS1.SSS1.p3.3.m3.1.1.12">𝑖</ci><ci id="S6.SS1.SSS1.p3.3.m3.1.1.13.cmml" xref="S6.SS1.SSS1.p3.3.m3.1.1.13">𝑚</ci><ci id="S6.SS1.SSS1.p3.3.m3.1.1.14.cmml" xref="S6.SS1.SSS1.p3.3.m3.1.1.14">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p3.3.m3.1c">1/inference\ time</annotation></semantics></math>]. Considering the FPS achieved on the CPU, it can be seen as being real-time, reaching an average of 11.711 FPS. Observe the small standard deviations for PCK and FPS at around <math id="S6.SS1.SSS1.p3.4.m4.1" class="ltx_Math" alttext="0.001" display="inline"><semantics id="S6.SS1.SSS1.p3.4.m4.1a"><mn id="S6.SS1.SSS1.p3.4.m4.1.1" xref="S6.SS1.SSS1.p3.4.m4.1.1.cmml">0.001</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p3.4.m4.1b"><cn type="float" id="S6.SS1.SSS1.p3.4.m4.1.1.cmml" xref="S6.SS1.SSS1.p3.4.m4.1.1">0.001</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p3.4.m4.1c">0.001</annotation></semantics></math> and <math id="S6.SS1.SSS1.p3.5.m5.1" class="ltx_Math" alttext="0.214" display="inline"><semantics id="S6.SS1.SSS1.p3.5.m5.1a"><mn id="S6.SS1.SSS1.p3.5.m5.1.1" xref="S6.SS1.SSS1.p3.5.m5.1.1.cmml">0.214</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p3.5.m5.1b"><cn type="float" id="S6.SS1.SSS1.p3.5.m5.1.1.cmml" xref="S6.SS1.SSS1.p3.5.m5.1.1">0.214</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p3.5.m5.1c">0.214</annotation></semantics></math>, respectively. Figure <a href="#S6.F14" title="Figure 14 ‣ VI-A1 Desktop Setup ‣ VI-A Experiment Description ‣ VI Experiments and Results ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a> displays some samples of the predicted images.</p>
</div>
<figure id="S6.F14" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S6.F14.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2205.13272/assets/images/AnyConv.com__frame_023_delay-0.05s.png" id="S6.F14.sf1.g1" class="ltx_graphics ltx_img_landscape" width="419" height="210" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Sample Frame</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S6.F14.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2205.13272/assets/images/AnyConv.com__frame_176_delay-0.05s.png" id="S6.F14.sf2.g1" class="ltx_graphics ltx_img_landscape" width="419" height="210" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Sample Frame</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Sample Frame. We have the keypoint on the left of the input image, and on the right are the segmentation maps. </figcaption>
</figure>
<div id="S6.SS1.SSS1.p4" class="ltx_para">
<p id="S6.SS1.SSS1.p4.1" class="ltx_p">The left side of Figure <a href="#S6.F14" title="Figure 14 ‣ VI-A1 Desktop Setup ‣ VI-A Experiment Description ‣ VI Experiments and Results ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a> displays the input image with the predicted keypoints. On the right side, one can see the set of predicted segmentation masks. The segmentation masks were joined to ease visualization. For each point, a predicted mask was joined. The mask referent to the skeleton is hidden. Figure <a href="#S6.F15" title="Figure 15 ‣ VI-A1 Desktop Setup ‣ VI-A Experiment Description ‣ VI Experiments and Results ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a> shows only the skeleton segmentation.</p>
</div>
<figure id="S6.F15" class="ltx_figure"><img src="/html/2205.13272/assets/images/segmentation.png" id="S6.F15.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="419" height="438" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>Generate segmentation mask for the robotic arm skeleton. First we have the input image (a), the segmentation mask (b), and finally the robotic arm segmented.</figcaption>
</figure>
<div id="S6.SS1.SSS1.p5" class="ltx_para">
<p id="S6.SS1.SSS1.p5.1" class="ltx_p">Observe that there is in Figure <a href="#S6.F15" title="Figure 15 ‣ VI-A1 Desktop Setup ‣ VI-A Experiment Description ‣ VI Experiments and Results ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a> a considerable segmentation mask of the robotic arm. Even in cases where the robotic arm presents some artifacts, the segmentation can focus only on the robotic arm. Our next step is to optimize the network inference time achieved by the pruning process.</p>
</div>
<div id="S6.SS1.SSS1.p6" class="ltx_para">
<p id="S6.SS1.SSS1.p6.1" class="ltx_p">We keep the same folds used for the training stage for the pruning process. We alter the pruning rate at a <math id="S6.SS1.SSS1.p6.1.m1.1" class="ltx_Math" alttext="10\%" display="inline"><semantics id="S6.SS1.SSS1.p6.1.m1.1a"><mrow id="S6.SS1.SSS1.p6.1.m1.1.1" xref="S6.SS1.SSS1.p6.1.m1.1.1.cmml"><mn id="S6.SS1.SSS1.p6.1.m1.1.1.2" xref="S6.SS1.SSS1.p6.1.m1.1.1.2.cmml">10</mn><mo id="S6.SS1.SSS1.p6.1.m1.1.1.1" xref="S6.SS1.SSS1.p6.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p6.1.m1.1b"><apply id="S6.SS1.SSS1.p6.1.m1.1.1.cmml" xref="S6.SS1.SSS1.p6.1.m1.1.1"><csymbol cd="latexml" id="S6.SS1.SSS1.p6.1.m1.1.1.1.cmml" xref="S6.SS1.SSS1.p6.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S6.SS1.SSS1.p6.1.m1.1.1.2.cmml" xref="S6.SS1.SSS1.p6.1.m1.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p6.1.m1.1c">10\%</annotation></semantics></math> step for each one. We calculate the average and standard deviation values for all metrics. After any pruning, we retrained the CNN for 100 epochs or until the convergence. Table <a href="#S6.T4" title="Table IV ‣ VI-A1 Desktop Setup ‣ VI-A Experiment Description ‣ VI Experiments and Results ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> lists the results obtained for the pruning in all rates and the metrics for the full model.</p>
</div>
<figure id="S6.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table IV: </span>Pruning Results</figcaption>
<table id="S6.T4.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S6.T4.1.1" class="ltx_tr">
<td id="S6.T4.1.1.1" class="ltx_td ltx_border_r"></td>
<td id="S6.T4.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S6.T4.1.1.2.1" class="ltx_text ltx_font_bold">PCK@0.5</span></td>
<td id="S6.T4.1.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" colspan="2"><span id="S6.T4.1.1.3.1" class="ltx_text ltx_font_bold">Inference Time</span></td>
<td id="S6.T4.1.1.4" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S6.T4.1.2" class="ltx_tr">
<td id="S6.T4.1.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T4.1.2.1.1" class="ltx_text ltx_font_bold">Pruning Rate</span></td>
<td id="S6.T4.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T4.1.2.2.1" class="ltx_text ltx_font_bold">Average</span></td>
<td id="S6.T4.1.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T4.1.2.3.1" class="ltx_text ltx_font_bold">STD</span></td>
<td id="S6.T4.1.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T4.1.2.4.1" class="ltx_text ltx_font_bold">Average</span></td>
<td id="S6.T4.1.2.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S6.T4.1.2.5.1" class="ltx_text ltx_font_bold">STD</span></td>
<td id="S6.T4.1.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T4.1.2.6.1" class="ltx_text ltx_font_bold">FPS - CPU</span></td>
</tr>
<tr id="S6.T4.1.3" class="ltx_tr">
<td id="S6.T4.1.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T4.1.3.1.1" class="ltx_text ltx_font_bold">Full Model</span></td>
<td id="S6.T4.1.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.997</td>
<td id="S6.T4.1.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.001</td>
<td id="S6.T4.1.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.085</td>
<td id="S6.T4.1.3.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.001</td>
<td id="S6.T4.1.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.711</td>
</tr>
<tr id="S6.T4.1.4" class="ltx_tr">
<td id="S6.T4.1.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T4.1.4.1.1" class="ltx_text ltx_font_bold">10</span></td>
<td id="S6.T4.1.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.998</td>
<td id="S6.T4.1.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.004</td>
<td id="S6.T4.1.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.089</td>
<td id="S6.T4.1.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.002</td>
<td id="S6.T4.1.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.128</td>
</tr>
<tr id="S6.T4.1.5" class="ltx_tr">
<td id="S6.T4.1.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T4.1.5.1.1" class="ltx_text ltx_font_bold">20</span></td>
<td id="S6.T4.1.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.995</td>
<td id="S6.T4.1.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.027</td>
<td id="S6.T4.1.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.070</td>
<td id="S6.T4.1.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.001</td>
<td id="S6.T4.1.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">14.130</td>
</tr>
<tr id="S6.T4.1.6" class="ltx_tr">
<td id="S6.T4.1.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T4.1.6.1.1" class="ltx_text ltx_font_bold">30</span></td>
<td id="S6.T4.1.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.991</td>
<td id="S6.T4.1.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.036</td>
<td id="S6.T4.1.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.072</td>
<td id="S6.T4.1.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.001</td>
<td id="S6.T4.1.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">13.716</td>
</tr>
<tr id="S6.T4.1.7" class="ltx_tr">
<td id="S6.T4.1.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T4.1.7.1.1" class="ltx_text ltx_font_bold">40</span></td>
<td id="S6.T4.1.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.991</td>
<td id="S6.T4.1.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.004</td>
<td id="S6.T4.1.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.062</td>
<td id="S6.T4.1.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.000</td>
<td id="S6.T4.1.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">16.057</td>
</tr>
<tr id="S6.T4.1.8" class="ltx_tr">
<td id="S6.T4.1.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T4.1.8.1.1" class="ltx_text ltx_font_bold">50</span></td>
<td id="S6.T4.1.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.985</td>
<td id="S6.T4.1.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.007</td>
<td id="S6.T4.1.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.035</td>
<td id="S6.T4.1.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.001</td>
<td id="S6.T4.1.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">27.823</td>
</tr>
<tr id="S6.T4.1.9" class="ltx_tr">
<td id="S6.T4.1.9.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T4.1.9.1.1" class="ltx_text ltx_font_bold">60</span></td>
<td id="S6.T4.1.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.980</td>
<td id="S6.T4.1.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.012</td>
<td id="S6.T4.1.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.034</td>
<td id="S6.T4.1.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.000</td>
<td id="S6.T4.1.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">29.251</td>
</tr>
<tr id="S6.T4.1.10" class="ltx_tr">
<td id="S6.T4.1.10.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T4.1.10.1.1" class="ltx_text ltx_font_bold">70</span></td>
<td id="S6.T4.1.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T4.1.10.2.1" class="ltx_text ltx_font_bold">0.980</span></td>
<td id="S6.T4.1.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T4.1.10.3.1" class="ltx_text ltx_font_bold">0.009</span></td>
<td id="S6.T4.1.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T4.1.10.4.1" class="ltx_text ltx_font_bold">0.023</span></td>
<td id="S6.T4.1.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T4.1.10.5.1" class="ltx_text ltx_font_bold">0.001</span></td>
<td id="S6.T4.1.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T4.1.10.6.1" class="ltx_text ltx_font_bold">41.900</span></td>
</tr>
<tr id="S6.T4.1.11" class="ltx_tr">
<td id="S6.T4.1.11.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T4.1.11.1.1" class="ltx_text ltx_font_bold">80</span></td>
<td id="S6.T4.1.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.912</td>
<td id="S6.T4.1.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.051</td>
<td id="S6.T4.1.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.022</td>
<td id="S6.T4.1.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.001</td>
<td id="S6.T4.1.11.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">44.949</td>
</tr>
<tr id="S6.T4.1.12" class="ltx_tr">
<td id="S6.T4.1.12.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T4.1.12.1.1" class="ltx_text ltx_font_bold">90</span></td>
<td id="S6.T4.1.12.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.284</td>
<td id="S6.T4.1.12.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.092</td>
<td id="S6.T4.1.12.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.016</td>
<td id="S6.T4.1.12.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.002</td>
<td id="S6.T4.1.12.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">61.892</td>
</tr>
</table>
</figure>
<div id="S6.SS1.SSS1.p7" class="ltx_para">
<p id="S6.SS1.SSS1.p7.5" class="ltx_p">Table <a href="#S6.T4" title="Table IV ‣ VI-A1 Desktop Setup ‣ VI-A Experiment Description ‣ VI Experiments and Results ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> demonstrates a successful pruning process. Observe that up to the <math id="S6.SS1.SSS1.p7.1.m1.1" class="ltx_Math" alttext="70\%" display="inline"><semantics id="S6.SS1.SSS1.p7.1.m1.1a"><mrow id="S6.SS1.SSS1.p7.1.m1.1.1" xref="S6.SS1.SSS1.p7.1.m1.1.1.cmml"><mn id="S6.SS1.SSS1.p7.1.m1.1.1.2" xref="S6.SS1.SSS1.p7.1.m1.1.1.2.cmml">70</mn><mo id="S6.SS1.SSS1.p7.1.m1.1.1.1" xref="S6.SS1.SSS1.p7.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p7.1.m1.1b"><apply id="S6.SS1.SSS1.p7.1.m1.1.1.cmml" xref="S6.SS1.SSS1.p7.1.m1.1.1"><csymbol cd="latexml" id="S6.SS1.SSS1.p7.1.m1.1.1.1.cmml" xref="S6.SS1.SSS1.p7.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S6.SS1.SSS1.p7.1.m1.1.1.2.cmml" xref="S6.SS1.SSS1.p7.1.m1.1.1.2">70</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p7.1.m1.1c">70\%</annotation></semantics></math> pruning rate, and the maximum PCK loss was approximately 0.05 perceptual points (p.p.). After <math id="S6.SS1.SSS1.p7.2.m2.1" class="ltx_Math" alttext="70\%" display="inline"><semantics id="S6.SS1.SSS1.p7.2.m2.1a"><mrow id="S6.SS1.SSS1.p7.2.m2.1.1" xref="S6.SS1.SSS1.p7.2.m2.1.1.cmml"><mn id="S6.SS1.SSS1.p7.2.m2.1.1.2" xref="S6.SS1.SSS1.p7.2.m2.1.1.2.cmml">70</mn><mo id="S6.SS1.SSS1.p7.2.m2.1.1.1" xref="S6.SS1.SSS1.p7.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p7.2.m2.1b"><apply id="S6.SS1.SSS1.p7.2.m2.1.1.cmml" xref="S6.SS1.SSS1.p7.2.m2.1.1"><csymbol cd="latexml" id="S6.SS1.SSS1.p7.2.m2.1.1.1.cmml" xref="S6.SS1.SSS1.p7.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S6.SS1.SSS1.p7.2.m2.1.1.2.cmml" xref="S6.SS1.SSS1.p7.2.m2.1.1.2">70</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p7.2.m2.1c">70\%</annotation></semantics></math> of pruning, CNN suffers a massive degradation. FPS increases largely, from <math id="S6.SS1.SSS1.p7.3.m3.1" class="ltx_Math" alttext="11.711" display="inline"><semantics id="S6.SS1.SSS1.p7.3.m3.1a"><mn id="S6.SS1.SSS1.p7.3.m3.1.1" xref="S6.SS1.SSS1.p7.3.m3.1.1.cmml">11.711</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p7.3.m3.1b"><cn type="float" id="S6.SS1.SSS1.p7.3.m3.1.1.cmml" xref="S6.SS1.SSS1.p7.3.m3.1.1">11.711</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p7.3.m3.1c">11.711</annotation></semantics></math> to <math id="S6.SS1.SSS1.p7.4.m4.1" class="ltx_Math" alttext="41.900" display="inline"><semantics id="S6.SS1.SSS1.p7.4.m4.1a"><mn id="S6.SS1.SSS1.p7.4.m4.1.1" xref="S6.SS1.SSS1.p7.4.m4.1.1.cmml">41.900</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p7.4.m4.1b"><cn type="float" id="S6.SS1.SSS1.p7.4.m4.1.1.cmml" xref="S6.SS1.SSS1.p7.4.m4.1.1">41.900</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p7.4.m4.1c">41.900</annotation></semantics></math>, becoming approximately <math id="S6.SS1.SSS1.p7.5.m5.1" class="ltx_Math" alttext="3.6" display="inline"><semantics id="S6.SS1.SSS1.p7.5.m5.1a"><mn id="S6.SS1.SSS1.p7.5.m5.1.1" xref="S6.SS1.SSS1.p7.5.m5.1.1.cmml">3.6</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p7.5.m5.1b"><cn type="float" id="S6.SS1.SSS1.p7.5.m5.1.1.cmml" xref="S6.SS1.SSS1.p7.5.m5.1.1">3.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p7.5.m5.1c">3.6</annotation></semantics></math> times faster. Figure <a href="#S6.F16" title="Figure 16 ‣ VI-A1 Desktop Setup ‣ VI-A Experiment Description ‣ VI Experiments and Results ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a> shows the efficiency plot, of the two metrics PCK versus FPS.</p>
</div>
<figure id="S6.F16" class="ltx_figure"><img src="/html/2205.13272/assets/images/pck_fps_desktop.png" id="S6.F16.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="322" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>PCK versus FPS. The FPS was normalized for a better visualization.</figcaption>
</figure>
<div id="S6.SS1.SSS1.p8" class="ltx_para">
<p id="S6.SS1.SSS1.p8.2" class="ltx_p">Figure <a href="#S6.F16" title="Figure 16 ‣ VI-A1 Desktop Setup ‣ VI-A Experiment Description ‣ VI Experiments and Results ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a> presents a sweet spot that comes innately starting from the <math id="S6.SS1.SSS1.p8.1.m1.1" class="ltx_Math" alttext="70\%" display="inline"><semantics id="S6.SS1.SSS1.p8.1.m1.1a"><mrow id="S6.SS1.SSS1.p8.1.m1.1.1" xref="S6.SS1.SSS1.p8.1.m1.1.1.cmml"><mn id="S6.SS1.SSS1.p8.1.m1.1.1.2" xref="S6.SS1.SSS1.p8.1.m1.1.1.2.cmml">70</mn><mo id="S6.SS1.SSS1.p8.1.m1.1.1.1" xref="S6.SS1.SSS1.p8.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p8.1.m1.1b"><apply id="S6.SS1.SSS1.p8.1.m1.1.1.cmml" xref="S6.SS1.SSS1.p8.1.m1.1.1"><csymbol cd="latexml" id="S6.SS1.SSS1.p8.1.m1.1.1.1.cmml" xref="S6.SS1.SSS1.p8.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S6.SS1.SSS1.p8.1.m1.1.1.2.cmml" xref="S6.SS1.SSS1.p8.1.m1.1.1.2">70</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p8.1.m1.1c">70\%</annotation></semantics></math> mark and increases until <math id="S6.SS1.SSS1.p8.2.m2.1" class="ltx_Math" alttext="90\%" display="inline"><semantics id="S6.SS1.SSS1.p8.2.m2.1a"><mrow id="S6.SS1.SSS1.p8.2.m2.1.1" xref="S6.SS1.SSS1.p8.2.m2.1.1.cmml"><mn id="S6.SS1.SSS1.p8.2.m2.1.1.2" xref="S6.SS1.SSS1.p8.2.m2.1.1.2.cmml">90</mn><mo id="S6.SS1.SSS1.p8.2.m2.1.1.1" xref="S6.SS1.SSS1.p8.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p8.2.m2.1b"><apply id="S6.SS1.SSS1.p8.2.m2.1.1.cmml" xref="S6.SS1.SSS1.p8.2.m2.1.1"><csymbol cd="latexml" id="S6.SS1.SSS1.p8.2.m2.1.1.1.cmml" xref="S6.SS1.SSS1.p8.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S6.SS1.SSS1.p8.2.m2.1.1.2.cmml" xref="S6.SS1.SSS1.p8.2.m2.1.1.2">90</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p8.2.m2.1c">90\%</annotation></semantics></math>; then we observe a severe degradation. Besides the response time, other important metrics are the storage requirement, the total number of parameters, and finally, the FLOPS, representing the capacity of computational calculation required by the network. Table <a href="#S6.T5" title="Table V ‣ VI-A1 Desktop Setup ‣ VI-A Experiment Description ‣ VI Experiments and Results ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> lists the collected results.</p>
</div>
<figure id="S6.T5" class="ltx_table">
<table id="S6.T5.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S6.T5.1.1" class="ltx_tr">
<td id="S6.T5.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S6.T5.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S6.T5.1.1.1.1.1" class="ltx_tr">
<td id="S6.T5.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T5.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Pruning</span></td>
</tr>
<tr id="S6.T5.1.1.1.1.2" class="ltx_tr">
<td id="S6.T5.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T5.1.1.1.1.2.1.1" class="ltx_text ltx_font_bold">Rate (%)</span></td>
</tr>
</table>
</td>
<td id="S6.T5.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S6.T5.1.1.2.1" class="ltx_text ltx_font_bold">FPS</span></td>
<td id="S6.T5.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S6.T5.1.1.3.1" class="ltx_text ltx_font_bold">Flops</span></td>
<td id="S6.T5.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S6.T5.1.1.4.1" class="ltx_text">
<span id="S6.T5.1.1.4.1.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T5.1.1.4.1.1.1" class="ltx_tr">
<span id="S6.T5.1.1.4.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T5.1.1.4.1.1.1.1.1" class="ltx_text ltx_font_bold">Size</span></span></span>
<span id="S6.T5.1.1.4.1.1.2" class="ltx_tr">
<span id="S6.T5.1.1.4.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T5.1.1.4.1.1.2.1.1" class="ltx_text ltx_font_bold">MB</span></span></span>
</span></span></td>
<td id="S6.T5.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S6.T5.1.1.5.1" class="ltx_text ltx_font_bold">Parameters</span></td>
</tr>
<tr id="S6.T5.1.2" class="ltx_tr">
<td id="S6.T5.1.2.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S6.T5.1.3" class="ltx_tr">
<td id="S6.T5.1.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table id="S6.T5.1.3.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S6.T5.1.3.1.1.1" class="ltx_tr">
<td id="S6.T5.1.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T5.1.3.1.1.1.1.1" class="ltx_text ltx_font_bold">Full</span></td>
</tr>
<tr id="S6.T5.1.3.1.1.2" class="ltx_tr">
<td id="S6.T5.1.3.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T5.1.3.1.1.2.1.1" class="ltx_text ltx_font_bold">Model</span></td>
</tr>
</table>
</td>
<td id="S6.T5.1.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.711</td>
<td id="S6.T5.1.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">262674</td>
<td id="S6.T5.1.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.702</td>
<td id="S6.T5.1.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">131705</td>
</tr>
<tr id="S6.T5.1.4" class="ltx_tr">
<td id="S6.T5.1.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T5.1.4.1.1" class="ltx_text ltx_font_bold">10</span></td>
<td id="S6.T5.1.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.129</td>
<td id="S6.T5.1.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">109909</td>
<td id="S6.T5.1.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.343</td>
<td id="S6.T5.1.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">110237</td>
</tr>
<tr id="S6.T5.1.5" class="ltx_tr">
<td id="S6.T5.1.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T5.1.5.1.1" class="ltx_text ltx_font_bold">20</span></td>
<td id="S6.T5.1.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">14.131</td>
<td id="S6.T5.1.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">88084</td>
<td id="S6.T5.1.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.092</td>
<td id="S6.T5.1.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">88375</td>
</tr>
<tr id="S6.T5.1.6" class="ltx_tr">
<td id="S6.T5.1.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T5.1.6.1.1" class="ltx_text ltx_font_bold">30</span></td>
<td id="S6.T5.1.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">13.717</td>
<td id="S6.T5.1.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">67762</td>
<td id="S6.T5.1.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.858</td>
<td id="S6.T5.1.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">68014</td>
</tr>
<tr id="S6.T5.1.7" class="ltx_tr">
<td id="S6.T5.1.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T5.1.7.1.1" class="ltx_text ltx_font_bold">40</span></td>
<td id="S6.T5.1.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">16.057</td>
<td id="S6.T5.1.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">51049</td>
<td id="S6.T5.1.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.668</td>
<td id="S6.T5.1.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">51264</td>
</tr>
<tr id="S6.T5.1.8" class="ltx_tr">
<td id="S6.T5.1.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T5.1.8.1.1" class="ltx_text ltx_font_bold">50</span></td>
<td id="S6.T5.1.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">27.823</td>
<td id="S6.T5.1.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">35011</td>
<td id="S6.T5.1.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.482</td>
<td id="S6.T5.1.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">35185</td>
</tr>
<tr id="S6.T5.1.9" class="ltx_tr">
<td id="S6.T5.1.9.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T5.1.9.1.1" class="ltx_text ltx_font_bold">60</span></td>
<td id="S6.T5.1.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">29.252</td>
<td id="S6.T5.1.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">24067</td>
<td id="S6.T5.1.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.358</td>
<td id="S6.T5.1.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">24209</td>
</tr>
<tr id="S6.T5.1.10" class="ltx_tr">
<td id="S6.T5.1.10.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T5.1.10.1.1" class="ltx_text ltx_font_bold">70</span></td>
<td id="S6.T5.1.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T5.1.10.2.1" class="ltx_text ltx_font_bold">41.901</span></td>
<td id="S6.T5.1.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T5.1.10.3.1" class="ltx_text ltx_font_bold">14563</span></td>
<td id="S6.T5.1.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T5.1.10.4.1" class="ltx_text ltx_font_bold">0.248</span></td>
<td id="S6.T5.1.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T5.1.10.5.1" class="ltx_text ltx_font_bold">14668</span></td>
</tr>
<tr id="S6.T5.1.11" class="ltx_tr">
<td id="S6.T5.1.11.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T5.1.11.1.1" class="ltx_text ltx_font_bold">80</span></td>
<td id="S6.T5.1.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">44.950</td>
<td id="S6.T5.1.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7138</td>
<td id="S6.T5.1.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.162</td>
<td id="S6.T5.1.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7206</td>
</tr>
<tr id="S6.T5.1.12" class="ltx_tr">
<td id="S6.T5.1.12.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T5.1.12.1.1" class="ltx_text ltx_font_bold">90</span></td>
<td id="S6.T5.1.12.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">61.893</td>
<td id="S6.T5.1.12.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">2449</td>
<td id="S6.T5.1.12.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.108</td>
<td id="S6.T5.1.12.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">2480</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table V: </span>Requirements CPU X Pruning Rate.</figcaption>
</figure>
<div id="S6.SS1.SSS1.p9" class="ltx_para">
<p id="S6.SS1.SSS1.p9.9" class="ltx_p">A close examination of Table <a href="#S6.F16" title="Figure 16 ‣ VI-A1 Desktop Setup ‣ VI-A Experiment Description ‣ VI Experiments and Results ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a> shows that the total number of the network parameters fell from <math id="S6.SS1.SSS1.p9.1.m1.1" class="ltx_Math" alttext="131705" display="inline"><semantics id="S6.SS1.SSS1.p9.1.m1.1a"><mn id="S6.SS1.SSS1.p9.1.m1.1.1" xref="S6.SS1.SSS1.p9.1.m1.1.1.cmml">131705</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p9.1.m1.1b"><cn type="integer" id="S6.SS1.SSS1.p9.1.m1.1.1.cmml" xref="S6.SS1.SSS1.p9.1.m1.1.1">131705</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p9.1.m1.1c">131705</annotation></semantics></math> to <math id="S6.SS1.SSS1.p9.2.m2.1" class="ltx_Math" alttext="14667" display="inline"><semantics id="S6.SS1.SSS1.p9.2.m2.1a"><mn id="S6.SS1.SSS1.p9.2.m2.1.1" xref="S6.SS1.SSS1.p9.2.m2.1.1.cmml">14667</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p9.2.m2.1b"><cn type="integer" id="S6.SS1.SSS1.p9.2.m2.1.1.cmml" xref="S6.SS1.SSS1.p9.2.m2.1.1">14667</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p9.2.m2.1c">14667</annotation></semantics></math>, corresponding to an <math id="S6.SS1.SSS1.p9.3.m3.1" class="ltx_Math" alttext="88.86\%" display="inline"><semantics id="S6.SS1.SSS1.p9.3.m3.1a"><mrow id="S6.SS1.SSS1.p9.3.m3.1.1" xref="S6.SS1.SSS1.p9.3.m3.1.1.cmml"><mn id="S6.SS1.SSS1.p9.3.m3.1.1.2" xref="S6.SS1.SSS1.p9.3.m3.1.1.2.cmml">88.86</mn><mo id="S6.SS1.SSS1.p9.3.m3.1.1.1" xref="S6.SS1.SSS1.p9.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p9.3.m3.1b"><apply id="S6.SS1.SSS1.p9.3.m3.1.1.cmml" xref="S6.SS1.SSS1.p9.3.m3.1.1"><csymbol cd="latexml" id="S6.SS1.SSS1.p9.3.m3.1.1.1.cmml" xref="S6.SS1.SSS1.p9.3.m3.1.1.1">percent</csymbol><cn type="float" id="S6.SS1.SSS1.p9.3.m3.1.1.2.cmml" xref="S6.SS1.SSS1.p9.3.m3.1.1.2">88.86</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p9.3.m3.1c">88.86\%</annotation></semantics></math> reduction. Similarly, the storage requirement fell from <math id="S6.SS1.SSS1.p9.4.m4.1" class="ltx_Math" alttext="1.702" display="inline"><semantics id="S6.SS1.SSS1.p9.4.m4.1a"><mn id="S6.SS1.SSS1.p9.4.m4.1.1" xref="S6.SS1.SSS1.p9.4.m4.1.1.cmml">1.702</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p9.4.m4.1b"><cn type="float" id="S6.SS1.SSS1.p9.4.m4.1.1.cmml" xref="S6.SS1.SSS1.p9.4.m4.1.1">1.702</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p9.4.m4.1c">1.702</annotation></semantics></math> to <math id="S6.SS1.SSS1.p9.5.m5.1" class="ltx_Math" alttext="0.248" display="inline"><semantics id="S6.SS1.SSS1.p9.5.m5.1a"><mn id="S6.SS1.SSS1.p9.5.m5.1.1" xref="S6.SS1.SSS1.p9.5.m5.1.1.cmml">0.248</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p9.5.m5.1b"><cn type="float" id="S6.SS1.SSS1.p9.5.m5.1.1.cmml" xref="S6.SS1.SSS1.p9.5.m5.1.1">0.248</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p9.5.m5.1c">0.248</annotation></semantics></math>, showing an <math id="S6.SS1.SSS1.p9.6.m6.1" class="ltx_Math" alttext="85.42\%" display="inline"><semantics id="S6.SS1.SSS1.p9.6.m6.1a"><mrow id="S6.SS1.SSS1.p9.6.m6.1.1" xref="S6.SS1.SSS1.p9.6.m6.1.1.cmml"><mn id="S6.SS1.SSS1.p9.6.m6.1.1.2" xref="S6.SS1.SSS1.p9.6.m6.1.1.2.cmml">85.42</mn><mo id="S6.SS1.SSS1.p9.6.m6.1.1.1" xref="S6.SS1.SSS1.p9.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p9.6.m6.1b"><apply id="S6.SS1.SSS1.p9.6.m6.1.1.cmml" xref="S6.SS1.SSS1.p9.6.m6.1.1"><csymbol cd="latexml" id="S6.SS1.SSS1.p9.6.m6.1.1.1.cmml" xref="S6.SS1.SSS1.p9.6.m6.1.1.1">percent</csymbol><cn type="float" id="S6.SS1.SSS1.p9.6.m6.1.1.2.cmml" xref="S6.SS1.SSS1.p9.6.m6.1.1.2">85.42</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p9.6.m6.1c">85.42\%</annotation></semantics></math> reduction. Processing requirements, reflected by the metric FLOPs, also decreased from <math id="S6.SS1.SSS1.p9.7.m7.1" class="ltx_Math" alttext="262674" display="inline"><semantics id="S6.SS1.SSS1.p9.7.m7.1a"><mn id="S6.SS1.SSS1.p9.7.m7.1.1" xref="S6.SS1.SSS1.p9.7.m7.1.1.cmml">262674</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p9.7.m7.1b"><cn type="integer" id="S6.SS1.SSS1.p9.7.m7.1.1.cmml" xref="S6.SS1.SSS1.p9.7.m7.1.1">262674</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p9.7.m7.1c">262674</annotation></semantics></math> to <math id="S6.SS1.SSS1.p9.8.m8.1" class="ltx_Math" alttext="12563" display="inline"><semantics id="S6.SS1.SSS1.p9.8.m8.1a"><mn id="S6.SS1.SSS1.p9.8.m8.1.1" xref="S6.SS1.SSS1.p9.8.m8.1.1.cmml">12563</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p9.8.m8.1b"><cn type="integer" id="S6.SS1.SSS1.p9.8.m8.1.1.cmml" xref="S6.SS1.SSS1.p9.8.m8.1.1">12563</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p9.8.m8.1c">12563</annotation></semantics></math>, achieving a <math id="S6.SS1.SSS1.p9.9.m9.1" class="ltx_Math" alttext="94.45\%" display="inline"><semantics id="S6.SS1.SSS1.p9.9.m9.1a"><mrow id="S6.SS1.SSS1.p9.9.m9.1.1" xref="S6.SS1.SSS1.p9.9.m9.1.1.cmml"><mn id="S6.SS1.SSS1.p9.9.m9.1.1.2" xref="S6.SS1.SSS1.p9.9.m9.1.1.2.cmml">94.45</mn><mo id="S6.SS1.SSS1.p9.9.m9.1.1.1" xref="S6.SS1.SSS1.p9.9.m9.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p9.9.m9.1b"><apply id="S6.SS1.SSS1.p9.9.m9.1.1.cmml" xref="S6.SS1.SSS1.p9.9.m9.1.1"><csymbol cd="latexml" id="S6.SS1.SSS1.p9.9.m9.1.1.1.cmml" xref="S6.SS1.SSS1.p9.9.m9.1.1.1">percent</csymbol><cn type="float" id="S6.SS1.SSS1.p9.9.m9.1.1.2.cmml" xref="S6.SS1.SSS1.p9.9.m9.1.1.2">94.45</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p9.9.m9.1c">94.45\%</annotation></semantics></math> reduction. For better comprehension, we plot some artifacts on the entire process, and figure <a href="#S6.F17" title="Figure 17 ‣ VI-A1 Desktop Setup ‣ VI-A Experiment Description ‣ VI Experiments and Results ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a> shows its significant steps.</p>
</div>
<figure id="S6.F17" class="ltx_figure"><img src="/html/2205.13272/assets/images/comparation_prune_crop.png" id="S6.F17.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="604" height="359" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>Robotic arm and keypoint segmentation.
We have the robotic arm keypoints on the left column, on the center, we have the robotic arm segmented, and finally, on the right, the keypoints.</figcaption>
</figure>
<div id="S6.SS1.SSS1.p10" class="ltx_para">
<p id="S6.SS1.SSS1.p10.1" class="ltx_p">Figure <a href="#S6.F17" title="Figure 17 ‣ VI-A1 Desktop Setup ‣ VI-A Experiment Description ‣ VI Experiments and Results ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a> compares the outcome before and after the proposed refinements. Sub-figures (a) represents the usage of the full CNN. Understandably, CNN segmentation and keypoints detection results are more precise. After the refinement and clustering, the output of pruning at <math id="S6.SS1.SSS1.p10.1.m1.1" class="ltx_Math" alttext="70\%" display="inline"><semantics id="S6.SS1.SSS1.p10.1.m1.1a"><mrow id="S6.SS1.SSS1.p10.1.m1.1.1" xref="S6.SS1.SSS1.p10.1.m1.1.1.cmml"><mn id="S6.SS1.SSS1.p10.1.m1.1.1.2" xref="S6.SS1.SSS1.p10.1.m1.1.1.2.cmml">70</mn><mo id="S6.SS1.SSS1.p10.1.m1.1.1.1" xref="S6.SS1.SSS1.p10.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p10.1.m1.1b"><apply id="S6.SS1.SSS1.p10.1.m1.1.1.cmml" xref="S6.SS1.SSS1.p10.1.m1.1.1"><csymbol cd="latexml" id="S6.SS1.SSS1.p10.1.m1.1.1.1.cmml" xref="S6.SS1.SSS1.p10.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S6.SS1.SSS1.p10.1.m1.1.1.2.cmml" xref="S6.SS1.SSS1.p10.1.m1.1.1.2">70</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p10.1.m1.1c">70\%</annotation></semantics></math>, shown in sub-figure (b), produces almost the same result as a full model after the post-processing. Figure <a href="#S6.F18" title="Figure 18 ‣ VI-A1 Desktop Setup ‣ VI-A Experiment Description ‣ VI Experiments and Results ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">18</span></a> shows in detail the predicted keypoint difference.</p>
</div>
<figure id="S6.F18" class="ltx_figure"><img src="/html/2205.13272/assets/images/zoom_comparation_prune.png" id="S6.F18.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="419" height="406" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 18: </span>Zoom comparison. Complete CNN versus <math id="S6.F18.3.2.m1.1" class="ltx_Math" alttext="70\%" display="inline"><semantics id="S6.F18.3.2.m1.1b"><mrow id="S6.F18.3.2.m1.1.1" xref="S6.F18.3.2.m1.1.1.cmml"><mn id="S6.F18.3.2.m1.1.1.2" xref="S6.F18.3.2.m1.1.1.2.cmml">70</mn><mo id="S6.F18.3.2.m1.1.1.1" xref="S6.F18.3.2.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.F18.3.2.m1.1c"><apply id="S6.F18.3.2.m1.1.1.cmml" xref="S6.F18.3.2.m1.1.1"><csymbol cd="latexml" id="S6.F18.3.2.m1.1.1.1.cmml" xref="S6.F18.3.2.m1.1.1.1">percent</csymbol><cn type="integer" id="S6.F18.3.2.m1.1.1.2.cmml" xref="S6.F18.3.2.m1.1.1.2">70</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F18.3.2.m1.1d">70\%</annotation></semantics></math> pruning rate.</figcaption>
</figure>
<div id="S6.SS1.SSS1.p11" class="ltx_para">
<p id="S6.SS1.SSS1.p11.1" class="ltx_p">Figure <a href="#S6.F18" title="Figure 18 ‣ VI-A1 Desktop Setup ‣ VI-A Experiment Description ‣ VI Experiments and Results ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">18</span></a> shows that the yellow points correspond to the results obtained when using the full CNN, whereas the blue points represent the results with <math id="S6.SS1.SSS1.p11.1.m1.1" class="ltx_Math" alttext="70\%" display="inline"><semantics id="S6.SS1.SSS1.p11.1.m1.1a"><mrow id="S6.SS1.SSS1.p11.1.m1.1.1" xref="S6.SS1.SSS1.p11.1.m1.1.1.cmml"><mn id="S6.SS1.SSS1.p11.1.m1.1.1.2" xref="S6.SS1.SSS1.p11.1.m1.1.1.2.cmml">70</mn><mo id="S6.SS1.SSS1.p11.1.m1.1.1.1" xref="S6.SS1.SSS1.p11.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p11.1.m1.1b"><apply id="S6.SS1.SSS1.p11.1.m1.1.1.cmml" xref="S6.SS1.SSS1.p11.1.m1.1.1"><csymbol cd="latexml" id="S6.SS1.SSS1.p11.1.m1.1.1.1.cmml" xref="S6.SS1.SSS1.p11.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S6.SS1.SSS1.p11.1.m1.1.1.2.cmml" xref="S6.SS1.SSS1.p11.1.m1.1.1.2">70</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p11.1.m1.1c">70\%</annotation></semantics></math> pruning. As it is noticeable, they are very close, and in some cases, the points are so close that it is impossible to distinguish one from another.</p>
</div>
</section>
<section id="S6.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S6.SS1.SSS2.5.1.1" class="ltx_text">VI-A</span>2 </span>Constrained Device Setup</h4>

<div id="S6.SS1.SSS2.p1" class="ltx_para">
<p id="S6.SS1.SSS2.p1.1" class="ltx_p">As seen in Table <a href="#S6.T2" title="Table II ‣ VI-A Experiment Description ‣ VI Experiments and Results ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, the Constrained Device scenario, with a Raspberry Pi 3, is very restricted and has only 1GB of RAM and a low-power four-core processor. Related to the pruning itself, it keeps the total number of parameters, but it reduces the total of FLOPS on the quantization process. Table <a href="#S6.T6" title="Table VI ‣ VI-A2 Constrained Device Setup ‣ VI-A Experiment Description ‣ VI Experiments and Results ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a> shows these results.</p>
</div>
<figure id="S6.T6" class="ltx_table">
<table id="S6.T6.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S6.T6.1.1" class="ltx_tr">
<td id="S6.T6.1.1.1" class="ltx_td ltx_border_r"></td>
<td id="S6.T6.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S6.T6.1.1.2.1" class="ltx_text ltx_font_bold">PCK@0.5</span></td>
<td id="S6.T6.1.1.3" class="ltx_td" colspan="2"></td>
<td id="S6.T6.1.1.4" class="ltx_td"></td>
</tr>
<tr id="S6.T6.1.2" class="ltx_tr">
<td id="S6.T6.1.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T6.1.2.1.1" class="ltx_text ltx_font_bold">Pruning Rate</span></td>
<td id="S6.T6.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T6.1.2.2.1" class="ltx_text ltx_font_bold">Average</span></td>
<td id="S6.T6.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T6.1.2.3.1" class="ltx_text ltx_font_bold">STD</span></td>
<td id="S6.T6.1.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T6.1.2.4.1" class="ltx_text ltx_font_bold">FPS</span></td>
<td id="S6.T6.1.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S6.T6.1.2.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S6.T6.1.2.5.1.1" class="ltx_tr">
<td id="S6.T6.1.2.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T6.1.2.5.1.1.1.1" class="ltx_text ltx_font_bold">Inference</span></td>
</tr>
<tr id="S6.T6.1.2.5.1.2" class="ltx_tr">
<td id="S6.T6.1.2.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T6.1.2.5.1.2.1.1" class="ltx_text ltx_font_bold">Time</span></td>
</tr>
</table>
</td>
<td id="S6.T6.1.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S6.T6.1.2.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S6.T6.1.2.6.1.1" class="ltx_tr">
<td id="S6.T6.1.2.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T6.1.2.6.1.1.1.1" class="ltx_text ltx_font_bold">Size</span></td>
</tr>
<tr id="S6.T6.1.2.6.1.2" class="ltx_tr">
<td id="S6.T6.1.2.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T6.1.2.6.1.2.1.1" class="ltx_text ltx_font_bold">MB</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S6.T6.1.3" class="ltx_tr">
<td id="S6.T6.1.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T6.1.3.1.1" class="ltx_text ltx_font_bold">10</span></td>
<td id="S6.T6.1.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.998</td>
<td id="S6.T6.1.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.004</td>
<td id="S6.T6.1.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.860</td>
<td id="S6.T6.1.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.002</td>
<td id="S6.T6.1.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.437</td>
</tr>
<tr id="S6.T6.1.4" class="ltx_tr">
<td id="S6.T6.1.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T6.1.4.1.1" class="ltx_text ltx_font_bold">20</span></td>
<td id="S6.T6.1.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.978</td>
<td id="S6.T6.1.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.027</td>
<td id="S6.T6.1.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.098</td>
<td id="S6.T6.1.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.001</td>
<td id="S6.T6.1.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.351</td>
</tr>
<tr id="S6.T6.1.5" class="ltx_tr">
<td id="S6.T6.1.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T6.1.5.1.1" class="ltx_text ltx_font_bold">30</span></td>
<td id="S6.T6.1.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.975</td>
<td id="S6.T6.1.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.036</td>
<td id="S6.T6.1.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.814</td>
<td id="S6.T6.1.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.001</td>
<td id="S6.T6.1.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.272</td>
</tr>
<tr id="S6.T6.1.6" class="ltx_tr">
<td id="S6.T6.1.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T6.1.6.1.1" class="ltx_text ltx_font_bold">40</span></td>
<td id="S6.T6.1.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.990</td>
<td id="S6.T6.1.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.004</td>
<td id="S6.T6.1.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.299</td>
<td id="S6.T6.1.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.000</td>
<td id="S6.T6.1.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.206</td>
</tr>
<tr id="S6.T6.1.7" class="ltx_tr">
<td id="S6.T6.1.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T6.1.7.1.1" class="ltx_text ltx_font_bold">50</span></td>
<td id="S6.T6.1.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.982</td>
<td id="S6.T6.1.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.007</td>
<td id="S6.T6.1.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.684</td>
<td id="S6.T6.1.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.001</td>
<td id="S6.T6.1.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.143</td>
</tr>
<tr id="S6.T6.1.8" class="ltx_tr">
<td id="S6.T6.1.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T6.1.8.1.1" class="ltx_text ltx_font_bold">60</span></td>
<td id="S6.T6.1.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.974</td>
<td id="S6.T6.1.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.012</td>
<td id="S6.T6.1.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7.764</td>
<td id="S6.T6.1.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.000</td>
<td id="S6.T6.1.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.101</td>
</tr>
<tr id="S6.T6.1.9" class="ltx_tr">
<td id="S6.T6.1.9.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T6.1.9.1.1" class="ltx_text ltx_font_bold">70</span></td>
<td id="S6.T6.1.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T6.1.9.2.1" class="ltx_text ltx_font_bold">0.969</span></td>
<td id="S6.T6.1.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T6.1.9.3.1" class="ltx_text ltx_font_bold">0.009</span></td>
<td id="S6.T6.1.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T6.1.9.4.1" class="ltx_text ltx_font_bold">10.040</span></td>
<td id="S6.T6.1.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T6.1.9.5.1" class="ltx_text ltx_font_bold">0.001</span></td>
<td id="S6.T6.1.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T6.1.9.6.1" class="ltx_text ltx_font_bold">0.063</span></td>
</tr>
<tr id="S6.T6.1.10" class="ltx_tr">
<td id="S6.T6.1.10.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T6.1.10.1.1" class="ltx_text ltx_font_bold">80</span></td>
<td id="S6.T6.1.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T6.1.10.2.1" class="ltx_text ltx_font_bold">0.911</span></td>
<td id="S6.T6.1.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T6.1.10.3.1" class="ltx_text ltx_font_bold">0.051</span></td>
<td id="S6.T6.1.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T6.1.10.4.1" class="ltx_text ltx_font_bold">15.873</span></td>
<td id="S6.T6.1.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T6.1.10.5.1" class="ltx_text ltx_font_bold">0.001</span></td>
<td id="S6.T6.1.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T6.1.10.6.1" class="ltx_text ltx_font_bold">0.034</span></td>
</tr>
<tr id="S6.T6.1.11" class="ltx_tr">
<td id="S6.T6.1.11.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T6.1.11.1.1" class="ltx_text ltx_font_bold">90</span></td>
<td id="S6.T6.1.11.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.087</td>
<td id="S6.T6.1.11.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.092</td>
<td id="S6.T6.1.11.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">27.778</td>
<td id="S6.T6.1.11.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.002</td>
<td id="S6.T6.1.11.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.016</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table VI: </span>Performance Results for Pruning and Quantization over a RaspberryPi 3.</figcaption>
</figure>
<div id="S6.SS1.SSS2.p2" class="ltx_para">
<p id="S6.SS1.SSS2.p2.3" class="ltx_p">Table <a href="#S6.T6" title="Table VI ‣ VI-A2 Constrained Device Setup ‣ VI-A Experiment Description ‣ VI Experiments and Results ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a> shows that the Raspberry could process 10 FPS at a <math id="S6.SS1.SSS2.p2.1.m1.1" class="ltx_Math" alttext="70\%" display="inline"><semantics id="S6.SS1.SSS2.p2.1.m1.1a"><mrow id="S6.SS1.SSS2.p2.1.m1.1.1" xref="S6.SS1.SSS2.p2.1.m1.1.1.cmml"><mn id="S6.SS1.SSS2.p2.1.m1.1.1.2" xref="S6.SS1.SSS2.p2.1.m1.1.1.2.cmml">70</mn><mo id="S6.SS1.SSS2.p2.1.m1.1.1.1" xref="S6.SS1.SSS2.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS2.p2.1.m1.1b"><apply id="S6.SS1.SSS2.p2.1.m1.1.1.cmml" xref="S6.SS1.SSS2.p2.1.m1.1.1"><csymbol cd="latexml" id="S6.SS1.SSS2.p2.1.m1.1.1.1.cmml" xref="S6.SS1.SSS2.p2.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S6.SS1.SSS2.p2.1.m1.1.1.2.cmml" xref="S6.SS1.SSS2.p2.1.m1.1.1.2">70</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS2.p2.1.m1.1c">70\%</annotation></semantics></math> prune rate while keeping a PCK above <math id="S6.SS1.SSS2.p2.2.m2.1" class="ltx_Math" alttext="0.96" display="inline"><semantics id="S6.SS1.SSS2.p2.2.m2.1a"><mn id="S6.SS1.SSS2.p2.2.m2.1.1" xref="S6.SS1.SSS2.p2.2.m2.1.1.cmml">0.96</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS2.p2.2.m2.1b"><cn type="float" id="S6.SS1.SSS2.p2.2.m2.1.1.cmml" xref="S6.SS1.SSS2.p2.2.m2.1.1">0.96</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS2.p2.2.m2.1c">0.96</annotation></semantics></math>. Another important metric is the disc storage requirement, where at <math id="S6.SS1.SSS2.p2.3.m3.1" class="ltx_Math" alttext="70\%" display="inline"><semantics id="S6.SS1.SSS2.p2.3.m3.1a"><mrow id="S6.SS1.SSS2.p2.3.m3.1.1" xref="S6.SS1.SSS2.p2.3.m3.1.1.cmml"><mn id="S6.SS1.SSS2.p2.3.m3.1.1.2" xref="S6.SS1.SSS2.p2.3.m3.1.1.2.cmml">70</mn><mo id="S6.SS1.SSS2.p2.3.m3.1.1.1" xref="S6.SS1.SSS2.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS2.p2.3.m3.1b"><apply id="S6.SS1.SSS2.p2.3.m3.1.1.cmml" xref="S6.SS1.SSS2.p2.3.m3.1.1"><csymbol cd="latexml" id="S6.SS1.SSS2.p2.3.m3.1.1.1.cmml" xref="S6.SS1.SSS2.p2.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S6.SS1.SSS2.p2.3.m3.1.1.2.cmml" xref="S6.SS1.SSS2.p2.3.m3.1.1.2">70</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS2.p2.3.m3.1c">70\%</annotation></semantics></math>, we only require 0.063 MB. Figure <a href="#S6.F19" title="Figure 19 ‣ VI-A2 Constrained Device Setup ‣ VI-A Experiment Description ‣ VI Experiments and Results ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">19</span></a> shows the corresponding performance curve.</p>
</div>
<figure id="S6.F19" class="ltx_figure"><img src="/html/2205.13272/assets/images/arm_pck.png" id="S6.F19.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="232" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 19: </span>Raspberry Pi3 performance curve. PCK versus FPS.</figcaption>
</figure>
<div id="S6.SS1.SSS2.p3" class="ltx_para">
<p id="S6.SS1.SSS2.p3.2" class="ltx_p">According to Figure <a href="#S6.F19" title="Figure 19 ‣ VI-A2 Constrained Device Setup ‣ VI-A Experiment Description ‣ VI Experiments and Results ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">19</span></a>, we maintain the same behavior as when using the full CNN to a <math id="S6.SS1.SSS2.p3.1.m1.1" class="ltx_Math" alttext="70\%" display="inline"><semantics id="S6.SS1.SSS2.p3.1.m1.1a"><mrow id="S6.SS1.SSS2.p3.1.m1.1.1" xref="S6.SS1.SSS2.p3.1.m1.1.1.cmml"><mn id="S6.SS1.SSS2.p3.1.m1.1.1.2" xref="S6.SS1.SSS2.p3.1.m1.1.1.2.cmml">70</mn><mo id="S6.SS1.SSS2.p3.1.m1.1.1.1" xref="S6.SS1.SSS2.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS2.p3.1.m1.1b"><apply id="S6.SS1.SSS2.p3.1.m1.1.1.cmml" xref="S6.SS1.SSS2.p3.1.m1.1.1"><csymbol cd="latexml" id="S6.SS1.SSS2.p3.1.m1.1.1.1.cmml" xref="S6.SS1.SSS2.p3.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S6.SS1.SSS2.p3.1.m1.1.1.2.cmml" xref="S6.SS1.SSS2.p3.1.m1.1.1.2">70</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS2.p3.1.m1.1c">70\%</annotation></semantics></math> pruning rate. A performance loss is observed with the <math id="S6.SS1.SSS2.p3.2.m2.1" class="ltx_Math" alttext="80\%" display="inline"><semantics id="S6.SS1.SSS2.p3.2.m2.1a"><mrow id="S6.SS1.SSS2.p3.2.m2.1.1" xref="S6.SS1.SSS2.p3.2.m2.1.1.cmml"><mn id="S6.SS1.SSS2.p3.2.m2.1.1.2" xref="S6.SS1.SSS2.p3.2.m2.1.1.2.cmml">80</mn><mo id="S6.SS1.SSS2.p3.2.m2.1.1.1" xref="S6.SS1.SSS2.p3.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS2.p3.2.m2.1b"><apply id="S6.SS1.SSS2.p3.2.m2.1.1.cmml" xref="S6.SS1.SSS2.p3.2.m2.1.1"><csymbol cd="latexml" id="S6.SS1.SSS2.p3.2.m2.1.1.1.cmml" xref="S6.SS1.SSS2.p3.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S6.SS1.SSS2.p3.2.m2.1.1.2.cmml" xref="S6.SS1.SSS2.p3.2.m2.1.1.2">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS2.p3.2.m2.1c">80\%</annotation></semantics></math> pruning rate. As a final step, we added a qualitative evaluation of keypoint detection, see Figure <a href="#S6.F20" title="Figure 20 ‣ VI-A2 Constrained Device Setup ‣ VI-A Experiment Description ‣ VI Experiments and Results ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">20</span></a> for a view of those samples.</p>
</div>
<figure id="S6.F20" class="ltx_figure"><img src="/html/2205.13272/assets/images/seg_arm_crop.png" id="S6.F20.1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="299" height="922" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 20: </span>The full process of keypoint detection. (a) Input image, (b) segmented arm, (c) and generated keypoints, and (d) a comparison between the real and predicted keypoints.</figcaption>
</figure>
<div id="S6.SS1.SSS2.p4" class="ltx_para">
<p id="S6.SS1.SSS2.p4.1" class="ltx_p">Figure <a href="#S6.F20" title="Figure 20 ‣ VI-A2 Constrained Device Setup ‣ VI-A Experiment Description ‣ VI Experiments and Results ‣ FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">20</span></a>(d) presents some promising results: blue depicts the real keypoint, and red the ones. Observe that it is difficult to even distinguish between the produced real and predicted keypoints in some cases.</p>
</div>
</section>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">The pose estimation process is usually heavy-duty and often demands robust systems with high computational power. Few approaches deal with pose estimation while trying to reduce a type of demand. In this work, besides proposing a new CNN for pose detection, we also aim to improve the overall response time of the model and its disk storage demands.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">We first generated a new CNN architecture for pose estimation (FCN-Pose) based on segmentation to achieve this goal. Then we proceed to the CNN compression, with pruning and quantization. We perform a combination of both techniques, and evaluate his results with considerable results. Additionally, we evaluate our proposed solution in a real constrained device, expanding the overall understatement on application, deploys, and usability of those solutions.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.6" class="ltx_p">The FCN-Pose reached a PCK above <math id="S7.p3.1.m1.1" class="ltx_Math" alttext="0.99\%" display="inline"><semantics id="S7.p3.1.m1.1a"><mrow id="S7.p3.1.m1.1.1" xref="S7.p3.1.m1.1.1.cmml"><mn id="S7.p3.1.m1.1.1.2" xref="S7.p3.1.m1.1.1.2.cmml">0.99</mn><mo id="S7.p3.1.m1.1.1.1" xref="S7.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.p3.1.m1.1b"><apply id="S7.p3.1.m1.1.1.cmml" xref="S7.p3.1.m1.1.1"><csymbol cd="latexml" id="S7.p3.1.m1.1.1.1.cmml" xref="S7.p3.1.m1.1.1.1">percent</csymbol><cn type="float" id="S7.p3.1.m1.1.1.2.cmml" xref="S7.p3.1.m1.1.1.2">0.99</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p3.1.m1.1c">0.99\%</annotation></semantics></math> without pruning and quantization, meaning that we keep a decision region close enough in <math id="S7.p3.2.m2.1" class="ltx_Math" alttext="99\%" display="inline"><semantics id="S7.p3.2.m2.1a"><mrow id="S7.p3.2.m2.1.1" xref="S7.p3.2.m2.1.1.cmml"><mn id="S7.p3.2.m2.1.1.2" xref="S7.p3.2.m2.1.1.2.cmml">99</mn><mo id="S7.p3.2.m2.1.1.1" xref="S7.p3.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.p3.2.m2.1b"><apply id="S7.p3.2.m2.1.1.cmml" xref="S7.p3.2.m2.1.1"><csymbol cd="latexml" id="S7.p3.2.m2.1.1.1.cmml" xref="S7.p3.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S7.p3.2.m2.1.1.2.cmml" xref="S7.p3.2.m2.1.1.2">99</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p3.2.m2.1c">99\%</annotation></semantics></math> of the cases even with our error, while the prune at <math id="S7.p3.3.m3.1" class="ltx_Math" alttext="70\%" display="inline"><semantics id="S7.p3.3.m3.1a"><mrow id="S7.p3.3.m3.1.1" xref="S7.p3.3.m3.1.1.cmml"><mn id="S7.p3.3.m3.1.1.2" xref="S7.p3.3.m3.1.1.2.cmml">70</mn><mo id="S7.p3.3.m3.1.1.1" xref="S7.p3.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.p3.3.m3.1b"><apply id="S7.p3.3.m3.1.1.cmml" xref="S7.p3.3.m3.1.1"><csymbol cd="latexml" id="S7.p3.3.m3.1.1.1.cmml" xref="S7.p3.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S7.p3.3.m3.1.1.2.cmml" xref="S7.p3.3.m3.1.1.2">70</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p3.3.m3.1c">70\%</annotation></semantics></math> only drops <math id="S7.p3.4.m4.1" class="ltx_Math" alttext="1\%" display="inline"><semantics id="S7.p3.4.m4.1a"><mrow id="S7.p3.4.m4.1.1" xref="S7.p3.4.m4.1.1.cmml"><mn id="S7.p3.4.m4.1.1.2" xref="S7.p3.4.m4.1.1.2.cmml">1</mn><mo id="S7.p3.4.m4.1.1.1" xref="S7.p3.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.p3.4.m4.1b"><apply id="S7.p3.4.m4.1.1.cmml" xref="S7.p3.4.m4.1.1"><csymbol cd="latexml" id="S7.p3.4.m4.1.1.1.cmml" xref="S7.p3.4.m4.1.1.1">percent</csymbol><cn type="integer" id="S7.p3.4.m4.1.1.2.cmml" xref="S7.p3.4.m4.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p3.4.m4.1c">1\%</annotation></semantics></math> in PCK with <math id="S7.p3.5.m5.1" class="ltx_Math" alttext="0.98\%" display="inline"><semantics id="S7.p3.5.m5.1a"><mrow id="S7.p3.5.m5.1.1" xref="S7.p3.5.m5.1.1.cmml"><mn id="S7.p3.5.m5.1.1.2" xref="S7.p3.5.m5.1.1.2.cmml">0.98</mn><mo id="S7.p3.5.m5.1.1.1" xref="S7.p3.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.p3.5.m5.1b"><apply id="S7.p3.5.m5.1.1.cmml" xref="S7.p3.5.m5.1.1"><csymbol cd="latexml" id="S7.p3.5.m5.1.1.1.cmml" xref="S7.p3.5.m5.1.1.1">percent</csymbol><cn type="float" id="S7.p3.5.m5.1.1.2.cmml" xref="S7.p3.5.m5.1.1.2">0.98</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p3.5.m5.1c">0.98\%</annotation></semantics></math>. We jump from 11.71 FPS to 41.90 FPS on the Desktop setup with <math id="S7.p3.6.m6.1" class="ltx_Math" alttext="70\%" display="inline"><semantics id="S7.p3.6.m6.1a"><mrow id="S7.p3.6.m6.1.1" xref="S7.p3.6.m6.1.1.cmml"><mn id="S7.p3.6.m6.1.1.2" xref="S7.p3.6.m6.1.1.2.cmml">70</mn><mo id="S7.p3.6.m6.1.1.1" xref="S7.p3.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.p3.6.m6.1b"><apply id="S7.p3.6.m6.1.1.cmml" xref="S7.p3.6.m6.1.1"><csymbol cd="latexml" id="S7.p3.6.m6.1.1.1.cmml" xref="S7.p3.6.m6.1.1.1">percent</csymbol><cn type="integer" id="S7.p3.6.m6.1.1.2.cmml" xref="S7.p3.6.m6.1.1.2">70</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p3.6.m6.1c">70\%</annotation></semantics></math> pruning. And finally, we the disc storage requirement from 1.7 MB to just 0.248 MB.</p>
</div>
<div id="S7.p4" class="ltx_para">
<p id="S7.p4.2" class="ltx_p">On the Constrained Device Setup, with the FCN-Pose, pruned and quantized, we could get a PCK of <math id="S7.p4.1.m1.1" class="ltx_Math" alttext="0.96\%" display="inline"><semantics id="S7.p4.1.m1.1a"><mrow id="S7.p4.1.m1.1.1" xref="S7.p4.1.m1.1.1.cmml"><mn id="S7.p4.1.m1.1.1.2" xref="S7.p4.1.m1.1.1.2.cmml">0.96</mn><mo id="S7.p4.1.m1.1.1.1" xref="S7.p4.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.p4.1.m1.1b"><apply id="S7.p4.1.m1.1.1.cmml" xref="S7.p4.1.m1.1.1"><csymbol cd="latexml" id="S7.p4.1.m1.1.1.1.cmml" xref="S7.p4.1.m1.1.1.1">percent</csymbol><cn type="float" id="S7.p4.1.m1.1.1.2.cmml" xref="S7.p4.1.m1.1.1.2">0.96</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p4.1.m1.1c">0.96\%</annotation></semantics></math> on <math id="S7.p4.2.m2.1" class="ltx_Math" alttext="70\%" display="inline"><semantics id="S7.p4.2.m2.1a"><mrow id="S7.p4.2.m2.1.1" xref="S7.p4.2.m2.1.1.cmml"><mn id="S7.p4.2.m2.1.1.2" xref="S7.p4.2.m2.1.1.2.cmml">70</mn><mo id="S7.p4.2.m2.1.1.1" xref="S7.p4.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.p4.2.m2.1b"><apply id="S7.p4.2.m2.1.1.cmml" xref="S7.p4.2.m2.1.1"><csymbol cd="latexml" id="S7.p4.2.m2.1.1.1.cmml" xref="S7.p4.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S7.p4.2.m2.1.1.2.cmml" xref="S7.p4.2.m2.1.1.2">70</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p4.2.m2.1c">70\%</annotation></semantics></math>, allowing us to run the FCN-Pose, from 2.8 FPS to 10.04 FPS, requiring only 0.063 MB on disc storage.</p>
</div>
<div id="S7.p5" class="ltx_para">
<p id="S7.p5.2" class="ltx_p">It is valid to notice that all those values of pruned were acquired on the performance curve, but may a PCK smaller can be applicable in some cases. For instance, on the Constrained Device, a prune of <math id="S7.p5.1.m1.1" class="ltx_Math" alttext="80\%" display="inline"><semantics id="S7.p5.1.m1.1a"><mrow id="S7.p5.1.m1.1.1" xref="S7.p5.1.m1.1.1.cmml"><mn id="S7.p5.1.m1.1.1.2" xref="S7.p5.1.m1.1.1.2.cmml">80</mn><mo id="S7.p5.1.m1.1.1.1" xref="S7.p5.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.p5.1.m1.1b"><apply id="S7.p5.1.m1.1.1.cmml" xref="S7.p5.1.m1.1.1"><csymbol cd="latexml" id="S7.p5.1.m1.1.1.1.cmml" xref="S7.p5.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S7.p5.1.m1.1.1.2.cmml" xref="S7.p5.1.m1.1.1.2">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p5.1.m1.1c">80\%</annotation></semantics></math> returns a PCK of <math id="S7.p5.2.m2.1" class="ltx_Math" alttext="0.91\%" display="inline"><semantics id="S7.p5.2.m2.1a"><mrow id="S7.p5.2.m2.1.1" xref="S7.p5.2.m2.1.1.cmml"><mn id="S7.p5.2.m2.1.1.2" xref="S7.p5.2.m2.1.1.2.cmml">0.91</mn><mo id="S7.p5.2.m2.1.1.1" xref="S7.p5.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.p5.2.m2.1b"><apply id="S7.p5.2.m2.1.1.cmml" xref="S7.p5.2.m2.1.1"><csymbol cd="latexml" id="S7.p5.2.m2.1.1.1.cmml" xref="S7.p5.2.m2.1.1.1">percent</csymbol><cn type="float" id="S7.p5.2.m2.1.1.2.cmml" xref="S7.p5.2.m2.1.1.2">0.91</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p5.2.m2.1c">0.91\%</annotation></semantics></math> but increases our FPS to 15.87.</p>
</div>
<div id="S7.p6" class="ltx_para">
<p id="S7.p6.1" class="ltx_p">Finally, for future works, our solution is applicable on constrained devices, but we could deploy them in distributed networks, as nodes, for instance. Another important point is that our solution was designed for robot pose estimation with the HRI. In the current stage, we can improve this tool by adding the human pose estimation as a factor and trying to make a decision based on both inputs.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
M. Javaid, A. Haleem, R. P. Singh, and R. Suman, “Substantial capabilities of
robotics in enhancing industry 4.0 implementation,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Cognitive
Robotics</em>, vol. 1, pp. 58–75, 2021. [Online]. Available:
<a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S2667241321000057" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.sciencedirect.com/science/article/pii/S2667241321000057</a>

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
V. Villani, F. Pini, F. Leali, C. Secchi, and C. Fantuzzi, “Survey on
human-robot interaction for robot programming in industrial applications,”
<em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">IFAC-PapersOnLine</em>, vol. 51, no. 11, pp. 66–71, 2018, 16th IFAC
Symposium on Information Control Problems in Manufacturing INCOM 2018.
[Online]. Available:
<a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S2405896318313600" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.sciencedirect.com/science/article/pii/S2405896318313600</a>

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
P. A. Lasota, T. Song, and J. A. Shah, 2017.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
C. Escobedo, M. Strong, M. West, A. Aramburu, and A. Roncone, “Contact
anticipation for physical human-robot interaction with robotic manipulators
using onboard proximity sensors,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2110.11516, 2021.
[Online]. Available: <a target="_blank" href="https://arxiv.org/abs/2110.11516" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2110.11516</a>

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
C. Dhawale, K. Dhawale, and R. Dubey, <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">A Review on Deep Learning
Applications</em>, 01 2020, pp. 21–31.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, “Language
models are unsupervised multitask learners,” 2019.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based
learning applied to document recognition,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE</em>,
vol. 86, no. 11, pp. 2278–2324, 1998.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A. Krizhevsky, I. Sutskever, and G. Hinton, “Imagenet classification with deep
convolutional neural networks,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Neural Information Processing
Systems</em>, vol. 25, 01 2012.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
R. Mishra, H. P. Gupta, and T. Dutta, “A survey on deep neural network
compression: Challenges, overview, and solutions,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2010.03954, 2020. [Online]. Available:
<a target="_blank" href="https://arxiv.org/abs/2010.03954" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2010.03954</a>

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
B. Singh, M. Najibi, and L. S. Davis, “Sniper: Efficient multi-scale
training,” in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2018.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Z. Zhang, T. He, H. Zhang, Z. Zhang, J. Xie, and M. Li, “Bag of freebies for
training object detection neural networks,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, vol.
abs/1902.04103, 2019.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-decoder
with atrous separable convolution for semantic image segmentation,” in
<em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Computer Vision – ECCV 2018</em>, V. Ferrari, M. Hebert, C. Sminchisescu,
and Y. Weiss, Eds.   Cham: Springer
International Publishing, 2018, pp. 833–851.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
H. Zhang, H. Zhang, C. Wang, and J. Xie, “Co-occurrent features in semantic
segmentation,” in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR)</em>, June 2019.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Q. Xie, M.-T. Luong, E. Hovy, and Q. V. Le, “Self-training with noisy student
improves imagenet classification,” in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR)</em>, June 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
A. I. Kolesnikov, L. Beyer, X. Zhai, J. Puigcerver, J. Yung, S. Gelly, and
N. Houlsby, “Big transfer (bit): General visual representation learning,”
<em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv: Computer Vision and Pattern Recognition</em>, 2019.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
S. A. Janowsky, “Pruning versus clipping in neural networks,” <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Physical
Review A</em>, vol. 39, no. 12, pp. 6600–6603, Jun. 1989. [Online]. Available:
<a target="_blank" href="https://doi.org/10.1103/physreva.39.6600" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1103/physreva.39.6600</a>

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Sietsma and Dow, “Neural net pruning-why and how,” in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">IEEE 1988
International Conference on Neural Networks</em>, 1988, pp. 325–333 vol.1.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
E. D. Karnin, “A simple procedure for pruning back-propagation trained
neural networks,” <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks</em>, vol. 1,
no. 2, pp. 239–242, 1990.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Y. Choi, M. El-Khamy, and J. Lee, “Jointly sparse convolutional neural
networks in dual spatial-winograd domains,” 05 2019, pp. 2792–2796.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
S. Alford, R. A. Robinett, L. Milechin, and J. Kepner, “Pruned and
structurally sparse neural networks,” <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, vol. abs/1810.00299,
2018.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
X. Ding, G. Ding, X. Zhou, Y. Guo, J. Han, and J. Liu, “Global sparse momentum
SGD for pruning very deep neural networks,” in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Advances in Neural
Information Processing Systems 32: Annual Conference on Neural Information
Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC,
Canada</em>, H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alch’e-Buc,
E. B. Fox, and R. Garnett, Eds., 2019, pp. 6379–6391. [Online]. Available:
<a target="_blank" href="http://papers.nips.cc/paper/8867-global-sparse-momentum-sgd-for-pruning-very-deep-neural-networks" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://papers.nips.cc/paper/8867-global-sparse-momentum-sgd-for-pruning-very-deep-neural-networks</a>

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
J. Wu, C. Leng, Y. Wang, Q. Hu, and J. Cheng, “Quantized convolutional neural
networks for mobile devices,” 06 2016, pp. 4820–4828.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
V. Vanhoucke, A. Senior, and M. Z. Mao, “Improving the speed of neural
networks on cpus,” in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Deep Learning and Unsupervised Feature Learning
Workshop, NIPS 2011</em>, 2011.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
S. Gupta, A. Agrawal, K. Gopalakrishnan, and P. Narayanan, “Deep learning with
limited numerical precision,” <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, vol. abs/1502.02551, 2015.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Y. Choukroun, E. Kravchik, F. Yang, and P. Kisilev, “Low-bit
quantization of neural networks for efficient inference,” in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">2019
IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</em>, 2019,
pp. 3009–3018.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
G. Yuan, X. Ma, C. Ding, S. Lin, T. Zhang, Z. S. Jalali, Y. Zhao,
L. Jiang, S. Soundarajan, and Y. Wang, “An ultra-efficient
memristor-based dnn framework with structured weight pruning and quantization
using admm,” in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">2019 IEEE/ACM International Symposium on Low Power
Electronics and Design (ISLPED)</em>, 2019, pp. 1–6.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
K. Paupamah, S. James, and R. Klein, “Quantisation and pruning for
neural network compression and regularisation,” in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">2020 International
SAUPEC/RobMech/PRASA Conference</em>, 2020, pp. 1–6.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
F. Tung and G. Mori, “Clip-q: Deep network compression learning by
in-parallel pruning-quantization,” in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">2018 IEEE/CVF Conference on
Computer Vision and Pattern Recognition</em>, 2018, pp. 7873–7882.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Q. Ding, R. Zhang, Y. Jiang, D. Zhai, and B. Li, “Regularized
training framework for combining pruning and quantization to compress neural
networks,” in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">2019 11th International Conference on Wireless
Communications and Signal Processing (WCSP)</em>, 2019, pp. 1–6.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
K. Patel and S. M. Patel, “Internet of things-iot : Definition ,
characteristics , architecture , enabling technologies , application and
future challenges,” 2016.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
S. Yao, Y. Zhao, A. Zhang, L. Su, and T. Abdelzaher, “Deepiot: Compressing
deep neural network structures for sensing systems with a compressor-critic
framework,” in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 15th ACM Conference on Embedded
Network Sensor Systems</em>, ser. SenSys ’17.   New York, NY, USA: Association for Computing Machinery, 2017.
[Online]. Available: <a target="_blank" href="https://doi.org/10.1145/3131672.3131675" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3131672.3131675</a>

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
S. Yao, Y. Zhao, H. Shao, A. Zhang, C. Zhang, S. Li, and T. Abdelzaher,
“Rdeepsense: Reliable deep mobile computing models with uncertainty
estimations,” <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.</em>,
vol. 1, no. 4, Jan. 2018. [Online]. Available:
<a target="_blank" href="https://doi.org/10.1145/3161181" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3161181</a>

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
S. Bhattacharya and N. D. Lane, “Sparsification and separation of deep
learning layers for constrained resource inference on wearables,” in
<em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 14th ACM Conference on Embedded Network Sensor
Systems CD-ROM</em>, ser. SenSys ’16.   New York, NY, USA: Association for Computing Machinery, 2016, p. 176–189.
[Online]. Available: <a target="_blank" href="https://doi.org/10.1145/2994551.2994564" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/2994551.2994564</a>

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
J. Bohg, J. Romero, A. Herzog, and S. Schaal, “Robot arm pose estimation
through pixel-wise part classification,” in <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">2014 IEEE International
Conference on Robotics and Automation (ICRA)</em>, 2014, pp. 3143–3150.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
T. E. Lee, J. Tremblay, T. To, J. Cheng, T. Mosier, O. Kroemer, D. Fox, and
S. Birchfield, “Camera-to-robot pose estimation from a single image,”
<em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1911.09231, 2019. [Online]. Available:
<a target="_blank" href="http://arxiv.org/abs/1911.09231" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1911.09231</a>

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
K. Cho, S. H. Baeg, and S. Park, “3d pose and target position estimation for a
quadruped walking robot,” in <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">2013 10th International Conference on
Ubiquitous Robots and Ambient Intelligence (URAI)</em>, 2013, pp. 466–467.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
I. R. Rodrigues, G. Barbosa, A. Oliveira Filho, C. Cani, D. H. Sadok,
J. Kelner, R. Souza, M. V. Marquezini, and S. Lins, “A new mechanism for
collision detection in human–robot collaboration using deep learning
techniques,” <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Journal of Control, Automation and Electrical Systems</em>,
vol. 33, no. 2, pp. 406–418, Apr 2022. [Online]. Available:
<a target="_blank" href="https://doi.org/10.1007/s40313-021-00829-3" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s40313-021-00829-3</a>

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
M. Rubagotti, I. Tusseyeva, S. Baltabayeva, D. Summers, and A. Sandygulova,
“Perceived safety in physical human robot interaction - A survey,”
<em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2105.14499, 2021. [Online]. Available:
<a target="_blank" href="https://arxiv.org/abs/2105.14499" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2105.14499</a>

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
A. Bicchi, M. A. Peshkin, and J. E. Colgate, <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Safety for Physical
Human–Robot Interaction</em>.   Berlin,
Heidelberg: Springer Berlin Heidelberg, 2008, pp. 1335–1348. [Online].
Available: <a target="_blank" href="https://doi.org/10.1007/978-3-540-30301-5_58" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/978-3-540-30301-5_58</a>

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
I. R. Silva, G. B. Barbosa, C. C. Ledebour, A. T. Oliveira Filho, J. Kelner,
D. Sadok, S. Lins, and R. Souza, “Assessing deep learning models for
human-robot collaboration collision detection in industrial environments,”
in <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Brazilian Conference on Intelligent Systems</em>.   Springer, 2020, pp. 240–255.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
G. Reis, M. Dantas, D. Bezerra, G. Nunes, P. Dreyer, C. Ledebour, J. Kelner,
D. Sadok, R. Souza, S. Lins <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Gripper design for radio base
station autonomous maintenance system,” <em id="bib.bib41.2.2" class="ltx_emph ltx_font_italic">International Journal of
Automation and Computing</em>, vol. 18, no. 4, pp. 645–653, 2021.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
A. Dutta and A. Zisserman, “The via annotation software for images, audio and
video,” in <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 27th ACM international conference on
multimedia</em>, 2019, pp. 2276–2279.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for
biomedical image segmentation,” <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1505.04597, 2015.
[Online]. Available: <a target="_blank" href="http://arxiv.org/abs/1505.04597" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1505.04597</a>

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand,
M. Andreetto, and H. Adam, “Mobilenets: Efficient convolutional neural
networks for mobile vision applications,” <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1704.04861,
2017. [Online]. Available: <a target="_blank" href="http://arxiv.org/abs/1704.04861" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1704.04861</a>

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for
semantic segmentation,” <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1411.4038, 2014. [Online].
Available: <a target="_blank" href="http://arxiv.org/abs/1411.4038" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1411.4038</a>

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
R. B. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature
hierarchies for accurate object detection and semantic segmentation,”
<em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1311.2524, 2013. [Online]. Available:
<a target="_blank" href="http://arxiv.org/abs/1311.2524" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1311.2524</a>

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
D. Bank, N. Koenigstein, and R. Giryes, “Autoencoders,” <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol.
abs/2003.05991, 2020. [Online]. Available:
<a target="_blank" href="https://arxiv.org/abs/2003.05991" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2003.05991</a>

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">arXiv 1409.1556</em>, 09 2014.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
S. Lloyd, “Least squares quantization in pcm,” <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on
Information Theory</em>, vol. 28, no. 2, pp. 129–137, 1982.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf, “Pruning filters for
efficient convnets,” <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, vol. abs/1608.08710, 2017.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
“Ieee standard for floating-point arithmetic,” <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">IEEE Std 754-2008</em>, pp.
1–70, 2008.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
S. Gupta, A. Agrawal, K. Gopalakrishnan, and P. Narayanan, “Deep learning with
limited numerical precision,” in <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 32nd International
Conference on International Conference on Machine Learning - Volume 37</em>, ser.
ICML’15.   JMLR.org, 2015, p.
1737–1746.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
T. L. Munea, Y. Z. Jembre, H. T. Weldegebriel, L. Chen, C. Huang, and C. Yang,
“The progress of human pose estimation: A survey and taxonomy of models
applied in 2d human pose estimation,” <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol. 8, pp.
133 330–133 348, 2020.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
P. Refaeilzadeh, L. Tang, and H. Liu, <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">Cross-Validation</em>.   Boston, MA: Springer US, 2009, pp. 532–538.
[Online]. Available: <a target="_blank" href="https://doi.org/10.1007/978-0-387-39940-9_565" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/978-0-387-39940-9_565</a>

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2205.13271" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2205.13272" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2205.13272">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2205.13272" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2205.13273" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar 11 13:34:05 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
