<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2205.13994] A framework for robotic arm pose estimation and movement prediction based on deep and extreme learning models</title><meta property="og:description" content="Human-robot collaboration has gained a notable prominence in Industry 4.0, as the use of collaborative robots increases efficiency and productivity in the automation process. However, it is necessary to consider the us‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A framework for robotic arm pose estimation and movement prediction based on deep and extreme learning models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="A framework for robotic arm pose estimation and movement prediction based on deep and extreme learning models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2205.13994">

<!--Generated on Tue Feb 27 05:15:40 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on Received: date / Accepted: date.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Robotic arm pose estimation Movement prediction Deep learning Extreme Learning Machine">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p">‚àé
</p>
</div>
<span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>F. Author </span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>first address 
<br class="ltx_break">Tel.: +123-45-678910
<br class="ltx_break">Fax: +123-45-678910
<br class="ltx_break"><span id="id2.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">email: </span>fauthor@example.com</span></span></span>
</span></span></span><span id="id3" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>S. Author </span></span></span><span id="id4" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_note_type">institutetext: </span>second address
</span></span></span>
<h1 class="ltx_title ltx_title_document">A framework for robotic arm pose estimation and movement prediction based on deep and extreme learning models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Iago Richard Rodrigues
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Marrone Dantas
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Assis Oliveira Filho
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gibson Barbosa
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Daniel Bezerra
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ricardo Souza
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Maria Val√©ria Marquezini
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Patricia Takako Endo
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Judith Kelner
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Djamel H. Sadok
</span></span>
</div>
<div class="ltx_dates">(Received: date / Accepted: date)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Human-robot collaboration has gained a notable prominence in Industry 4.0, as the use of collaborative robots increases efficiency and productivity in the automation process. However, it is necessary to consider the use of mechanisms that increase security in these environments, as the literature reports that risk situations may exist in the context of human-robot collaboration. One of the strategies that can be adopted is the visual recognition of the collaboration environment using machine learning techniques, which can automatically identify what is happening in the scene and what may happen in the future. In this work, we are proposing a new framework that is capable of detecting robotic arm keypoints commonly used in Industry 4.0. In addition to detecting, the proposed framework is able to predict the future movement of these robotic arms, thus providing relevant information that can be considered in the recognition of the human-robot collaboration scenario. The proposed framework is based on deep and extreme learning machine techniques. Results show that the proposed framework is capable of detecting and predicting with low error, contributing to the mitigation of risks in human-robot collaboration.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Robotic arm pose estimation Movement prediction Deep learning Extreme Learning Machine
</div>
<span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_journal"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_type">journal: </span>The Journal of Supercomputing</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The usage of robots to perform human tasks has been increasingly applied in the production process, manufacturing, and other areas of the industrial sector <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib1" title="" class="ltx_ref">goel2020robotics </a></cite>. Robots are also assisting humans in undertaking specific tasks in what is referred to as human-robot collaboration (HRC) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib2" title="" class="ltx_ref">ajoudani2018progress </a></cite>. Currently HRC spans diverse contexts, including manufacturing, homes, offices, and hospitals <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib3" title="" class="ltx_ref">bauer2008human </a></cite>. More significantly, HRC has been introduced in Industry 4.0 manufacturing activities. This triggered interest and set a new trend in the research community seeking to evaluate the impact and ensure efficient use of HRC <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib4" title="" class="ltx_ref">Microsoft:2019trends </a></cite>. Advocates of this technology claim that it benefits the production process by reducing the time to execute some complex activities traditionally allocated to humans only.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The present study looks at HRC in the context of the maintenance of computer network equipment, such as a radio base station (RBS), as the one described in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib5" title="" class="ltx_ref">reis2021gripper </a></cite>. Cellular network operators allocate considerable human and financial resources for the maintenance of RBS equipment. This is a task that requires frequent visits by technicians to geographically distributed sites over small and large urban environments. An Telecom operator must ensure high RBS availability in order to avoid financial loss. The inherent cost of these visits and the time it takes to locally perform maintenance tasks are a concern. This problem is likely to worsen with the introduction of a large number of small cells as planned by the 5G standard <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib6" title="" class="ltx_ref">thors2017time </a></cite>. Collaborative robots (COBOTS), specifically robotic arms, can be successfully deployed jointly with technicians to automate many of the RBS related maintenance tasks. Their collaborative tasks may cover a range of activities including faulty cable verification and their replacement, insertion and removal of cables into and from network device ports, device configuration, among others.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Despite the many apparent advantages, the direct contact between humans with robots can pose some risks. One of these is collision, see <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib7" title="" class="ltx_ref">vasic2013safety </a></cite>. Such events may lead to accidents that can cause irreparable physical damage to humans <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib8" title="" class="ltx_ref">rodrigues2021modeling </a></cite>. Robots are also likely to suffer from collisions, possibly leading to financial losses to owners. There is as a result a need to design, deploy and evaluate new safety mechanisms that govern HRC and ensure its safety. Anticipating and mitigating risks in HRC certainly provides a promising solution to this problem.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Considering the previously identified research gaps, the need for safe HRC and the promising high accuracy of DL models for detection tasks, the present work develops and evaluates a new framework for the accurate detection of robot poses and the prediction of future movement. The proposed framework consists of the following two parts:</p>
</div>
<div id="S1.p5" class="ltx_para">
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Robotic arm pose estimation: relies on re-training a CNN model named SCNet-50-V1-d <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib9" title="" class="ltx_ref">liu2020improving </a></cite> for the pose detection task of the robot through regression. The SCNet-50-V1-d model uses self-calibrated convolutions (SCConvs) and enjoyed a great deal of success in a wide range of application including image classification, object detection and instance segmentation. In this work, we also demonstrate the efficacy of using SCConvs for regression tasks. In addition, we improve pose estimation through the use of the extreme learning machine (ELM) neural network <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib10" title="" class="ltx_ref">huang2006extreme </a></cite>, which when combined with CNN models tends to provide a better learning outcome <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib11" title="" class="ltx_ref">rodrigues2021convolutional </a></cite>.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Robotic arm movement prediction: contemplates the training a long-short term memory (LSTM) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib12" title="" class="ltx_ref">hochreiter1997long </a></cite> and gated recurrent unit (GRU) models <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib13" title="" class="ltx_ref">cho-etal-2014-learning </a></cite> to predict the future movement of a robotic arm.</p>
</div>
</li>
</ol>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The framework is instantiated with several DL models that support pose detection and prediction of future robot movement. These are evaluated and compared in a well-controlled scenario, as defined by Silva et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib14" title="" class="ltx_ref">silva2020assessing </a></cite>. Despite being a well-controlled setting, the adopted testbed includes real equipment such as the UR-5 robotic arm, networking and RBS devices installed in a rack. The scenario also inserts a person responsible for performing collaborative activities with the robotic arm. These activities are recorded through a strategically positioned camera. We also annotate the data to carry out the regression of the robot‚Äôs keypoints and obtain the current robot pose. We compared all the evaluated models using the two performance metrics: mean squared error (MSE) and mean absolute error (MAE) metrics. The obtained result demonstrate the suitability of using the proposed framework as an efficient solution for the detection and prediction of the future movement of a robotic arm while suffering a very low error.</p>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Organization of this article</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">The remainder of this article is organized as follows. Section <a href="#S2" title="2 Related Works ‚Ä£ A framework for robotic arm pose estimation and movement prediction based on deep and extreme learning models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents similar works in the robot keypoint detection field. Section <a href="#S3" title="3 Proposed framework ‚Ä£ A framework for robotic arm pose estimation and movement prediction based on deep and extreme learning models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents the proposed framework and details its two main modules, namely, robotic arm pose estimation and robotic arm movement prediction. Section <a href="#S4" title="4 Materials and methods ‚Ä£ A framework for robotic arm pose estimation and movement prediction based on deep and extreme learning models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> describes the methods adopted in this work for developing the experimental scenario, supported image acquisition process, used DL models, used parameters and metrics, and finally model validation. Section <a href="#S5" title="5 Results and discussion ‚Ä£ A framework for robotic arm pose estimation and movement prediction based on deep and extreme learning models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> explains the obtained results 0along with the experiments performed in this study. Finally, Section <a href="#S6" title="6 Conclusion ‚Ä£ A framework for robotic arm pose estimation and movement prediction based on deep and extreme learning models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> concludes this article and suggests some future research directions.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Across the literature, there are several technologies that can actually contribute towards detecting and mitigating risks in HRC <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib15" title="" class="ltx_ref">robla2017working </a></cite>, <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib16" title="" class="ltx_ref">lasota2017survey </a></cite>, <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib14" title="" class="ltx_ref">silva2020assessing </a></cite>, <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib17" title="" class="ltx_ref">zhang2020recurrent </a></cite>, <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib18" title="" class="ltx_ref">anvaripour2019collision </a></cite>, <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib19" title="" class="ltx_ref">maceira2020recurrent </a></cite>. Among these, we highlight the special role of machine learning (ML), which provides intelligent systems to solve the most diverse engineering problems, including HRC. In particular, we draw the attention to an emerging sub-field of ML in the literature, deep learning (DL) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib20" title="" class="ltx_ref">lecun2015deep </a></cite>. DL has proven to beat the state of the art in solving most common problems involving ML, such as the image classification challenge in the ImageNet database <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib21" title="" class="ltx_ref">deng2009imagenet </a></cite>, <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib22" title="" class="ltx_ref">tan2019efficientnet </a></cite>. Furthermore, DL has gained notable prominence in regression, clustering, among other tasks, through its use of convolutional neural networks (CNN) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib23" title="" class="ltx_ref">liu2017survey </a></cite> and recurrent neural networks (RNN) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib24" title="" class="ltx_ref">lipton2015critical </a></cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">As a result, there are several approaches in the literature that use DL models for risk mitigation in HRC <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib14" title="" class="ltx_ref">silva2020assessing </a></cite>, <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib25" title="" class="ltx_ref">sharkawy2020human </a></cite>, <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib26" title="" class="ltx_ref">park2020learning </a></cite>. Nonetheless, many of the already listed works are limited to carrying out the immediate detection of collision events. They do not predict eminent risk situations before they happen. To reduce damage to workers and agents involved in close collaboration scenarios with robots, one should ideally train new models that predict risk situations and analyze relevant data to predict possible risks in a timely manner and prior to taking place <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib17" title="" class="ltx_ref">zhang2020recurrent </a></cite>, <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib18" title="" class="ltx_ref">anvaripour2019collision </a></cite>, <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib19" title="" class="ltx_ref">maceira2020recurrent </a></cite>.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">The authors are of the view that there are limited contributions that take into consideration the detection of human intention to estimate the likelihood of future risks. Such works suffer from a limited scope as they focus on detecting human movement <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib17" title="" class="ltx_ref">zhang2020recurrent </a></cite>, <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib18" title="" class="ltx_ref">anvaripour2019collision </a></cite>, <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib19" title="" class="ltx_ref">maceira2020recurrent </a></cite> and do not combine this with robot detection and the analysis of its movement. Note however that there are literary works that tackle robotic arms detection in an isolated context. Unlike these, this study develops and evaluates detection mechanisms for robots to assess their proximity, speed, collision risks, future movement, etc., in a HRC scenario. These assessments should provide a more robust solution for risk mitigation in HRC.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Pose estimation (or detection) is widely used in several computer vision applications such as medical assistance, games, and human intention detection. All previously cited applications refer to human pose estimation. Human pose estimation consists of detecting the human body keypoints through regression models. In a similar view, robot pose estimation consists of detecting the robot keypoints through regression models, these keypoints are the joints that compose the robot skeleton. With the advance of deep learning studies, there has been a notable advance in robot pose detection with several CNN applications with different types of robots. Robot pose detection aims to obtain the keypoints which represent the robot joints, thus constituting its pose. Bellow, we present recent papers on robot pose detection using CNNs.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Miseikis et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib27" title="" class="ltx_ref">miseikis2018robot </a></cite> present an approach for detecting robot poses in RGB images. Their method consists of a cascade of two different convolutional networks. The first network performs the segmentation of the robot, with an architecture reminiscent of AlexNet <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib28" title="" class="ltx_ref">krizhevsky2012imagenet </a></cite>. In contrast, the second network is responsible for mapping the robot‚Äôs key points from the mask extracted with the first network. The UR-5 robotic arm pose detection reached an accuracy of 98.1% and outperformed that of UR-3 and UR-10 with 93.1% and 92.8%, respectively.
In a similar research, Mivseikis et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib29" title="" class="ltx_ref">mivseikis2018transfer </a></cite> propose the use of transfer learning to detect various tasks (as <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib27" title="" class="ltx_ref">miseikis2018robot </a></cite>), including the detection of keypoints of another type of robot, the Kuka LBR iiwa. The approach achieved an accuacy of 97.3% for detecting the new robot‚Äôs masks. This demonstrates that transfer learning also contributes to detecting keypoints of robots, even though there are fewer examples used for training the neural network.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">Zhou et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib30" title="" class="ltx_ref">zhou20193d </a></cite> also propose an architecture for detecting the pose of robots from RGB images. The architecture is divided into two parts, Base-Net and Pose-Net. In Pose-Net, the positions of the keypoints in the image are initially estimated to guide training. Base-Net obtains the 3D position of the robot base by calculating the depth and using the robot base position in the image evaluated by proposed system.</p>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p">Lee et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib31" title="" class="ltx_ref">lee2020camera </a></cite> present an approach to estimate the pose of the robot using a single RGB image. As with similar previous works, a convolutional network is used to detect the robot‚Äôs keypoints in artificial images due to the lack of the availability of real data reported by the authors. However, unlike previous works, the projection of keypoints is in 2D. Perspective-n-point (PnP) is then used to retrieve the camera extrinsic, allowing the approach to dismiss the need from an offline calibration step from a single frame.</p>
</div>
<div id="S2.p8" class="ltx_para">
<p id="S2.p8.1" class="ltx_p">Heindl et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib32" title="" class="ltx_ref">heindl2019learning </a></cite> use a recurrent convolutional network based on ConvGRU-type convolutions <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib33" title="" class="ltx_ref">ballas2015delving </a></cite>. Despite the promising approach and reaching a mean average precision or mAP (threshold 50%) of 88.7%, all images used in their entirety were artificially generated to train the proposed architecture. Although the paper presents inferences in authentic images, there are no experiments or other evidence that the proposed approach can work in real scenarios. The idea of using artificial images was also adopted in previous work <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib34" title="" class="ltx_ref">heindl20193d </a></cite> by the same authors in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib32" title="" class="ltx_ref">heindl2019learning </a></cite>, however, the neural networks were modeled with vanilla convolutions this time.</p>
</div>
<div id="S2.p9" class="ltx_para">
<p id="S2.p9.1" class="ltx_p">With regard to robot pose detection, there is a need to consider the importance of robots‚Äô keypoints in the context of safety for HRC. Note also that none of the reviewed works considers the analysis of robots‚Äô keypoints information into a recurrent neural network in order to classify or predict robots‚Äô movements. In a typical HRC scenario, it is necessary to understand and classify/predict the robot‚Äôs intention. This approach should help to avoid or decrease the impacts of collision/accident events.</p>
</div>
<div id="S2.p10" class="ltx_para">
<p id="S2.p10.1" class="ltx_p">Considering the current state-of-art, we can cite the following main contributions of our work:</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">This work develops a new framework designed to predict robotic arm pose detection and its future movement in the context of HRC using DL, ELM and one 2D camera in a well controlled scenario.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">Our framework is applied and evaluated in the proposed controlled setup that highly mimics a real scenario, generating real data during the experiments for analysis. Unlike most existing related works, this one did not used synthetic images, e.g. robot located in random scenarios. It is also important to highlight that in our experiments the camera calibration process was not necessary. The additional steps part of the calibration processr, such as checkerboard position, camera positioning, acquisition of intrinsic and extrinsic parameters are error sensitive. Our goal is to use a generic environment where we could use devices present in-loco.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p">To the best of our knowledge, this work is first to use ELM to solve a regression problem using features generated by a neural network that uses self-calibrated convolutions (SCNet). It is first to obtain robot pose detection using a convolutional extreme learning machine (CELM) approach. We demonstrate through extensive experiments that the DL model SCNet50 used in conjunction with the ELM method provide better detection results in our scenario.</p>
</div>
</li>
<li id="S2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I1.i4.p1" class="ltx_para">
<p id="S2.I1.i4.p1.1" class="ltx_p">Another important contribution of this work is the proposition of deep learning models to predict the future movement of the robotic arm. Models of this type had not been previously proposed in the literature to tackle this problem. There are works that apply DL only to human movement prediction models in the HRI context. Using DL, it becomes possible to analyze the future movement of the robot. Furthermore, DL models will also be considered, as part of future work, in the analysis of collision risk between humans and robots.</p>
</div>
</li>
<li id="S2.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I1.i5.p1" class="ltx_para">
<p id="S2.I1.i5.p1.1" class="ltx_p">Finally, the last contribution of this work is that the proposed framework is easy to deploy. The testbed relies on infrastructure that is commonly found in workspaces including cameras, robotics arms, and networking devices.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Proposed framework</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">This section provides detailed information on the composition of the framework proposed in this work. Its adopted models, their comparison and experimental parameters will be presented in Section <a href="#S4" title="4 Materials and methods ‚Ä£ A framework for robotic arm pose estimation and movement prediction based on deep and extreme learning models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">As illustrated in Fig. <a href="#S3.F1" title="Figure 1 ‚Ä£ 3 Proposed framework ‚Ä£ A framework for robotic arm pose estimation and movement prediction based on deep and extreme learning models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, our framework consists of two main modules. The first one is the robotic arm pose estimation module. It uses a new proposed neural network model that combines the self-calibrated CNN (SCNet-50) and ELM. The second module is the robotic arm movement prediction. It executes well known RNN models to make future inferences. The next two subsections detail the operation of these two modules.</p>
</div>
<figure id="S3.F1" class="ltx_figure">
<div id="S3.F1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:739.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-59.6pt,101.7pt) scale(0.784313725490196,0.784313725490196) ;"><img src="/html/2205.13994/assets/figures/proposed_framework.jpg" id="S3.F1.1.g1" class="ltx_graphics ltx_img_portrait" width="765" height="1305" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Framework for robotic arm pose estimation and prediction of future movement.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Robotic arm pose estimation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Fig. <a href="#S3.F2" title="Figure 2 ‚Ä£ 3.1 Robotic arm pose estimation ‚Ä£ 3 Proposed framework ‚Ä£ A framework for robotic arm pose estimation and movement prediction based on deep and extreme learning models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> depicts this module. The framework proposes a new neural network model for detecting keypoints used to estimate the pose of the robotic arm.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<div id="S3.F2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:531.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-109.5pt,134.1pt) scale(0.664451821028146,0.664451821028146) ;"><img src="/html/2205.13994/assets/figures/module1.jpg" id="S3.F2.1.g1" class="ltx_graphics ltx_img_square" width="903" height="1106" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Module for robotic arm pose estimation.</figcaption>
</figure>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The first part of the proposed new model, responsible for the training in keypoints detection, uses the SCNet-50 network with self-calibrated convolutions (SCConvs). As mentioned by Liu <span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib9" title="" class="ltx_ref">liu2020improving </a></cite>, in the SCConvs ‚Äú<span id="S3.SS1.p2.1.2" class="ltx_text ltx_font_italic">each spatial location is allowed to not only adaptively consider its surrounding informative context as embeddings from the latent space functioning as scalars in the responses from the original scale space, but also model inter-channel dependencies</span>‚Äù. This allows SCConvs to learn better discriminative representations by adding basic convolution transformations, identity, and sigmoid functions to each layer <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib9" title="" class="ltx_ref">liu2020improving </a></cite>.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.3" class="ltx_p">The SCNet-50 version that we use has a modification also proposed by the SCConv authors <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib9" title="" class="ltx_ref">liu2020improving </a></cite>, the SCNet-50-V1D, which implements the original SCNet-50 network with modifications inspired by bag of tricks for CNNs <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib35" title="" class="ltx_ref">he2019bag </a></cite>. According to Liu, SCNet-50-V1D replaces the original <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="7\times 7" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mrow id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mn id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">7</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p3.1.m1.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.cmml">√ó</mo><mn id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml">7</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><times id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1"></times><cn type="integer" id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2">7</cn><cn type="integer" id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">7\times 7</annotation></semantics></math> convolutions by three <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mrow id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml"><mn id="S3.SS1.p3.2.m2.1.1.2" xref="S3.SS1.p3.2.m2.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p3.2.m2.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.cmml">√ó</mo><mn id="S3.SS1.p3.2.m2.1.1.3" xref="S3.SS1.p3.2.m2.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1"><times id="S3.SS1.p3.2.m2.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1"></times><cn type="integer" id="S3.SS1.p3.2.m2.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.2">3</cn><cn type="integer" id="S3.SS1.p3.2.m2.1.1.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">3\times 3</annotation></semantics></math> convolutions, and in the downsampling block, a <math id="S3.SS1.p3.3.m3.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S3.SS1.p3.3.m3.1a"><mrow id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml"><mn id="S3.SS1.p3.3.m3.1.1.2" xref="S3.SS1.p3.3.m3.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p3.3.m3.1.1.1" xref="S3.SS1.p3.3.m3.1.1.1.cmml">√ó</mo><mn id="S3.SS1.p3.3.m3.1.1.3" xref="S3.SS1.p3.3.m3.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><apply id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1"><times id="S3.SS1.p3.3.m3.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1.1"></times><cn type="integer" id="S3.SS1.p3.3.m3.1.1.2.cmml" xref="S3.SS1.p3.3.m3.1.1.2">3</cn><cn type="integer" id="S3.SS1.p3.3.m3.1.1.3.cmml" xref="S3.SS1.p3.3.m3.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">3\times 3</annotation></semantics></math> average pooling with stride 2 is added before the convolution operations, whose stride is changed to 1 <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib9" title="" class="ltx_ref">liu2020improving </a></cite>.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">These modifications follow some of the bag of tricks paper <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib35" title="" class="ltx_ref">he2019bag </a></cite>. We first take the SCNet-50-V1D model trained in the ImageNet database <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib21" title="" class="ltx_ref">deng2009imagenet </a></cite> and then attempt to use it for transfer learning in our work. Here, we we re-train all the weights of the network in our own database (described in Section <a href="#S4.SS2" title="4.2 Data acquisition and processing ‚Ä£ 4 Materials and methods ‚Ä£ A framework for robotic arm pose estimation and movement prediction based on deep and extreme learning models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>) for robotic arm pose detection. The training process consists of removing the last fully connected layer and adding a new linear layer containing 16 activations. There are 16 activations because in this work we aim to make the regression of the eight keypoints that form the pose of the robotic arm, for each keypoint there is a coordinate <math id="S3.SS1.p4.1.m1.2" class="ltx_Math" alttext="(x,y)" display="inline"><semantics id="S3.SS1.p4.1.m1.2a"><mrow id="S3.SS1.p4.1.m1.2.3.2" xref="S3.SS1.p4.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.SS1.p4.1.m1.2.3.2.1" xref="S3.SS1.p4.1.m1.2.3.1.cmml">(</mo><mi id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml">x</mi><mo id="S3.SS1.p4.1.m1.2.3.2.2" xref="S3.SS1.p4.1.m1.2.3.1.cmml">,</mo><mi id="S3.SS1.p4.1.m1.2.2" xref="S3.SS1.p4.1.m1.2.2.cmml">y</mi><mo stretchy="false" id="S3.SS1.p4.1.m1.2.3.2.3" xref="S3.SS1.p4.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.2b"><interval closure="open" id="S3.SS1.p4.1.m1.2.3.1.cmml" xref="S3.SS1.p4.1.m1.2.3.2"><ci id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">ùë•</ci><ci id="S3.SS1.p4.1.m1.2.2.cmml" xref="S3.SS1.p4.1.m1.2.2">ùë¶</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.2c">(x,y)</annotation></semantics></math>.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">The second part of the proposed new model, responsible for the inference in keypoints detection, uses ELM to improve the detection of poses of the robotic arm. According to a survey carried out by Rodrigues <span id="S3.SS1.p5.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib11" title="" class="ltx_ref">rodrigues2021convolutional </a></cite>, ELM can be used together with CNNs to improve the results in classification tasks and pre-trained CNNs can be used for feature extraction. ELM models were trained with the extracted features, providing better accuracy results compared to using only CNNs.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.1" class="ltx_p">In this work, we hypothesize that the pose estimation generated by the SCNet-50-V1D model can be improved by using the ELM network for regression. Also, we hypothesize that it is possible to have better convergence and better error results in detection for robotic arm pose estimation, considering that ELM proved itself accurate in another regression problem, according to Huang <span id="S3.SS1.p6.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib36" title="" class="ltx_ref">huang2015trends </a></cite>.</p>
</div>
<div id="S3.SS1.p7" class="ltx_para">
<p id="S3.SS1.p7.1" class="ltx_p">For training, we perform the feature extraction process through our CNN network already trained in our dataset. Then we use the ELM network for a new training with the extracted features. As for inference, with all the aforementioned models, we also execute the feature extraction process using the SCNet-50-v1-d network. The already trained ELM network will also predict the keypoints, which together build the pose of the robotic arm. As a result, for each image processed, our proposed learning scheme produces outputs containing all 8 <math id="S3.SS1.p7.1.m1.2" class="ltx_Math" alttext="(x,y)" display="inline"><semantics id="S3.SS1.p7.1.m1.2a"><mrow id="S3.SS1.p7.1.m1.2.3.2" xref="S3.SS1.p7.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.SS1.p7.1.m1.2.3.2.1" xref="S3.SS1.p7.1.m1.2.3.1.cmml">(</mo><mi id="S3.SS1.p7.1.m1.1.1" xref="S3.SS1.p7.1.m1.1.1.cmml">x</mi><mo id="S3.SS1.p7.1.m1.2.3.2.2" xref="S3.SS1.p7.1.m1.2.3.1.cmml">,</mo><mi id="S3.SS1.p7.1.m1.2.2" xref="S3.SS1.p7.1.m1.2.2.cmml">y</mi><mo stretchy="false" id="S3.SS1.p7.1.m1.2.3.2.3" xref="S3.SS1.p7.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.1.m1.2b"><interval closure="open" id="S3.SS1.p7.1.m1.2.3.1.cmml" xref="S3.SS1.p7.1.m1.2.3.2"><ci id="S3.SS1.p7.1.m1.1.1.cmml" xref="S3.SS1.p7.1.m1.1.1">ùë•</ci><ci id="S3.SS1.p7.1.m1.2.2.cmml" xref="S3.SS1.p7.1.m1.2.2">ùë¶</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.1.m1.2c">(x,y)</annotation></semantics></math> points, corresponding to the robotic arm joints. We name our proposed model as SCNet-50-V1D+ELM to make the robotic arm pose estimation.</p>
</div>
<div id="S3.SS1.p8" class="ltx_para">
<p id="S3.SS1.p8.1" class="ltx_p">Since we now have the model SCNet-50-V1D+ELM to make inferences about the pose of the robotic arm, there is now also a need to make inferences from future frames to predict its movement. This type of forecast can facilitate the understanding of what is happening in collaborative activities. To this end, the proposed framework is also covered with the robotic arm movement prediction module, which we will explain later.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Robotic arm movement prediction</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">This module proposes the use of several RNN models to predict future robotic arm movements. As part of this task, we consider two RNN models: LSTM and GRU. These models are frequently used for time series prediction and. Overall, they outperform traditional ML algorithms, depending on the applications at hand <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib37" title="" class="ltx_ref">ribeiro2020short </a></cite>. It is important to highlight that we are proposing a double-stacked RNN (with LSTM and GRU) with encoders and decoders <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib38" title="" class="ltx_ref">cho2014learning </a></cite> (Fig. <a href="#S3.F3" title="Figure 3 ‚Ä£ 3.2 Robotic arm movement prediction ‚Ä£ 3 Proposed framework ‚Ä£ A framework for robotic arm pose estimation and movement prediction based on deep and extreme learning models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>), for better learning of long term data, storing in memory various data from the past, and considering them in future prediction. There are two encoder layers, in which we store the sequences for subsequent use by the decoder layers. At last, there is a time-distributed and dense layer that makes predictions.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<div id="S3.F3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:496.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-120.7pt,138.1pt) scale(0.642398279192958,0.642398279192958) ;"><img src="/html/2205.13994/assets/figures/rnn.jpg" id="S3.F3.1.g1" class="ltx_graphics ltx_img_square" width="934" height="1069" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Detailed view of the module for robotic arm movement prediction. Orange arrows refer to copy operations, and blue arrows refer to the normal feedforward process of the neural network.</figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.4" class="ltx_p">For each set of images <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="In" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.1.m1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><times id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1"></times><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">ùêº</ci><ci id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">ùëõ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">In</annotation></semantics></math> supplied to the first detection module, this will return an array of dimension <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="(n\times 16)" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p2.2.m2.1.1.1.2" xref="S3.SS2.p2.2.m2.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p2.2.m2.1.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.1.1.2" xref="S3.SS2.p2.2.m2.1.1.1.1.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.2.m2.1.1.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.1.1.cmml">√ó</mo><mn id="S3.SS2.p2.2.m2.1.1.1.1.3" xref="S3.SS2.p2.2.m2.1.1.1.1.3.cmml">16</mn></mrow><mo stretchy="false" id="S3.SS2.p2.2.m2.1.1.1.3" xref="S3.SS2.p2.2.m2.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1"><times id="S3.SS2.p2.2.m2.1.1.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.1"></times><ci id="S3.SS2.p2.2.m2.1.1.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.2">ùëõ</ci><cn type="integer" id="S3.SS2.p2.2.m2.1.1.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">(n\times 16)</annotation></semantics></math>, where <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mi id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><ci id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">n</annotation></semantics></math> corresponds to the number of input images, and 16 corresponds to the number of coordinates generated for the identification of the robotic arm in the <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="In" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><mrow id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><mi id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.4.m4.1.1.1" xref="S3.SS2.p2.4.m4.1.1.1.cmml">‚Äã</mo><mi id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><times id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1"></times><ci id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2">ùêº</ci><ci id="S3.SS2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3">ùëõ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">In</annotation></semantics></math> images. This matrix corresponds to a time series containing the keypoint coordinates of the robotic arm for the entire set of input images.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.5" class="ltx_p">Thus, the initial predicted data is sequential (of size <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="(n\times 16)" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mrow id="S3.SS2.p3.1.m1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p3.1.m1.1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p3.1.m1.1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.1.1.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p3.1.m1.1.1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.1.1.1.cmml">√ó</mo><mn id="S3.SS2.p3.1.m1.1.1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.1.1.3.cmml">16</mn></mrow><mo stretchy="false" id="S3.SS2.p3.1.m1.1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1"><times id="S3.SS2.p3.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1.1"></times><ci id="S3.SS2.p3.1.m1.1.1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1.2">ùëõ</ci><cn type="integer" id="S3.SS2.p3.1.m1.1.1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">(n\times 16)</annotation></semantics></math> containing just robotic arm poses. The RNN models are used to make future inferences with this initial data. They return another sequential data containing the predictions of the robotic arm movement. The out sequential data has a size of <math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="(f\times 16)" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><mrow id="S3.SS2.p3.2.m2.1.1.1" xref="S3.SS2.p3.2.m2.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p3.2.m2.1.1.1.2" xref="S3.SS2.p3.2.m2.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p3.2.m2.1.1.1.1" xref="S3.SS2.p3.2.m2.1.1.1.1.cmml"><mi id="S3.SS2.p3.2.m2.1.1.1.1.2" xref="S3.SS2.p3.2.m2.1.1.1.1.2.cmml">f</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p3.2.m2.1.1.1.1.1" xref="S3.SS2.p3.2.m2.1.1.1.1.1.cmml">√ó</mo><mn id="S3.SS2.p3.2.m2.1.1.1.1.3" xref="S3.SS2.p3.2.m2.1.1.1.1.3.cmml">16</mn></mrow><mo stretchy="false" id="S3.SS2.p3.2.m2.1.1.1.3" xref="S3.SS2.p3.2.m2.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1.1"><times id="S3.SS2.p3.2.m2.1.1.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1.1.1.1"></times><ci id="S3.SS2.p3.2.m2.1.1.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.1.1.2">ùëì</ci><cn type="integer" id="S3.SS2.p3.2.m2.1.1.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">(f\times 16)</annotation></semantics></math>, where <math id="S3.SS2.p3.3.m3.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S3.SS2.p3.3.m3.1a"><mi id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><ci id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1">ùëì</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">f</annotation></semantics></math> is the number of future frames set to be predicted. With these data provided, it is possible to verify where the robot is moving, enabling the creation of policies in the robot‚Äôs operation. An example of an applicable policy is to stop the robot when it is moving towards a human. The output size <math id="S3.SS2.p3.4.m4.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S3.SS2.p3.4.m4.1a"><mi id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><ci id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1">ùëì</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">f</annotation></semantics></math> is an adjustable parameter, together with the <math id="S3.SS2.p3.5.m5.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS2.p3.5.m5.1a"><mi id="S3.SS2.p3.5.m5.1.1" xref="S3.SS2.p3.5.m5.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m5.1b"><ci id="S3.SS2.p3.5.m5.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m5.1c">n</annotation></semantics></math> parameter, both are defined in Section <a href="#S4.SS3.SSS2" title="4.3.2 Robot movement prediction ‚Ä£ 4.3 Experimental settings ‚Ä£ 4 Materials and methods ‚Ä£ A framework for robotic arm pose estimation and movement prediction based on deep and extreme learning models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3.2</span></a>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Materials and methods</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">This section provides insights on the way the experiments were setup and executed.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental scenario</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Fig. <a href="#S4.F4" title="Figure 4 ‚Ä£ 4.1 Experimental scenario ‚Ä£ 4 Materials and methods ‚Ä£ A framework for robotic arm pose estimation and movement prediction based on deep and extreme learning models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents our well-controlled experimental testbed implemented within a laboratory as well as all its main elements. The adopted scenario is one where a worker collaboratively interacts with a robotic arm that performs maintenance activities on an RBS network rack.</p>
</div>
<figure id="S4.F4" class="ltx_figure">
<div id="S4.F4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:404.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;"><img src="/html/2205.13994/assets/figures/cenario.png" id="S4.F4.1.g1" class="ltx_graphics ltx_img_square" width="600" height="560" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>System general architecture (same scenario of Silva et. al <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib14" title="" class="ltx_ref">silva2020assessing </a></cite>).</figcaption>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">The robotic arm used in our scenario is an UR-5 <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib39" title="" class="ltx_ref">ur2021robots </a></cite>. It performs maintenance operations such as adding, removing, or exchanging network cables in a rack similar to the one used in an RBS site. In addition, a camera is strategically placed to capture all activities performed in our well-controlled scenario. With the interaction of the robotic arm, all images are captured at a 21 FPS rate in high definition by a webcam and processed by a nearby computer. This machine also hosts the software for training using deep learning models and making inferences.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Table <a href="#S4.T1" title="Table 1 ‚Ä£ 4.1 Experimental scenario ‚Ä£ 4 Materials and methods ‚Ä£ A framework for robotic arm pose estimation and movement prediction based on deep and extreme learning models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents, in detail, the characteristics of the camera and computer used.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Specifications of the camera and computer used in this work.</figcaption>
<div id="S4.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:272.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(73.4pt,-46.1pt) scale(1.51211298614449,1.51211298614449) ;">
<table id="S4.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Device</span></th>
<td id="S4.T1.1.1.1.1.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Specification</span></td>
<td id="S4.T1.1.1.1.1.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Value</span></td>
</tr>
<tr id="S4.T1.1.1.2.2" class="ltx_tr">
<th id="S4.T1.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Camera</th>
<td id="S4.T1.1.1.2.2.2" class="ltx_td ltx_align_left ltx_border_t">Type</td>
<td id="S4.T1.1.1.2.2.3" class="ltx_td ltx_align_left ltx_border_t">Webcam</td>
</tr>
<tr id="S4.T1.1.1.3.3" class="ltx_tr">
<th id="S4.T1.1.1.3.3.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S4.T1.1.1.3.3.2" class="ltx_td ltx_align_left">Model</td>
<td id="S4.T1.1.1.3.3.3" class="ltx_td ltx_align_left">Logitech C270</td>
</tr>
<tr id="S4.T1.1.1.4.4" class="ltx_tr">
<th id="S4.T1.1.1.4.4.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S4.T1.1.1.4.4.2" class="ltx_td ltx_align_left">Transmission rate</td>
<td id="S4.T1.1.1.4.4.3" class="ltx_td ltx_align_left">21 fps</td>
</tr>
<tr id="S4.T1.1.1.5.5" class="ltx_tr">
<th id="S4.T1.1.1.5.5.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S4.T1.1.1.5.5.2" class="ltx_td ltx_align_left">Image Quality</td>
<td id="S4.T1.1.1.5.5.3" class="ltx_td ltx_align_left">HD - 1080p</td>
</tr>
<tr id="S4.T1.1.1.6.6" class="ltx_tr">
<th id="S4.T1.1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Computer</th>
<td id="S4.T1.1.1.6.6.2" class="ltx_td ltx_align_left ltx_border_t">OS</td>
<td id="S4.T1.1.1.6.6.3" class="ltx_td ltx_align_left ltx_border_t">Ubuntu 18.04</td>
</tr>
<tr id="S4.T1.1.1.7.7" class="ltx_tr">
<th id="S4.T1.1.1.7.7.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S4.T1.1.1.7.7.2" class="ltx_td ltx_align_left">Processor</td>
<td id="S4.T1.1.1.7.7.3" class="ltx_td ltx_align_left">Core i7-2600 CPU - 3.4 GHz</td>
</tr>
<tr id="S4.T1.1.1.8.8" class="ltx_tr">
<th id="S4.T1.1.1.8.8.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S4.T1.1.1.8.8.2" class="ltx_td ltx_align_left">RAM</td>
<td id="S4.T1.1.1.8.8.3" class="ltx_td ltx_align_left">16GB</td>
</tr>
<tr id="S4.T1.1.1.9.9" class="ltx_tr">
<th id="S4.T1.1.1.9.9.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S4.T1.1.1.9.9.2" class="ltx_td ltx_align_left">System type</td>
<td id="S4.T1.1.1.9.9.3" class="ltx_td ltx_align_left">64 bits</td>
</tr>
<tr id="S4.T1.1.1.10.10" class="ltx_tr">
<th id="S4.T1.1.1.10.10.1" class="ltx_td ltx_th ltx_th_row ltx_border_b"></th>
<td id="S4.T1.1.1.10.10.2" class="ltx_td ltx_align_left ltx_border_b">Video card</td>
<td id="S4.T1.1.1.10.10.3" class="ltx_td ltx_align_left ltx_border_b">GeForce GTX 1060 6GB</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Data acquisition and processing</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">All scene images captured by a single camera are next processed by a dedicated computer. In order to ensure the capture of representative scenes with HRC risk, we established some activity protocols to be followed by the robot and humans to simulate collaborative activities. Based on our previous work <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib14" title="" class="ltx_ref">silva2020assessing </a></cite>, we defined the following activities:</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">a worker making movements inside and outside the robot‚Äôs workspace;</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">a worker performing delivering or exchanging cables to the robot;</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">accidental collision and contact situations caused by the worker or the robot by teleoperation;</p>
</div>
</li>
</ul>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">The captured the images are stored in video format (.AVI) during the recording process. At the end of the recording, we extracted all frames from the video which resulted in a total of 23,135 frames (approximately 18 minutes with 20 FPS). We use the OpenCV library in Python to perform these two procedures.</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Data for robot pose estimation</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">With the obtained frames at hand, we next selected a small number of frames to annotate robot keypoints. We considered 1,000 frames representing around one frame per second. We decided to select frames in this way to avoid excessive redundant information that does not contribute to the efficiency of the learning process. It is known that frames are given with temporal interdependence, and they generally are similar to each other. As a result, it is often not necessary to use all frames for the detection task.</p>
</div>
<div id="S4.SS2.SSS1.p2" class="ltx_para">
<p id="S4.SS2.SSS1.p2.1" class="ltx_p">To detect the pose of the robotic arm, it is necessary to define, through annotations, the keypoints that compose the robotic arm. To annotate the data for robotics arm pose detection, we use the VGG Image Annotator <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib40" title="" class="ltx_ref">dutta2019via </a></cite>. Fig. <a href="#S4.F5" title="Figure 5 ‚Ä£ 4.2.1 Data for robot pose estimation ‚Ä£ 4.2 Data acquisition and processing ‚Ä£ 4 Materials and methods ‚Ä£ A framework for robotic arm pose estimation and movement prediction based on deep and extreme learning models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows an example of how the data was annotated.</p>
</div>
<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<table id="S4.F5.1" class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.F5.1.1.1" class="ltx_tr">
<td id="S4.F5.1.1.1.1" class="ltx_td ltx_align_center"></td>
<td id="S4.F5.1.1.1.2" class="ltx_td ltx_align_center"></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2205.13994/assets/figures/example_img.png" id="S4.F5.sf1.g1" class="ltx_graphics ltx_img_square" width="146" height="146" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2205.13994/assets/figures/example_gt.png" id="S4.F5.sf2.g1" class="ltx_graphics ltx_img_square" width="147" height="146" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Example of robot keypoints annotation with the VGG Image Annotator software.</figcaption>
</figure>
<div id="S4.SS2.SSS1.p3" class="ltx_para">
<p id="S4.SS2.SSS1.p3.1" class="ltx_p">Using the VGG Image Annotator (VIA), we annotate eight points that correspond to the robot‚Äôs joints in all frames. The generated output by the tool is in JSON format. After the frame selection process, we obtained a number of 508 annotated images with the pose of the robotic arm.</p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Data for robot movement prediction</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">With regard to the data used for training the recurrent models, we did not perform frame selection because for movement prediction one needs to maintain a temporal dependency in the data. But, one problem emerges as a result of this decision, there are more than 20,000 frames that make the ground truth and which can all be used to train the recurrent models.</p>
</div>
<div id="S4.SS2.SSS2.p2" class="ltx_para">
<p id="S4.SS2.SSS2.p2.1" class="ltx_p">We hypothesize that our proposed model for robotic arm pose estimation (SCNet-50-V1D+ELM) can extract the keypoints with an acceptable level of precision, so we assume that it is the source of our annotations. We make predictions with the SCNet-50-V1D+ELM architecture in all frames extracted, then create a dataset for robotic arm movement prediction. In other words, we obtain the position of the robotic arm in all frames of the video. With that, we created a new time series dataset containing the eight robotic arm points, with coordinates <math id="S4.SS2.SSS2.p2.1.m1.2" class="ltx_Math" alttext="(x,y)" display="inline"><semantics id="S4.SS2.SSS2.p2.1.m1.2a"><mrow id="S4.SS2.SSS2.p2.1.m1.2.3.2" xref="S4.SS2.SSS2.p2.1.m1.2.3.1.cmml"><mo stretchy="false" id="S4.SS2.SSS2.p2.1.m1.2.3.2.1" xref="S4.SS2.SSS2.p2.1.m1.2.3.1.cmml">(</mo><mi id="S4.SS2.SSS2.p2.1.m1.1.1" xref="S4.SS2.SSS2.p2.1.m1.1.1.cmml">x</mi><mo id="S4.SS2.SSS2.p2.1.m1.2.3.2.2" xref="S4.SS2.SSS2.p2.1.m1.2.3.1.cmml">,</mo><mi id="S4.SS2.SSS2.p2.1.m1.2.2" xref="S4.SS2.SSS2.p2.1.m1.2.2.cmml">y</mi><mo stretchy="false" id="S4.SS2.SSS2.p2.1.m1.2.3.2.3" xref="S4.SS2.SSS2.p2.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p2.1.m1.2b"><interval closure="open" id="S4.SS2.SSS2.p2.1.m1.2.3.1.cmml" xref="S4.SS2.SSS2.p2.1.m1.2.3.2"><ci id="S4.SS2.SSS2.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p2.1.m1.1.1">ùë•</ci><ci id="S4.SS2.SSS2.p2.1.m1.2.2.cmml" xref="S4.SS2.SSS2.p2.1.m1.2.2">ùë¶</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p2.1.m1.2c">(x,y)</annotation></semantics></math>. The result totals to 16 coordinate information representing the robotic arm, for each frame, as shown in Fig. <a href="#S4.F6" title="Figure 6 ‚Ä£ 4.2.2 Data for robot movement prediction ‚Ä£ 4.2 Data acquisition and processing ‚Ä£ 4 Materials and methods ‚Ä£ A framework for robotic arm pose estimation and movement prediction based on deep and extreme learning models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. After runnning the automatic annotation process using the SCNet-50-V1D+ELM model, we obtained 23,135 sequential images annotated with the pose of the robotic arm.</p>
</div>
<figure id="S4.F6" class="ltx_figure">
<div id="S4.F6.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:552.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-103.0pt,131.3pt) scale(0.677966101694915,0.677966101694915) ;"><img src="/html/2205.13994/assets/figures/automatic_annot.jpg" id="S4.F6.1.g1" class="ltx_graphics ltx_img_portrait" width="885" height="1128" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Automatic process for keypoint annotation proposed in this work.</figcaption>
</figure>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Experimental settings</h3>

<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Robot pose estimation</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">Section <a href="#S3" title="3 Proposed framework ‚Ä£ A framework for robotic arm pose estimation and movement prediction based on deep and extreme learning models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> describes the main model for detecting the pose of the robotic arm, namely, the SCNet-50-V1D+ELM model . To prove the model‚Äôs effectiveness, we compare it with some state-of-the-art DL models, such as AlexNet <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib28" title="" class="ltx_ref">krizhevsky2012imagenet </a></cite>, SqueezeNet <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib41" title="" class="ltx_ref">iandola2016squeezenet </a></cite>, VGG-11 <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib42" title="" class="ltx_ref">SimonyanZ14a </a></cite>, ResNet-34 <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib43" title="" class="ltx_ref">he2016deep </a></cite>, and DenseNet-121 <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib44" title="" class="ltx_ref">huang2017densely </a></cite>. For all of these models, we removed the last fully connected (or dense) layer and replaced it with another dense layer with linear activation, now containing 16 neurons responsible for predicting the keypoints of the robotic arm. During the training process, we retrained all models using their pre-trained weights obtained from ImageNet.</p>
</div>
<div id="S4.SS3.SSS1.p2" class="ltx_para">
<p id="S4.SS3.SSS1.p2.1" class="ltx_p">We trained all the models mentioned above (in addition to our proposed model) in 500 epochs with the Adam optimizer. A learning rate of 0.0001 and batch size of 8 were used. Data validation for this training process was carried out using cross-validation with 5 folds. Considering that we have a regression problem, we used the loss function of mean squared error (MSE) to calculate the error in the propagation of neural networks.</p>
</div>
<div id="S4.SS3.SSS1.p3" class="ltx_para">
<p id="S4.SS3.SSS1.p3.1" class="ltx_p">For the ELM network, we have four different kernel activation functions, including the radial basis function (RBF) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib45" title="" class="ltx_ref">huang2005extreme </a></cite>, RBF with L2-norm (RBF-L2) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib45" title="" class="ltx_ref">huang2005extreme </a></cite>, hyperbolic tangent (Tanh) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib46" title="" class="ltx_ref">baraha2017implementation </a></cite>, and the linear function. For all ELM evaluations, we varied the number of neurons between 100 and 1000 with a step of 50.</p>
</div>
<div id="S4.SS3.SSS1.p4" class="ltx_para">
<p id="S4.SS3.SSS1.p4.1" class="ltx_p">In addition to its role as a loss function, the MSE was also used as an evaluation metric. In addition, the proposed framework adopted the mean absolute error (MAE) as a secondary metric to evaluate the models. Eqs. <a href="#S4.E1" title="In 4.3.1 Robot pose estimation ‚Ä£ 4.3 Experimental settings ‚Ä£ 4 Materials and methods ‚Ä£ A framework for robotic arm pose estimation and movement prediction based on deep and extreme learning models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and <a href="#S4.E2" title="In 4.3.1 Robot pose estimation ‚Ä£ 4.3 Experimental settings ‚Ä£ 4 Materials and methods ‚Ä£ A framework for robotic arm pose estimation and movement prediction based on deep and extreme learning models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> detail the definitions of the adopted MSE and MAE metrics.</p>
</div>
<div id="S4.SS3.SSS1.p5" class="ltx_para">
<table id="S4.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E1.m1.1" class="ltx_Math" alttext="MSE=\frac{1}{N}\sum_{i=1}^{N}(y_{i}-p_{i})^{2}" display="block"><semantics id="S4.E1.m1.1a"><mrow id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml"><mrow id="S4.E1.m1.1.1.3" xref="S4.E1.m1.1.1.3.cmml"><mi id="S4.E1.m1.1.1.3.2" xref="S4.E1.m1.1.1.3.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.3.1" xref="S4.E1.m1.1.1.3.1.cmml">‚Äã</mo><mi id="S4.E1.m1.1.1.3.3" xref="S4.E1.m1.1.1.3.3.cmml">S</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.3.1a" xref="S4.E1.m1.1.1.3.1.cmml">‚Äã</mo><mi id="S4.E1.m1.1.1.3.4" xref="S4.E1.m1.1.1.3.4.cmml">E</mi></mrow><mo id="S4.E1.m1.1.1.2" xref="S4.E1.m1.1.1.2.cmml">=</mo><mrow id="S4.E1.m1.1.1.1" xref="S4.E1.m1.1.1.1.cmml"><mfrac id="S4.E1.m1.1.1.1.3" xref="S4.E1.m1.1.1.1.3.cmml"><mn id="S4.E1.m1.1.1.1.3.2" xref="S4.E1.m1.1.1.1.3.2.cmml">1</mn><mi id="S4.E1.m1.1.1.1.3.3" xref="S4.E1.m1.1.1.1.3.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.2" xref="S4.E1.m1.1.1.1.2.cmml">‚Äã</mo><mrow id="S4.E1.m1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.cmml"><munderover id="S4.E1.m1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S4.E1.m1.1.1.1.1.2.2.2" xref="S4.E1.m1.1.1.1.1.2.2.2.cmml">‚àë</mo><mrow id="S4.E1.m1.1.1.1.1.2.2.3" xref="S4.E1.m1.1.1.1.1.2.2.3.cmml"><mi id="S4.E1.m1.1.1.1.1.2.2.3.2" xref="S4.E1.m1.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S4.E1.m1.1.1.1.1.2.2.3.1" xref="S4.E1.m1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S4.E1.m1.1.1.1.1.2.2.3.3" xref="S4.E1.m1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S4.E1.m1.1.1.1.1.2.3" xref="S4.E1.m1.1.1.1.1.2.3.cmml">N</mi></munderover><msup id="S4.E1.m1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.cmml"><mrow id="S4.E1.m1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E1.m1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E1.m1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.1.cmml"><msub id="S4.E1.m1.1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S4.E1.m1.1.1.1.1.1.1.1.1.2.2" xref="S4.E1.m1.1.1.1.1.1.1.1.1.2.2.cmml">y</mi><mi id="S4.E1.m1.1.1.1.1.1.1.1.1.2.3" xref="S4.E1.m1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S4.E1.m1.1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.cmml">‚àí</mo><msub id="S4.E1.m1.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E1.m1.1.1.1.1.1.1.1.1.3.2" xref="S4.E1.m1.1.1.1.1.1.1.1.1.3.2.cmml">p</mi><mi id="S4.E1.m1.1.1.1.1.1.1.1.1.3.3" xref="S4.E1.m1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo stretchy="false" id="S4.E1.m1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="S4.E1.m1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.3.cmml">2</mn></msup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.1b"><apply id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1"><eq id="S4.E1.m1.1.1.2.cmml" xref="S4.E1.m1.1.1.2"></eq><apply id="S4.E1.m1.1.1.3.cmml" xref="S4.E1.m1.1.1.3"><times id="S4.E1.m1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.3.1"></times><ci id="S4.E1.m1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.3.2">ùëÄ</ci><ci id="S4.E1.m1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.3.3">ùëÜ</ci><ci id="S4.E1.m1.1.1.3.4.cmml" xref="S4.E1.m1.1.1.3.4">ùê∏</ci></apply><apply id="S4.E1.m1.1.1.1.cmml" xref="S4.E1.m1.1.1.1"><times id="S4.E1.m1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.2"></times><apply id="S4.E1.m1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.3"><divide id="S4.E1.m1.1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.1.3"></divide><cn type="integer" id="S4.E1.m1.1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.1.3.2">1</cn><ci id="S4.E1.m1.1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.1.3.3">ùëÅ</ci></apply><apply id="S4.E1.m1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1"><apply id="S4.E1.m1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.2.1.cmml" xref="S4.E1.m1.1.1.1.1.2">superscript</csymbol><apply id="S4.E1.m1.1.1.1.1.2.2.cmml" xref="S4.E1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.2.2.1.cmml" xref="S4.E1.m1.1.1.1.1.2">subscript</csymbol><sum id="S4.E1.m1.1.1.1.1.2.2.2.cmml" xref="S4.E1.m1.1.1.1.1.2.2.2"></sum><apply id="S4.E1.m1.1.1.1.1.2.2.3.cmml" xref="S4.E1.m1.1.1.1.1.2.2.3"><eq id="S4.E1.m1.1.1.1.1.2.2.3.1.cmml" xref="S4.E1.m1.1.1.1.1.2.2.3.1"></eq><ci id="S4.E1.m1.1.1.1.1.2.2.3.2.cmml" xref="S4.E1.m1.1.1.1.1.2.2.3.2">ùëñ</ci><cn type="integer" id="S4.E1.m1.1.1.1.1.2.2.3.3.cmml" xref="S4.E1.m1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S4.E1.m1.1.1.1.1.2.3.cmml" xref="S4.E1.m1.1.1.1.1.2.3">ùëÅ</ci></apply><apply id="S4.E1.m1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1">superscript</csymbol><apply id="S4.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1"><minus id="S4.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1"></minus><apply id="S4.E1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.2.2">ùë¶</ci><ci id="S4.E1.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.2.3">ùëñ</ci></apply><apply id="S4.E1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.3.2">ùëù</ci><ci id="S4.E1.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.3.3">ùëñ</ci></apply></apply><cn type="integer" id="S4.E1.m1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.1c">MSE=\frac{1}{N}\sum_{i=1}^{N}(y_{i}-p_{i})^{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS3.SSS1.p6" class="ltx_para">
<table id="S4.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E2.m1.1" class="ltx_Math" alttext="MAE=\frac{1}{N}\sum_{i=1}^{N}|y_{i}-p_{i}|" display="block"><semantics id="S4.E2.m1.1a"><mrow id="S4.E2.m1.1.1" xref="S4.E2.m1.1.1.cmml"><mrow id="S4.E2.m1.1.1.3" xref="S4.E2.m1.1.1.3.cmml"><mi id="S4.E2.m1.1.1.3.2" xref="S4.E2.m1.1.1.3.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.1" xref="S4.E2.m1.1.1.3.1.cmml">‚Äã</mo><mi id="S4.E2.m1.1.1.3.3" xref="S4.E2.m1.1.1.3.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.1a" xref="S4.E2.m1.1.1.3.1.cmml">‚Äã</mo><mi id="S4.E2.m1.1.1.3.4" xref="S4.E2.m1.1.1.3.4.cmml">E</mi></mrow><mo id="S4.E2.m1.1.1.2" xref="S4.E2.m1.1.1.2.cmml">=</mo><mrow id="S4.E2.m1.1.1.1" xref="S4.E2.m1.1.1.1.cmml"><mfrac id="S4.E2.m1.1.1.1.3" xref="S4.E2.m1.1.1.1.3.cmml"><mn id="S4.E2.m1.1.1.1.3.2" xref="S4.E2.m1.1.1.1.3.2.cmml">1</mn><mi id="S4.E2.m1.1.1.1.3.3" xref="S4.E2.m1.1.1.1.3.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.1.2" xref="S4.E2.m1.1.1.1.2.cmml">‚Äã</mo><mrow id="S4.E2.m1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.cmml"><munderover id="S4.E2.m1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S4.E2.m1.1.1.1.1.2.2.2" xref="S4.E2.m1.1.1.1.1.2.2.2.cmml">‚àë</mo><mrow id="S4.E2.m1.1.1.1.1.2.2.3" xref="S4.E2.m1.1.1.1.1.2.2.3.cmml"><mi id="S4.E2.m1.1.1.1.1.2.2.3.2" xref="S4.E2.m1.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S4.E2.m1.1.1.1.1.2.2.3.1" xref="S4.E2.m1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S4.E2.m1.1.1.1.1.2.2.3.3" xref="S4.E2.m1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S4.E2.m1.1.1.1.1.2.3" xref="S4.E2.m1.1.1.1.1.2.3.cmml">N</mi></munderover><mrow id="S4.E2.m1.1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.E2.m1.1.1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.1.2.1.cmml">|</mo><mrow id="S4.E2.m1.1.1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.1.1.cmml"><msub id="S4.E2.m1.1.1.1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S4.E2.m1.1.1.1.1.1.1.1.2.2" xref="S4.E2.m1.1.1.1.1.1.1.1.2.2.cmml">y</mi><mi id="S4.E2.m1.1.1.1.1.1.1.1.2.3" xref="S4.E2.m1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S4.E2.m1.1.1.1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.1.1.1.cmml">‚àí</mo><msub id="S4.E2.m1.1.1.1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E2.m1.1.1.1.1.1.1.1.3.2" xref="S4.E2.m1.1.1.1.1.1.1.1.3.2.cmml">p</mi><mi id="S4.E2.m1.1.1.1.1.1.1.1.3.3" xref="S4.E2.m1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo stretchy="false" id="S4.E2.m1.1.1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.1.2.1.cmml">|</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.1b"><apply id="S4.E2.m1.1.1.cmml" xref="S4.E2.m1.1.1"><eq id="S4.E2.m1.1.1.2.cmml" xref="S4.E2.m1.1.1.2"></eq><apply id="S4.E2.m1.1.1.3.cmml" xref="S4.E2.m1.1.1.3"><times id="S4.E2.m1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.3.1"></times><ci id="S4.E2.m1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.3.2">ùëÄ</ci><ci id="S4.E2.m1.1.1.3.3.cmml" xref="S4.E2.m1.1.1.3.3">ùê¥</ci><ci id="S4.E2.m1.1.1.3.4.cmml" xref="S4.E2.m1.1.1.3.4">ùê∏</ci></apply><apply id="S4.E2.m1.1.1.1.cmml" xref="S4.E2.m1.1.1.1"><times id="S4.E2.m1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.2"></times><apply id="S4.E2.m1.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.3"><divide id="S4.E2.m1.1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.1.3"></divide><cn type="integer" id="S4.E2.m1.1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.1.3.2">1</cn><ci id="S4.E2.m1.1.1.1.3.3.cmml" xref="S4.E2.m1.1.1.1.3.3">ùëÅ</ci></apply><apply id="S4.E2.m1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1"><apply id="S4.E2.m1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.2.1.cmml" xref="S4.E2.m1.1.1.1.1.2">superscript</csymbol><apply id="S4.E2.m1.1.1.1.1.2.2.cmml" xref="S4.E2.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.2.2.1.cmml" xref="S4.E2.m1.1.1.1.1.2">subscript</csymbol><sum id="S4.E2.m1.1.1.1.1.2.2.2.cmml" xref="S4.E2.m1.1.1.1.1.2.2.2"></sum><apply id="S4.E2.m1.1.1.1.1.2.2.3.cmml" xref="S4.E2.m1.1.1.1.1.2.2.3"><eq id="S4.E2.m1.1.1.1.1.2.2.3.1.cmml" xref="S4.E2.m1.1.1.1.1.2.2.3.1"></eq><ci id="S4.E2.m1.1.1.1.1.2.2.3.2.cmml" xref="S4.E2.m1.1.1.1.1.2.2.3.2">ùëñ</ci><cn type="integer" id="S4.E2.m1.1.1.1.1.2.2.3.3.cmml" xref="S4.E2.m1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S4.E2.m1.1.1.1.1.2.3.cmml" xref="S4.E2.m1.1.1.1.1.2.3">ùëÅ</ci></apply><apply id="S4.E2.m1.1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1"><abs id="S4.E2.m1.1.1.1.1.1.2.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.2"></abs><apply id="S4.E2.m1.1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1"><minus id="S4.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1"></minus><apply id="S4.E2.m1.1.1.1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.2.2">ùë¶</ci><ci id="S4.E2.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.2.3">ùëñ</ci></apply><apply id="S4.E2.m1.1.1.1.1.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.3.2">ùëù</ci><ci id="S4.E2.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.3.3">ùëñ</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.1c">MAE=\frac{1}{N}\sum_{i=1}^{N}|y_{i}-p_{i}|</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS3.SSS1.p7" class="ltx_para">
<p id="S4.SS3.SSS1.p7.3" class="ltx_p">Where <math id="S4.SS3.SSS1.p7.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS3.SSS1.p7.1.m1.1a"><mi id="S4.SS3.SSS1.p7.1.m1.1.1" xref="S4.SS3.SSS1.p7.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p7.1.m1.1b"><ci id="S4.SS3.SSS1.p7.1.m1.1.1.cmml" xref="S4.SS3.SSS1.p7.1.m1.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p7.1.m1.1c">N</annotation></semantics></math> is the number of samples in the set, <math id="S4.SS3.SSS1.p7.2.m2.1" class="ltx_Math" alttext="y_{i}" display="inline"><semantics id="S4.SS3.SSS1.p7.2.m2.1a"><msub id="S4.SS3.SSS1.p7.2.m2.1.1" xref="S4.SS3.SSS1.p7.2.m2.1.1.cmml"><mi id="S4.SS3.SSS1.p7.2.m2.1.1.2" xref="S4.SS3.SSS1.p7.2.m2.1.1.2.cmml">y</mi><mi id="S4.SS3.SSS1.p7.2.m2.1.1.3" xref="S4.SS3.SSS1.p7.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p7.2.m2.1b"><apply id="S4.SS3.SSS1.p7.2.m2.1.1.cmml" xref="S4.SS3.SSS1.p7.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p7.2.m2.1.1.1.cmml" xref="S4.SS3.SSS1.p7.2.m2.1.1">subscript</csymbol><ci id="S4.SS3.SSS1.p7.2.m2.1.1.2.cmml" xref="S4.SS3.SSS1.p7.2.m2.1.1.2">ùë¶</ci><ci id="S4.SS3.SSS1.p7.2.m2.1.1.3.cmml" xref="S4.SS3.SSS1.p7.2.m2.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p7.2.m2.1c">y_{i}</annotation></semantics></math> corresponds to the current real value, and <math id="S4.SS3.SSS1.p7.3.m3.1" class="ltx_Math" alttext="p_{i}" display="inline"><semantics id="S4.SS3.SSS1.p7.3.m3.1a"><msub id="S4.SS3.SSS1.p7.3.m3.1.1" xref="S4.SS3.SSS1.p7.3.m3.1.1.cmml"><mi id="S4.SS3.SSS1.p7.3.m3.1.1.2" xref="S4.SS3.SSS1.p7.3.m3.1.1.2.cmml">p</mi><mi id="S4.SS3.SSS1.p7.3.m3.1.1.3" xref="S4.SS3.SSS1.p7.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p7.3.m3.1b"><apply id="S4.SS3.SSS1.p7.3.m3.1.1.cmml" xref="S4.SS3.SSS1.p7.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p7.3.m3.1.1.1.cmml" xref="S4.SS3.SSS1.p7.3.m3.1.1">subscript</csymbol><ci id="S4.SS3.SSS1.p7.3.m3.1.1.2.cmml" xref="S4.SS3.SSS1.p7.3.m3.1.1.2">ùëù</ci><ci id="S4.SS3.SSS1.p7.3.m3.1.1.3.cmml" xref="S4.SS3.SSS1.p7.3.m3.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p7.3.m3.1c">p_{i}</annotation></semantics></math> is the predicted value by the regression model.</p>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Robot movement prediction</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.2" class="ltx_p">We train the LSTM and GRU models with the Adam optimizer over 500 epochs. The learning rate equals 0.0001, and the batch size is 4096. Another variation in the experiments with the recurrent models is the window size of the data used to make inferences (past window - <math id="S4.SS3.SSS2.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.SS3.SSS2.p1.1.m1.1a"><mi id="S4.SS3.SSS2.p1.1.m1.1.1" xref="S4.SS3.SSS2.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.1.m1.1b"><ci id="S4.SS3.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS2.p1.1.m1.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.1.m1.1c">n</annotation></semantics></math>) and the window size of the predicted data (future window - <math id="S4.SS3.SSS2.p1.2.m2.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S4.SS3.SSS2.p1.2.m2.1a"><mi id="S4.SS3.SSS2.p1.2.m2.1.1" xref="S4.SS3.SSS2.p1.2.m2.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.2.m2.1b"><ci id="S4.SS3.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS3.SSS2.p1.2.m2.1.1">ùëì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.2.m2.1c">f</annotation></semantics></math>). We designed grid search experiments to analyze the impact of past and future window variation in the prediction results. Table <a href="#S4.T2" title="Table 2 ‚Ä£ 4.3.2 Robot movement prediction ‚Ä£ 4.3 Experimental settings ‚Ä£ 4 Materials and methods ‚Ä£ A framework for robotic arm pose estimation and movement prediction based on deep and extreme learning models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the parameters for the grid search.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Grid search parameters used in the robotic movement prediction.</figcaption>
<table id="S4.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.2.3.1" class="ltx_tr">
<th id="S4.T2.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Parameter</th>
<th id="S4.T2.2.3.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Values</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Past window (<math id="S4.T2.1.1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.T2.1.1.1.m1.1a"><mi id="S4.T2.1.1.1.m1.1.1" xref="S4.T2.1.1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">n</annotation></semantics></math>)</th>
<td id="S4.T2.1.1.2" class="ltx_td ltx_align_left ltx_border_t">10, 20, 30, 45, 60</td>
</tr>
<tr id="S4.T2.2.2" class="ltx_tr">
<th id="S4.T2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">Future window (<math id="S4.T2.2.2.1.m1.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S4.T2.2.2.1.m1.1a"><mi id="S4.T2.2.2.1.m1.1.1" xref="S4.T2.2.2.1.m1.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.1.m1.1b"><ci id="S4.T2.2.2.1.m1.1.1.cmml" xref="S4.T2.2.2.1.m1.1.1">ùëì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.1.m1.1c">f</annotation></semantics></math>)</th>
<td id="S4.T2.2.2.2" class="ltx_td ltx_align_left ltx_border_b">1, 5, 15, 30, 60, 90, 120</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and discussion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This section presents the main results for both pose detection and movement prediction of a robotic arm.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Robotic arm pose estimation</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">First, we present the experimental results for the robot pose detection. Table <a href="#S5.T3" title="Table 3 ‚Ä£ 5.1 Robotic arm pose estimation ‚Ä£ 5 Results and discussion ‚Ä£ A framework for robotic arm pose estimation and movement prediction based on deep and extreme learning models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the results obtained for robot pose detection using DL models, considering the mean and standard deviation of MSE and MAE reported for each DL model in the cross-validation training. All MSE and MAE results reported in this work are real values (difference in pixels). That is, they are not normalized.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>MSE and MAE results reported by each DL models for robotic arm pose detection.</figcaption>
<table id="S5.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T3.1.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Model</th>
<td id="S5.T3.1.1.1.2" class="ltx_td ltx_align_left ltx_border_t">MSE</td>
<td id="S5.T3.1.1.1.3" class="ltx_td ltx_align_left ltx_border_t">MAE</td>
</tr>
<tr id="S5.T3.1.2.2" class="ltx_tr">
<th id="S5.T3.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">AlexNet</th>
<td id="S5.T3.1.2.2.2" class="ltx_td ltx_align_left ltx_border_t">13.65¬±8.27</td>
<td id="S5.T3.1.2.2.3" class="ltx_td ltx_align_left ltx_border_t">2.37¬±0.29</td>
</tr>
<tr id="S5.T3.1.3.3" class="ltx_tr">
<th id="S5.T3.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">DenseNet-121</th>
<td id="S5.T3.1.3.3.2" class="ltx_td ltx_align_left">07.33¬±4.67</td>
<td id="S5.T3.1.3.3.3" class="ltx_td ltx_align_left">1.67¬±0.22</td>
</tr>
<tr id="S5.T3.1.4.4" class="ltx_tr">
<th id="S5.T3.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">ResNet-34</th>
<td id="S5.T3.1.4.4.2" class="ltx_td ltx_align_left">12.27¬±7.61</td>
<td id="S5.T3.1.4.4.3" class="ltx_td ltx_align_left">2.31¬±0.91</td>
</tr>
<tr id="S5.T3.1.5.5" class="ltx_tr">
<th id="S5.T3.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">SCNet50-V1D</th>
<td id="S5.T3.1.5.5.2" class="ltx_td ltx_align_left">
<span id="S5.T3.1.5.5.2.1" class="ltx_text ltx_font_bold">04.12</span>¬±<span id="S5.T3.1.5.5.2.2" class="ltx_text ltx_font_bold">3.20</span>
</td>
<td id="S5.T3.1.5.5.3" class="ltx_td ltx_align_left">
<span id="S5.T3.1.5.5.3.1" class="ltx_text ltx_font_bold">1.24</span>¬±<span id="S5.T3.1.5.5.3.2" class="ltx_text ltx_font_bold">0.13</span>
</td>
</tr>
<tr id="S5.T3.1.6.6" class="ltx_tr">
<th id="S5.T3.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">SqueezeNet</th>
<td id="S5.T3.1.6.6.2" class="ltx_td ltx_align_left">08.91¬±4.91</td>
<td id="S5.T3.1.6.6.3" class="ltx_td ltx_align_left">1.90¬±0.17</td>
</tr>
<tr id="S5.T3.1.7.7" class="ltx_tr">
<th id="S5.T3.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">VGG-11</th>
<td id="S5.T3.1.7.7.2" class="ltx_td ltx_align_left ltx_border_b">27.34¬±3.56</td>
<td id="S5.T3.1.7.7.3" class="ltx_td ltx_align_left ltx_border_b">3.92¬±0.20</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">By analyzing the results, we can verify that the SCNet-50-V1D model outperformed the others. The exception is for observed the standard deviation. We can attribute the superiority of the SCNet-50-V1d model to its robustness due to performing more operations on its self-calibrated convolutions, unlike vanilla convolutions. The SCNet-50-V1D model reached an MSE equal to 4.12 and an MAE equal to 1.24.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">The model that comes closest to SCNet-50-V1D was the DenseNet-121 model. It achieved an MSE equal to 7.33 and an MAE equal to 1.67. According to Zhang <span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib47" title="" class="ltx_ref">zhang2021resnet </a></cite>, DenseNet models are known for their superior feature generalization with fewer parameters than ResNet and other older DL models. ResNet-34 and AlexNet models could not reach error like the SCNet-50-V1D models. While ResNet-34 reached MSE of 12.27 and MAE of 2.31, AlexNet reached MSE of 13.65 and MAE of 2.37. Note that they stand only a little behind in terms of errors from what was reported by the two best models SCNet-50-V1D and DenseNet-121.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">Between these two best models and ResNet-34, we find SqueezeNet that achieved an MSE and and an MAE of8.91 and 1.90 respectively. The SqueezeNet model has a smaller amount of parameters compared to ResNet-34, and this fact may have weighed on the results obtained. ResNet-34 models generally converge better when there is a large amount of data used for training <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib43" title="" class="ltx_ref">he2016deep </a></cite>. This may have directly impacted the performance and feature generalization of the model, given that we are facing a learning problem with relatively a small amount of data.</p>
</div>
<div id="S5.SS1.p5" class="ltx_para">
<p id="S5.SS1.p5.1" class="ltx_p">Last but not least, we have the AlexNet model, with an MSE equal to 13.65 and an MAE equal to 2.37, and the VGG-11 model which resulted in am MSE of 27.34 and an MAE value of 3.92. The AlexNet and VGG-11 models contain, in their architectures, several stacked convolutional layers, that is, without concatenation operations or residual operations, as is the case with the DenseNet or ResNet models. The fact of not having transfer features in their layers can impoverish the feature generalization process, causing a more significant error.</p>
</div>
<div id="S5.SS1.p6" class="ltx_para">
<p id="S5.SS1.p6.1" class="ltx_p">All the analysis previously performed considered only the average of the errors of the five cross-validation runs. However, we still need to consider the standard deviation. To carry out a more concise analysis of the behavior of the models, we can also present the results in boxplot form in Fig. <a href="#S5.F7" title="Figure 7 ‚Ä£ 5.1 Robotic arm pose estimation ‚Ä£ 5 Results and discussion ‚Ä£ A framework for robotic arm pose estimation and movement prediction based on deep and extreme learning models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. We consider the MSE error reached by each model into the boxplot.</p>
</div>
<figure id="S5.F7" class="ltx_figure">
<div id="S5.F7.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:307.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-55.6pt,39.5pt) scale(0.795755956285579,0.795755956285579) ;"><img src="/html/2205.13994/assets/figures/boxplot1.png" id="S5.F7.1.g1" class="ltx_graphics ltx_img_landscape" width="754" height="535" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Boxplot representation for MSE results reached by the DL models.</figcaption>
</figure>
<div id="S5.SS1.p7" class="ltx_para">
<p id="S5.SS1.p7.1" class="ltx_p">One can observe the error variance of each of the analyzed models. VGG and ResNet models reached the highest standard deviation and also provided a more significant variation in the boxplot. The other models did not show much variation, except for the presentation of outliers, which impacted the increase in the standard deviation for the models in question.</p>
</div>
<div id="S5.SS1.p8" class="ltx_para">
<p id="S5.SS1.p8.1" class="ltx_p">Regarding outliers, we can observe this behavior in the AlexNet, SqueezeNet, ResNet-34, and SCNet-50V1D models. This behavior may have happened coincidentally in one of the cross-validation executions, which impacts that the data used in training causing poor learning in the models compared to the other executions. The VGG-11 and ResNet-34 models showed, in general, poor learning performances in all executions. For this reason, they may not present outliers in their executions.</p>
</div>
<div id="S5.SS1.p9" class="ltx_para">
<p id="S5.SS1.p9.1" class="ltx_p">The boxplot analysis gives us even greater certainty about the models that performed better in learning. We can see that the DenseNet-121 and SCNet-50-V1D models do not overlap with other models in the boxplot. More specifically, we can see that the SCNet-50-V1D model does not overlap with the DenseNet-121 model, which shows that the samples are statistically different. What could counter this argument are the outliers that both models have in common. However, the error obtained in the outlier with SCNet-50-V1D was lower.</p>
</div>
<div id="S5.SS1.p10" class="ltx_para">
<p id="S5.SS1.p10.1" class="ltx_p">This last statistical analysis reinforces the argument that the SCNet-50-V1D model provides better results for detecting the pose of the robotic arm. Furthermore, we can verify that SCConvs offer better results for a regression task, in addition to the functions indicated by the model proponents, Liu et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib9" title="" class="ltx_ref">liu2020improving </a></cite>. In this work, SCConvs contributed to the construction of feature representation at different scales through self-calibrating operations, contributing to a better generalization of the data. It was known in the literature that SCConvs could present better results in large-scale problems with a high amount of data. According to the experiments presented in this work, there is evidence that SCConvs can also provide better representation and generalization of data in small-scale problems.</p>
</div>
<div id="S5.SS1.p11" class="ltx_para">
<p id="S5.SS1.p11.1" class="ltx_p">Previously, we presented the results for detecting the robotic arm pose using only the DL models. Next, we analyze the impact of using an ELM network to refine the results obtained by the SCNet-50-V1D model. Fig. <a href="#S5.F8" title="Figure 8 ‚Ä£ 5.1 Robotic arm pose estimation ‚Ä£ 5 Results and discussion ‚Ä£ A framework for robotic arm pose estimation and movement prediction based on deep and extreme learning models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> presents a graph containing the number of neurons <span id="S5.SS1.p11.1.1" class="ltx_text ltx_font_italic">vs.</span> error (MSE) reached by the specific ELM network.</p>
</div>
<figure id="S5.F8" class="ltx_figure">
<div id="S5.F8.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:292.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(76.2pt,-51.4pt) scale(1.54241646360444,1.54241646360444) ;"><img src="/html/2205.13994/assets/figures/elm_graph.png" id="S5.F8.1.g1" class="ltx_graphics ltx_img_landscape" width="389" height="262" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Relation between the number of neurons and the error reached by the ELM with different activation.</figcaption>
</figure>
<div id="S5.SS1.p12" class="ltx_para">
<p id="S5.SS1.p12.1" class="ltx_p">The refinement of the poses of the robotic arm with ELM can occur with any activation function for the neurons. However, the number of neurons is an essential factor that must be considered for the refinement step. The ELM networks with RBF-L2 kernel and Tanh as activation functions presented similar MSE results during all the variations of the number of neurons. While, ELM networks with Linear kernel and RBF provide results closer to the other two mentioned above, using more than 700 neurons.</p>
</div>
<div id="S5.SS1.p13" class="ltx_para">
<p id="S5.SS1.p13.1" class="ltx_p">By using 1000 neurons, we observe that the results of all ELM networks are similar. However, it is worth mentioning that, if the production environment chooses to use the more lightweight ELM model, the correct option would be to use ELM with RBF-L2 kernel or Tanh with a smaller number of neurons. These last two activation functions provide similar results when using 1000 neurons with the other activation functions. On the other hand, if one chooses to use the ELM network with better learning performance, a more detailed analysis of the obtained results is necessary.</p>
</div>
<div id="S5.SS1.p14" class="ltx_para">
<p id="S5.SS1.p14.1" class="ltx_p">For a better and fairer analysis, we equaled the number of neurons to 1000 for the use of all activation functions. In this analysis, we consider the MSE and MAE metrics. Table <a href="#S5.T4" title="Table 4 ‚Ä£ 5.1 Robotic arm pose estimation ‚Ä£ 5 Results and discussion ‚Ä£ A framework for robotic arm pose estimation and movement prediction based on deep and extreme learning models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents the results obtained with this analysis, also considering 5-fold cross-validation.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>MSE and MAE results reported by each activation functions in the ELM network for robotic arm pose refinement.</figcaption>
<table id="S5.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.1.1.1" class="ltx_tr">
<th id="S5.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Model</th>
<th id="S5.T4.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">MSE</th>
<th id="S5.T4.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">MAE</th>
</tr>
<tr id="S5.T4.1.2.2" class="ltx_tr">
<th id="S5.T4.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">SCNet50-V1D</th>
<th id="S5.T4.1.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">4.12¬±3.20</th>
<th id="S5.T4.1.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">1.24¬±0.13</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.1.3.1" class="ltx_tr">
<td id="S5.T4.1.3.1.1" class="ltx_td ltx_align_left ltx_border_t">SCNet50-V1D + ELM-Linear</td>
<td id="S5.T4.1.3.1.2" class="ltx_td ltx_align_left ltx_border_t">3.76¬±3.11</td>
<td id="S5.T4.1.3.1.3" class="ltx_td ltx_align_left ltx_border_t">1.15¬±0.14</td>
</tr>
<tr id="S5.T4.1.4.2" class="ltx_tr">
<td id="S5.T4.1.4.2.1" class="ltx_td ltx_align_left">SCNet50-V1D + ELM-RBF</td>
<td id="S5.T4.1.4.2.2" class="ltx_td ltx_align_left">5.24¬±3.47</td>
<td id="S5.T4.1.4.2.3" class="ltx_td ltx_align_left">1.47¬±0.13</td>
</tr>
<tr id="S5.T4.1.5.3" class="ltx_tr">
<td id="S5.T4.1.5.3.1" class="ltx_td ltx_align_left">SCNet50-V1D + ELM-RBF-L2</td>
<td id="S5.T4.1.5.3.2" class="ltx_td ltx_align_left">3.74¬±3.20</td>
<td id="S5.T4.1.5.3.3" class="ltx_td ltx_align_left">1.14¬±0.13</td>
</tr>
<tr id="S5.T4.1.6.4" class="ltx_tr">
<td id="S5.T4.1.6.4.1" class="ltx_td ltx_align_left ltx_border_b">SCNet50-V1D + ELM-Tanh</td>
<td id="S5.T4.1.6.4.2" class="ltx_td ltx_align_left ltx_border_b">4.73¬±3.19</td>
<td id="S5.T4.1.6.4.3" class="ltx_td ltx_align_left ltx_border_b">1.37¬±0.13</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS1.p15" class="ltx_para">
<p id="S5.SS1.p15.1" class="ltx_p">We observed that ELM with RBF-L2 kernel and Linear reached the smallest errors, considering only an average of the errors obtained in each validation fold. The ELM with the RBF-L2 kernel achieved an MSE of 3.74 and an MAE of 1.14, while the ELM with Linear activation achieved an MSE of 3.76 and an MAE of 1.15. Both generate results that are very close to each other and below those for SCNet-50-V1D (Table <a href="#S5.T3" title="Table 3 ‚Ä£ 5.1 Robotic arm pose estimation ‚Ä£ 5 Results and discussion ‚Ä£ A framework for robotic arm pose estimation and movement prediction based on deep and extreme learning models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). RBF-L2 provides a squared error-based regularization for regression problems, which may explain its smaller errors.</p>
</div>
<div id="S5.SS1.p16" class="ltx_para">
<p id="S5.SS1.p16.1" class="ltx_p">We also analyze ELM with RBF kernel, which achieved an MSE of 5.24, and an MAE of 1.47, while with Tanh it reached an MSE of 4.73 and an MAE of 1.37. Observe that both configurations worsened the error results that SCNet-50-V1D achieved. Despite always obtaining error results that varied little regardless of the number of neurons (compared to ELM-RBF and ELM-Linear), ELM with Tanh activation did not show interesting results with 1000 neurons. The distribution of neurons into 1000 data may have impacted the low results of Tanh (which is more suitable for binary classification) and RBF (that could not provide a better approximation of the function).</p>
</div>
<div id="S5.SS1.p17" class="ltx_para">
<p id="S5.SS1.p17.1" class="ltx_p">Considering an analysis from the standard deviation viewpoint, we can see that ELM with RBF-L2 kernel and Linear do overlap. Fig. <a href="#S5.F9" title="Figure 9 ‚Ä£ 5.1 Robotic arm pose estimation ‚Ä£ 5 Results and discussion ‚Ä£ A framework for robotic arm pose estimation and movement prediction based on deep and extreme learning models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> shows a boxplot with all the results obtained by SCNet-50-V1D with ELM. For comparison purposes, the boxplot also shows the results of SCNet-50-V1D without ELM.</p>
</div>
<figure id="S5.F9" class="ltx_figure">
<div id="S5.F9.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:307.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-55.6pt,39.5pt) scale(0.795755956285579,0.795755956285579) ;"><img src="/html/2205.13994/assets/figures/boxplot2.png" id="S5.F9.1.g1" class="ltx_graphics ltx_img_landscape" width="754" height="535" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Boxplot representation for MSE results reached by SCNet-50-V1D with and without ELM network.</figcaption>
</figure>
<div id="S5.SS1.p18" class="ltx_para">
<p id="S5.SS1.p18.1" class="ltx_p">We verified that, regarding the models with ELM, the best learning performance with RBF-L2 and Linear is confirmed. These two models obtained smaller error results in all five execution runs, while not denoting any overlap with the ELM models with RBF and Tanh. In addition, the outlier that had been previously evidenced only with SCNet-50-V1D appears again, proving that in one of the execution rounds, there was indeed some difficulty in the modeling and learning process of the models. In addition, the ELM with Tanh, in particular, presented a second outlier, however with a smaller MSE, while having a significant convergence in one of the validation folds.</p>
</div>
<div id="S5.SS1.p19" class="ltx_para">
<p id="S5.SS1.p19.1" class="ltx_p">The boxplot analysis also indicates that it is impossible to state which of the models is better, ELM with RBF-L2, or Linear ELM. This is because result‚Äôs samples are not statistically different. Both models reached similar results and behavior across all the validation fold, as reflected by the MSE. Still, about these last two models, their results generally do not overlap with the results of the SCNet-50-V1D model without ELM. This phenomenon demonstrates that the samples may not be statistically different and that the use of ELM networks refines the robotic arm poses regression error results. Improving detection results is essential for a better representation of poses, as wrongly detected keypoints can negatively impact the results, causing risks in a critical HRC system.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Robotic arm movement prediction</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">In this section, we present the experimental results for predicting the future movement of the robotic arm. First, we start by showing the results using the LSTM model. Then we present the results related to the GRU model. Due to the many results for each model, we opted to show only MSE results. All experiments considered SCNet-50-V1D with ELM-RBF-L2 as a robotic arm pose detector over the video recording.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">Table <a href="#S5.T5" title="Table 5 ‚Ä£ 5.2 Robotic arm movement prediction ‚Ä£ 5 Results and discussion ‚Ä£ A framework for robotic arm pose estimation and movement prediction based on deep and extreme learning models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the results obtained for predicting the robot‚Äôs future movement using only the LSTM model for each configuration defined in the grid search experiments.</p>
</div>
<figure id="S5.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>MSE results reported by LSTM model for robotic arm movement prediction.</figcaption>
<p id="S5.T5.1" class="ltx_p ltx_align_center"><span id="S5.T5.1.1" class="ltx_text">
<span id="S5.T5.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:401.9pt;height:126pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S5.T5.1.1.1.1" class="ltx_p"><span id="S5.T5.1.1.1.1.1" class="ltx_text">
<span id="S5.T5.1.1.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_tbody">
<span id="S5.T5.1.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S5.T5.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S5.T5.1.1.1.1.1.1.1.1.1.1" class="ltx_ERROR undefined">\cellcolor</span>[HTML]FFFFFF</span>
<span id="S5.T5.1.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_left ltx_border_t ltx_colspan ltx_colspan_7"><span id="S5.T5.1.1.1.1.1.1.1.1.2.1" class="ltx_ERROR undefined">\cellcolor</span>[HTML]FFFFFFFuture Window</span></span>
<span id="S5.T5.1.1.1.1.1.1.2.2" class="ltx_tr">
<span id="S5.T5.1.1.1.1.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T5.1.1.1.1.1.1.2.2.1.1" class="ltx_text"><span id="S5.T5.1.1.1.1.1.1.2.2.1.1.1" class="ltx_ERROR undefined">\cellcolor</span>[HTML]FFFFFFPast window</span></span>
<span id="S5.T5.1.1.1.1.1.1.2.2.2" class="ltx_td ltx_align_left ltx_border_t">1</span>
<span id="S5.T5.1.1.1.1.1.1.2.2.3" class="ltx_td ltx_align_left ltx_border_t">5</span>
<span id="S5.T5.1.1.1.1.1.1.2.2.4" class="ltx_td ltx_align_left ltx_border_t">15</span>
<span id="S5.T5.1.1.1.1.1.1.2.2.5" class="ltx_td ltx_align_left ltx_border_t">30</span>
<span id="S5.T5.1.1.1.1.1.1.2.2.6" class="ltx_td ltx_align_left ltx_border_t">60</span>
<span id="S5.T5.1.1.1.1.1.1.2.2.7" class="ltx_td ltx_align_left ltx_border_t">90</span>
<span id="S5.T5.1.1.1.1.1.1.2.2.8" class="ltx_td ltx_align_left ltx_border_t">120</span></span>
<span id="S5.T5.1.1.1.1.1.1.3.3" class="ltx_tr">
<span id="S5.T5.1.1.1.1.1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">10</span>
<span id="S5.T5.1.1.1.1.1.1.3.3.2" class="ltx_td ltx_align_left ltx_border_t">28.89</span>
<span id="S5.T5.1.1.1.1.1.1.3.3.3" class="ltx_td ltx_align_left ltx_border_t">27.14</span>
<span id="S5.T5.1.1.1.1.1.1.3.3.4" class="ltx_td ltx_align_left ltx_border_t">33.27</span>
<span id="S5.T5.1.1.1.1.1.1.3.3.5" class="ltx_td ltx_align_left ltx_border_t">40.81</span>
<span id="S5.T5.1.1.1.1.1.1.3.3.6" class="ltx_td ltx_align_left ltx_border_t">59.46</span>
<span id="S5.T5.1.1.1.1.1.1.3.3.7" class="ltx_td ltx_align_left ltx_border_t">74.02</span>
<span id="S5.T5.1.1.1.1.1.1.3.3.8" class="ltx_td ltx_align_left ltx_border_t">89.69</span></span>
<span id="S5.T5.1.1.1.1.1.1.4.4" class="ltx_tr">
<span id="S5.T5.1.1.1.1.1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">20</span>
<span id="S5.T5.1.1.1.1.1.1.4.4.2" class="ltx_td ltx_align_left">32.61</span>
<span id="S5.T5.1.1.1.1.1.1.4.4.3" class="ltx_td ltx_align_left">29.95</span>
<span id="S5.T5.1.1.1.1.1.1.4.4.4" class="ltx_td ltx_align_left">36.31</span>
<span id="S5.T5.1.1.1.1.1.1.4.4.5" class="ltx_td ltx_align_left">40.03</span>
<span id="S5.T5.1.1.1.1.1.1.4.4.6" class="ltx_td ltx_align_left">60.98</span>
<span id="S5.T5.1.1.1.1.1.1.4.4.7" class="ltx_td ltx_align_left">77.65</span>
<span id="S5.T5.1.1.1.1.1.1.4.4.8" class="ltx_td ltx_align_left">100.03</span></span>
<span id="S5.T5.1.1.1.1.1.1.5.5" class="ltx_tr">
<span id="S5.T5.1.1.1.1.1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">30</span>
<span id="S5.T5.1.1.1.1.1.1.5.5.2" class="ltx_td ltx_align_left">25.69</span>
<span id="S5.T5.1.1.1.1.1.1.5.5.3" class="ltx_td ltx_align_left">32.79</span>
<span id="S5.T5.1.1.1.1.1.1.5.5.4" class="ltx_td ltx_align_left">31.86</span>
<span id="S5.T5.1.1.1.1.1.1.5.5.5" class="ltx_td ltx_align_left">44.02</span>
<span id="S5.T5.1.1.1.1.1.1.5.5.6" class="ltx_td ltx_align_left">51.01</span>
<span id="S5.T5.1.1.1.1.1.1.5.5.7" class="ltx_td ltx_align_left">71.69</span>
<span id="S5.T5.1.1.1.1.1.1.5.5.8" class="ltx_td ltx_align_left">93.87</span></span>
<span id="S5.T5.1.1.1.1.1.1.6.6" class="ltx_tr">
<span id="S5.T5.1.1.1.1.1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">45</span>
<span id="S5.T5.1.1.1.1.1.1.6.6.2" class="ltx_td ltx_align_left">34.19</span>
<span id="S5.T5.1.1.1.1.1.1.6.6.3" class="ltx_td ltx_align_left">33.38</span>
<span id="S5.T5.1.1.1.1.1.1.6.6.4" class="ltx_td ltx_align_left">33.11</span>
<span id="S5.T5.1.1.1.1.1.1.6.6.5" class="ltx_td ltx_align_left">39.51</span>
<span id="S5.T5.1.1.1.1.1.1.6.6.6" class="ltx_td ltx_align_left">61.49</span>
<span id="S5.T5.1.1.1.1.1.1.6.6.7" class="ltx_td ltx_align_left">72.38</span>
<span id="S5.T5.1.1.1.1.1.1.6.6.8" class="ltx_td ltx_align_left">101.39</span></span>
<span id="S5.T5.1.1.1.1.1.1.7.7" class="ltx_tr">
<span id="S5.T5.1.1.1.1.1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">60</span>
<span id="S5.T5.1.1.1.1.1.1.7.7.2" class="ltx_td ltx_align_left ltx_border_b">29.65</span>
<span id="S5.T5.1.1.1.1.1.1.7.7.3" class="ltx_td ltx_align_left ltx_border_b">27.56</span>
<span id="S5.T5.1.1.1.1.1.1.7.7.4" class="ltx_td ltx_align_left ltx_border_b">37.24</span>
<span id="S5.T5.1.1.1.1.1.1.7.7.5" class="ltx_td ltx_align_left ltx_border_b">42.62</span>
<span id="S5.T5.1.1.1.1.1.1.7.7.6" class="ltx_td ltx_align_left ltx_border_b">49.88</span>
<span id="S5.T5.1.1.1.1.1.1.7.7.7" class="ltx_td ltx_align_left ltx_border_b">71.88</span>
<span id="S5.T5.1.1.1.1.1.1.7.7.8" class="ltx_td ltx_align_left ltx_border_b">98.97</span></span>
</span>
</span></span></span>
</span></span></span></p>
</figure>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">The results show that the future window parameter has a significant impact on the results. We first highlight the use the prediction window values of 1 and 5, because in some cases, prediction with a window equal to 5 outperformed prediction using a window equal to 1, independently of almost all sizes of the past data window, except when this is equal to 30. This demonstrates one of the characteristics of LSTM: it is easier to learn long-term data (despite the the use of a small dataset) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib12" title="" class="ltx_ref">hochreiter1997long </a></cite>. We also highlight that the LSTM algorithm presents good results of future prediction of the robot with a small past window with a future window equal to 5. The smaller the past window, the less data will be processed and propagated in the network, which may decrease the amount of processing required to arrive at a low MSE result.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">Despite highlighting the prediction window of 5, LSTM tended to increase MSE as the prediction window size is more significant, regardless of the size of the past data window. MSE results start to increase considerably after using 30 frames (or 1 second) in the future forecast window. Despite this, we can still observe that the use of a past window equal to 10 provided better forecast results, in general, except against the past window similar to 30 (in some cases). Nonetheless, the results obtained in these two prediction windows remained very close.</p>
</div>
<div id="S5.SS2.p5" class="ltx_para">
<p id="S5.SS2.p5.1" class="ltx_p">Table <a href="#S5.T6" title="Table 6 ‚Ä£ 5.2 Robotic arm movement prediction ‚Ä£ 5 Results and discussion ‚Ä£ A framework for robotic arm pose estimation and movement prediction based on deep and extreme learning models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the results obtained for predicting the robot‚Äôs future movement using only the GRU model for each configuration defined in the grid search experiments.</p>
</div>
<figure id="S5.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>MSE results reported by GRU model for robotic arm movement prediction.</figcaption>
<p id="S5.T6.1" class="ltx_p ltx_align_center"><span id="S5.T6.1.1" class="ltx_text">
<span id="S5.T6.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:314.5pt;height:126pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S5.T6.1.1.1.1" class="ltx_p"><span id="S5.T6.1.1.1.1.1" class="ltx_text">
<span id="S5.T6.1.1.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="S5.T6.1.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S5.T6.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></span>
<span id="S5.T6.1.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t ltx_colspan ltx_colspan_7">Future Window</span></span>
<span id="S5.T6.1.1.1.1.1.1.2.2" class="ltx_tr">
<span id="S5.T6.1.1.1.1.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row"><span id="S5.T6.1.1.1.1.1.1.2.2.1.1" class="ltx_text">Past window</span></span>
<span id="S5.T6.1.1.1.1.1.1.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">1</span>
<span id="S5.T6.1.1.1.1.1.1.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">5</span>
<span id="S5.T6.1.1.1.1.1.1.2.2.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">15</span>
<span id="S5.T6.1.1.1.1.1.1.2.2.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">30</span>
<span id="S5.T6.1.1.1.1.1.1.2.2.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">60</span>
<span id="S5.T6.1.1.1.1.1.1.2.2.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">90</span>
<span id="S5.T6.1.1.1.1.1.1.2.2.8" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">120</span></span>
</span>
<span class="ltx_tbody">
<span id="S5.T6.1.1.1.1.1.1.3.1" class="ltx_tr">
<span id="S5.T6.1.1.1.1.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">10</span>
<span id="S5.T6.1.1.1.1.1.1.3.1.2" class="ltx_td ltx_align_left ltx_border_t">15.58</span>
<span id="S5.T6.1.1.1.1.1.1.3.1.3" class="ltx_td ltx_align_left ltx_border_t">20.36</span>
<span id="S5.T6.1.1.1.1.1.1.3.1.4" class="ltx_td ltx_align_left ltx_border_t">21.25</span>
<span id="S5.T6.1.1.1.1.1.1.3.1.5" class="ltx_td ltx_align_left ltx_border_t">26.27</span>
<span id="S5.T6.1.1.1.1.1.1.3.1.6" class="ltx_td ltx_align_left ltx_border_t">40.72</span>
<span id="S5.T6.1.1.1.1.1.1.3.1.7" class="ltx_td ltx_align_left ltx_border_t">57.43</span>
<span id="S5.T6.1.1.1.1.1.1.3.1.8" class="ltx_td ltx_align_left ltx_border_t">82.49</span></span>
<span id="S5.T6.1.1.1.1.1.1.4.2" class="ltx_tr">
<span id="S5.T6.1.1.1.1.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">20</span>
<span id="S5.T6.1.1.1.1.1.1.4.2.2" class="ltx_td ltx_align_left">20.54</span>
<span id="S5.T6.1.1.1.1.1.1.4.2.3" class="ltx_td ltx_align_left">21.91</span>
<span id="S5.T6.1.1.1.1.1.1.4.2.4" class="ltx_td ltx_align_left">21.67</span>
<span id="S5.T6.1.1.1.1.1.1.4.2.5" class="ltx_td ltx_align_left">24.50</span>
<span id="S5.T6.1.1.1.1.1.1.4.2.6" class="ltx_td ltx_align_left">42.89</span>
<span id="S5.T6.1.1.1.1.1.1.4.2.7" class="ltx_td ltx_align_left">59.90</span>
<span id="S5.T6.1.1.1.1.1.1.4.2.8" class="ltx_td ltx_align_left">83.84</span></span>
<span id="S5.T6.1.1.1.1.1.1.5.3" class="ltx_tr">
<span id="S5.T6.1.1.1.1.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">30</span>
<span id="S5.T6.1.1.1.1.1.1.5.3.2" class="ltx_td ltx_align_left">18.07</span>
<span id="S5.T6.1.1.1.1.1.1.5.3.3" class="ltx_td ltx_align_left">19.76</span>
<span id="S5.T6.1.1.1.1.1.1.5.3.4" class="ltx_td ltx_align_left">17.85</span>
<span id="S5.T6.1.1.1.1.1.1.5.3.5" class="ltx_td ltx_align_left">26.59</span>
<span id="S5.T6.1.1.1.1.1.1.5.3.6" class="ltx_td ltx_align_left">43.63</span>
<span id="S5.T6.1.1.1.1.1.1.5.3.7" class="ltx_td ltx_align_left">58.53</span>
<span id="S5.T6.1.1.1.1.1.1.5.3.8" class="ltx_td ltx_align_left">85.25</span></span>
<span id="S5.T6.1.1.1.1.1.1.6.4" class="ltx_tr">
<span id="S5.T6.1.1.1.1.1.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">45</span>
<span id="S5.T6.1.1.1.1.1.1.6.4.2" class="ltx_td ltx_align_left">16.49</span>
<span id="S5.T6.1.1.1.1.1.1.6.4.3" class="ltx_td ltx_align_left">18.64</span>
<span id="S5.T6.1.1.1.1.1.1.6.4.4" class="ltx_td ltx_align_left">22.20</span>
<span id="S5.T6.1.1.1.1.1.1.6.4.5" class="ltx_td ltx_align_left">26.26</span>
<span id="S5.T6.1.1.1.1.1.1.6.4.6" class="ltx_td ltx_align_left">37.58</span>
<span id="S5.T6.1.1.1.1.1.1.6.4.7" class="ltx_td ltx_align_left">62.38</span>
<span id="S5.T6.1.1.1.1.1.1.6.4.8" class="ltx_td ltx_align_left">81.75</span></span>
<span id="S5.T6.1.1.1.1.1.1.7.5" class="ltx_tr">
<span id="S5.T6.1.1.1.1.1.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">60</span>
<span id="S5.T6.1.1.1.1.1.1.7.5.2" class="ltx_td ltx_align_left ltx_border_b">18.77</span>
<span id="S5.T6.1.1.1.1.1.1.7.5.3" class="ltx_td ltx_align_left ltx_border_b">16.58</span>
<span id="S5.T6.1.1.1.1.1.1.7.5.4" class="ltx_td ltx_align_left ltx_border_b">20.29</span>
<span id="S5.T6.1.1.1.1.1.1.7.5.5" class="ltx_td ltx_align_left ltx_border_b">24.31</span>
<span id="S5.T6.1.1.1.1.1.1.7.5.6" class="ltx_td ltx_align_left ltx_border_b">38.38</span>
<span id="S5.T6.1.1.1.1.1.1.7.5.7" class="ltx_td ltx_align_left ltx_border_b">65.47</span>
<span id="S5.T6.1.1.1.1.1.1.7.5.8" class="ltx_td ltx_align_left ltx_border_b">83.05</span></span>
</span>
</span></span></span>
</span></span></span></p>
</figure>
<div id="S5.SS2.p6" class="ltx_para">
<p id="S5.SS2.p6.1" class="ltx_p">First, we can highlight the superiority of GRU over LSTM in our experiments. A likely explanation for this phenomenon is that the fact that the small database may have weighed in favor of the GRU <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib48" title="" class="ltx_ref">yang2020lstm </a></cite>. The GRU model is more straightforward than LSTM, having only two gates. It is more likely to obtain better results when using less data. On the other hand, LSTM tends to provide a better outcome when exposed to a large amount of data (total number of samples in the dataset). As observed with the LSTM model, the GRU also presented better MSE results with prediction windows equal to 1 and 5 regardless of the past data window, except when this is equal to 30.</p>
</div>
<div id="S5.SS2.p7" class="ltx_para">
<p id="S5.SS2.p7.1" class="ltx_p">Similar to the LSTM, the GRU model tends to increase the error as we increased the size of the prediction window of the future. With the increasing of the future prediction window, the error can also to increase <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib49" title="" class="ltx_ref">patel2020deep </a></cite>. The error tendency in the first frames of the prediction sequence is smaller, and as you move away from the starting point of the prediction window, the error increases.</p>
</div>
<div id="S5.SS2.p8" class="ltx_para">
<p id="S5.SS2.p8.1" class="ltx_p">Unlike the pose detection demonstrated in the previous subsection, future prediction does not consider images as input. The only robot poses data in a grouping of frames from the past, making the inference in a ‚Äúblindly way‚Äù. In addition, the MSE metric penalizes the highest error results (farthest frames) during inference. This fact also explains why the MSE results were much higher than the robot poses detection.</p>
</div>
<div id="S5.SS2.p9" class="ltx_para">
<p id="S5.SS2.p9.1" class="ltx_p">With the results obtained by the GRU, we can also highlight the competitiveness of the results when we use a past window equal to 10, obtaining results close to those of other past data windows. In addition, it is also highlighted that the GRU promoted a significant increase in MSE when prediction windows equal to or greater than 60 were used. This result can be considered another advantage for LSTM.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we proposed a new framework for robotic arm pose estimation and future movement prediction in the context of human-robot collaboration in a well-controlled scenario. This framework consists of two modules, the first module estimates robotic arm pose using self-calibrated convolutions and ELM, while the second module predicts its future movement.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Results listed in this paper suggest that our framework provides satisfactory results with a low detection error. It was possible to reach an MSE of 4.12 using the SCNet-50-V1D, while the proposed model SCNet-50-V1D+ELM reached an MSE of 3.74. It outperforms all analyzed baselines. In this paper, we showed that the use of ELM with self-calibrated convolutions can provide low error results or better generalization for the regression task. Also, the results reached by the LSTM and GRU could ensure low errors for robotic arm movement prediction.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">Although this work contributes to risk mitigation is a HRC, it does not claim solving the collision risk problem in its entirety. Our goal was to address the open gaps previously presented and propose a new framework that mainly provides future movement prediction and detect the pose of the robotic arm. This last mentioned step can be essential for risk assessment in the HRC context. The use of this framework allows for example a human agent to know where a robot will move to, providing it ample time for making decisions and reacting in order to remain safe and avoid any damage to a moving robot and humans nearby.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">As future work, we intend to extend our approach by using the proposed models for risk evaluation in a well-controlled scenario. We plan to use the models for a joint human and robotic pose estimation, besides using deep learning forecasting models to analyze the extracted keypoints to predict possible risk or collision situations in the human-robot collaboration. These extensions stand to enable us to map future collision situations even before they occur. We also intend to extend our mechanism to work in a more advanced and complete environment with more people, robots, and interactions in order to develop a more robust and realistic system for collision detection.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Declarations</h2>

<div id="Sx1.p1" class="ltx_para">
<ul id="Sx1.I1" class="ltx_itemize">
<li id="Sx1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="Sx1.I1.i1.p1" class="ltx_para">
<p id="Sx1.I1.i1.p1.1" class="ltx_p"><span id="Sx1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Data availability:</span> The datasets generated during and/or analysed during the current study are available from the corresponding author on reasonable request.</p>
</div>
</li>
<li id="Sx1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="Sx1.I1.i2.p1" class="ltx_para">
<p id="Sx1.I1.i2.p1.1" class="ltx_p"><span id="Sx1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Conflict of interest:</span> The authors declare that they have no conflict of interest.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
This work was financed in part by the Conselho Nacional de Desenvolvimento Cient√≠fico e Tecnol√≥gico (CNPq), Funda√ß√£o de Amparo a Ci√™ncia e Tecnologia de Pernambuco (FACEPE), Coordena√ß√£o de Aperfei√ßoamento de Pessoal de N√≠vel Superior (CAPES), and Research, Development and Innovation Center, Ericsson Telecommunications Inc., Brazil.

</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Ruchi Goel and Pooja Gupta.

</span>
<span class="ltx_bibblock">Robotics and industry 4.0.

</span>
<span class="ltx_bibblock">In <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">A Roadmap to Industry 4.0: Smart Production, Sharp Business
and Sustainable Development</span>, pages 157‚Äì169. Springer, 2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Arash Ajoudani, Andrea¬†Maria Zanchettin, Serena Ivaldi, Alin Albu-Sch√§ffer,
Kazuhiro Kosuge, and Oussama Khatib.

</span>
<span class="ltx_bibblock">Progress and prospects of the human‚Äìrobot collaboration.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Autonomous Robots</span>, 42(5):957‚Äì975, 2018.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Andrea Bauer, Dirk Wollherr, and Martin Buss.

</span>
<span class="ltx_bibblock">Human‚Äìrobot collaboration: a survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">International Journal of Humanoid Robotics</span>, 5(01):47‚Äì66, 2008.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Microsoft.

</span>
<span class="ltx_bibblock">2019 microsoft dynamics 365 manufacturing trends report, 2019.

</span>
<span class="ltx_bibblock">Accessed: 2019-09-09.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Gabriel Reis, Marrone Dantas, Daniel Bezerra, Gibson Nunes, Pedro Dreyer,
Carolina Ledebour, Judith Kelner, Djamel Sadok, Ricardo Souza, Silvia Lins,
et¬†al.

</span>
<span class="ltx_bibblock">Gripper design for radio base station autonomous maintenance system.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">International Journal of Automation and Computing</span>, 18:1‚Äì9,
2021.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Bj√∂rn Thors, Anders Furusk√§r, Davide Colombi, and Christer
T√∂rnevik.

</span>
<span class="ltx_bibblock">Time-averaged realistic maximum power levels for the assessment of
radio frequency exposure for 5g radio base stations using massive mimo.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, 5:19711‚Äì19719, 2017.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Milos Vasic and Aude Billard.

</span>
<span class="ltx_bibblock">Safety issues in human-robot interactions.

</span>
<span class="ltx_bibblock">In <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">2013 ieee international conference on robotics and
automation</span>, pages 197‚Äì204. IEEE, 2013.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Iago¬†Richard Rodrigues, Gibson Barbosa, Assis Oliveira¬†Filho, Carolina Cani,
Marrone Dantas, Djamel¬†H Sadok, Judith Kelner, Ricardo¬†Silva Souza,
Maria¬†Val√©ria Marquezini, and Silvia Lins.

</span>
<span class="ltx_bibblock">Modeling and assessing an intelligent system for safety in
human-robot collaboration using deep and machine learning techniques.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Multimedia Tools and Applications</span>, pages 1‚Äì27, 2021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Jiang-Jiang Liu, Qibin Hou, Ming-Ming Cheng, Changhu Wang, and Jiashi Feng.

</span>
<span class="ltx_bibblock">Improving convolutional networks with self-calibrated convolutions.

</span>
<span class="ltx_bibblock">In <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 10096‚Äì10105, 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Guang-Bin Huang, Qin-Yu Zhu, and Chee-Kheong Siew.

</span>
<span class="ltx_bibblock">Extreme learning machine: theory and applications.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Neurocomputing</span>, 70(1-3):489‚Äì501, 2006.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Iago¬†Richard Rodrigues, Sebasti√£o¬†Rog√©rio da¬†Silva¬†Neto, Judith Kelner,
Djamel Sadok, and Patricia¬†Takako Endo.

</span>
<span class="ltx_bibblock">Convolutional extreme learning machines: A systematic review.

</span>
<span class="ltx_bibblock">In <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Informatics</span>, volume¬†8, page¬†33, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Sepp Hochreiter and J√ºrgen Schmidhuber.

</span>
<span class="ltx_bibblock">Long short-term memory.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Neural computation</span>, 9(8):1735‚Äì1780, 1997.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Kyunghyun Cho, Bart van Merri√´nboer, Caglar Gulcehre, Dzmitry Bahdanau,
Fethi Bougares, Holger Schwenk, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Learning phrase representations using RNN encoder‚Äìdecoder for
statistical machine translation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2014 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</span>, pages 1724‚Äì1734, Doha, Qatar,
October 2014. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Iago¬†RR Silva, Gibson¬†BN Barbosa, Carolina¬†CD Ledebour, Assis¬†T Oliveira¬†Filho,
Judith Kelner, Djamel Sadok, Silvia Lins, and Ricardo Souza.

</span>
<span class="ltx_bibblock">Assessing deep learning models for human-robot collaboration
collision detection in industrial environments.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Brazilian Conference on Intelligent Systems</span>, pages 240‚Äì255.
Springer, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Sandra Robla-G√≥mez, Victor¬†M Becerra, Jos√©¬†Ram√≥n Llata, Esther
Gonzalez-Sarabia, Carlos Torre-Ferrero, and Juan Perez-Oria.

</span>
<span class="ltx_bibblock">Working together: A review on safe human-robot collaboration in
industrial environments.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, 5:26754‚Äì26773, 2017.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Przemyslaw¬†A Lasota, Terrence Fong, Julie¬†A Shah, et¬†al.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">A survey of methods for safe human-robot interaction</span>.

</span>
<span class="ltx_bibblock">Now Publishers, 2017.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Jianjing Zhang, Hongyi Liu, Qing Chang, Lihui Wang, and Robert¬†X Gao.

</span>
<span class="ltx_bibblock">Recurrent neural network for motion trajectory prediction in
human-robot collaborative assembly.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">CIRP annals</span>, 69(1):9‚Äì12, 2020.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Mohammad Anvaripour and Mehrdad Saif.

</span>
<span class="ltx_bibblock">Collision detection for human-robot interaction in an industrial
setting using force myography and a deep learning approach.

</span>
<span class="ltx_bibblock">In <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">2019 IEEE International Conference on Systems, Man and
Cybernetics (SMC)</span>, pages 2149‚Äì2154. IEEE, 2019.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Marc Maceira, Alberto Olivares-Alarcos, and Guillem Aleny√†.

</span>
<span class="ltx_bibblock">Recurrent neural networks for inferring intentions in shared tasks
for industrial collaborative robots.

</span>
<span class="ltx_bibblock">In <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">2020 29th IEEE International Conference on Robot and Human
Interactive Communication (RO-MAN)</span>, pages 665‚Äì670. IEEE, 2020.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.

</span>
<span class="ltx_bibblock">Deep learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">nature</span>, 521(7553):436‚Äì444, 2015.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li¬†Fei-Fei.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">2009 IEEE conference on computer vision and pattern
recognition</span>, pages 248‚Äì255. Ieee, 2009.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Mingxing Tan and Quoc Le.

</span>
<span class="ltx_bibblock">Efficientnet: Rethinking model scaling for convolutional neural
networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
6105‚Äì6114. PMLR, 2019.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Weibo Liu, Zidong Wang, Xiaohui Liu, Nianyin Zeng, Yurong Liu, and Fuad¬†E
Alsaadi.

</span>
<span class="ltx_bibblock">A survey of deep neural network architectures and their applications.

</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Neurocomputing</span>, 234:11‚Äì26, 2017.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Zachary¬†C Lipton, John Berkowitz, and Charles Elkan.

</span>
<span class="ltx_bibblock">A critical review of recurrent neural networks for sequence learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1506.00019</span>, 2015.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Abdel-Nasser Sharkawy, Panagiotis¬†N Koustoumpardis, and Nikos Aspragathos.

</span>
<span class="ltx_bibblock">Human‚Äìrobot collisions detection for safe human‚Äìrobot interaction
using one multi-input‚Äìoutput neural network.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Soft Computing</span>, 24(9):6687‚Äì6719, 2020.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Kyu¬†Min Park, Jihwan Kim, Jinhyuk Park, and Frank¬†C Park.

</span>
<span class="ltx_bibblock">Learning-based real-time detection of robot collisions without joint
torque sensors.

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">IEEE Robotics and Automation Letters</span>, 6(1):103‚Äì110, 2021.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Justinas Miseikis, Patrick Knobelreiter, Inka Brijacak, Saeed Yahyanejad, Kyrre
Glette, Ole¬†Jakob Elle, and Jim Torresen.

</span>
<span class="ltx_bibblock">Robot localisation and 3d position estimation using a free-moving
camera and cascaded convolutional neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">2018 IEEE/ASME International Conference on Advanced
Intelligent Mechatronics (AIM)</span>, pages 181‚Äì187. IEEE, 2018.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Alex Krizhevsky, Ilya Sutskever, and Geoffrey¬†E Hinton.

</span>
<span class="ltx_bibblock">Imagenet classification with deep convolutional neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>,
25:1097‚Äì1105, 2012.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Justinas Mi≈°eikis, Inka Brijacak, Saeed Yahyanejad, Kyrre Glette,
Ole¬†Jakob Elle, and Jim Torresen.

</span>
<span class="ltx_bibblock">Transfer learning for unseen robot detection and joint estimation on
a multi-objective convolutional neural network.

</span>
<span class="ltx_bibblock">In <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">2018 IEEE International Conference on Intelligence and Safety
for Robotics (ISR)</span>, pages 337‚Äì342. IEEE, 2018.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Fan Zhou, Zijing Chi, Chungang Zhuang, and Han Ding.

</span>
<span class="ltx_bibblock">3d pose estimation of robot arm with rgb images based on deep
learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">International Conference on Intelligent Robotics and
Applications</span>, pages 541‚Äì553. Springer, 2019.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Timothy¬†E Lee, Jonathan Tremblay, Thang To, Jia Cheng, Terry Mosier, Oliver
Kroemer, Dieter Fox, and Stan Birchfield.

</span>
<span class="ltx_bibblock">Camera-to-robot pose estimation from a single image.

</span>
<span class="ltx_bibblock">In <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">2020 IEEE International Conference on Robotics and Automation
(ICRA)</span>, pages 9426‚Äì9432. IEEE, 2020.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Christoph Heindl, Sebastian Zambal, and Josef Scharinger.

</span>
<span class="ltx_bibblock">Learning to predict robot keypoints using artificially generated
images.

</span>
<span class="ltx_bibblock">In <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">2019 24th IEEE International Conference on Emerging
Technologies and Factory Automation (ETFA)</span>, pages 1536‚Äì1539. IEEE, 2019.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Nicolas Ballas, Li¬†Yao, Chris Pal, and Aaron Courville.

</span>
<span class="ltx_bibblock">Delving deeper into convolutional networks for learning video
representations.

</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1511.06432</span>, 2015.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Christoph Heindl, Sebastian Zambal, Thomas Ponitz, Andreas Pichler, and Josef
Scharinger.

</span>
<span class="ltx_bibblock">3d robot pose estimation from 2d images.

</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1902.04987</span>, 2019.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu¬†Li.

</span>
<span class="ltx_bibblock">Bag of tricks for image classification with convolutional neural
networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 558‚Äì567, 2019.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Gao Huang, Guang-Bin Huang, Shiji Song, and Keyou You.

</span>
<span class="ltx_bibblock">Trends in extreme learning machines: A review.

</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">Neural Networks</span>, 61:32‚Äì48, 2015.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Andrea Maria¬†NC Ribeiro, Pedro Rafael¬†X do¬†Carmo, Iago¬†Richard Rodrigues,
Djamel Sadok, Theo Lynn, and Patricia¬†Takako Endo.

</span>
<span class="ltx_bibblock">Short-term firm-level energy-consumption forecasting for
energy-intensive manufacturing: A comparison of machine learning and deep
learning models.

</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">Algorithms</span>, 13(11):274, 2020.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Kyunghyun Cho, Bart Van¬†Merri√´nboer, Caglar Gulcehre, Dzmitry Bahdanau,
Fethi Bougares, Holger Schwenk, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Learning phrase representations using rnn encoder-decoder for
statistical machine translation.

</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1406.1078</span>, 2014.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Universal Robots.

</span>
<span class="ltx_bibblock">Universial robots. https://www.universal-robots.com/. accessed in
november 2021.

</span>
<span class="ltx_bibblock">2021.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Abhishek Dutta and Andrew Zisserman.

</span>
<span class="ltx_bibblock">The via annotation software for images, audio and video.

</span>
<span class="ltx_bibblock">In <span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">Proceedings of the 27th ACM international conference on
multimedia</span>, pages 2276‚Äì2279, 2019.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Forrest¬†N Iandola, Song Han, Matthew¬†W Moskewicz, Khalid Ashraf, William¬†J
Dally, and Kurt Keutzer.

</span>
<span class="ltx_bibblock">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¬° 0.5
mb model size.

</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1602.07360</span>, 2016.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Karen Simonyan and Andrew Zisserman.

</span>
<span class="ltx_bibblock">Very deep convolutional networks for large-scale image recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">3rd International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
Proceedings</span>, 2015.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 770‚Äì778, 2016.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Gao Huang, Zhuang Liu, Laurens Van Der¬†Maaten, and Kilian¬†Q Weinberger.

</span>
<span class="ltx_bibblock">Densely connected convolutional networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib44.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 4700‚Äì4708, 2017.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Guang-Bin Huang and Chee-Kheong Siew.

</span>
<span class="ltx_bibblock">Extreme learning machine with randomly assigned rbf kernels.

</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">International Journal of Information Technology</span>, 11(1):16‚Äì24,
2005.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Satyakam Baraha and Pradyut¬†Kumar Biswal.

</span>
<span class="ltx_bibblock">Implementation of activation functions for elm based classifiers.

</span>
<span class="ltx_bibblock">In <span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">2017 International Conference on Wireless Communications,
Signal Processing and Networking (WiSPNET)</span>, pages 1038‚Äì1042. IEEE, 2017.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Chaoning Zhang, Philipp Benz, Dawit¬†Mureja Argaw, Seokju Lee, Junsik Kim,
Francois Rameau, Jean-Charles Bazin, and In¬†So Kweon.

</span>
<span class="ltx_bibblock">Resnet or densenet? introducing dense shortcuts to resnet.

</span>
<span class="ltx_bibblock">In <span id="bib.bib47.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Winter Conference on Applications
of Computer Vision</span>, pages 3550‚Äì3559, 2021.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Shudong Yang, Xueying Yu, and Ying Zhou.

</span>
<span class="ltx_bibblock">Lstm and gru neural network performance comparison study: Taking yelp
review dataset as an example.

</span>
<span class="ltx_bibblock">In <span id="bib.bib48.1.1" class="ltx_text ltx_font_italic">2020 International workshop on electronic communication and
artificial intelligence (IWECAI)</span>, pages 98‚Äì101. IEEE, 2020.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Mohil¬†Maheshkumar Patel, Sudeep Tanwar, Rajesh Gupta, and Neeraj Kumar.

</span>
<span class="ltx_bibblock">A deep learning-based cryptocurrency price prediction scheme for
financial institutions.

</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">Journal of Information Security and Applications</span>, 55:102583,
2020.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2205.13993" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2205.13994" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2205.13994">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2205.13994" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2205.13995" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 05:15:40 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
