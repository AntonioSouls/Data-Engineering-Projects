<!DOCTYPE html><html lang="en" data-theme="light"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention</title>
<!--Generated on Mon Oct  7 14:25:23 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">




<base href="https://arxiv.org/html/2410.05076v1/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2410.05076v1">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" title="Toggle dark/light mode" aria-label="System preference">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        
        
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="https://arxiv.org/html/2410.05076v1/#myForm">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2410.05076v1">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2410.05076v1" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" title="Toggle dark/light mode">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        
        
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist"><li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#abstract" title="Abstract">
      <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref"></span>
        Abstract
      </span>
    </a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S1" title="In TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S2" title="In TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S2.SS0.SSS0.Px1" title="In 2 Related Work ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_title">Long-context model.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S2.SS0.SSS0.Px2" title="In 2 Related Work ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_title">Eviction-based sparse attention.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S2.SS0.SSS0.Px3" title="In 2 Related Work ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_title">Selection-based sparse attention.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S3" title="In TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S3.SS1" title="In 3 Methodology ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Position Persistent Sparse Attention (PPSA)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S3.SS2" title="In 3 Methodology ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>KV Cache Correction</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S4" title="In TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S4.SS1" title="In 4 Experiments ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experiment Setting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S4.SS2" title="In 4 Experiments ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Performance Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S4.SS2.SSS1" title="In 4.2 Performance Evaluation ‚Ä£ 4 Experiments ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Needle-in-the-Haystack</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S4.SS2.SSS2" title="In 4.2 Performance Evaluation ‚Ä£ 4 Experiments ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Language Modeling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S4.SS2.SSS3" title="In 4.2 Performance Evaluation ‚Ä£ 4 Experiments ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.3 </span>LongBench Experiment</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S4.SS3" title="In 4 Experiments ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Efficiency Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S4.SS4" title="In 4 Experiments ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Sensitivity analysis on token re-selection layer</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S5" title="In TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#A1" title="In TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#A1.SS1" title="In Appendix A Appendix ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>LongBench for LLaMA-3.1-8B-Instruct</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#A1.SS2" title="In Appendix A Appendix ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>End-to-end efficiency evaluation results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#A1.SS3" title="In Appendix A Appendix ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Full sensitivity studies on different token re-selection layer</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib" title="References">
      <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref"></span>
        References
      </span>
    </a></li></ol></nav>

<div class="ltx_page_content"><div class="package-alerts ltx_document" role="status" aria-label="Conversion errors have been found">
      <button aria-label="Dismiss alert">
          <span aria-hidden="true"><svg role="presentation" width="20" height="20" viewBox="0 0 44 44" aria-hidden="true" focusable="false">
          <path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
          <path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
          </svg></span>
      </button>
      <p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p>
          <ul arial-label="Unsupported packages used in this paper">
              <li>failed: xltabular</li>
          </ul>
      <p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p>
    </div><div id="target-section" class="section"><a id="license-tr" href="https://info.arxiv.org/help/license/index.html#licenses-available">License: CC BY-SA 4.0</a><div id="watermark-tr">arXiv:2410.05076v1 [cs.LG] 07 Oct 2024</div></div>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Lijie Yang 
<br class="ltx_break">Carnegie Mellon University
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter" id="id3.2.id1">lijiey@andrew.cmu.edu</span>
&amp;Zhihao Zhang<sup class="ltx_sup" id="id4.3.id2">‚àó</sup>
<br class="ltx_break">Carnegie Mellon University
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter" id="id5.4.id3">zhihaoz3@cs.cmu.edu</span>
&amp;Zhuofu Chen 
<br class="ltx_break">Carnegie Mellon University
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter" id="id6.5.id4">zhuofuc@cs.cmu.edu</span>
<br class="ltx_break">&amp;Zikun Li 
<br class="ltx_break">Carnegie Mellon University
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter" id="id7.6.id5">zikunl@cs.cmu.edu</span>
<br class="ltx_break">&amp;Zhihao Jia 
<br class="ltx_break">Carnegie Mellon University
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter" id="id8.7.id6">zhihao@cmu.edu</span>
<br class="ltx_break">
</span><span class="ltx_author_notes">Equal contribution</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract" id="abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id2.1">Large language models (LLMs) have driven significant advancements across diverse NLP tasks, with long-context models gaining prominence for handling extended inputs. However, the expanding key-value (KV) cache size required by Transformer architectures intensifies the memory constraints, particularly during the decoding phase, creating a significant bottleneck.
Existing sparse attention mechanisms designed to address this bottleneck have two limitations: (1) they often fail to reliably identify the most relevant tokens for attention, and (2) they overlook the spatial coherence of token selection across consecutive Transformer layers, which can lead to performance degradation and substantial overhead in token selection.
This paper introduces TidalDecode, a simple yet effective algorithm and system for fast and accurate LLM decoding through position persistent sparse attention. TidalDecode leverages the spatial coherence of tokens selected by existing sparse attention methods and introduces a few token selection layers that perform full attention to identify the tokens with the highest attention scores, while all other layers perform sparse attention with the pre-selected tokens.
This design enables TidalDecode to substantially reduce the overhead of token selection for sparse
attention without sacrificing the quality of the generated results.
Evaluation on a diverse set of LLMs and tasks shows that TidalDecode closely matches the generative performance of full attention methods while reducing the LLM decoding latency by up to <math alttext="2.1\times" class="ltx_math_unparsed" display="inline" id="id2.1.m1.1"><semantics id="id2.1.m1.1a"><mrow id="id2.1.m1.1b"><mn id="id2.1.m1.1.1">2.1</mn><mo id="id2.1.m1.1.2" lspace="0.222em">√ó</mo></mrow><annotation encoding="application/x-tex" id="id2.1.m1.1c">2.1\times</annotation><annotation encoding="application/x-llamapun" id="id2.1.m1.1d">2.1 √ó</annotation></semantics></math><span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The codebase to reproduce performance and efficiency results for TidalDecode included in this paper can be found at <a class="ltx_ref ltx_href" href="https://github.com/DerrickYLJ/TidalDecode" title="">https://github.com/DerrickYLJ/TidalDecode</a></span></span></span>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large language models (LLMs) have revolutionized natural language processing (NLP) by achieving state-of-the-art performance on various applications.
As LLMs evolve, they are increasingly being adapted to manage tasks with long contexts, such as Chain-of-Thought reasoning <cite class="ltx_cite ltx_citemacro_citep">(Wei et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib24" title="">2023</a>)</cite>, document summarization <cite class="ltx_cite ltx_citemacro_citep">(Huang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib10" title="">2021</a>)</cite>, and retrieval-augmented generation <cite class="ltx_cite ltx_citemacro_citep">(Ram et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib16" title="">2023</a>; Zhang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib29" title="">2024b</a>)</cite>.
However, quickly and efficiently serving long-context LLMs is challenging due to the inherent memory and compute bottlenecks in the Transformer architectures&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Vaswani et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib22" title="">2023</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">LLM inference involves two separate stages: <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">prefilling</span> and <span class="ltx_text ltx_font_italic" id="S1.p2.1.2">decoding</span>.
The prefilling stage computes the activations for all input tokens and stores the keys and values for all tokens in the <span class="ltx_text ltx_font_italic" id="S1.p2.1.3">key-value (KV) cache</span>, allowing the LLM to reuse these keys and values to compute attention for future tokens.
In each decoding stage, the LLM decodes one new token using all input tokens and previously generated tokens.
The KV cache size grows linearly in the sequence length&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Kwon et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib12" title="">2023</a>)</cite>.
For instance, with a context length of 128K tokens, the KV cache of LLama2-7B with half-precision can easily reach 64 GB<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The KV cache size is computed as:
<math alttext="\text{Layers}\times\text{KV Heads}\times\text{Head Dim}\times\text{Seq Len}%
\times\text{FP16 Size}\times 2\ (\text{for K+V})=32\times 32\times 128\times 1%
28K\times 2\ \text{bytes}\times 2=64\ \text{GB}" class="ltx_Math" display="inline" id="footnote2.m1.1"><semantics id="footnote2.m1.1b"><mrow id="footnote2.m1.1.2" xref="footnote2.m1.1.2.cmml"><mrow id="footnote2.m1.1.2.2" xref="footnote2.m1.1.2.2.cmml"><mrow id="footnote2.m1.1.2.2.2" xref="footnote2.m1.1.2.2.2.cmml"><mtext id="footnote2.m1.1.2.2.2.2" xref="footnote2.m1.1.2.2.2.2a.cmml">Layers</mtext><mo id="footnote2.m1.1.2.2.2.1" lspace="0.222em" rspace="0.222em" xref="footnote2.m1.1.2.2.2.1.cmml">√ó</mo><mtext id="footnote2.m1.1.2.2.2.3" xref="footnote2.m1.1.2.2.2.3a.cmml">KV Heads</mtext><mo id="footnote2.m1.1.2.2.2.1b" lspace="0.222em" rspace="0.222em" xref="footnote2.m1.1.2.2.2.1.cmml">√ó</mo><mtext id="footnote2.m1.1.2.2.2.4" xref="footnote2.m1.1.2.2.2.4a.cmml">Head Dim</mtext><mo id="footnote2.m1.1.2.2.2.1c" lspace="0.222em" rspace="0.222em" xref="footnote2.m1.1.2.2.2.1.cmml">√ó</mo><mtext id="footnote2.m1.1.2.2.2.5" xref="footnote2.m1.1.2.2.2.5a.cmml">Seq Len</mtext><mo id="footnote2.m1.1.2.2.2.1d" lspace="0.222em" rspace="0.222em" xref="footnote2.m1.1.2.2.2.1.cmml">√ó</mo><mtext id="footnote2.m1.1.2.2.2.6" xref="footnote2.m1.1.2.2.2.6a.cmml">FP16 Size</mtext><mo id="footnote2.m1.1.2.2.2.1e" lspace="0.222em" rspace="0.222em" xref="footnote2.m1.1.2.2.2.1.cmml">√ó</mo><mn id="footnote2.m1.1.2.2.2.7" xref="footnote2.m1.1.2.2.2.7.cmml">2</mn></mrow><mo id="footnote2.m1.1.2.2.1" lspace="0.500em" xref="footnote2.m1.1.2.2.1.cmml">‚Å¢</mo><mrow id="footnote2.m1.1.2.2.3.2" xref="footnote2.m1.1.1a.cmml"><mo id="footnote2.m1.1.2.2.3.2.1" stretchy="false" xref="footnote2.m1.1.1a.cmml">(</mo><mtext id="footnote2.m1.1.1" xref="footnote2.m1.1.1.cmml">for K+V</mtext><mo id="footnote2.m1.1.2.2.3.2.2" stretchy="false" xref="footnote2.m1.1.1a.cmml">)</mo></mrow></mrow><mo id="footnote2.m1.1.2.3" xref="footnote2.m1.1.2.3.cmml">=</mo><mrow id="footnote2.m1.1.2.4" xref="footnote2.m1.1.2.4.cmml"><mrow id="footnote2.m1.1.2.4.2" xref="footnote2.m1.1.2.4.2.cmml"><mrow id="footnote2.m1.1.2.4.2.2" xref="footnote2.m1.1.2.4.2.2.cmml"><mrow id="footnote2.m1.1.2.4.2.2.2" xref="footnote2.m1.1.2.4.2.2.2.cmml"><mrow id="footnote2.m1.1.2.4.2.2.2.2" xref="footnote2.m1.1.2.4.2.2.2.2.cmml"><mn id="footnote2.m1.1.2.4.2.2.2.2.2" xref="footnote2.m1.1.2.4.2.2.2.2.2.cmml">32</mn><mo id="footnote2.m1.1.2.4.2.2.2.2.1" lspace="0.222em" rspace="0.222em" xref="footnote2.m1.1.2.4.2.2.2.2.1.cmml">√ó</mo><mn id="footnote2.m1.1.2.4.2.2.2.2.3" xref="footnote2.m1.1.2.4.2.2.2.2.3.cmml">32</mn><mo id="footnote2.m1.1.2.4.2.2.2.2.1b" lspace="0.222em" rspace="0.222em" xref="footnote2.m1.1.2.4.2.2.2.2.1.cmml">√ó</mo><mn id="footnote2.m1.1.2.4.2.2.2.2.4" xref="footnote2.m1.1.2.4.2.2.2.2.4.cmml">128</mn><mo id="footnote2.m1.1.2.4.2.2.2.2.1c" lspace="0.222em" rspace="0.222em" xref="footnote2.m1.1.2.4.2.2.2.2.1.cmml">√ó</mo><mn id="footnote2.m1.1.2.4.2.2.2.2.5" xref="footnote2.m1.1.2.4.2.2.2.2.5.cmml">128</mn></mrow><mo id="footnote2.m1.1.2.4.2.2.2.1" xref="footnote2.m1.1.2.4.2.2.2.1.cmml">‚Å¢</mo><mi id="footnote2.m1.1.2.4.2.2.2.3" xref="footnote2.m1.1.2.4.2.2.2.3.cmml">K</mi></mrow><mo id="footnote2.m1.1.2.4.2.2.1" lspace="0.222em" rspace="0.222em" xref="footnote2.m1.1.2.4.2.2.1.cmml">√ó</mo><mn id="footnote2.m1.1.2.4.2.2.3" xref="footnote2.m1.1.2.4.2.2.3.cmml">2</mn></mrow><mo id="footnote2.m1.1.2.4.2.1" lspace="0.500em" xref="footnote2.m1.1.2.4.2.1.cmml">‚Å¢</mo><mtext id="footnote2.m1.1.2.4.2.3" xref="footnote2.m1.1.2.4.2.3a.cmml">bytes</mtext></mrow><mo id="footnote2.m1.1.2.4.1" lspace="0.222em" rspace="0.222em" xref="footnote2.m1.1.2.4.1.cmml">√ó</mo><mn id="footnote2.m1.1.2.4.3" xref="footnote2.m1.1.2.4.3.cmml">2</mn></mrow><mo id="footnote2.m1.1.2.5" xref="footnote2.m1.1.2.5.cmml">=</mo><mrow id="footnote2.m1.1.2.6" xref="footnote2.m1.1.2.6.cmml"><mn id="footnote2.m1.1.2.6.2" xref="footnote2.m1.1.2.6.2.cmml">64</mn><mo id="footnote2.m1.1.2.6.1" lspace="0.500em" xref="footnote2.m1.1.2.6.1.cmml">‚Å¢</mo><mtext id="footnote2.m1.1.2.6.3" xref="footnote2.m1.1.2.6.3a.cmml">GB</mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="footnote2.m1.1c"><apply id="footnote2.m1.1.2.cmml" xref="footnote2.m1.1.2"><and id="footnote2.m1.1.2a.cmml" xref="footnote2.m1.1.2"></and><apply id="footnote2.m1.1.2b.cmml" xref="footnote2.m1.1.2"><eq id="footnote2.m1.1.2.3.cmml" xref="footnote2.m1.1.2.3"></eq><apply id="footnote2.m1.1.2.2.cmml" xref="footnote2.m1.1.2.2"><times id="footnote2.m1.1.2.2.1.cmml" xref="footnote2.m1.1.2.2.1"></times><apply id="footnote2.m1.1.2.2.2.cmml" xref="footnote2.m1.1.2.2.2"><times id="footnote2.m1.1.2.2.2.1.cmml" xref="footnote2.m1.1.2.2.2.1"></times><ci id="footnote2.m1.1.2.2.2.2a.cmml" xref="footnote2.m1.1.2.2.2.2"><mtext id="footnote2.m1.1.2.2.2.2.cmml" xref="footnote2.m1.1.2.2.2.2">Layers</mtext></ci><ci id="footnote2.m1.1.2.2.2.3a.cmml" xref="footnote2.m1.1.2.2.2.3"><mtext id="footnote2.m1.1.2.2.2.3.cmml" xref="footnote2.m1.1.2.2.2.3">KV Heads</mtext></ci><ci id="footnote2.m1.1.2.2.2.4a.cmml" xref="footnote2.m1.1.2.2.2.4"><mtext id="footnote2.m1.1.2.2.2.4.cmml" xref="footnote2.m1.1.2.2.2.4">Head Dim</mtext></ci><ci id="footnote2.m1.1.2.2.2.5a.cmml" xref="footnote2.m1.1.2.2.2.5"><mtext id="footnote2.m1.1.2.2.2.5.cmml" xref="footnote2.m1.1.2.2.2.5">Seq Len</mtext></ci><ci id="footnote2.m1.1.2.2.2.6a.cmml" xref="footnote2.m1.1.2.2.2.6"><mtext id="footnote2.m1.1.2.2.2.6.cmml" xref="footnote2.m1.1.2.2.2.6">FP16 Size</mtext></ci><cn id="footnote2.m1.1.2.2.2.7.cmml" type="integer" xref="footnote2.m1.1.2.2.2.7">2</cn></apply><ci id="footnote2.m1.1.1a.cmml" xref="footnote2.m1.1.2.2.3.2"><mtext id="footnote2.m1.1.1.cmml" xref="footnote2.m1.1.1">for K+V</mtext></ci></apply><apply id="footnote2.m1.1.2.4.cmml" xref="footnote2.m1.1.2.4"><times id="footnote2.m1.1.2.4.1.cmml" xref="footnote2.m1.1.2.4.1"></times><apply id="footnote2.m1.1.2.4.2.cmml" xref="footnote2.m1.1.2.4.2"><times id="footnote2.m1.1.2.4.2.1.cmml" xref="footnote2.m1.1.2.4.2.1"></times><apply id="footnote2.m1.1.2.4.2.2.cmml" xref="footnote2.m1.1.2.4.2.2"><times id="footnote2.m1.1.2.4.2.2.1.cmml" xref="footnote2.m1.1.2.4.2.2.1"></times><apply id="footnote2.m1.1.2.4.2.2.2.cmml" xref="footnote2.m1.1.2.4.2.2.2"><times id="footnote2.m1.1.2.4.2.2.2.1.cmml" xref="footnote2.m1.1.2.4.2.2.2.1"></times><apply id="footnote2.m1.1.2.4.2.2.2.2.cmml" xref="footnote2.m1.1.2.4.2.2.2.2"><times id="footnote2.m1.1.2.4.2.2.2.2.1.cmml" xref="footnote2.m1.1.2.4.2.2.2.2.1"></times><cn id="footnote2.m1.1.2.4.2.2.2.2.2.cmml" type="integer" xref="footnote2.m1.1.2.4.2.2.2.2.2">32</cn><cn id="footnote2.m1.1.2.4.2.2.2.2.3.cmml" type="integer" xref="footnote2.m1.1.2.4.2.2.2.2.3">32</cn><cn id="footnote2.m1.1.2.4.2.2.2.2.4.cmml" type="integer" xref="footnote2.m1.1.2.4.2.2.2.2.4">128</cn><cn id="footnote2.m1.1.2.4.2.2.2.2.5.cmml" type="integer" xref="footnote2.m1.1.2.4.2.2.2.2.5">128</cn></apply><ci id="footnote2.m1.1.2.4.2.2.2.3.cmml" xref="footnote2.m1.1.2.4.2.2.2.3">ùêæ</ci></apply><cn id="footnote2.m1.1.2.4.2.2.3.cmml" type="integer" xref="footnote2.m1.1.2.4.2.2.3">2</cn></apply><ci id="footnote2.m1.1.2.4.2.3a.cmml" xref="footnote2.m1.1.2.4.2.3"><mtext id="footnote2.m1.1.2.4.2.3.cmml" xref="footnote2.m1.1.2.4.2.3">bytes</mtext></ci></apply><cn id="footnote2.m1.1.2.4.3.cmml" type="integer" xref="footnote2.m1.1.2.4.3">2</cn></apply></apply><apply id="footnote2.m1.1.2c.cmml" xref="footnote2.m1.1.2"><eq id="footnote2.m1.1.2.5.cmml" xref="footnote2.m1.1.2.5"></eq><share href="https://arxiv.org/html/2410.05076v1#footnote2.m1.1.2.4.cmml" id="footnote2.m1.1.2d.cmml" xref="footnote2.m1.1.2"></share><apply id="footnote2.m1.1.2.6.cmml" xref="footnote2.m1.1.2.6"><times id="footnote2.m1.1.2.6.1.cmml" xref="footnote2.m1.1.2.6.1"></times><cn id="footnote2.m1.1.2.6.2.cmml" type="integer" xref="footnote2.m1.1.2.6.2">64</cn><ci id="footnote2.m1.1.2.6.3a.cmml" xref="footnote2.m1.1.2.6.3"><mtext id="footnote2.m1.1.2.6.3.cmml" xref="footnote2.m1.1.2.6.3">GB</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote2.m1.1d">\text{Layers}\times\text{KV Heads}\times\text{Head Dim}\times\text{Seq Len}%
\times\text{FP16 Size}\times 2\ (\text{for K+V})=32\times 32\times 128\times 1%
28K\times 2\ \text{bytes}\times 2=64\ \text{GB}</annotation><annotation encoding="application/x-llamapun" id="footnote2.m1.1e">Layers √ó KV Heads √ó Head Dim √ó Seq Len √ó FP16 Size √ó 2 ( for K+V ) = 32 √ó 32 √ó 128 √ó 128 italic_K √ó 2 bytes √ó 2 = 64 GB</annotation></semantics></math>.</span></span></span>, creating substantial memory pressure for LLM serving.
In addition, the LLM decoding stage is memory-bounded since decoding one new token requires accessing all previous tokens in the KV cache, making KV cache access the primary bottleneck for long-context LLM decoding.
This memory-bound nature severely limits the scalability and efficiency of LLM serving.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To address this problem, recent work has introduced <span class="ltx_text ltx_font_italic" id="S1.p3.1.1">sparse attention</span>, which approximates full attention using a small portion of tokens with the highest attention scores. Compared to full attention, sparse attention reduces computation cost and memory access while preserving the LLM‚Äôs generative performance&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ge et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib9" title="">2024</a>; Zhang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib27" title="">2023</a>)</cite>.
Existing sparse attention techniques can be classified into two categories: eviction- and selection-based methods.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">First, <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">eviction-based</span> sparse attention reduces memory usage for the KV cache by selectively discarding less relevant tokens from the KV cache, therefore reducing the number of tokens computed in attention mechanisms&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Xiao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib25" title="">2023</a>; Zhang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib27" title="">2023</a>)</cite>.
While these methods decrease the size of the KV cache, they can be inadequate for tasks where critical information is carried by tokens that are prematurely evicted, such as the needle-in-the-haystack tasks&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Peng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib14" title="">2023</a>)</cite>.
On the other hand, <span class="ltx_text ltx_font_italic" id="S1.p4.1.2">selection-based</span> sparse attention maintains <span class="ltx_text ltx_font_italic" id="S1.p4.1.3">all</span> tokens in the KV cache, estimates their attention scores, and selects a small subset of tokens to participate in each LLM decoding step.
This approach is prone to issues related to distribution shifts caused by appending sparsely attended, biased KV representations back into the cache.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="497" id="S1.F1.g1" src="https://arxiv.org/html/2410.05076v1/x1.png" width="571">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The heatmap for one decoding step of Llama3-8B-Instruct&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(AI, <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib1" title="">2024a</a>)</cite>, where columns and rows indicate different Transformer layers and tokens in the KV cache, respectively. For each layer, the 5 tokens (10% sparsity) with the highest attention scores of the first attention head are highlighted in yellow, which are the tokens used for sparse attention.
We feed an input prompt ‚ÄúUse only the provided search results to write a high-quality, concise answer to the question.\n&lt;‚Äîbegin_of_text‚Äî&gt;\n The magic number is: 15213. \n\n\n Question: What is the magic number? Keep the response short and direct. Answer: ‚Äù, and the LLM outputs ‚Äú15213‚Äù. The results show strong spatial coherence of tokens chosen for sparse attention in the decoding step.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">This paper presents TidalDecode, an algorithm and system for fast and precise LLM decoding, utilizing <span class="ltx_text ltx_font_italic" id="S1.p5.1.1">position persistent sparse attention</span> (PPSA).
A key insight behind TidalDecode is the observation that tokens chosen for sparse attention ‚Äî based on their highest attention scores ‚Äî exhibit <span class="ltx_text ltx_font_italic" id="S1.p5.1.2">significant overlap across consecutive Transformer layers</span> within each decoding phase.
<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S1.F1" title="In 1 Introduction ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">1</span></a> illustrates this overlap in a single decoding step of LLaMA-3-8B instruct&nbsp;<cite class="ltx_cite ltx_citemacro_cite">AI (<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib1" title="">2024a</a>)</cite> with an input of 51 tokens.
Each column in the figure corresponds to a Transformer layer, and each row indicates one token in the KV cache.
Selection-based sparse attention methods select the 5 tokens with the highest attention scores (highlighted in yellow) for attention computation in each head.
As the figure depicts, there is a recurring pattern where consecutive layers <span class="ltx_text ltx_font_italic" id="S1.p5.1.3">consistently focus on the same set of tokens</span>, indicating a <span class="ltx_text ltx_font_italic" id="S1.p5.1.4">spatial coherence in the selection of tokens</span> for sparse attention.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Instead of independently selecting tokens for sparse attention at each layer, TidalDecode introduces a few token selection layers, which perform full attention to identify the tokens with the highest attention scores.
All remaining layers implement position persistent sparse attention, where only the tokens selected by the token selection layers are retrieved from the KV cache for attention.
Consequently, all other layers between two token selection layers operate on the same set of tokens, reducing the overhead of token selection.
Experiments across a diverse set of LLMs and datasets demonstrate that using just two token selection layers ‚Äî one at the beginning and one in the middle ‚Äî is sufficient to achieve high generative performance while minimizing computation and memory overheads.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">This design enables TidalDecode to substantially reduce the overhead of token selection for sparse attention without sacrificing the quality of the generated results. Additionally, to address the KV cache distribution shift, TidalDecode introduces a cache-correction mechanism that periodically refills the KV cache using full attention for all sparsely decoded tokens to mitigate bias in the KV representations.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p8">
<p class="ltx_p" id="S1.p8.2">Comprehensive evaluation with the LongChat-7b-v1.5-32k, Llama-3-8B, Llama-3-70B, and Llama-3.1-8B models on the Needle-in-the-Haystack, PG-19, and LongBench tasks demonstrates that TidalDecode can consistently achieve the best performance efficiency trade-off compared with the best existing sparse attention methods.
We have implemented custom GPU kernels for PPSA and an end-to-end system for TidalDecode. Compared with existing full and sparse attention implementations, our system reduced the end-to-end inference latency by up to <math alttext="2.1\times" class="ltx_math_unparsed" display="inline" id="S1.p8.1.m1.1"><semantics id="S1.p8.1.m1.1a"><mrow id="S1.p8.1.m1.1b"><mn id="S1.p8.1.m1.1.1">2.1</mn><mo id="S1.p8.1.m1.1.2" lspace="0.222em">√ó</mo></mrow><annotation encoding="application/x-tex" id="S1.p8.1.m1.1c">2.1\times</annotation><annotation encoding="application/x-llamapun" id="S1.p8.1.m1.1d">2.1 √ó</annotation></semantics></math> and <math alttext="1.2\times" class="ltx_math_unparsed" display="inline" id="S1.p8.2.m2.1"><semantics id="S1.p8.2.m2.1a"><mrow id="S1.p8.2.m2.1b"><mn id="S1.p8.2.m2.1.1">1.2</mn><mo id="S1.p8.2.m2.1.2" lspace="0.222em">√ó</mo></mrow><annotation encoding="application/x-tex" id="S1.p8.2.m2.1c">1.2\times</annotation><annotation encoding="application/x-llamapun" id="S1.p8.2.m2.1d">1.2 √ó</annotation></semantics></math>, respectively.
In conclusion, our contributions are:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We propose TidalDecode, a streamlined and efficient algorithm and system for fast and high-quality LLM decoding, utilizing position persistent sparse attention.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">To address KV cache distribution shifts, we introduce a cache-correction mechanism that periodically refills the KV cache with using full attention for sparsely decoded tokens.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para ltx_noindent" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Empirically, we demonstrate the effectiveness and efficiency of TidalDecode through comprehensive evaluation, showing that TidalDecode significantly outperforms existing sparse attention methods.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Long-context model.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">Efficiently handling long-context inputs is essential for various LLM tasks in real-world applications such as document summarization, question answering, and dialogue systems&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib23" title="">2024</a>)</cite>.
Recent advancements, including rotary positional encoding (RoPE) <cite class="ltx_cite ltx_citemacro_citep">(Su et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib19" title="">2023</a>)</cite>, have enabled models to manage extended context lengths effectively.
The LLaMA-3 model series supports up to 8K tokens, with enhanced versions such as Gradient-AI-Llama3 <cite class="ltx_cite ltx_citemacro_citep">(AI, <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib1" title="">2024a</a>)</cite> and LLaMA 3.1 <cite class="ltx_cite ltx_citemacro_citep">(AI, <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib2" title="">2024b</a>)</cite> extending this limit to 128K tokens.
Additionally, proprietary LLMs such as GPT-4 Turbo and GPT-4o <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib13" title="">2024</a>)</cite> support up to 128K tokens, and Claude 3.5 Sonnet allows up to 200K tokens <cite class="ltx_cite ltx_citemacro_citep">(Anthropic, <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib3" title="">2024</a>)</cite>.
While recent work has introduced efficient attention kernel implementation <cite class="ltx_cite ltx_citemacro_citep">(Dao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib8" title="">2022</a>; Dao, <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib7" title="">2023</a>)</cite>, processing long-context inputs continues to be constrained by significant memory usage and computational costs from the extended KV cache.
TidalDecode is designed to mitigate these challenges by reducing latency and memory overhead through an efficient strategy for selecting tokens with the highest attention scores and one-time intermediate re-calibration, ensuring both efficiency and high-quality output.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p2.1">To alleviate the intrinsic computational and memory bottleneck in long-context LLM inference, recent works on sparse attention have approached this problem from two main perspectives: <span class="ltx_text ltx_font_italic" id="S2.SS0.SSS0.Px1.p2.1.1">eviction-</span> and <span class="ltx_text ltx_font_italic" id="S2.SS0.SSS0.Px1.p2.1.2">selection-based</span> methods.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Eviction-based sparse attention.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1"><cite class="ltx_cite ltx_citemacro_citet">Xiao et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib25" title="">2023</a>); Zhang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib28" title="">2024a</a>)</cite> propose to reduce KV cache memory usage by evicting tokens that are considered less relevant during inference.
These suffer from potential performance degradation, especially in tasks where every token may carry crucial information (e.g., needle-in-the-haystack tasks), since tokens with high importance for a future decoding step can be mistakenly evicted as the generation proceeds, which makes <span class="ltx_text ltx_font_italic" id="S2.SS0.SSS0.Px2.p1.1.1">selection-based methods</span> more popular choices in latest sparse attention works.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Selection-based sparse attention.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.1">Instead of evicting past tokens in the KV cache, <cite class="ltx_cite ltx_citemacro_citet">Child et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib5" title="">2019</a>); Kitaev et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib11" title="">2020</a>); Choromanski et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib6" title="">2020</a>); Tang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib20" title="">2024</a>); Ribar et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib17" title="">2023</a>)</cite> preserve the full KV cache and only select important tokens to attend with the attention module on the fly. More specifically, <cite class="ltx_cite ltx_citemacro_citet">Child et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib5" title="">2019</a>)</cite> leverages a fixed attention mask to select tokens while <cite class="ltx_cite ltx_citemacro_citet">Tang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib20" title="">2024</a>); Ribar et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib17" title="">2023</a>); Choromanski et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib6" title="">2020</a>); Kitaev et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib11" title="">2020</a>)</cite> aim to identify and retain the most relevant tokens at each layer by approximating attention scores.
Although these methods are more selective, they operate independently at each layer and are not guaranteed to obtain the ground-truth tokens with the highest attention scores, failing to capture token relevance patterns that persist across layers.
Moreover, attention score estimation algorithms sometimes introduce unnecessary complexity, diminishing the practical efficiency gains they are designed to achieve.
Improving upon prior works, TidalDecode leverages a shared pattern of most important tokens across consecutive layers to further reduce the computational overhead and memory access required for token selection.

</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="240" id="S3.F2.g1" src="https://arxiv.org/html/2410.05076v1/x2.png" width="723">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An overview of the decoding step in TidalDecode, which performs full attention for the first two layers, full attention with token selection for the third layer and a middle layer, and position persistent sparse attention for all other layers.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">This section introduces TidalDecode, an efficient algorithm and system for fast LLM decoding using <span class="ltx_text ltx_font_italic" id="S3.p1.1.1">position persistent sparse attention</span> and <span class="ltx_text ltx_font_italic" id="S3.p1.1.2">KV cache correction</span>. <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S3.F2" title="In 3 Methodology ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a> shows an overview of TidalDecode.
TidalDecode uses the same prefilling mechanism as existing systems and performs full attention to compute the key-value (KV) cache for all prompt tokens.
In each decoding step, TidalDecode uses three types of attention layers: full attention, full attention with token selection, and position persistent sparse attention.
First, TidalDecode performs full attention for the initial Transformer layers to avoid early performance degradation as identified by prior work&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Tang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib20" title="">2024</a>)</cite>.
Second, the layer immediately after full attention and a single middle layer (e.g., layer 2 and 13 in <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S3.F2" title="In 3 Methodology ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a>) perform full attention with token selection, where TidalDecode stores the inner product<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>We don‚Äôt store attention score as state-of-the-art attention kernels don‚Äôt materialize the attention score. Since the softmax operation is ordering invariant, we store the inner product value instead.</span></span></span> between the current query and key vectors of all tokens in KV cache during full attention and then selects <math alttext="k" class="ltx_Math" display="inline" id="S3.p1.1.m1.1"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.1d">italic_k</annotation></semantics></math> tokens contributing to the highest attention scores.
Third, all other layers perform <span class="ltx_text ltx_font_italic" id="S3.p1.1.3">position persistent sparse attention</span>, where only tokens selected from the previous token selection layer are loaded from the KV cache to perform attention computation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Position Persistent Sparse Attention (PPSA)</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.9">Attention mechanisms have been widely used in today‚Äôs LLMs. For each attention head, the output is computed via scaled multiplicative formulation as follows.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="A_{i}=Q_{i}K_{i}/\sqrt{d},\quad H_{i}={\rm softmax}(A_{i})V_{i}" class="ltx_Math" display="block" id="S3.E1.m1.2"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.3.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.2.2.cmml">A</mi><mi id="S3.E1.m1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml"><mrow id="S3.E1.m1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.3.2.cmml"><msub id="S3.E1.m1.1.1.1.1.3.2.2" xref="S3.E1.m1.1.1.1.1.3.2.2.cmml"><mi id="S3.E1.m1.1.1.1.1.3.2.2.2" xref="S3.E1.m1.1.1.1.1.3.2.2.2.cmml">Q</mi><mi id="S3.E1.m1.1.1.1.1.3.2.2.3" xref="S3.E1.m1.1.1.1.1.3.2.2.3.cmml">i</mi></msub><mo id="S3.E1.m1.1.1.1.1.3.2.1" xref="S3.E1.m1.1.1.1.1.3.2.1.cmml">‚Å¢</mo><msub id="S3.E1.m1.1.1.1.1.3.2.3" xref="S3.E1.m1.1.1.1.1.3.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1.3.2.3.2" xref="S3.E1.m1.1.1.1.1.3.2.3.2.cmml">K</mi><mi id="S3.E1.m1.1.1.1.1.3.2.3.3" xref="S3.E1.m1.1.1.1.1.3.2.3.3.cmml">i</mi></msub></mrow><mo id="S3.E1.m1.1.1.1.1.3.1" xref="S3.E1.m1.1.1.1.1.3.1.cmml">/</mo><msqrt id="S3.E1.m1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.3.3.cmml"><mi id="S3.E1.m1.1.1.1.1.3.3.2" xref="S3.E1.m1.1.1.1.1.3.3.2.cmml">d</mi></msqrt></mrow></mrow><mo id="S3.E1.m1.2.2.2.3" rspace="1.167em" xref="S3.E1.m1.2.2.3a.cmml">,</mo><mrow id="S3.E1.m1.2.2.2.2" xref="S3.E1.m1.2.2.2.2.cmml"><msub id="S3.E1.m1.2.2.2.2.3" xref="S3.E1.m1.2.2.2.2.3.cmml"><mi id="S3.E1.m1.2.2.2.2.3.2" xref="S3.E1.m1.2.2.2.2.3.2.cmml">H</mi><mi id="S3.E1.m1.2.2.2.2.3.3" xref="S3.E1.m1.2.2.2.2.3.3.cmml">i</mi></msub><mo id="S3.E1.m1.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.cmml">=</mo><mrow id="S3.E1.m1.2.2.2.2.1" xref="S3.E1.m1.2.2.2.2.1.cmml"><mi id="S3.E1.m1.2.2.2.2.1.3" xref="S3.E1.m1.2.2.2.2.1.3.cmml">softmax</mi><mo id="S3.E1.m1.2.2.2.2.1.2" xref="S3.E1.m1.2.2.2.2.1.2.cmml">‚Å¢</mo><mrow id="S3.E1.m1.2.2.2.2.1.1.1" xref="S3.E1.m1.2.2.2.2.1.1.1.1.cmml"><mo id="S3.E1.m1.2.2.2.2.1.1.1.2" stretchy="false" xref="S3.E1.m1.2.2.2.2.1.1.1.1.cmml">(</mo><msub id="S3.E1.m1.2.2.2.2.1.1.1.1" xref="S3.E1.m1.2.2.2.2.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.2.2.1.1.1.1.2" xref="S3.E1.m1.2.2.2.2.1.1.1.1.2.cmml">A</mi><mi id="S3.E1.m1.2.2.2.2.1.1.1.1.3" xref="S3.E1.m1.2.2.2.2.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E1.m1.2.2.2.2.1.1.1.3" stretchy="false" xref="S3.E1.m1.2.2.2.2.1.1.1.1.cmml">)</mo></mrow><mo id="S3.E1.m1.2.2.2.2.1.2a" xref="S3.E1.m1.2.2.2.2.1.2.cmml">‚Å¢</mo><msub id="S3.E1.m1.2.2.2.2.1.4" xref="S3.E1.m1.2.2.2.2.1.4.cmml"><mi id="S3.E1.m1.2.2.2.2.1.4.2" xref="S3.E1.m1.2.2.2.2.1.4.2.cmml">V</mi><mi id="S3.E1.m1.2.2.2.2.1.4.3" xref="S3.E1.m1.2.2.2.2.1.4.3.cmml">i</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.2.3.cmml" xref="S3.E1.m1.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.3a.cmml" xref="S3.E1.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"></eq><apply id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2">ùê¥</ci><ci id="S3.E1.m1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.3">ùëñ</ci></apply><apply id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"><divide id="S3.E1.m1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.1"></divide><apply id="S3.E1.m1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2"><times id="S3.E1.m1.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3.2.1"></times><apply id="S3.E1.m1.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3.2.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2.2.2">ùëÑ</ci><ci id="S3.E1.m1.1.1.1.1.3.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.3.2.2.3">ùëñ</ci></apply><apply id="S3.E1.m1.1.1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3.2">ùêæ</ci><ci id="S3.E1.m1.1.1.1.1.3.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3.3">ùëñ</ci></apply></apply><apply id="S3.E1.m1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3"><root id="S3.E1.m1.1.1.1.1.3.3a.cmml" xref="S3.E1.m1.1.1.1.1.3.3"></root><ci id="S3.E1.m1.1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2">ùëë</ci></apply></apply></apply><apply id="S3.E1.m1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2"><eq id="S3.E1.m1.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2"></eq><apply id="S3.E1.m1.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.3.1.cmml" xref="S3.E1.m1.2.2.2.2.3">subscript</csymbol><ci id="S3.E1.m1.2.2.2.2.3.2.cmml" xref="S3.E1.m1.2.2.2.2.3.2">ùêª</ci><ci id="S3.E1.m1.2.2.2.2.3.3.cmml" xref="S3.E1.m1.2.2.2.2.3.3">ùëñ</ci></apply><apply id="S3.E1.m1.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2.1"><times id="S3.E1.m1.2.2.2.2.1.2.cmml" xref="S3.E1.m1.2.2.2.2.1.2"></times><ci id="S3.E1.m1.2.2.2.2.1.3.cmml" xref="S3.E1.m1.2.2.2.2.1.3">softmax</ci><apply id="S3.E1.m1.2.2.2.2.1.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.2.2.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1.2">ùê¥</ci><ci id="S3.E1.m1.2.2.2.2.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1.3">ùëñ</ci></apply><apply id="S3.E1.m1.2.2.2.2.1.4.cmml" xref="S3.E1.m1.2.2.2.2.1.4"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.1.4.1.cmml" xref="S3.E1.m1.2.2.2.2.1.4">subscript</csymbol><ci id="S3.E1.m1.2.2.2.2.1.4.2.cmml" xref="S3.E1.m1.2.2.2.2.1.4.2">ùëâ</ci><ci id="S3.E1.m1.2.2.2.2.1.4.3.cmml" xref="S3.E1.m1.2.2.2.2.1.4.3">ùëñ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">A_{i}=Q_{i}K_{i}/\sqrt{d},\quad H_{i}={\rm softmax}(A_{i})V_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.2d">italic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_K start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT / square-root start_ARG italic_d end_ARG , italic_H start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = roman_softmax ( italic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p1.8">where <math alttext="Q_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><msub id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">Q</mi><mi id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">ùëÑ</ci><ci id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">Q_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="K_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><msub id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">K</mi><mi id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">ùêæ</ci><ci id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">K_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">italic_K start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext="V_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><msub id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">V</mi><mi id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">ùëâ</ci><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">V_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> are the query, key, and value tensors for the <math alttext="i" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.1"><semantics id="S3.SS1.p1.4.m4.1a"><mi id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">ùëñ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.1d">italic_i</annotation></semantics></math>-th attention head. <math alttext="A_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5.1"><semantics id="S3.SS1.p1.5.m5.1a"><msub id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml">A</mi><mi id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2">ùê¥</ci><ci id="S3.SS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">A_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m5.1d">italic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is a matrix representing the attention scores between tokens, and <math alttext="H_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.6.m6.1"><semantics id="S3.SS1.p1.6.m6.1a"><msub id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml"><mi id="S3.SS1.p1.6.m6.1.1.2" xref="S3.SS1.p1.6.m6.1.1.2.cmml">H</mi><mi id="S3.SS1.p1.6.m6.1.1.3" xref="S3.SS1.p1.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><apply id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.p1.6.m6.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.2">ùêª</ci><ci id="S3.SS1.p1.6.m6.1.1.3.cmml" xref="S3.SS1.p1.6.m6.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">H_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.6.m6.1d">italic_H start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the output of the <math alttext="i" class="ltx_Math" display="inline" id="S3.SS1.p1.7.m7.1"><semantics id="S3.SS1.p1.7.m7.1a"><mi id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><ci id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">ùëñ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.7.m7.1d">italic_i</annotation></semantics></math>-th attention head.
Instead of attending to all input tokens, existing sparse attention methods approximate attention computation by attending the query <math alttext="Q_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.8.m8.1"><semantics id="S3.SS1.p1.8.m8.1a"><msub id="S3.SS1.p1.8.m8.1.1" xref="S3.SS1.p1.8.m8.1.1.cmml"><mi id="S3.SS1.p1.8.m8.1.1.2" xref="S3.SS1.p1.8.m8.1.1.2.cmml">Q</mi><mi id="S3.SS1.p1.8.m8.1.1.3" xref="S3.SS1.p1.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><apply id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.8.m8.1.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1">subscript</csymbol><ci id="S3.SS1.p1.8.m8.1.1.2.cmml" xref="S3.SS1.p1.8.m8.1.1.2">ùëÑ</ci><ci id="S3.SS1.p1.8.m8.1.1.3.cmml" xref="S3.SS1.p1.8.m8.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">Q_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.8.m8.1d">italic_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> to a subset of previous tokens the highest attention scores.
Prior work generally performs token selection for individual attention heads and Transformer layers, introducing high runtime overhead.
For example, selecting the tokens with highest attention scores using top-k can take longer than computing full attention (see&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S4.F7" title="In 4.3 Efficiency Evaluation ‚Ä£ 4 Experiments ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">7</span></a>), thus diminishing the benefits of performing sparse attention.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">The key insight behind TidalDecode‚Äôs position persistent sparse attention is an observation that <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.1">tokens with highest attention scores for consecutive Transformer layers highly overlap</span>.
We use the LLaMA-3-8B model and the needle-in-the-haystack test on PG-19-mini dataset with a context length of 100K tokens to quantify this observation.
We randomly select 100 requests from the dataset, compute full attention, and analyze the top 256 tokens with the highest attention scores for each Transformer layer.
<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S3.F3.sf1" title="In Figure 3 ‚Ä£ 3.1 Position Persistent Sparse Attention (PPSA) ‚Ä£ 3 Methodology ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">3(a)</span></a> shows the overlap ratios for all pairs of transformer layers, where an overlap ratio of 1 indicates that the tokens with highest attention scores are always identical in these layers, and an overlap ratio of 0 means the top tokens do not overlap in the two layers. Note that we select top 256 tokens from 100K tokens in the KV cache, so randomly selected tokens hardly overlap.
In <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S3.F3.sf2" title="In Figure 3 ‚Ä£ 3.1 Position Persistent Sparse Attention (PPSA) ‚Ä£ 3 Methodology ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">3(b)</span></a>, we compute average recall rates of selected tokens by choosing different re-selection layers. We observe that without re-selection layers, where all layers possess a low overlap ratio with Layer 3 shown in the purple box in <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S3.F3.sf1" title="In Figure 3 ‚Ä£ 3.1 Position Persistent Sparse Attention (PPSA) ‚Ä£ 3 Methodology ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">3(a)</span></a>, the average recall rates are less than 20%. When we choose Layer 13 to perform re-selection, the average recall rates boost to almost 40% due to higher overlap ratios between Layer 13 and its subsequent layers, shown by red boxes in <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S3.F3.sf1" title="In Figure 3 ‚Ä£ 3.1 Position Persistent Sparse Attention (PPSA) ‚Ä£ 3 Methodology ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">3(a)</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F3.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="637" id="S3.F3.sf1.g1" src="https://arxiv.org/html/2410.05076v1/x3.png" width="761">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Overlap of Tokens with the Highest Attention Scores between Layers</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F3.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="605" id="S3.F3.sf2.g1" src="https://arxiv.org/html/2410.05076v1/x4.png" width="761">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Recall Rate by Re-selection Layer</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>By retrieving the top-256 tokens from a 100K-context-length Needle-in-the-Haystack test conducted on PG-19-mini, <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S3.F3.sf1" title="Figure 3(a) ‚Ä£ Figure 3 ‚Ä£ 3.1 Position Persistent Sparse Attention (PPSA) ‚Ä£ 3 Methodology ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_tag">3(a)</span></a> shows the overlap ratio of tokens with the highest attention scores across layers, showing that consecutive layers tend to share a large number of critical tokens. <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S3.F3.sf2" title="Figure 3(b) ‚Ä£ Figure 3 ‚Ä£ 3.1 Position Persistent Sparse Attention (PPSA) ‚Ä£ 3 Methodology ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_tag">3(b)</span></a> depicts the recall rates, indicating that different choices of re-selection layers have a high impact on the recall rates ‚Äî there is a clear peak, delineating the optimal layers for token re-selection.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold" id="alg1.2.1.1">Algorithm 1</span> </span> TidalDecode</figcaption>
<div class="ltx_listing ltx_listing" id="alg1.3">
<div class="ltx_listingline" id="alg1.l1">
<span class="ltx_tag ltx_tag_listingline">1:</span><span class="ltx_text ltx_font_bold" id="alg1.l1.1">Input:</span> Current embedding <math alttext="h" class="ltx_Math" display="inline" id="alg1.l1.m1.1"><semantics id="alg1.l1.m1.1a"><mi id="alg1.l1.m1.1.1" xref="alg1.l1.m1.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m1.1b"><ci id="alg1.l1.m1.1.1.cmml" xref="alg1.l1.m1.1.1">‚Ñé</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m1.1c">h</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m1.1d">italic_h</annotation></semantics></math>, KV cache <math alttext="\mathcal{C}" class="ltx_Math" display="inline" id="alg1.l1.m2.1"><semantics id="alg1.l1.m2.1a"><mi class="ltx_font_mathcaligraphic" id="alg1.l1.m2.1.1" xref="alg1.l1.m2.1.1.cmml">ùíû</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m2.1b"><ci id="alg1.l1.m2.1.1.cmml" xref="alg1.l1.m2.1.1">ùíû</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m2.1c">\mathcal{C}</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m2.1d">caligraphic_C</annotation></semantics></math>, token budget <math alttext="m" class="ltx_Math" display="inline" id="alg1.l1.m3.1"><semantics id="alg1.l1.m3.1a"><mi id="alg1.l1.m3.1.1" xref="alg1.l1.m3.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m3.1b"><ci id="alg1.l1.m3.1.1.cmml" xref="alg1.l1.m3.1.1">ùëö</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m3.1c">m</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m3.1d">italic_m</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l2">
<span class="ltx_tag ltx_tag_listingline">2:</span><span class="ltx_text ltx_font_bold" id="alg1.l2.1">Output:</span> Logits

</div>
<div class="ltx_listingline" id="alg1.l3">
<span class="ltx_tag ltx_tag_listingline">3:</span><span class="ltx_text ltx_font_bold" id="alg1.l3.2">Initialize:</span> <math alttext="\rho=[]" class="ltx_Math" display="inline" id="alg1.l3.m1.1"><semantics id="alg1.l3.m1.1a"><mrow id="alg1.l3.m1.1.1" xref="alg1.l3.m1.1.1.cmml"><mi id="alg1.l3.m1.1.1.2" xref="alg1.l3.m1.1.1.2.cmml">œÅ</mi><mo id="alg1.l3.m1.1.1.1" xref="alg1.l3.m1.1.1.1.cmml">=</mo><mrow id="alg1.l3.m1.1.1.3.2" xref="alg1.l3.m1.1.1.cmml"><mo id="alg1.l3.m1.1.1.3.2.1" stretchy="false" xref="alg1.l3.m1.1.1.3.1.cmml">[</mo><mo id="alg1.l3.m1.1.1.3.2.2" stretchy="false" xref="alg1.l3.m1.1.1.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l3.m1.1b"><apply id="alg1.l3.m1.1.1.cmml" xref="alg1.l3.m1.1.1"><eq id="alg1.l3.m1.1.1.1.cmml" xref="alg1.l3.m1.1.1.1"></eq><ci id="alg1.l3.m1.1.1.2.cmml" xref="alg1.l3.m1.1.1.2">ùúå</ci><list id="alg1.l3.m1.1.1.3.1.cmml" xref="alg1.l3.m1.1.1.3.2.1"></list></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m1.1c">\rho=[]</annotation><annotation encoding="application/x-llamapun" id="alg1.l3.m1.1d">italic_œÅ = [ ]</annotation></semantics></math> <span class="ltx_text" id="alg1.l3.1" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg1.l3.1.m1.1"><semantics id="alg1.l3.1.m1.1a"><mo id="alg1.l3.1.m1.1.1" xref="alg1.l3.1.m1.1.1.cmml">‚ñ∑</mo><annotation-xml encoding="MathML-Content" id="alg1.l3.1.m1.1b"><ci id="alg1.l3.1.m1.1.1.cmml" xref="alg1.l3.1.m1.1.1">‚ñ∑</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.1.m1.1c">\triangleright</annotation><annotation encoding="application/x-llamapun" id="alg1.l3.1.m1.1d">‚ñ∑</annotation></semantics></math> <span class="ltx_text" id="alg1.l3.1.1" style="color:#800080;">Initialize the token buffer to store selected tokens</span>
</span>
</div>
<div class="ltx_listingline" id="alg1.l4">
<span class="ltx_tag ltx_tag_listingline">4:</span><span class="ltx_text ltx_font_bold" id="alg1.l4.1">for</span>&nbsp;each decoder layer <math alttext="i" class="ltx_Math" display="inline" id="alg1.l4.m1.1"><semantics id="alg1.l4.m1.1a"><mi id="alg1.l4.m1.1.1" xref="alg1.l4.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="alg1.l4.m1.1b"><ci id="alg1.l4.m1.1.1.cmml" xref="alg1.l4.m1.1.1">ùëñ</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m1.1c">i</annotation><annotation encoding="application/x-llamapun" id="alg1.l4.m1.1d">italic_i</annotation></semantics></math>&nbsp;<span class="ltx_text ltx_font_bold" id="alg1.l4.2">do</span>
</div>
<div class="ltx_listingline" id="alg1.l5">
<span class="ltx_tag ltx_tag_listingline">5:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<math alttext="q,k,v=f(W_{qkv},h)" class="ltx_Math" display="inline" id="alg1.l5.m1.5"><semantics id="alg1.l5.m1.5a"><mrow id="alg1.l5.m1.5.5" xref="alg1.l5.m1.5.5.cmml"><mrow id="alg1.l5.m1.5.5.3.2" xref="alg1.l5.m1.5.5.3.1.cmml"><mi id="alg1.l5.m1.2.2" xref="alg1.l5.m1.2.2.cmml">q</mi><mo id="alg1.l5.m1.5.5.3.2.1" xref="alg1.l5.m1.5.5.3.1.cmml">,</mo><mi id="alg1.l5.m1.3.3" xref="alg1.l5.m1.3.3.cmml">k</mi><mo id="alg1.l5.m1.5.5.3.2.2" xref="alg1.l5.m1.5.5.3.1.cmml">,</mo><mi id="alg1.l5.m1.4.4" xref="alg1.l5.m1.4.4.cmml">v</mi></mrow><mo id="alg1.l5.m1.5.5.2" xref="alg1.l5.m1.5.5.2.cmml">=</mo><mrow id="alg1.l5.m1.5.5.1" xref="alg1.l5.m1.5.5.1.cmml"><mi id="alg1.l5.m1.5.5.1.3" xref="alg1.l5.m1.5.5.1.3.cmml">f</mi><mo id="alg1.l5.m1.5.5.1.2" xref="alg1.l5.m1.5.5.1.2.cmml">‚Å¢</mo><mrow id="alg1.l5.m1.5.5.1.1.1" xref="alg1.l5.m1.5.5.1.1.2.cmml"><mo id="alg1.l5.m1.5.5.1.1.1.2" stretchy="false" xref="alg1.l5.m1.5.5.1.1.2.cmml">(</mo><msub id="alg1.l5.m1.5.5.1.1.1.1" xref="alg1.l5.m1.5.5.1.1.1.1.cmml"><mi id="alg1.l5.m1.5.5.1.1.1.1.2" xref="alg1.l5.m1.5.5.1.1.1.1.2.cmml">W</mi><mrow id="alg1.l5.m1.5.5.1.1.1.1.3" xref="alg1.l5.m1.5.5.1.1.1.1.3.cmml"><mi id="alg1.l5.m1.5.5.1.1.1.1.3.2" xref="alg1.l5.m1.5.5.1.1.1.1.3.2.cmml">q</mi><mo id="alg1.l5.m1.5.5.1.1.1.1.3.1" xref="alg1.l5.m1.5.5.1.1.1.1.3.1.cmml">‚Å¢</mo><mi id="alg1.l5.m1.5.5.1.1.1.1.3.3" xref="alg1.l5.m1.5.5.1.1.1.1.3.3.cmml">k</mi><mo id="alg1.l5.m1.5.5.1.1.1.1.3.1a" xref="alg1.l5.m1.5.5.1.1.1.1.3.1.cmml">‚Å¢</mo><mi id="alg1.l5.m1.5.5.1.1.1.1.3.4" xref="alg1.l5.m1.5.5.1.1.1.1.3.4.cmml">v</mi></mrow></msub><mo id="alg1.l5.m1.5.5.1.1.1.3" xref="alg1.l5.m1.5.5.1.1.2.cmml">,</mo><mi id="alg1.l5.m1.1.1" xref="alg1.l5.m1.1.1.cmml">h</mi><mo id="alg1.l5.m1.5.5.1.1.1.4" stretchy="false" xref="alg1.l5.m1.5.5.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l5.m1.5b"><apply id="alg1.l5.m1.5.5.cmml" xref="alg1.l5.m1.5.5"><eq id="alg1.l5.m1.5.5.2.cmml" xref="alg1.l5.m1.5.5.2"></eq><list id="alg1.l5.m1.5.5.3.1.cmml" xref="alg1.l5.m1.5.5.3.2"><ci id="alg1.l5.m1.2.2.cmml" xref="alg1.l5.m1.2.2">ùëû</ci><ci id="alg1.l5.m1.3.3.cmml" xref="alg1.l5.m1.3.3">ùëò</ci><ci id="alg1.l5.m1.4.4.cmml" xref="alg1.l5.m1.4.4">ùë£</ci></list><apply id="alg1.l5.m1.5.5.1.cmml" xref="alg1.l5.m1.5.5.1"><times id="alg1.l5.m1.5.5.1.2.cmml" xref="alg1.l5.m1.5.5.1.2"></times><ci id="alg1.l5.m1.5.5.1.3.cmml" xref="alg1.l5.m1.5.5.1.3">ùëì</ci><interval closure="open" id="alg1.l5.m1.5.5.1.1.2.cmml" xref="alg1.l5.m1.5.5.1.1.1"><apply id="alg1.l5.m1.5.5.1.1.1.1.cmml" xref="alg1.l5.m1.5.5.1.1.1.1"><csymbol cd="ambiguous" id="alg1.l5.m1.5.5.1.1.1.1.1.cmml" xref="alg1.l5.m1.5.5.1.1.1.1">subscript</csymbol><ci id="alg1.l5.m1.5.5.1.1.1.1.2.cmml" xref="alg1.l5.m1.5.5.1.1.1.1.2">ùëä</ci><apply id="alg1.l5.m1.5.5.1.1.1.1.3.cmml" xref="alg1.l5.m1.5.5.1.1.1.1.3"><times id="alg1.l5.m1.5.5.1.1.1.1.3.1.cmml" xref="alg1.l5.m1.5.5.1.1.1.1.3.1"></times><ci id="alg1.l5.m1.5.5.1.1.1.1.3.2.cmml" xref="alg1.l5.m1.5.5.1.1.1.1.3.2">ùëû</ci><ci id="alg1.l5.m1.5.5.1.1.1.1.3.3.cmml" xref="alg1.l5.m1.5.5.1.1.1.1.3.3">ùëò</ci><ci id="alg1.l5.m1.5.5.1.1.1.1.3.4.cmml" xref="alg1.l5.m1.5.5.1.1.1.1.3.4">ùë£</ci></apply></apply><ci id="alg1.l5.m1.1.1.cmml" xref="alg1.l5.m1.1.1">‚Ñé</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m1.5c">q,k,v=f(W_{qkv},h)</annotation><annotation encoding="application/x-llamapun" id="alg1.l5.m1.5d">italic_q , italic_k , italic_v = italic_f ( italic_W start_POSTSUBSCRIPT italic_q italic_k italic_v end_POSTSUBSCRIPT , italic_h )</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l6">
<span class="ltx_tag ltx_tag_listingline">6:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<math alttext="\mathcal{C}" class="ltx_Math" display="inline" id="alg1.l6.m1.1"><semantics id="alg1.l6.m1.1a"><mi class="ltx_font_mathcaligraphic" id="alg1.l6.m1.1.1" xref="alg1.l6.m1.1.1.cmml">ùíû</mi><annotation-xml encoding="MathML-Content" id="alg1.l6.m1.1b"><ci id="alg1.l6.m1.1.1.cmml" xref="alg1.l6.m1.1.1">ùíû</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m1.1c">\mathcal{C}</annotation><annotation encoding="application/x-llamapun" id="alg1.l6.m1.1d">caligraphic_C</annotation></semantics></math>.append(<math alttext="k,v" class="ltx_Math" display="inline" id="alg1.l6.m2.2"><semantics id="alg1.l6.m2.2a"><mrow id="alg1.l6.m2.2.3.2" xref="alg1.l6.m2.2.3.1.cmml"><mi id="alg1.l6.m2.1.1" xref="alg1.l6.m2.1.1.cmml">k</mi><mo id="alg1.l6.m2.2.3.2.1" xref="alg1.l6.m2.2.3.1.cmml">,</mo><mi id="alg1.l6.m2.2.2" xref="alg1.l6.m2.2.2.cmml">v</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l6.m2.2b"><list id="alg1.l6.m2.2.3.1.cmml" xref="alg1.l6.m2.2.3.2"><ci id="alg1.l6.m2.1.1.cmml" xref="alg1.l6.m2.1.1">ùëò</ci><ci id="alg1.l6.m2.2.2.cmml" xref="alg1.l6.m2.2.2">ùë£</ci></list></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m2.2c">k,v</annotation><annotation encoding="application/x-llamapun" id="alg1.l6.m2.2d">italic_k , italic_v</annotation></semantics></math>)

</div>
<div class="ltx_listingline" id="alg1.l7">
<span class="ltx_tag ltx_tag_listingline">7:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="ltx_text ltx_font_bold" id="alg1.l7.1">if</span>&nbsp;<math alttext="i" class="ltx_Math" display="inline" id="alg1.l7.m1.1"><semantics id="alg1.l7.m1.1a"><mi id="alg1.l7.m1.1.1" xref="alg1.l7.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="alg1.l7.m1.1b"><ci id="alg1.l7.m1.1.1.cmml" xref="alg1.l7.m1.1.1">ùëñ</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m1.1c">i</annotation><annotation encoding="application/x-llamapun" id="alg1.l7.m1.1d">italic_i</annotation></semantics></math> is Full Attention Layer&nbsp;<span class="ltx_text ltx_font_bold" id="alg1.l7.2">then</span>
</div>
<div class="ltx_listingline" id="alg1.l8">
<span class="ltx_tag ltx_tag_listingline">8:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<math alttext="o=\text{FullAttention}(q,\mathcal{C}[:])" class="ltx_Math" display="inline" id="alg1.l8.m1.3"><semantics id="alg1.l8.m1.3a"><mrow id="alg1.l8.m1.3.3" xref="alg1.l8.m1.3.3.cmml"><mi id="alg1.l8.m1.3.3.3" xref="alg1.l8.m1.3.3.3.cmml">o</mi><mo id="alg1.l8.m1.3.3.2" xref="alg1.l8.m1.3.3.2.cmml">=</mo><mrow id="alg1.l8.m1.3.3.1" xref="alg1.l8.m1.3.3.1.cmml"><mtext id="alg1.l8.m1.3.3.1.3" xref="alg1.l8.m1.3.3.1.3a.cmml">FullAttention</mtext><mo id="alg1.l8.m1.3.3.1.2" xref="alg1.l8.m1.3.3.1.2.cmml">‚Å¢</mo><mrow id="alg1.l8.m1.3.3.1.1.1" xref="alg1.l8.m1.3.3.1.1.2.cmml"><mo id="alg1.l8.m1.3.3.1.1.1.2" stretchy="false" xref="alg1.l8.m1.3.3.1.1.2.cmml">(</mo><mi id="alg1.l8.m1.2.2" xref="alg1.l8.m1.2.2.cmml">q</mi><mo id="alg1.l8.m1.3.3.1.1.1.3" xref="alg1.l8.m1.3.3.1.1.2.cmml">,</mo><mrow id="alg1.l8.m1.3.3.1.1.1.1" xref="alg1.l8.m1.3.3.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="alg1.l8.m1.3.3.1.1.1.1.2" xref="alg1.l8.m1.3.3.1.1.1.1.2.cmml">ùíû</mi><mo id="alg1.l8.m1.3.3.1.1.1.1.1" xref="alg1.l8.m1.3.3.1.1.1.1.1.cmml">‚Å¢</mo><mrow id="alg1.l8.m1.3.3.1.1.1.1.3.2" xref="alg1.l8.m1.3.3.1.1.1.1.3.1.cmml"><mo id="alg1.l8.m1.3.3.1.1.1.1.3.2.1" stretchy="false" xref="alg1.l8.m1.3.3.1.1.1.1.3.1.1.cmml">[</mo><mo id="alg1.l8.m1.1.1" rspace="0em" xref="alg1.l8.m1.1.1.cmml">:</mo><mo id="alg1.l8.m1.3.3.1.1.1.1.3.2.2" stretchy="false" xref="alg1.l8.m1.3.3.1.1.1.1.3.1.1.cmml">]</mo></mrow></mrow><mo id="alg1.l8.m1.3.3.1.1.1.4" stretchy="false" xref="alg1.l8.m1.3.3.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l8.m1.3b"><apply id="alg1.l8.m1.3.3.cmml" xref="alg1.l8.m1.3.3"><eq id="alg1.l8.m1.3.3.2.cmml" xref="alg1.l8.m1.3.3.2"></eq><ci id="alg1.l8.m1.3.3.3.cmml" xref="alg1.l8.m1.3.3.3">ùëú</ci><apply id="alg1.l8.m1.3.3.1.cmml" xref="alg1.l8.m1.3.3.1"><times id="alg1.l8.m1.3.3.1.2.cmml" xref="alg1.l8.m1.3.3.1.2"></times><ci id="alg1.l8.m1.3.3.1.3a.cmml" xref="alg1.l8.m1.3.3.1.3"><mtext id="alg1.l8.m1.3.3.1.3.cmml" xref="alg1.l8.m1.3.3.1.3">FullAttention</mtext></ci><interval closure="open" id="alg1.l8.m1.3.3.1.1.2.cmml" xref="alg1.l8.m1.3.3.1.1.1"><ci id="alg1.l8.m1.2.2.cmml" xref="alg1.l8.m1.2.2">ùëû</ci><apply id="alg1.l8.m1.3.3.1.1.1.1.cmml" xref="alg1.l8.m1.3.3.1.1.1.1"><times id="alg1.l8.m1.3.3.1.1.1.1.1.cmml" xref="alg1.l8.m1.3.3.1.1.1.1.1"></times><ci id="alg1.l8.m1.3.3.1.1.1.1.2.cmml" xref="alg1.l8.m1.3.3.1.1.1.1.2">ùíû</ci><apply id="alg1.l8.m1.3.3.1.1.1.1.3.1.cmml" xref="alg1.l8.m1.3.3.1.1.1.1.3.2"><csymbol cd="latexml" id="alg1.l8.m1.3.3.1.1.1.1.3.1.1.cmml" xref="alg1.l8.m1.3.3.1.1.1.1.3.2.1">delimited-[]</csymbol><ci id="alg1.l8.m1.1.1.cmml" xref="alg1.l8.m1.1.1">:</ci></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l8.m1.3c">o=\text{FullAttention}(q,\mathcal{C}[:])</annotation><annotation encoding="application/x-llamapun" id="alg1.l8.m1.3d">italic_o = FullAttention ( italic_q , caligraphic_C [ : ] )</annotation></semantics></math> <span class="ltx_text" id="alg1.l8.1" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg1.l8.1.m1.1"><semantics id="alg1.l8.1.m1.1a"><mo id="alg1.l8.1.m1.1.1" xref="alg1.l8.1.m1.1.1.cmml">‚ñ∑</mo><annotation-xml encoding="MathML-Content" id="alg1.l8.1.m1.1b"><ci id="alg1.l8.1.m1.1.1.cmml" xref="alg1.l8.1.m1.1.1">‚ñ∑</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l8.1.m1.1c">\triangleright</annotation><annotation encoding="application/x-llamapun" id="alg1.l8.1.m1.1d">‚ñ∑</annotation></semantics></math> <span class="ltx_text" id="alg1.l8.1.1" style="color:#800080;">Dense attention with the full KVCache</span>
</span>
</div>
<div class="ltx_listingline" id="alg1.l9">
<span class="ltx_tag ltx_tag_listingline">9:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="ltx_text ltx_font_bold" id="alg1.l9.1">else</span>&nbsp;<span class="ltx_text ltx_font_bold" id="alg1.l9.2">if</span>&nbsp;<math alttext="i" class="ltx_Math" display="inline" id="alg1.l9.m1.1"><semantics id="alg1.l9.m1.1a"><mi id="alg1.l9.m1.1.1" xref="alg1.l9.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="alg1.l9.m1.1b"><ci id="alg1.l9.m1.1.1.cmml" xref="alg1.l9.m1.1.1">ùëñ</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l9.m1.1c">i</annotation><annotation encoding="application/x-llamapun" id="alg1.l9.m1.1d">italic_i</annotation></semantics></math> is Token Selection Layer&nbsp;<span class="ltx_text ltx_font_bold" id="alg1.l9.3">then</span>
</div>
<div class="ltx_listingline" id="alg1.l10">
<span class="ltx_tag ltx_tag_listingline">10:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<math alttext="o=\text{FullAttention}(q,\mathcal{C}[:])" class="ltx_Math" display="inline" id="alg1.l10.m1.3"><semantics id="alg1.l10.m1.3a"><mrow id="alg1.l10.m1.3.3" xref="alg1.l10.m1.3.3.cmml"><mi id="alg1.l10.m1.3.3.3" xref="alg1.l10.m1.3.3.3.cmml">o</mi><mo id="alg1.l10.m1.3.3.2" xref="alg1.l10.m1.3.3.2.cmml">=</mo><mrow id="alg1.l10.m1.3.3.1" xref="alg1.l10.m1.3.3.1.cmml"><mtext id="alg1.l10.m1.3.3.1.3" xref="alg1.l10.m1.3.3.1.3a.cmml">FullAttention</mtext><mo id="alg1.l10.m1.3.3.1.2" xref="alg1.l10.m1.3.3.1.2.cmml">‚Å¢</mo><mrow id="alg1.l10.m1.3.3.1.1.1" xref="alg1.l10.m1.3.3.1.1.2.cmml"><mo id="alg1.l10.m1.3.3.1.1.1.2" stretchy="false" xref="alg1.l10.m1.3.3.1.1.2.cmml">(</mo><mi id="alg1.l10.m1.2.2" xref="alg1.l10.m1.2.2.cmml">q</mi><mo id="alg1.l10.m1.3.3.1.1.1.3" xref="alg1.l10.m1.3.3.1.1.2.cmml">,</mo><mrow id="alg1.l10.m1.3.3.1.1.1.1" xref="alg1.l10.m1.3.3.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="alg1.l10.m1.3.3.1.1.1.1.2" xref="alg1.l10.m1.3.3.1.1.1.1.2.cmml">ùíû</mi><mo id="alg1.l10.m1.3.3.1.1.1.1.1" xref="alg1.l10.m1.3.3.1.1.1.1.1.cmml">‚Å¢</mo><mrow id="alg1.l10.m1.3.3.1.1.1.1.3.2" xref="alg1.l10.m1.3.3.1.1.1.1.3.1.cmml"><mo id="alg1.l10.m1.3.3.1.1.1.1.3.2.1" stretchy="false" xref="alg1.l10.m1.3.3.1.1.1.1.3.1.1.cmml">[</mo><mo id="alg1.l10.m1.1.1" rspace="0em" xref="alg1.l10.m1.1.1.cmml">:</mo><mo id="alg1.l10.m1.3.3.1.1.1.1.3.2.2" stretchy="false" xref="alg1.l10.m1.3.3.1.1.1.1.3.1.1.cmml">]</mo></mrow></mrow><mo id="alg1.l10.m1.3.3.1.1.1.4" stretchy="false" xref="alg1.l10.m1.3.3.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l10.m1.3b"><apply id="alg1.l10.m1.3.3.cmml" xref="alg1.l10.m1.3.3"><eq id="alg1.l10.m1.3.3.2.cmml" xref="alg1.l10.m1.3.3.2"></eq><ci id="alg1.l10.m1.3.3.3.cmml" xref="alg1.l10.m1.3.3.3">ùëú</ci><apply id="alg1.l10.m1.3.3.1.cmml" xref="alg1.l10.m1.3.3.1"><times id="alg1.l10.m1.3.3.1.2.cmml" xref="alg1.l10.m1.3.3.1.2"></times><ci id="alg1.l10.m1.3.3.1.3a.cmml" xref="alg1.l10.m1.3.3.1.3"><mtext id="alg1.l10.m1.3.3.1.3.cmml" xref="alg1.l10.m1.3.3.1.3">FullAttention</mtext></ci><interval closure="open" id="alg1.l10.m1.3.3.1.1.2.cmml" xref="alg1.l10.m1.3.3.1.1.1"><ci id="alg1.l10.m1.2.2.cmml" xref="alg1.l10.m1.2.2">ùëû</ci><apply id="alg1.l10.m1.3.3.1.1.1.1.cmml" xref="alg1.l10.m1.3.3.1.1.1.1"><times id="alg1.l10.m1.3.3.1.1.1.1.1.cmml" xref="alg1.l10.m1.3.3.1.1.1.1.1"></times><ci id="alg1.l10.m1.3.3.1.1.1.1.2.cmml" xref="alg1.l10.m1.3.3.1.1.1.1.2">ùíû</ci><apply id="alg1.l10.m1.3.3.1.1.1.1.3.1.cmml" xref="alg1.l10.m1.3.3.1.1.1.1.3.2"><csymbol cd="latexml" id="alg1.l10.m1.3.3.1.1.1.1.3.1.1.cmml" xref="alg1.l10.m1.3.3.1.1.1.1.3.2.1">delimited-[]</csymbol><ci id="alg1.l10.m1.1.1.cmml" xref="alg1.l10.m1.1.1">:</ci></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l10.m1.3c">o=\text{FullAttention}(q,\mathcal{C}[:])</annotation><annotation encoding="application/x-llamapun" id="alg1.l10.m1.3d">italic_o = FullAttention ( italic_q , caligraphic_C [ : ] )</annotation></semantics></math> <span class="ltx_text" id="alg1.l10.1" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg1.l10.1.m1.1"><semantics id="alg1.l10.1.m1.1a"><mo id="alg1.l10.1.m1.1.1" xref="alg1.l10.1.m1.1.1.cmml">‚ñ∑</mo><annotation-xml encoding="MathML-Content" id="alg1.l10.1.m1.1b"><ci id="alg1.l10.1.m1.1.1.cmml" xref="alg1.l10.1.m1.1.1">‚ñ∑</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l10.1.m1.1c">\triangleright</annotation><annotation encoding="application/x-llamapun" id="alg1.l10.1.m1.1d">‚ñ∑</annotation></semantics></math> <span class="ltx_text" id="alg1.l10.1.1" style="color:#800080;">Dense attention with the full KVCache</span>
</span>
</div>
<div class="ltx_listingline" id="alg1.l11">
<span class="ltx_tag ltx_tag_listingline">11:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<math alttext="K\leftarrow\mathcal{C}.\text{getKey}" class="ltx_Math" display="inline" id="alg1.l11.m1.2"><semantics id="alg1.l11.m1.2a"><mrow id="alg1.l11.m1.2.2.1" xref="alg1.l11.m1.2.2.2.cmml"><mrow id="alg1.l11.m1.2.2.1.1" xref="alg1.l11.m1.2.2.1.1.cmml"><mi id="alg1.l11.m1.2.2.1.1.2" xref="alg1.l11.m1.2.2.1.1.2.cmml">K</mi><mo id="alg1.l11.m1.2.2.1.1.1" stretchy="false" xref="alg1.l11.m1.2.2.1.1.1.cmml">‚Üê</mo><mi class="ltx_font_mathcaligraphic" id="alg1.l11.m1.2.2.1.1.3" xref="alg1.l11.m1.2.2.1.1.3.cmml">ùíû</mi></mrow><mo id="alg1.l11.m1.2.2.1.2" lspace="0em" rspace="0.167em" xref="alg1.l11.m1.2.2.2a.cmml">.</mo><mtext id="alg1.l11.m1.1.1" xref="alg1.l11.m1.1.1a.cmml">getKey</mtext></mrow><annotation-xml encoding="MathML-Content" id="alg1.l11.m1.2b"><apply id="alg1.l11.m1.2.2.2.cmml" xref="alg1.l11.m1.2.2.1"><csymbol cd="ambiguous" id="alg1.l11.m1.2.2.2a.cmml" xref="alg1.l11.m1.2.2.1.2">formulae-sequence</csymbol><apply id="alg1.l11.m1.2.2.1.1.cmml" xref="alg1.l11.m1.2.2.1.1"><ci id="alg1.l11.m1.2.2.1.1.1.cmml" xref="alg1.l11.m1.2.2.1.1.1">‚Üê</ci><ci id="alg1.l11.m1.2.2.1.1.2.cmml" xref="alg1.l11.m1.2.2.1.1.2">ùêæ</ci><ci id="alg1.l11.m1.2.2.1.1.3.cmml" xref="alg1.l11.m1.2.2.1.1.3">ùíû</ci></apply><ci id="alg1.l11.m1.1.1a.cmml" xref="alg1.l11.m1.1.1"><mtext id="alg1.l11.m1.1.1.cmml" xref="alg1.l11.m1.1.1">getKey</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l11.m1.2c">K\leftarrow\mathcal{C}.\text{getKey}</annotation><annotation encoding="application/x-llamapun" id="alg1.l11.m1.2d">italic_K ‚Üê caligraphic_C . getKey</annotation></semantics></math>, <math alttext="\rho:=\text{argTopK}(\langle q,K\rangle,m)" class="ltx_Math" display="inline" id="alg1.l11.m2.4"><semantics id="alg1.l11.m2.4a"><mrow id="alg1.l11.m2.4.4" xref="alg1.l11.m2.4.4.cmml"><mi id="alg1.l11.m2.4.4.3" xref="alg1.l11.m2.4.4.3.cmml">œÅ</mi><mo id="alg1.l11.m2.4.4.2" lspace="0.278em" rspace="0.278em" xref="alg1.l11.m2.4.4.2.cmml">:=</mo><mrow id="alg1.l11.m2.4.4.1" xref="alg1.l11.m2.4.4.1.cmml"><mtext id="alg1.l11.m2.4.4.1.3" xref="alg1.l11.m2.4.4.1.3a.cmml">argTopK</mtext><mo id="alg1.l11.m2.4.4.1.2" xref="alg1.l11.m2.4.4.1.2.cmml">‚Å¢</mo><mrow id="alg1.l11.m2.4.4.1.1.1" xref="alg1.l11.m2.4.4.1.1.2.cmml"><mo id="alg1.l11.m2.4.4.1.1.1.2" stretchy="false" xref="alg1.l11.m2.4.4.1.1.2.cmml">(</mo><mrow id="alg1.l11.m2.4.4.1.1.1.1.2" xref="alg1.l11.m2.4.4.1.1.1.1.1.cmml"><mo id="alg1.l11.m2.4.4.1.1.1.1.2.1" stretchy="false" xref="alg1.l11.m2.4.4.1.1.1.1.1.cmml">‚ü®</mo><mi id="alg1.l11.m2.1.1" xref="alg1.l11.m2.1.1.cmml">q</mi><mo id="alg1.l11.m2.4.4.1.1.1.1.2.2" xref="alg1.l11.m2.4.4.1.1.1.1.1.cmml">,</mo><mi id="alg1.l11.m2.2.2" xref="alg1.l11.m2.2.2.cmml">K</mi><mo id="alg1.l11.m2.4.4.1.1.1.1.2.3" stretchy="false" xref="alg1.l11.m2.4.4.1.1.1.1.1.cmml">‚ü©</mo></mrow><mo id="alg1.l11.m2.4.4.1.1.1.3" xref="alg1.l11.m2.4.4.1.1.2.cmml">,</mo><mi id="alg1.l11.m2.3.3" xref="alg1.l11.m2.3.3.cmml">m</mi><mo id="alg1.l11.m2.4.4.1.1.1.4" stretchy="false" xref="alg1.l11.m2.4.4.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l11.m2.4b"><apply id="alg1.l11.m2.4.4.cmml" xref="alg1.l11.m2.4.4"><csymbol cd="latexml" id="alg1.l11.m2.4.4.2.cmml" xref="alg1.l11.m2.4.4.2">assign</csymbol><ci id="alg1.l11.m2.4.4.3.cmml" xref="alg1.l11.m2.4.4.3">ùúå</ci><apply id="alg1.l11.m2.4.4.1.cmml" xref="alg1.l11.m2.4.4.1"><times id="alg1.l11.m2.4.4.1.2.cmml" xref="alg1.l11.m2.4.4.1.2"></times><ci id="alg1.l11.m2.4.4.1.3a.cmml" xref="alg1.l11.m2.4.4.1.3"><mtext id="alg1.l11.m2.4.4.1.3.cmml" xref="alg1.l11.m2.4.4.1.3">argTopK</mtext></ci><interval closure="open" id="alg1.l11.m2.4.4.1.1.2.cmml" xref="alg1.l11.m2.4.4.1.1.1"><list id="alg1.l11.m2.4.4.1.1.1.1.1.cmml" xref="alg1.l11.m2.4.4.1.1.1.1.2"><ci id="alg1.l11.m2.1.1.cmml" xref="alg1.l11.m2.1.1">ùëû</ci><ci id="alg1.l11.m2.2.2.cmml" xref="alg1.l11.m2.2.2">ùêæ</ci></list><ci id="alg1.l11.m2.3.3.cmml" xref="alg1.l11.m2.3.3">ùëö</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l11.m2.4c">\rho:=\text{argTopK}(\langle q,K\rangle,m)</annotation><annotation encoding="application/x-llamapun" id="alg1.l11.m2.4d">italic_œÅ := argTopK ( ‚ü® italic_q , italic_K ‚ü© , italic_m )</annotation></semantics></math> <span class="ltx_text" id="alg1.l11.1" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg1.l11.1.m1.1"><semantics id="alg1.l11.1.m1.1a"><mo id="alg1.l11.1.m1.1.1" xref="alg1.l11.1.m1.1.1.cmml">‚ñ∑</mo><annotation-xml encoding="MathML-Content" id="alg1.l11.1.m1.1b"><ci id="alg1.l11.1.m1.1.1.cmml" xref="alg1.l11.1.m1.1.1">‚ñ∑</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l11.1.m1.1c">\triangleright</annotation><annotation encoding="application/x-llamapun" id="alg1.l11.1.m1.1d">‚ñ∑</annotation></semantics></math> <span class="ltx_text" id="alg1.l11.1.1" style="color:#800080;">Update token buffer</span>
</span>
</div>
<div class="ltx_listingline" id="alg1.l12">
<span class="ltx_tag ltx_tag_listingline">12:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="ltx_text ltx_font_bold" id="alg1.l12.1">else</span>
</div>
<div class="ltx_listingline" id="alg1.l13">
<span class="ltx_tag ltx_tag_listingline">13:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<math alttext="o=\text{SparseAttention}(q,\mathcal{C}[\rho])" class="ltx_Math" display="inline" id="alg1.l13.m1.3"><semantics id="alg1.l13.m1.3a"><mrow id="alg1.l13.m1.3.3" xref="alg1.l13.m1.3.3.cmml"><mi id="alg1.l13.m1.3.3.3" xref="alg1.l13.m1.3.3.3.cmml">o</mi><mo id="alg1.l13.m1.3.3.2" xref="alg1.l13.m1.3.3.2.cmml">=</mo><mrow id="alg1.l13.m1.3.3.1" xref="alg1.l13.m1.3.3.1.cmml"><mtext id="alg1.l13.m1.3.3.1.3" xref="alg1.l13.m1.3.3.1.3a.cmml">SparseAttention</mtext><mo id="alg1.l13.m1.3.3.1.2" xref="alg1.l13.m1.3.3.1.2.cmml">‚Å¢</mo><mrow id="alg1.l13.m1.3.3.1.1.1" xref="alg1.l13.m1.3.3.1.1.2.cmml"><mo id="alg1.l13.m1.3.3.1.1.1.2" stretchy="false" xref="alg1.l13.m1.3.3.1.1.2.cmml">(</mo><mi id="alg1.l13.m1.2.2" xref="alg1.l13.m1.2.2.cmml">q</mi><mo id="alg1.l13.m1.3.3.1.1.1.3" xref="alg1.l13.m1.3.3.1.1.2.cmml">,</mo><mrow id="alg1.l13.m1.3.3.1.1.1.1" xref="alg1.l13.m1.3.3.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="alg1.l13.m1.3.3.1.1.1.1.2" xref="alg1.l13.m1.3.3.1.1.1.1.2.cmml">ùíû</mi><mo id="alg1.l13.m1.3.3.1.1.1.1.1" xref="alg1.l13.m1.3.3.1.1.1.1.1.cmml">‚Å¢</mo><mrow id="alg1.l13.m1.3.3.1.1.1.1.3.2" xref="alg1.l13.m1.3.3.1.1.1.1.3.1.cmml"><mo id="alg1.l13.m1.3.3.1.1.1.1.3.2.1" stretchy="false" xref="alg1.l13.m1.3.3.1.1.1.1.3.1.1.cmml">[</mo><mi id="alg1.l13.m1.1.1" xref="alg1.l13.m1.1.1.cmml">œÅ</mi><mo id="alg1.l13.m1.3.3.1.1.1.1.3.2.2" stretchy="false" xref="alg1.l13.m1.3.3.1.1.1.1.3.1.1.cmml">]</mo></mrow></mrow><mo id="alg1.l13.m1.3.3.1.1.1.4" stretchy="false" xref="alg1.l13.m1.3.3.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l13.m1.3b"><apply id="alg1.l13.m1.3.3.cmml" xref="alg1.l13.m1.3.3"><eq id="alg1.l13.m1.3.3.2.cmml" xref="alg1.l13.m1.3.3.2"></eq><ci id="alg1.l13.m1.3.3.3.cmml" xref="alg1.l13.m1.3.3.3">ùëú</ci><apply id="alg1.l13.m1.3.3.1.cmml" xref="alg1.l13.m1.3.3.1"><times id="alg1.l13.m1.3.3.1.2.cmml" xref="alg1.l13.m1.3.3.1.2"></times><ci id="alg1.l13.m1.3.3.1.3a.cmml" xref="alg1.l13.m1.3.3.1.3"><mtext id="alg1.l13.m1.3.3.1.3.cmml" xref="alg1.l13.m1.3.3.1.3">SparseAttention</mtext></ci><interval closure="open" id="alg1.l13.m1.3.3.1.1.2.cmml" xref="alg1.l13.m1.3.3.1.1.1"><ci id="alg1.l13.m1.2.2.cmml" xref="alg1.l13.m1.2.2">ùëû</ci><apply id="alg1.l13.m1.3.3.1.1.1.1.cmml" xref="alg1.l13.m1.3.3.1.1.1.1"><times id="alg1.l13.m1.3.3.1.1.1.1.1.cmml" xref="alg1.l13.m1.3.3.1.1.1.1.1"></times><ci id="alg1.l13.m1.3.3.1.1.1.1.2.cmml" xref="alg1.l13.m1.3.3.1.1.1.1.2">ùíû</ci><apply id="alg1.l13.m1.3.3.1.1.1.1.3.1.cmml" xref="alg1.l13.m1.3.3.1.1.1.1.3.2"><csymbol cd="latexml" id="alg1.l13.m1.3.3.1.1.1.1.3.1.1.cmml" xref="alg1.l13.m1.3.3.1.1.1.1.3.2.1">delimited-[]</csymbol><ci id="alg1.l13.m1.1.1.cmml" xref="alg1.l13.m1.1.1">ùúå</ci></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l13.m1.3c">o=\text{SparseAttention}(q,\mathcal{C}[\rho])</annotation><annotation encoding="application/x-llamapun" id="alg1.l13.m1.3d">italic_o = SparseAttention ( italic_q , caligraphic_C [ italic_œÅ ] )</annotation></semantics></math> <span class="ltx_text" id="alg1.l13.1" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="alg1.l13.1.m1.1"><semantics id="alg1.l13.1.m1.1a"><mo id="alg1.l13.1.m1.1.1" xref="alg1.l13.1.m1.1.1.cmml">‚ñ∑</mo><annotation-xml encoding="MathML-Content" id="alg1.l13.1.m1.1b"><ci id="alg1.l13.1.m1.1.1.cmml" xref="alg1.l13.1.m1.1.1">‚ñ∑</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l13.1.m1.1c">\triangleright</annotation><annotation encoding="application/x-llamapun" id="alg1.l13.1.m1.1d">‚ñ∑</annotation></semantics></math> <span class="ltx_text" id="alg1.l13.1.1" style="color:#800080;">Sparse attention with the tokens in the token buffer</span>
</span>
</div>
<div class="ltx_listingline" id="alg1.l14">
<span class="ltx_tag ltx_tag_listingline">14:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="ltx_text ltx_font_bold" id="alg1.l14.1">end</span>&nbsp;<span class="ltx_text ltx_font_bold" id="alg1.l14.2">if</span>
</div>
<div class="ltx_listingline" id="alg1.l15">
<span class="ltx_tag ltx_tag_listingline">15:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<math alttext="h=\text{FFN}(o)" class="ltx_Math" display="inline" id="alg1.l15.m1.1"><semantics id="alg1.l15.m1.1a"><mrow id="alg1.l15.m1.1.2" xref="alg1.l15.m1.1.2.cmml"><mi id="alg1.l15.m1.1.2.2" xref="alg1.l15.m1.1.2.2.cmml">h</mi><mo id="alg1.l15.m1.1.2.1" xref="alg1.l15.m1.1.2.1.cmml">=</mo><mrow id="alg1.l15.m1.1.2.3" xref="alg1.l15.m1.1.2.3.cmml"><mtext id="alg1.l15.m1.1.2.3.2" xref="alg1.l15.m1.1.2.3.2a.cmml">FFN</mtext><mo id="alg1.l15.m1.1.2.3.1" xref="alg1.l15.m1.1.2.3.1.cmml">‚Å¢</mo><mrow id="alg1.l15.m1.1.2.3.3.2" xref="alg1.l15.m1.1.2.3.cmml"><mo id="alg1.l15.m1.1.2.3.3.2.1" stretchy="false" xref="alg1.l15.m1.1.2.3.cmml">(</mo><mi id="alg1.l15.m1.1.1" xref="alg1.l15.m1.1.1.cmml">o</mi><mo id="alg1.l15.m1.1.2.3.3.2.2" stretchy="false" xref="alg1.l15.m1.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l15.m1.1b"><apply id="alg1.l15.m1.1.2.cmml" xref="alg1.l15.m1.1.2"><eq id="alg1.l15.m1.1.2.1.cmml" xref="alg1.l15.m1.1.2.1"></eq><ci id="alg1.l15.m1.1.2.2.cmml" xref="alg1.l15.m1.1.2.2">‚Ñé</ci><apply id="alg1.l15.m1.1.2.3.cmml" xref="alg1.l15.m1.1.2.3"><times id="alg1.l15.m1.1.2.3.1.cmml" xref="alg1.l15.m1.1.2.3.1"></times><ci id="alg1.l15.m1.1.2.3.2a.cmml" xref="alg1.l15.m1.1.2.3.2"><mtext id="alg1.l15.m1.1.2.3.2.cmml" xref="alg1.l15.m1.1.2.3.2">FFN</mtext></ci><ci id="alg1.l15.m1.1.1.cmml" xref="alg1.l15.m1.1.1">ùëú</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l15.m1.1c">h=\text{FFN}(o)</annotation><annotation encoding="application/x-llamapun" id="alg1.l15.m1.1d">italic_h = FFN ( italic_o )</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l16">
<span class="ltx_tag ltx_tag_listingline">16:</span><span class="ltx_text ltx_font_bold" id="alg1.l16.1">end</span>&nbsp;<span class="ltx_text ltx_font_bold" id="alg1.l16.2">for</span>
</div>
<div class="ltx_listingline" id="alg1.l17">
<span class="ltx_tag ltx_tag_listingline">17:</span>logits <math alttext="=" class="ltx_Math" display="inline" id="alg1.l17.m1.1"><semantics id="alg1.l17.m1.1a"><mo id="alg1.l17.m1.1.1" xref="alg1.l17.m1.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="alg1.l17.m1.1b"><eq id="alg1.l17.m1.1.1.cmml" xref="alg1.l17.m1.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="alg1.l17.m1.1c">=</annotation><annotation encoding="application/x-llamapun" id="alg1.l17.m1.1d">=</annotation></semantics></math> lm_head<math alttext="(h)" class="ltx_Math" display="inline" id="alg1.l17.m2.1"><semantics id="alg1.l17.m2.1a"><mrow id="alg1.l17.m2.1.2.2"><mo id="alg1.l17.m2.1.2.2.1" stretchy="false">(</mo><mi id="alg1.l17.m2.1.1" xref="alg1.l17.m2.1.1.cmml">h</mi><mo id="alg1.l17.m2.1.2.2.2" stretchy="false">)</mo></mrow><annotation-xml encoding="MathML-Content" id="alg1.l17.m2.1b"><ci id="alg1.l17.m2.1.1.cmml" xref="alg1.l17.m2.1.1">‚Ñé</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l17.m2.1c">(h)</annotation><annotation encoding="application/x-llamapun" id="alg1.l17.m2.1d">( italic_h )</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l18">
<span class="ltx_tag ltx_tag_listingline">18:</span><span class="ltx_text ltx_font_bold" id="alg1.l18.1">return</span> logits

</div>
</div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.5">Based on this observation, we design position persistent sparse attention to maximally leverage the token overlaps between consecutive Transformer layers to reduce the computation cost for token selection while achieving high predictive performance. <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#alg1" title="In 3.1 Position Persistent Sparse Attention (PPSA) ‚Ä£ 3 Methodology ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_tag">Algorithm</span>&nbsp;<span class="ltx_text ltx_ref_tag">1</span></a> shows the TidalDecode algorithm for interleaving full attention and PPSA layers.
After the initial full attention layers, TidalDecode uses a token selection layer that computes full attention and selects tokens with the highest attention scores.
To select tokens, TidalDecode stores the inner product <math alttext="\langle Q,K\rangle" class="ltx_Math" display="inline" id="S3.SS1.p3.1.m1.2"><semantics id="S3.SS1.p3.1.m1.2a"><mrow id="S3.SS1.p3.1.m1.2.3.2" xref="S3.SS1.p3.1.m1.2.3.1.cmml"><mo id="S3.SS1.p3.1.m1.2.3.2.1" stretchy="false" xref="S3.SS1.p3.1.m1.2.3.1.cmml">‚ü®</mo><mi id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">Q</mi><mo id="S3.SS1.p3.1.m1.2.3.2.2" xref="S3.SS1.p3.1.m1.2.3.1.cmml">,</mo><mi id="S3.SS1.p3.1.m1.2.2" xref="S3.SS1.p3.1.m1.2.2.cmml">K</mi><mo id="S3.SS1.p3.1.m1.2.3.2.3" stretchy="false" xref="S3.SS1.p3.1.m1.2.3.1.cmml">‚ü©</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.2b"><list id="S3.SS1.p3.1.m1.2.3.1.cmml" xref="S3.SS1.p3.1.m1.2.3.2"><ci id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">ùëÑ</ci><ci id="S3.SS1.p3.1.m1.2.2.cmml" xref="S3.SS1.p3.1.m1.2.2">ùêæ</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.2c">\langle Q,K\rangle</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.1.m1.2d">‚ü® italic_Q , italic_K ‚ü©</annotation></semantics></math> on the fly together with full attention calculation. TidalDecode then selects the top <math alttext="k" class="ltx_Math" display="inline" id="S3.SS1.p3.2.m2.1"><semantics id="S3.SS1.p3.2.m2.1a"><mi id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><ci id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.2.m2.1d">italic_k</annotation></semantics></math> tokens with the highest inner product values to form a token set <math alttext="\mathcal{T}" class="ltx_Math" display="inline" id="S3.SS1.p3.3.m3.1"><semantics id="S3.SS1.p3.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml">ùíØ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><ci id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">ùíØ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">\mathcal{T}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.3.m3.1d">caligraphic_T</annotation></semantics></math>. Note that using the inner product to select top-<math alttext="k" class="ltx_Math" display="inline" id="S3.SS1.p3.4.m4.1"><semantics id="S3.SS1.p3.4.m4.1a"><mi id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><ci id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.4.m4.1d">italic_k</annotation></semantics></math> is equivalent to the post-softmax attention score as the softmax operator is ordering invariant.
All PPSA layers after a token selection layer computes sparse attention by only loading the keys and values for tokens in <math alttext="\mathcal{T}" class="ltx_Math" display="inline" id="S3.SS1.p3.5.m5.1"><semantics id="S3.SS1.p3.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p3.5.m5.1.1" xref="S3.SS1.p3.5.m5.1.1.cmml">ùíØ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m5.1b"><ci id="S3.SS1.p3.5.m5.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1">ùíØ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m5.1c">\mathcal{T}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.5.m5.1d">caligraphic_T</annotation></semantics></math>, thus limiting the number of tokens participating in attention computations and reducing memory access.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.3">A straightforward approach to designing TidalDecode is to select the tokens <math alttext="\mathcal{T}" class="ltx_Math" display="inline" id="S3.SS1.p4.1.m1.1"><semantics id="S3.SS1.p4.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml">ùíØ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><ci id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">ùíØ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">\mathcal{T}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.1.m1.1d">caligraphic_T</annotation></semantics></math> once after full attention and perform PPSA using the same set of tokens for all subsequent layers.
However, our preliminary experimentation shows that using a single token set for all Transformer layers reduces the LLM‚Äôs predictive performance by a large margin since distant Transformer layers are less correlated compared to consecutive layers, as shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S3.F3.sf1" title="In Figure 3 ‚Ä£ 3.1 Position Persistent Sparse Attention (PPSA) ‚Ä£ 3 Methodology ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">3(a)</span></a>.
To address this issue, TidalDecode performs <span class="ltx_text ltx_font_italic" id="S3.SS1.p4.3.1">token re-selection</span> in a middle layer, where TidalDecode recalibrates the selected tokens with the highest attention scores by applying full attention and re-selecting top-<math alttext="k" class="ltx_Math" display="inline" id="S3.SS1.p4.2.m2.1"><semantics id="S3.SS1.p4.2.m2.1a"><mi id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><ci id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.2.m2.1d">italic_k</annotation></semantics></math> token to update <math alttext="\mathcal{T}" class="ltx_Math" display="inline" id="S3.SS1.p4.3.m3.1"><semantics id="S3.SS1.p4.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p4.3.m3.1.1" xref="S3.SS1.p4.3.m3.1.1.cmml">ùíØ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.1b"><ci id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1">ùíØ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.1c">\mathcal{T}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.3.m3.1d">caligraphic_T</annotation></semantics></math>, ensuring that token selection remains optimal for the remaining layers.
This re-selection mechanism significantly boosts the model performance and promotes accurate and efficient PPSA throughout the model.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1">Extensive evaluation on both small and large models on a wide range of datasets shows that using a single middle layer for token re-selection is sufficient to preserve the LLM‚Äôs generative performance, while introducing small runtime overhead.
However, deciding which layer to perform token re-selection is critical to model performance.
As shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S3.F3" title="In 3.1 Position Persistent Sparse Attention (PPSA) ‚Ä£ 3 Methodology ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">3</span></a>, choosing different layers for token re-selection results in different recall rates, where layer 11 and 13 achieve optimal performance.
Introducing a one-time token re-selection at an optimal layer ensures the selected tokens are re-calibrated, effectively mitigating the drift in token importance and elevates accuracy from 15% (without re-selection) to almost 40%.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>KV Cache Correction</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.2">For tokens decoded by sparse attention methods, their key/value representations can deviate from the original representation of full attention decoded ones, which we refer to as polluted tokens. The problem can be further exacerbated as their KV pairs are added to the KV cache, resulting in the error accumulation or distribution shift of the KV cache. This can lead to model performance drop in scenarios where the generation length is fairly long. To this end, TidalDecode uses a cache-correction mechanism as shown in&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S3.F4" title="In 3.2 KV Cache Correction ‚Ä£ 3 Methodology ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">4</span></a> to periodically correct the polluted tokens in the KV cache. For every <math alttext="T" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">ùëá</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_T</annotation></semantics></math> decoding step performed by TidalDecode, there will be a cache correction step through a prefill over all polluted tokens to update their KV representations in the cache. The choice of <math alttext="T" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">ùëá</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">italic_T</annotation></semantics></math> can be at the level of thousands of decoding steps but also depend on different models and tasks. Notice that the cache correction step can be performed concurrently with the sparse decoding step. Nevertheless, we haven‚Äôt used cache correction in our evaluations to make it a fair comparison against existing methods.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="191" id="S3.F4.g1" src="https://arxiv.org/html/2410.05076v1/x5.png" width="456">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Cache Correction</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experiment Setting</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">In this section, we conduct extensive experiments to assess both the performance and efficiency of TidalDecode.
Our evaluations are performed on widely used open-source models, including Llama-2-7B <cite class="ltx_cite ltx_citemacro_cite">Touvron et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib21" title="">2023</a>)</cite> and Llama-3-8/70B.
Both models are pretrained decoder-only transformers, exhibiting similar yet distinct architectural features.
For instance, Llama 3-8B incorporates group query attention (GQA), a feature not present in Llama 2-7B. In&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S4.SS2" title="4.2 Performance Evaluation ‚Ä£ 4 Experiments ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">4.2</span></a>, we evaluate TidalDecode‚Äôs performance on various tasks, including needle-in-the-haystack, language modeling on PG-19, and LongBench. In&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S4.SS3" title="4.3 Efficiency Evaluation ‚Ä£ 4 Experiments ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">4.3</span></a>, we write customized attention kernels and compare TidalDecode‚Äôs kernel efficiency against existing state-of-the-art sparse attention methods. Finally, in&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S4.SS4" title="4.4 Sensitivity analysis on token re-selection layer ‚Ä£ 4 Experiments ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">4.4</span></a>, we conclude our evaluations with a detailed sensitivity analysis on the choice of different token selection layers. We use TD+LX to denote TidalDecode with layer X selected as the token re-selection layer throughout this section.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Performance Evaluation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">To evaluate the effectiveness of TidalDecode, we conduct two key downstream NLP experiments: the needle-in-the-haystack test and perplexity evaluation on the PG-19 dataset&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Rae et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib15" title="">2019</a>)</cite>. These tasks provide robust benchmarks for measuring both sparse attention models‚Äô ability to retrieve critical information in challenging scenarios and their performance on long-context language modeling tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Needle-in-the-Haystack</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Results of 10k-context-length Needle-in-the-Haystack test on LongChat-7b-v1.5-32k. TidalDecode achieves the same or better results than Quest and significantly better results than cache eviction algorithms such as H2O, TOVA, and StreamingLLM. TidalDecode achieves full accuracy with only a 512 token budget.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T1.1.1.1.1">Method / Budget</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.2">K=32</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.3">K=64</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.4">K=128</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.5">K=256</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.6">K=512</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.2.1.1">H2O</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.1.2.1.2">0%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.1.2.1.3">1%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.1.2.1.4">1%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.1.2.1.5">1%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.1.2.1.6">3%</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.3.2.1">TOVA</th>
<td class="ltx_td ltx_align_right" id="S4.T1.1.3.2.2">0%</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.3.2.3">1%</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.3.2.4">1%</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.3.2.5">3%</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.3.2.6">8%</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.4.3.1">StreamingLLM</th>
<td class="ltx_td ltx_align_right" id="S4.T1.1.4.3.2">1%</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.4.3.3">1%</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.4.3.4">1%</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.4.3.5">3%</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.4.3.6">5%</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T1.1.5.4.1">Quest</th>
<td class="ltx_td ltx_align_right" id="S4.T1.1.5.4.2">65%</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.5.4.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.5.4.3.1">99%</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.5.4.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.5.4.4.1">99%</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.5.4.5"><span class="ltx_text ltx_font_bold" id="S4.T1.1.5.4.5.1">99%</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.5.4.6"><span class="ltx_text ltx_font_bold" id="S4.T1.1.5.4.6.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S4.T1.1.6.5.1">TD+L7(Ours)</th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.1.6.5.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.6.5.2.1">73%</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.1.6.5.3">92%</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.1.6.5.4">98%</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.1.6.5.5"><span class="ltx_text ltx_font_bold" id="S4.T1.1.6.5.5.1">99%</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.1.6.5.6"><span class="ltx_text ltx_font_bold" id="S4.T1.1.6.5.6.1">100%</span></td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comprehensive results of 10K-, 32K-, and 100K-context-length Needle-in-the-Haystack test on Llama-3-8B-Instruct-Gradient-1048k, Llama-3.1-8B-Instruct, and Llama-3-70B-Instruct-Gradient-1048k with PG-19-mini dataset. Across all models, TidalDecode consistently outperforms Quest, showing that TidalDecode with only two token selection layers can effectively retain critical information. TidalDecode achieves full accuracy with 64, 64, and 128 tokens in 10K-, 32K-, and 100K-context-length tests, which is only 0.6%, 0.2%, and 0.1% of total input lengths, respectively.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.1">Model (context length)</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.2">Method / Budget</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.3">K=32</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.4">K=64</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.5">K=128</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.6">K=256</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.7">K=512</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.1" rowspan="3"><span class="ltx_text" id="S4.T2.1.2.1.1.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.1.2.1.1.1.1">
<span class="ltx_tr" id="S4.T2.1.2.1.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.2.1.1.1.1.1.1">LLaMA-3-8B</span></span>
<span class="ltx_tr" id="S4.T2.1.2.1.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.2.1.1.1.1.2.1">(10K)</span></span>
</span></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.2.1.2">Quest</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.2.1.3">74%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.2.1.4">84%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.2.1.5">99%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.2.1.6">98%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.2.1.7"><span class="ltx_text ltx_font_bold" id="S4.T2.1.2.1.7.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.2">
<td class="ltx_td ltx_align_left" id="S4.T2.1.3.2.1">TD+L13(Ours)</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.3.2.2">88%</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.3.2.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.3.2.3.1">98%</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.3.2.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.3.2.4.1">100%</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.3.2.5"><span class="ltx_text ltx_font_bold" id="S4.T2.1.3.2.5.1">100%</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.3.2.6"><span class="ltx_text ltx_font_bold" id="S4.T2.1.3.2.6.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.3">
<td class="ltx_td ltx_align_left" id="S4.T2.1.4.3.1">TD+L15(Ours)</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.4.3.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.4.3.2.1">92%</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.4.3.3">88%</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.4.3.4">94%</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.4.3.5">94%</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.4.3.6"><span class="ltx_text ltx_font_bold" id="S4.T2.1.4.3.6.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.5.4.1" rowspan="3"><span class="ltx_text" id="S4.T2.1.5.4.1.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.1.5.4.1.1.1">
<span class="ltx_tr" id="S4.T2.1.5.4.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.5.4.1.1.1.1.1">LLaMA-3-8B</span></span>
<span class="ltx_tr" id="S4.T2.1.5.4.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.5.4.1.1.1.2.1">(100K)</span></span>
</span></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.5.4.2">Quest</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.5.4.3">38%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.5.4.4">50%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.5.4.5">65%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.5.4.6">87%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.5.4.7">98%</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.6.5">
<td class="ltx_td ltx_align_left" id="S4.T2.1.6.5.1">TD+L13(Ours)</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.6.5.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.6.5.2.1">86%</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.6.5.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.6.5.3.1">92%</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.6.5.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.6.5.4.1">100%</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.6.5.5"><span class="ltx_text ltx_font_bold" id="S4.T2.1.6.5.5.1">100%</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.6.5.6"><span class="ltx_text ltx_font_bold" id="S4.T2.1.6.5.6.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.7.6">
<td class="ltx_td ltx_align_left" id="S4.T2.1.7.6.1">TD+L15(Ours)</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.7.6.2">84%</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.7.6.3">90%</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.7.6.4">92%</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.7.6.5">98%</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.7.6.6"><span class="ltx_text ltx_font_bold" id="S4.T2.1.7.6.6.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.8.7">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.8.7.1" rowspan="3"><span class="ltx_text" id="S4.T2.1.8.7.1.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.1.8.7.1.1.1">
<span class="ltx_tr" id="S4.T2.1.8.7.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.8.7.1.1.1.1.1">LLaMA-3.1-8B</span></span>
<span class="ltx_tr" id="S4.T2.1.8.7.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.8.7.1.1.1.2.1">(10K)</span></span>
</span></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.8.7.2">Quest</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.8.7.3">74%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.8.7.4">86%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.8.7.5">94%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.8.7.6"><span class="ltx_text ltx_font_bold" id="S4.T2.1.8.7.6.1">100%</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.8.7.7">98%</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.9.8">
<td class="ltx_td ltx_align_left" id="S4.T2.1.9.8.1">TD+L13(Ours)</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.9.8.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.9.8.2.1">100%</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.9.8.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.9.8.3.1">100%</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.9.8.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.9.8.4.1">100%</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.9.8.5"><span class="ltx_text ltx_font_bold" id="S4.T2.1.9.8.5.1">100%</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.9.8.6"><span class="ltx_text ltx_font_bold" id="S4.T2.1.9.8.6.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.10.9">
<td class="ltx_td ltx_align_left" id="S4.T2.1.10.9.1">TD+L14(Ours)</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.10.9.2">98%</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.10.9.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.10.9.3.1">100%</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.10.9.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.10.9.4.1">100%</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.10.9.5"><span class="ltx_text ltx_font_bold" id="S4.T2.1.10.9.5.1">100%</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.10.9.6"><span class="ltx_text ltx_font_bold" id="S4.T2.1.10.9.6.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.11.10">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.11.10.1" rowspan="3"><span class="ltx_text" id="S4.T2.1.11.10.1.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.1.11.10.1.1.1">
<span class="ltx_tr" id="S4.T2.1.11.10.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.11.10.1.1.1.1.1">LLaMA-3.1-8B</span></span>
<span class="ltx_tr" id="S4.T2.1.11.10.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.11.10.1.1.1.2.1">(32K)</span></span>
</span></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.11.10.2">Quest</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.11.10.3">78%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.11.10.4">88%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.11.10.5">92%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.11.10.6"><span class="ltx_text ltx_font_bold" id="S4.T2.1.11.10.6.1">100%</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.11.10.7"><span class="ltx_text ltx_font_bold" id="S4.T2.1.11.10.7.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.12.11">
<td class="ltx_td ltx_align_left" id="S4.T2.1.12.11.1">TD+L13(Ours)</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.12.11.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.12.11.2.1">98%</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.12.11.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.12.11.3.1">100%</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.12.11.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.12.11.4.1">100%</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.12.11.5"><span class="ltx_text ltx_font_bold" id="S4.T2.1.12.11.5.1">100%</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.12.11.6"><span class="ltx_text ltx_font_bold" id="S4.T2.1.12.11.6.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.13.12">
<td class="ltx_td ltx_align_left" id="S4.T2.1.13.12.1">TD+L14(Ours)</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.13.12.2">80%</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.13.12.3">98%</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.13.12.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.13.12.4.1">100%</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.13.12.5"><span class="ltx_text ltx_font_bold" id="S4.T2.1.13.12.5.1">100%</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.13.12.6"><span class="ltx_text ltx_font_bold" id="S4.T2.1.13.12.6.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.14.13">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.14.13.1" rowspan="3"><span class="ltx_text" id="S4.T2.1.14.13.1.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.1.14.13.1.1.1">
<span class="ltx_tr" id="S4.T2.1.14.13.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.14.13.1.1.1.1.1">LLaMA-3-70B</span></span>
<span class="ltx_tr" id="S4.T2.1.14.13.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.14.13.1.1.1.2.1">(10K)</span></span>
</span></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.14.13.2">Quest</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.14.13.3">68%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.14.13.4">72%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.14.13.5">90%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.14.13.6">98%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.14.13.7"><span class="ltx_text ltx_font_bold" id="S4.T2.1.14.13.7.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.15.14">
<td class="ltx_td ltx_align_left" id="S4.T2.1.15.14.1">TD+L14(Ours)</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.15.14.2">87%</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.15.14.3">93%</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.15.14.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.15.14.4.1">100%</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.15.14.5"><span class="ltx_text ltx_font_bold" id="S4.T2.1.15.14.5.1">100%</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.15.14.6"><span class="ltx_text ltx_font_bold" id="S4.T2.1.15.14.6.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.16.15">
<td class="ltx_td ltx_align_left" id="S4.T2.1.16.15.1">TD+L31(Ours)</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.16.15.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.16.15.2.1">90%</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.16.15.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.16.15.3.1">97%</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.16.15.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.16.15.4.1">100%</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.16.15.5"><span class="ltx_text ltx_font_bold" id="S4.T2.1.16.15.5.1">100%</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.16.15.6"><span class="ltx_text ltx_font_bold" id="S4.T2.1.16.15.6.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.17.16">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.1.17.16.1" rowspan="3"><span class="ltx_text" id="S4.T2.1.17.16.1.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.1.17.16.1.1.1">
<span class="ltx_tr" id="S4.T2.1.17.16.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.17.16.1.1.1.1.1">LLaMA-3-70B</span></span>
<span class="ltx_tr" id="S4.T2.1.17.16.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.17.16.1.1.1.2.1">(32K)</span></span>
</span></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.17.16.2">Quest</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.17.16.3">50%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.17.16.4">80%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.17.16.5">88%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.17.16.6">92%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.17.16.7">78%</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.18.17">
<td class="ltx_td ltx_align_left" id="S4.T2.1.18.17.1">TD+L14(Ours)</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.18.17.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.18.17.2.1">82%</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.18.17.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.18.17.3.1">98%</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.18.17.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.18.17.4.1">98%</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.18.17.5"><span class="ltx_text ltx_font_bold" id="S4.T2.1.18.17.5.1">100%</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.18.17.6"><span class="ltx_text ltx_font_bold" id="S4.T2.1.18.17.6.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.19.18">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.1.19.18.1">TD+L31(Ours)</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T2.1.19.18.2">80%</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T2.1.19.18.3">82%</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T2.1.19.18.4">92%</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T2.1.19.18.5">98%</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T2.1.19.18.6"><span class="ltx_text ltx_font_bold" id="S4.T2.1.19.18.6.1">100%</span></td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">The Needle-in-the-Haystack test assesses LLMs‚Äô ability to handle long-dependency tasks, which is particularly critical for sparse attention algorithms.
Eviction-based methods <cite class="ltx_cite ltx_citemacro_cite">Xiao et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib25" title="">2023</a>); Zhang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib27" title="">2023</a>)</cite> may discard essential tokens, while selection-based approaches often fail to consistently identify the ground-truth tokens with the highest attention scores in long contexts.
Since Quest is the current state-of-the-art approach on this task, we first run TidalDecode on the same test as Quest on the LongChat-7b-v1.5-32k model and obtained&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S4.T1" title="In 4.2.1 Needle-in-the-Haystack ‚Ä£ 4.2 Performance Evaluation ‚Ä£ 4 Experiments ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">1</span></a> with competitive performance.
To demonstrate the effectiveness of TidalDecode on long-dependency tasks, we further evaluate TidalDecode on tasks with 10K-, 32K-, and 100K-context-window lengths with the LLaMA-3-70B, LLaMA-3-8B, LLaMA-3.1-8B model using the PG-19-mini dataset, shown in&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S4.T2" title="In 4.2.1 Needle-in-the-Haystack ‚Ä£ 4.2 Performance Evaluation ‚Ä£ 4 Experiments ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a>.
To ensure fairness, both TidalDecode and Quest use dense attention in the first two layers. In each test, we inserted a random password within the text and tested whether the specific method could retrieve the password correctly.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS1.p2">
<p class="ltx_p" id="S4.SS2.SSS1.p2.1">From&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S4.T2" title="In 4.2.1 Needle-in-the-Haystack ‚Ä£ 4.2 Performance Evaluation ‚Ä£ 4 Experiments ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a>, TidalDecode consistently outperforms Quest and achieves full accuracy with an extremely low sparsity (about 0.5% across all context lengths and models). These results demonstrate TidalDecode can achieve state-of-the-art performance with only two token selection layers.
While Quest relies on page-level importance estimation for token selection, TidalDecode‚Äôs exact selection with token reuse approach proves more effective for this task. Also, note that TidalDecode can reduce the token budget by up to <math alttext="8\times" class="ltx_math_unparsed" display="inline" id="S4.SS2.SSS1.p2.1.m1.1"><semantics id="S4.SS2.SSS1.p2.1.m1.1a"><mrow id="S4.SS2.SSS1.p2.1.m1.1b"><mn id="S4.SS2.SSS1.p2.1.m1.1.1">8</mn><mo id="S4.SS2.SSS1.p2.1.m1.1.2" lspace="0.222em">√ó</mo></mrow><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p2.1.m1.1c">8\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p2.1.m1.1d">8 √ó</annotation></semantics></math> when achieving a 100% accuracy compared with Quest. This further demonstrates that TidalDecode‚Äôs exact token selection layer can obtain more relevant tokens than Quest.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Language Modeling</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S4.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F5.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="502" id="S4.F5.sf1.g1" src="https://arxiv.org/html/2410.05076v1/x6.png" width="761">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Token Budget = 2048</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F5.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="502" id="S4.F5.sf2.g1" src="https://arxiv.org/html/2410.05076v1/x7.png" width="761">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Token Budget = 4096</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Perplexity evaluation on the PG-19 dataset from 0 to 32K tokens. The results compare TidalDecode with different token re-selection layers (L9, L13, L15) to Quest across token budgets (2048 <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S4.F5.sf1" title="Figure 5(a) ‚Ä£ Figure 5 ‚Ä£ 4.2.2 Language Modeling ‚Ä£ 4.2 Performance Evaluation ‚Ä£ 4 Experiments ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_tag">5(a)</span></a>, 4096 <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S4.F5.sf2" title="Figure 5(b) ‚Ä£ Figure 5 ‚Ä£ 4.2.2 Language Modeling ‚Ä£ 4.2 Performance Evaluation ‚Ä£ 4 Experiments ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_tag">5(b)</span></a>). Lower perplexity indicates better model performance. Full refers to dense attention as baseline.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">Perplexity measures the negative likelihood of how well a model predicts the next word in a sequence, with lower values indicating better performance. We evaluate TidalDecode on Llama-3-8B-Instruct-Gradient-1048k with the PG-19 dataset, which includes up to 100 books, providing a comprehensive long-context benchmark.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS2.p2">
<p class="ltx_p" id="S4.SS2.SSS2.p2.1">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S4.F5" title="Figure 5 ‚Ä£ 4.2.2 Language Modeling ‚Ä£ 4.2 Performance Evaluation ‚Ä£ 4 Experiments ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_tag">5</span></a>, TidalDecode+L9/13/15 consistently achieves lower perplexity than Quest across all token budget options (2048, 4096). This indicates that TidalDecode‚Äôs position persistent sparse attention mechanism can effectively retain critical information without significantly sacrificing model accuracy, even as the sequence length grows, demonstrating its robustness for long-context inputs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>LongBench Experiment</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Performance comparison on eight LongBench datasets evaluating single/multi-document QA, summarization, and retrieval tasks using Llama-3-8B-Instruct-Gradient-1048k. TidalDecode outperforms Quest at a 4096 token budget and achieves an average score higher than full-weight attention. The maximum F1-score for each task is in bold.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T3.1.1.1.1" style="padding-left:5.0pt;padding-right:5.0pt;">Method (K)/Task</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.2" style="padding-left:5.0pt;padding-right:5.0pt;">MFQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.3" style="padding-left:5.0pt;padding-right:5.0pt;">NrtQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.4" style="padding-left:5.0pt;padding-right:5.0pt;">Qasp</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.5" style="padding-left:5.0pt;padding-right:5.0pt;">2Wiki</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.6" style="padding-left:5.0pt;padding-right:5.0pt;">HotQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.7" style="padding-left:5.0pt;padding-right:5.0pt;">QMSm</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.8" style="padding-left:5.0pt;padding-right:5.0pt;">TrQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.9" style="padding-left:5.0pt;padding-right:5.0pt;">PRe</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.10" style="padding-left:5.0pt;padding-right:5.0pt;">Avg</th>
</tr>
<tr class="ltx_tr" id="S4.T3.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T3.1.2.2.1" style="padding-left:5.0pt;padding-right:5.0pt;">Full</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.2.2.2" style="padding-left:5.0pt;padding-right:5.0pt;">30.76</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.2.2.3" style="padding-left:5.0pt;padding-right:5.0pt;">5.52</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.2.2.4" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.2.2.4.1">14.56</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.2.2.5" style="padding-left:5.0pt;padding-right:5.0pt;">13.32</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.2.2.6" style="padding-left:5.0pt;padding-right:5.0pt;">11.50</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.2.2.7" style="padding-left:5.0pt;padding-right:5.0pt;">19.43</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.2.2.8" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.2.2.8.1">86.56</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.2.2.9" style="padding-left:5.0pt;padding-right:5.0pt;">77.00</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.2.2.10" style="padding-left:5.0pt;padding-right:5.0pt;">32.33</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.1.3.1.1" style="padding-left:5.0pt;padding-right:5.0pt;">Quest  (1024)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.2" style="padding-left:5.0pt;padding-right:5.0pt;">26.21</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.3" style="padding-left:5.0pt;padding-right:5.0pt;">4.08</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.4" style="padding-left:5.0pt;padding-right:5.0pt;">12.19</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.5" style="padding-left:5.0pt;padding-right:5.0pt;">12.61</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.6" style="padding-left:5.0pt;padding-right:5.0pt;">10.75</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.7" style="padding-left:5.0pt;padding-right:5.0pt;">19.56</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.8" style="padding-left:5.0pt;padding-right:5.0pt;">83.47</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.9" style="padding-left:5.0pt;padding-right:5.0pt;">63.84</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.10" style="padding-left:5.0pt;padding-right:5.0pt;">29.09</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.4.2.1" style="padding-left:5.0pt;padding-right:5.0pt;">TD+L13  (1024)</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.2" style="padding-left:5.0pt;padding-right:5.0pt;">28.57</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.4.2.3.1">7.63</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.4" style="padding-left:5.0pt;padding-right:5.0pt;">11.11</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.5" style="padding-left:5.0pt;padding-right:5.0pt;">13.56</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.6" style="padding-left:5.0pt;padding-right:5.0pt;">9.82</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.7" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.4.2.7.1">20.37</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.8" style="padding-left:5.0pt;padding-right:5.0pt;">79.78</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.9" style="padding-left:5.0pt;padding-right:5.0pt;">75.17</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.10" style="padding-left:5.0pt;padding-right:5.0pt;">30.75</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.1.5.3.1" style="padding-left:5.0pt;padding-right:5.0pt;">Quest  (4096)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.5.3.2" style="padding-left:5.0pt;padding-right:5.0pt;">28.92</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.5.3.3" style="padding-left:5.0pt;padding-right:5.0pt;">3.74</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.5.3.4" style="padding-left:5.0pt;padding-right:5.0pt;">13.63</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.5.3.5" style="padding-left:5.0pt;padding-right:5.0pt;">12.83</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.5.3.6" style="padding-left:5.0pt;padding-right:5.0pt;">12.15</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.5.3.7" style="padding-left:5.0pt;padding-right:5.0pt;">19.36</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.5.3.8" style="padding-left:5.0pt;padding-right:5.0pt;">85.91</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.5.3.9" style="padding-left:5.0pt;padding-right:5.0pt;">72.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.5.3.10" style="padding-left:5.0pt;padding-right:5.0pt;">31.13</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T3.1.6.4.1" style="padding-left:5.0pt;padding-right:5.0pt;">TD+L13  (4096)</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.6.4.2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.6.4.2.1">30.94</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.6.4.3" style="padding-left:5.0pt;padding-right:5.0pt;">6.19</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.6.4.4" style="padding-left:5.0pt;padding-right:5.0pt;">13.85</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.6.4.5" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.6.4.5.1">14.40</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.6.4.6" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.6.4.6.1">13.71</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.6.4.7" style="padding-left:5.0pt;padding-right:5.0pt;">19.48</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.6.4.8" style="padding-left:5.0pt;padding-right:5.0pt;">86.30</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.6.4.9" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.6.4.9.1">78.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.6.4.10" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.1.6.4.10.1">32.86</span></td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.1">We also evaluate TidalDecode on LongBench, a benchmark designed to test LLMs on long-context tasks across diverse NLP domains&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Bai et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib4" title="">2023</a>)</cite>. We focus on eight tasks: MultiFieldQA (MFQA), NarrativeQA (NrtQA), Qasper (Qasp), 2WikiMQA (2Wiki), HotpotQA (HotQA), QMSum (QMSm), TriviaQA (TrQA), and Passage Retrieval (PRe), which collectively composite a comprehensive evaluation benchmark in single/multi-document QA, summarization, and retrieval.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS3.p2">
<p class="ltx_p" id="S4.SS2.SSS3.p2.2">We evaluate all methods with LLaMA-3-8B-Instruct-Gradient-1048k. TidalDecode is compared against full-weight attention and Quest at token budgets of 1024 and 4096. As shown in <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:longbench</span>, TidalDecode consistently outperforms Quest on all tasks at <math alttext="K=4096" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p2.1.m1.1"><semantics id="S4.SS2.SSS3.p2.1.m1.1a"><mrow id="S4.SS2.SSS3.p2.1.m1.1.1" xref="S4.SS2.SSS3.p2.1.m1.1.1.cmml"><mi id="S4.SS2.SSS3.p2.1.m1.1.1.2" xref="S4.SS2.SSS3.p2.1.m1.1.1.2.cmml">K</mi><mo id="S4.SS2.SSS3.p2.1.m1.1.1.1" xref="S4.SS2.SSS3.p2.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS3.p2.1.m1.1.1.3" xref="S4.SS2.SSS3.p2.1.m1.1.1.3.cmml">4096</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p2.1.m1.1b"><apply id="S4.SS2.SSS3.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS3.p2.1.m1.1.1"><eq id="S4.SS2.SSS3.p2.1.m1.1.1.1.cmml" xref="S4.SS2.SSS3.p2.1.m1.1.1.1"></eq><ci id="S4.SS2.SSS3.p2.1.m1.1.1.2.cmml" xref="S4.SS2.SSS3.p2.1.m1.1.1.2">ùêæ</ci><cn id="S4.SS2.SSS3.p2.1.m1.1.1.3.cmml" type="integer" xref="S4.SS2.SSS3.p2.1.m1.1.1.3">4096</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p2.1.m1.1c">K=4096</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p2.1.m1.1d">italic_K = 4096</annotation></semantics></math> and on five tasks at <math alttext="K=1024" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p2.2.m2.1"><semantics id="S4.SS2.SSS3.p2.2.m2.1a"><mrow id="S4.SS2.SSS3.p2.2.m2.1.1" xref="S4.SS2.SSS3.p2.2.m2.1.1.cmml"><mi id="S4.SS2.SSS3.p2.2.m2.1.1.2" xref="S4.SS2.SSS3.p2.2.m2.1.1.2.cmml">K</mi><mo id="S4.SS2.SSS3.p2.2.m2.1.1.1" xref="S4.SS2.SSS3.p2.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS3.p2.2.m2.1.1.3" xref="S4.SS2.SSS3.p2.2.m2.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p2.2.m2.1b"><apply id="S4.SS2.SSS3.p2.2.m2.1.1.cmml" xref="S4.SS2.SSS3.p2.2.m2.1.1"><eq id="S4.SS2.SSS3.p2.2.m2.1.1.1.cmml" xref="S4.SS2.SSS3.p2.2.m2.1.1.1"></eq><ci id="S4.SS2.SSS3.p2.2.m2.1.1.2.cmml" xref="S4.SS2.SSS3.p2.2.m2.1.1.2">ùêæ</ci><cn id="S4.SS2.SSS3.p2.2.m2.1.1.3.cmml" type="integer" xref="S4.SS2.SSS3.p2.2.m2.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p2.2.m2.1c">K=1024</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p2.2.m2.1d">italic_K = 1024</annotation></semantics></math>. Surprisingly, TidalDecode, in most cases, matches or exceeds full attention baseline with notable sparsity: 14% on NrtQA, 50% on MFQA, 80% on Qasp, 50% on 2WikiMQA, 32% on HotQA, 29% on QMSm, 35% on TrQA, and 33% on PRe. We hypothesize this is because TidalDecode‚Äôs token selection process can filter out irrelevant information, thus leading to higher performance.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS3.p3">
<p class="ltx_p" id="S4.SS2.SSS3.p3.1">These results demonstrate TidalDecode‚Äôs generic ability to select tokens with the highest attention scores, achieving competitive or superior performance while significantly reducing token usage, making it ideal for long-context scenarios.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Efficiency Evaluation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="239" id="S4.F6.g1" src="https://arxiv.org/html/2410.05076v1/x8.png" width="746">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>End-to-end latency results on LLaMA-2-7B model for Full attention baseline(Full), Quest, and TidalDecode(TD) when context length is 10K, 32K, and 100K, respectively.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">To show the efficiency of TidalDecode, we write customized kernels for our approach and measure the end-to-end decoding latency. We conduct evaluation under the configuration of Llama-2-7B on one Nvidia A100 (80 GB HBM, SXM4) with CUDA 12.2. We compare TidalDecode with state-of-the-art full attention serving library FlashInfer&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ye et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib26" title="">2024</a>)</cite> and also the Quest implementation. As shown in&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S4.F6" title="In 4.3 Efficiency Evaluation ‚Ä£ 4 Experiments ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">6</span></a>, we can observe that TidalDecode can consistently outperform full attention baseline and Quest by a large margin under all token budgets and context lengths. TidalDecode achieves this through token pattern reuse to minimize the token selection overhead. Notice that the latest LLaMA-3 model shares the same architecture as LLaMA-2, except it uses Group-Query-Attention instead of Multi-Head-Attention. However, this does not affect the relative efficiency comparison against Quest and full attention.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S4.F7">
<figcaption class="ltx_caption ltx_centering"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="57" id="S4.F7.1.g1" src="https://arxiv.org/html/2410.05076v1/x9.png" width="571"></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="507" id="S4.F6.sf1.g1" src="https://arxiv.org/html/2410.05076v1/x10.png" width="761">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>32 Layers</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="507" id="S4.F6.sf2.g1" src="https://arxiv.org/html/2410.05076v1/x11.png" width="761">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>64 Layers</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Overall attention latency results for different methods on the LLaMA model with (a) 32 and (b) 64 layers. We use the full attention model as a reference and show TidalDecode and Quest‚Äôs overall attention latency ratio. For each group of the bar plots, the left/middle/right bar denotes the full attention baseline, Quest, and TidalDecode, respectively. </figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.2">In&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S4.F7" title="In 4.3 Efficiency Evaluation ‚Ä£ 4 Experiments ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">7</span></a>, we compare the overall attention latency between different methods on the LLaMA model with 32/64 layers. For the 32-layer LLaMA model, we have 2 full attention layers + 2 token selection layers + 28 sparse attention layers, while Quest has 2 full attention layers + 30 Quest attention layers. For the 64-layer LLaMA model, we have 2 full attention layers + 2 token selection layers + 60 sparse attention layers, while Quest has 2 full attention layers + 62 Quest attention layers. Thus, by completely removing the token estimation overhead in the sparse attention layers, for the 32-layer and 64-layer LLaMA model under all context lengths, TidalDecode can consistently achieve the lowest serving latency while bringing up to <math alttext="5.56\times" class="ltx_math_unparsed" display="inline" id="S4.SS3.p2.1.m1.1"><semantics id="S4.SS3.p2.1.m1.1a"><mrow id="S4.SS3.p2.1.m1.1b"><mn id="S4.SS3.p2.1.m1.1.1">5.56</mn><mo id="S4.SS3.p2.1.m1.1.2" lspace="0.222em">√ó</mo></mrow><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">5.56\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.1.m1.1d">5.56 √ó</annotation></semantics></math> speed-up ratio against the full attention baseline and <math alttext="2.17\times" class="ltx_math_unparsed" display="inline" id="S4.SS3.p2.2.m2.1"><semantics id="S4.SS3.p2.2.m2.1a"><mrow id="S4.SS3.p2.2.m2.1b"><mn id="S4.SS3.p2.2.m2.1.1">2.17</mn><mo id="S4.SS3.p2.2.m2.1.2" lspace="0.222em">√ó</mo></mrow><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">2.17\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.2.m2.1d">2.17 √ó</annotation></semantics></math> speed-up ratio against Quest. When the context length is 10K, Quest has a higher latency due to the token selection overhead, which aligns with the end-to-end results in&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S4.F6" title="In 4.3 Efficiency Evaluation ‚Ä£ 4 Experiments ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">6</span></a>. In contrast, TidalDecode still achieves significant speed-up by utilizing the position persistent sparse attention mechanism.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S4.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="315" id="S4.F8.g1" src="https://arxiv.org/html/2410.05076v1/x12.png" width="609">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>The breakdown latency results for the full attention, token selection attention, sparse attention, and Quest attention kernels over 10K, 32K, and 100K context length. We use full attention latency as a reference and report other kernels‚Äô relative latency ratio. We use a token budget of K=512 for TidalDecode and Quest across all evaluations.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">In&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S4.F8" title="In 4.3 Efficiency Evaluation ‚Ä£ 4 Experiments ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">8</span></a>, we further break down the latency comparison for different attention modules to show why TidalDecode can bring significant speed-up consistently. We compare different attention modules, namely, the full attention layer, the token selection layer, TidalDecode‚Äôs sparse attention layer, and the Quest attention layer over the 10K, 32K, and 100K context length. We can observe that, as TidalDecode‚Äôs sparse attention kernel can directly reuse previous token patterns, it completely removes the important token estimation overhead in the Quest attention kernel, resulting in up to <math alttext="3.36\times" class="ltx_math_unparsed" display="inline" id="S4.SS3.p3.1.m1.1"><semantics id="S4.SS3.p3.1.m1.1a"><mrow id="S4.SS3.p3.1.m1.1b"><mn id="S4.SS3.p3.1.m1.1.1">3.36</mn><mo id="S4.SS3.p3.1.m1.1.2" lspace="0.222em">√ó</mo></mrow><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">3.36\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p3.1.m1.1d">3.36 √ó</annotation></semantics></math> speed-up compared with the Quest implementation. On the other hand, even though TidalDecode‚Äôs token selection layer has a slightly higher latency, we only have two token selection layers even in the 70B LLaMA model that has 64 layers in total.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Sensitivity analysis on token re-selection layer</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S4.F9">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F9.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="696" id="S4.F9.sf1.g1" src="https://arxiv.org/html/2410.05076v1/x13.png" width="761">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>LLaMA-2 Model</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F9.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="696" id="S4.F9.sf2.g1" src="https://arxiv.org/html/2410.05076v1/x14.png" width="761">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>LLaMA-3 Model</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Sensitivity study on the choice of different token re-selection layer. We evaluate LLaMA-2-7B-LongChat, LLaMA-2-7B-Yarn, LLaMA-3-8B, and LLaMA-3.1-8B with TidalDecode with a token budget of 256.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">In this section, we conduct sensitivity studies for different choices of the token re-selection layer. As TidalDecode only has one token re-selection layer in the middle, it is critical to choose the best-performed one. As shown in&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#S4.F9" title="In 4.4 Sensitivity analysis on token re-selection layer ‚Ä£ 4 Experiments ‚Ä£ TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">9</span></a>, we have two interesting findings: (1). Different choices of token re-selection layers can significantly affect the accuracy of the results (2). For models within the same model family, the optimal token re-selection layer is consistent over different tasks. In our setup, the optimal token re-selection layer for the LLaMA-2-7B model is layer 7, while for the LLaMA-3-8B/LLaMA-3.1-8B model is layer 13. A concurrent KV cache compression work also identifies that layer 13 is surprisingly important for their approach as well&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Shi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2410.05076v1#bib.bib18" title="">2024</a>)</cite>. For a more detailed sensitivity results on the choice of different token re-selection layers, please refer to the appendix for more results.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">To conclude, we introduce TidalDecode, an efficient LLM decoding framework with sparse attention. On observing the correlation of the pattern of tokens with the highest attention scores across different consecutive layers, TidalDecode proposes only to select tokens twice: once at the beginning layers and once in the middle layer to serve as a token re-selection layer. We find that using two token selection layers is necessary and sufficient to achieve high-generation quality. Additionally, by reusing the token patterns throughout the sparse attention layer, TidalDecode greatly reduces the token selection overhead, resulting in a significant end-to-end speed-up ratio against existing methods. More interestingly, the optimal choice of the token re-selection layer is consistent across different tasks if the model is in the same model family.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgment</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This research is supported by NSF awards CNS-2147909, CNS-2211882, CNS-2239351, and research awards from Amazon, Cisco, Google, Meta, Oracle, Qualcomm, and Samsung.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">AI (2024a)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Gradient AI.

</span>
<span class="ltx_bibblock">Llama-3-8b-instruct-gradient-1048k.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k" title="">https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k</a>, 2024a.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-26.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">AI (2024b)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Meta AI.

</span>
<span class="ltx_bibblock">Llama 3.1: Advanced open-source language model.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ai.meta.com/blog/meta-llama-3-1/" title="">https://ai.meta.com/blog/meta-llama-3-1/</a>, 2024b.

</span>
<span class="ltx_bibblock">Accessed: 2024-09-26.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anthropic (2024)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">Claude 3.5 sonnet.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.anthropic.com/news/claude-3-5-sonnet" title="">https://www.anthropic.com/news/claude-3-5-sonnet</a>, 2024.

</span>
<span class="ltx_bibblock">[Accessed 20-06-2024].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li.

</span>
<span class="ltx_bibblock">Longbench: A bilingual, multitask benchmark for long context understanding, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Child et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Generating long sequences with sparse transformers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:1904.10509</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choromanski et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et&nbsp;al.

</span>
<span class="ltx_bibblock">Rethinking attention with performers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2009.14794</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dao (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Tri Dao.

</span>
<span class="ltx_bibblock">Flashattention-2: Faster attention with better parallelism and work partitioning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">arXiv preprint arXiv:2307.08691</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dao et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Tri Dao, Daniel&nbsp;Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R√©.

</span>
<span class="ltx_bibblock">Flashattention: Fast and memory-efficient exact attention with io-awareness, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ge et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao.

</span>
<span class="ltx_bibblock">Model tells you what to discard: Adaptive kv cache compression for llms, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu&nbsp;Wang.

</span>
<span class="ltx_bibblock">Efficient attentions for long document summarization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pp.&nbsp; 1419‚Äì1436, Online, June 2021. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2021.naacl-main.112</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.naacl-main.112" title="">https://aclanthology.org/2021.naacl-main.112</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kitaev et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Nikita Kitaev, ≈Åukasz Kaiser, and Anselm Levskaya.

</span>
<span class="ltx_bibblock">Reformer: The efficient transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2001.04451</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwon et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody&nbsp;Hao Yu, Joseph&nbsp;E. Gonzalez, Hao Zhang, and Ion Stoica.

</span>
<span class="ltx_bibblock">Efficient memory management for large language model serving with pagedattention.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2024)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Introducing gpt-4o: our fastest and most affordable flagship model.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://platform.openai.com/docs/models" title="">https://platform.openai.com/docs/models</a>, 2024.

</span>
<span class="ltx_bibblock">[Accessed 28-05-2024].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.

</span>
<span class="ltx_bibblock">Yarn: Efficient context window extension of large language models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rae et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Jack&nbsp;W Rae, Anna Potapenko, Siddhant&nbsp;M Jayakumar, Chloe Hillier, and Timothy&nbsp;P Lillicrap.

</span>
<span class="ltx_bibblock">Compressive transformers for long-range sequence modelling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">arXiv preprint</em>, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1911.05507" title="">https://arxiv.org/abs/1911.05507</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ram et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham.

</span>
<span class="ltx_bibblock">In-context retrieval-augmented language models, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2302.00083" title="">https://arxiv.org/abs/2302.00083</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ribar et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, and Douglas Orr.

</span>
<span class="ltx_bibblock">Sparq attention: Bandwidth-efficient llm inference, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Zhenmei Shi, Yifei Ming, Xuan-Phi Nguyen, Yingyu Liang, and Shafiq Joty.

</span>
<span class="ltx_bibblock">Discovering the gems in early layers: Accelerating long-context llms with 1000x input token reduction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:2409.17422</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Jianlin Su, Yu&nbsp;Lu, Shengfeng Pan, Ahmed Murtadha, Bo&nbsp;Wen, and Yunfeng Liu.

</span>
<span class="ltx_bibblock">Roformer: Enhanced transformer with rotary position embedding, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, and Song Han.

</span>
<span class="ltx_bibblock">Quest: Query-aware sparsity for efficient long-context llm inference, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2406.10774" title="">https://arxiv.org/abs/2406.10774</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan&nbsp;N. Gomez, Lukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1706.03762" title="">https://arxiv.org/abs/1706.03762</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Xindi Wang, Mahsa Salmani, Parsa Omidi, Xiangyu Ren, Mehdi Rezagholizadeh, and Armaghan Eshaghi.

</span>
<span class="ltx_bibblock">Beyond the limits: A survey of techniques to extend the context length in large language models, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2402.02244" title="">https://arxiv.org/abs/2402.02244</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed&nbsp;Chi, Quoc Le, and Denny Zhou.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2201.11903" title="">https://arxiv.org/abs/2201.11903</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis.

</span>
<span class="ltx_bibblock">Efficient streaming language models with attention sinks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Zihao Ye, Ruihang Lai, Roy Lu, Chien-Yu Lin, Size Zheng, Lequn Chen, Tianqi Chen, and Luis Ceze.

</span>
<span class="ltx_bibblock">Cascade inference: Memory bandwidth efficient shared prefix batch decoding.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://flashinfer.ai/2024/01/08/cascade-inference.html" title="">https://flashinfer.ai/2024/01/08/cascade-inference.html</a>, Jan 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://flashinfer.ai/2024/01/08/cascade-inference.html" title="">https://flashinfer.ai/2024/01/08/cascade-inference.html</a>.

</span>
<span class="ltx_bibblock">Accessed on 2024-02-01.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher R√©, Clark Barrett, Zhangyang Wang, and Beidi Chen.

</span>
<span class="ltx_bibblock">H<sub class="ltx_sub" id="bib.bib27.2.1">2</sub>o: Heavy-hitter oracle for efficient generative inference of large language models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2024a)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher R√©, Clark Barrett, et&nbsp;al.

</span>
<span class="ltx_bibblock">H2o: Heavy-hitter oracle for efficient generative inference of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2024b)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Zhihao Zhang, Alan Zhu, Lijie Yang, Yihua Xu, Lanting Li, Phitchaya&nbsp;Mangpo Phothilimthana, and Zhihao Jia.

</span>
<span class="ltx_bibblock">Accelerating iterative retrieval-augmented language model serving with speculation.

</span>
<span class="ltx_bibblock">In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the 41st International Conference on Machine Learning</em>, volume 235 of <em class="ltx_emph ltx_font_italic" id="bib.bib29.2.2">Proceedings of Machine Learning Research</em>, pp.&nbsp; 60626‚Äì60643. PMLR, 21‚Äì27 Jul 2024b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v235/zhang24cq.html" title="">https://proceedings.mlr.press/v235/zhang24cq.html</a>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>LongBench for LLaMA-3.1-8B-Instruct</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A1.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Performance comparison on eight LongBench datasets evaluating single/multi-document QA, summarization, and retrieval tasks using Llama-3.1-8B-Instruct. The maximum F1-score for each task is in bold.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="A1.T4.1.1.1.1" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="A1.T4.1.1.1.2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.2.1">MFQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="A1.T4.1.1.1.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.3.1">NrtQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="A1.T4.1.1.1.4" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.4.1">Qasp</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="A1.T4.1.1.1.5" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.5.1">2Wiki</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="A1.T4.1.1.1.6" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.6.1">HotQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="A1.T4.1.1.1.7" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.7.1">QSm</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="A1.T4.1.1.1.8" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.8.1">TrQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="A1.T4.1.1.1.9" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.9.1">Pre</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T4.1.1.1.10" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.10.1">Avg</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T4.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T4.1.2.1.1" style="padding-left:5.0pt;padding-right:5.0pt;">Full</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.2.1.2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.1.2.1.2.1">27.02</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.2.1.3" style="padding-left:5.0pt;padding-right:5.0pt;">25.59</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.2.1.4" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.1.2.1.4.1">13.05</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.2.1.5" style="padding-left:5.0pt;padding-right:5.0pt;">16.64</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.2.1.6" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.1.2.1.6.1">16.86</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.2.1.7" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.1.2.1.7.1">23.88</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.2.1.8" style="padding-left:5.0pt;padding-right:5.0pt;">91.48</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.2.1.9" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.1.2.1.9.1">97.67</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.2.1.10" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.1.2.1.10.1">39.02</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T4.1.3.2.1" style="padding-left:5.0pt;padding-right:5.0pt;">Quest (1024)</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.3.2.2" style="padding-left:5.0pt;padding-right:5.0pt;">22.35</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.3.2.3" style="padding-left:5.0pt;padding-right:5.0pt;">14.89</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.3.2.4" style="padding-left:5.0pt;padding-right:5.0pt;">12.44</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.3.2.5" style="padding-left:5.0pt;padding-right:5.0pt;">14.24</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.3.2.6" style="padding-left:5.0pt;padding-right:5.0pt;">14.12</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.3.2.7" style="padding-left:5.0pt;padding-right:5.0pt;">23.86</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.3.2.8" style="padding-left:5.0pt;padding-right:5.0pt;">81.71</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.3.2.9" style="padding-left:5.0pt;padding-right:5.0pt;">95.73</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.3.2.10" style="padding-left:5.0pt;padding-right:5.0pt;">34.92</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T4.1.4.3.1" style="padding-left:5.0pt;padding-right:5.0pt;">TD+13 (1024)</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.4.3.2" style="padding-left:5.0pt;padding-right:5.0pt;">23.70</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.4.3.3" style="padding-left:5.0pt;padding-right:5.0pt;">23.25</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.4.3.4" style="padding-left:5.0pt;padding-right:5.0pt;">11.14</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.4.3.5" style="padding-left:5.0pt;padding-right:5.0pt;">13.53</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.4.3.6" style="padding-left:5.0pt;padding-right:5.0pt;">13.72</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.4.3.7" style="padding-left:5.0pt;padding-right:5.0pt;">22.69</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.4.3.8" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.1.4.3.8.1">92.35</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.4.3.9" style="padding-left:5.0pt;padding-right:5.0pt;">92.15</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.4.3.10" style="padding-left:5.0pt;padding-right:5.0pt;">36.57</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T4.1.5.4.1" style="padding-left:5.0pt;padding-right:5.0pt;">Quest (4096)</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.5.4.2" style="padding-left:5.0pt;padding-right:5.0pt;">26.34</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.5.4.3" style="padding-left:5.0pt;padding-right:5.0pt;">21.17</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.5.4.4" style="padding-left:5.0pt;padding-right:5.0pt;">11.99</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.5.4.5" style="padding-left:5.0pt;padding-right:5.0pt;">15.61</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.5.4.6" style="padding-left:5.0pt;padding-right:5.0pt;">16.26</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.5.4.7" style="padding-left:5.0pt;padding-right:5.0pt;">23.61</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.5.4.8" style="padding-left:5.0pt;padding-right:5.0pt;">90.73</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T4.1.5.4.9" style="padding-left:5.0pt;padding-right:5.0pt;">96.35</td>
<td class="ltx_td ltx_align_center" id="A1.T4.1.5.4.10" style="padding-left:5.0pt;padding-right:5.0pt;">37.76</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="A1.T4.1.6.5.1" style="padding-left:5.0pt;padding-right:5.0pt;">TD+13 (4096)</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T4.1.6.5.2" style="padding-left:5.0pt;padding-right:5.0pt;">25.89</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T4.1.6.5.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.1.6.5.3.1">26.29</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T4.1.6.5.4" style="padding-left:5.0pt;padding-right:5.0pt;">12.65</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T4.1.6.5.5" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.1.6.5.5.1">16.86</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T4.1.6.5.6" style="padding-left:5.0pt;padding-right:5.0pt;">15.94</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T4.1.6.5.7" style="padding-left:5.0pt;padding-right:5.0pt;">23.27</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T4.1.6.5.8" style="padding-left:5.0pt;padding-right:5.0pt;">90.22</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T4.1.6.5.9" style="padding-left:5.0pt;padding-right:5.0pt;">95.47</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T4.1.6.5.10" style="padding-left:5.0pt;padding-right:5.0pt;">38.32</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">TidalDecode and full-weight attention share the maximum F1 scores for all tasks, achieving the best scores in three tasks (NrtQA, 2Wiki, and TrQA). TidalDecode significantly outperforms Quest in 4/8 tasks (NrtQA, Qasp, 2Wiki, and TrQA) and full-attention in 3/8 tasks (NrtQA, 2Wiki, and TrQA). For other tasks, we stay close to the full attention and also obtains a higher average score than Quest.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>End-to-end efficiency evaluation results</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A1.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>TidalDecode end-to-end efficiency results on LLaMA-2-7B</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T5.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T5.1.1.1.1" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A1.T5.1.1.1.1.1">Context Length</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T5.1.1.1.2" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A1.T5.1.1.1.2.1">
<span class="ltx_tabular ltx_align_middle" id="A1.T5.1.1.1.2.1.1">
<span class="ltx_tr" id="A1.T5.1.1.1.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T5.1.1.1.2.1.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">Full Attention</span></span>
<span class="ltx_tr" id="A1.T5.1.1.1.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T5.1.1.1.2.1.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">(ms)</span></span>
</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="8" id="A1.T5.1.1.1.3" style="padding-left:2.0pt;padding-right:2.0pt;">TidalDecode(ms)</th>
</tr>
<tr class="ltx_tr" id="A1.T5.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T5.1.2.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">K=32</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T5.1.2.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">K=64</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T5.1.2.2.3" style="padding-left:2.0pt;padding-right:2.0pt;">K=128</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T5.1.2.2.4" style="padding-left:2.0pt;padding-right:2.0pt;">K=256</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T5.1.2.2.5" style="padding-left:2.0pt;padding-right:2.0pt;">K=512</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T5.1.2.2.6" style="padding-left:2.0pt;padding-right:2.0pt;">K=1024</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T5.1.2.2.7" style="padding-left:2.0pt;padding-right:2.0pt;">K=2048</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T5.1.2.2.8" style="padding-left:2.0pt;padding-right:2.0pt;">K=4096</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T5.1.3.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.1.3.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">10K</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.1.3.1.2" style="padding-left:2.0pt;padding-right:2.0pt;">19.22</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.1.3.1.3" style="padding-left:2.0pt;padding-right:2.0pt;">16.94</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.1.3.1.4" style="padding-left:2.0pt;padding-right:2.0pt;">17.18</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.1.3.1.5" style="padding-left:2.0pt;padding-right:2.0pt;">17.15</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.1.3.1.6" style="padding-left:2.0pt;padding-right:2.0pt;">16.98</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.1.3.1.7" style="padding-left:2.0pt;padding-right:2.0pt;">16.96</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.1.3.1.8" style="padding-left:2.0pt;padding-right:2.0pt;">17.32</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.1.3.1.9" style="padding-left:2.0pt;padding-right:2.0pt;">17.19</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.1.3.1.10" style="padding-left:2.0pt;padding-right:2.0pt;">17.63</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.4.2">
<td class="ltx_td ltx_align_center" id="A1.T5.1.4.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">32K</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.4.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">25.71</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.4.2.3" style="padding-left:2.0pt;padding-right:2.0pt;">17.89</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.4.2.4" style="padding-left:2.0pt;padding-right:2.0pt;">17.92</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.4.2.5" style="padding-left:2.0pt;padding-right:2.0pt;">17.64</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.4.2.6" style="padding-left:2.0pt;padding-right:2.0pt;">17.70</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.4.2.7" style="padding-left:2.0pt;padding-right:2.0pt;">17.91</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.4.2.8" style="padding-left:2.0pt;padding-right:2.0pt;">17.97</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.4.2.9" style="padding-left:2.0pt;padding-right:2.0pt;">18.48</td>
<td class="ltx_td ltx_align_center" id="A1.T5.1.4.2.10" style="padding-left:2.0pt;padding-right:2.0pt;">18.98</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.5.3">
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.1.5.3.1" style="padding-left:2.0pt;padding-right:2.0pt;">100K</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.1.5.3.2" style="padding-left:2.0pt;padding-right:2.0pt;">45.70</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.1.5.3.3" style="padding-left:2.0pt;padding-right:2.0pt;">21.26</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.1.5.3.4" style="padding-left:2.0pt;padding-right:2.0pt;">21.09</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.1.5.3.5" style="padding-left:2.0pt;padding-right:2.0pt;">21.38</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.1.5.3.6" style="padding-left:2.0pt;padding-right:2.0pt;">21.19</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.1.5.3.7" style="padding-left:2.0pt;padding-right:2.0pt;">21.13</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.1.5.3.8" style="padding-left:2.0pt;padding-right:2.0pt;">21.38</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.1.5.3.9" style="padding-left:2.0pt;padding-right:2.0pt;">21.65</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.1.5.3.10" style="padding-left:2.0pt;padding-right:2.0pt;">22.34</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A1.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Quest end-to-end efficiency results on LLaMA-2-7B</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T6.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T6.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T6.1.1.1.1" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A1.T6.1.1.1.1.1">Context Length</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T6.1.1.1.2" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A1.T6.1.1.1.2.1">
<span class="ltx_tabular ltx_align_middle" id="A1.T6.1.1.1.2.1.1">
<span class="ltx_tr" id="A1.T6.1.1.1.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T6.1.1.1.2.1.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">Full Attention</span></span>
<span class="ltx_tr" id="A1.T6.1.1.1.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T6.1.1.1.2.1.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">(ms)</span></span>
</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="8" id="A1.T6.1.1.1.3" style="padding-left:2.0pt;padding-right:2.0pt;">TidalDecode(ms)</th>
</tr>
<tr class="ltx_tr" id="A1.T6.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T6.1.2.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">K=32</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T6.1.2.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">K=64</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T6.1.2.2.3" style="padding-left:2.0pt;padding-right:2.0pt;">K=128</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T6.1.2.2.4" style="padding-left:2.0pt;padding-right:2.0pt;">K=256</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T6.1.2.2.5" style="padding-left:2.0pt;padding-right:2.0pt;">K=512</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T6.1.2.2.6" style="padding-left:2.0pt;padding-right:2.0pt;">K=1024</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T6.1.2.2.7" style="padding-left:2.0pt;padding-right:2.0pt;">K=2048</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A1.T6.1.2.2.8" style="padding-left:2.0pt;padding-right:2.0pt;">K=4096</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T6.1.3.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.3.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">10K</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.3.1.2" style="padding-left:2.0pt;padding-right:2.0pt;">19.22</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.3.1.3" style="padding-left:2.0pt;padding-right:2.0pt;">20.39</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.3.1.4" style="padding-left:2.0pt;padding-right:2.0pt;">19.86</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.3.1.5" style="padding-left:2.0pt;padding-right:2.0pt;">19.44</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.3.1.6" style="padding-left:2.0pt;padding-right:2.0pt;">19.35</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.3.1.7" style="padding-left:2.0pt;padding-right:2.0pt;">20.18</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.3.1.8" style="padding-left:2.0pt;padding-right:2.0pt;">19.91</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.3.1.9" style="padding-left:2.0pt;padding-right:2.0pt;">20.23</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.3.1.10" style="padding-left:2.0pt;padding-right:2.0pt;">21.09</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.4.2">
<td class="ltx_td ltx_align_center" id="A1.T6.1.4.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">32K</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.4.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">25.71</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.4.2.3" style="padding-left:2.0pt;padding-right:2.0pt;">20.47</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.4.2.4" style="padding-left:2.0pt;padding-right:2.0pt;">20.85</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.4.2.5" style="padding-left:2.0pt;padding-right:2.0pt;">20.73</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.4.2.6" style="padding-left:2.0pt;padding-right:2.0pt;">21.06</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.4.2.7" style="padding-left:2.0pt;padding-right:2.0pt;">20.62</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.4.2.8" style="padding-left:2.0pt;padding-right:2.0pt;">20.94</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.4.2.9" style="padding-left:2.0pt;padding-right:2.0pt;">21.35</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.4.2.10" style="padding-left:2.0pt;padding-right:2.0pt;">22.11</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.5.3">
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.1.5.3.1" style="padding-left:2.0pt;padding-right:2.0pt;">100K</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.1.5.3.2" style="padding-left:2.0pt;padding-right:2.0pt;">45.70</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.1.5.3.3" style="padding-left:2.0pt;padding-right:2.0pt;">24.93</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.1.5.3.4" style="padding-left:2.0pt;padding-right:2.0pt;">25.18</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.1.5.3.5" style="padding-left:2.0pt;padding-right:2.0pt;">24.77</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.1.5.3.6" style="padding-left:2.0pt;padding-right:2.0pt;">24.90</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.1.5.3.7" style="padding-left:2.0pt;padding-right:2.0pt;">24.84</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.1.5.3.8" style="padding-left:2.0pt;padding-right:2.0pt;">25.10</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.1.5.3.9" style="padding-left:2.0pt;padding-right:2.0pt;">25.77</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.1.5.3.10" style="padding-left:2.0pt;padding-right:2.0pt;">26.17</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Full sensitivity studies on different token re-selection layer</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A1.T7">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Sensitivity study of re-selection layer (RL) on 10k-context-length Needle-in-the-Haystack test for LLaMA-3.2-3B-Instruct with TidalDecode. The best accuracy for each token budget (K) is in bold.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T7.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T7.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T7.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T7.1.1.1.1.1">RL/K</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T7.1.1.1.2">32</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T7.1.1.1.3">64</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T7.1.1.1.4">128</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T7.1.1.1.5">256</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T7.1.1.1.6">512</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T7.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.1.2.1.1">TidalDecode+L2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.1.2.1.2">2%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.1.2.1.3">12%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.1.2.1.4">14%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.1.2.1.5">16%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.2.1.6">30%</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.3.2.1">TidalDecode+L3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.3.2.2">2%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.3.2.3">6%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.3.2.4">8%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.3.2.5">10%</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.3.2.6">24%</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.4.3.1">TidalDecode+L4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.4.3.2">6%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.4.3.3">14%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.4.3.4">16%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.4.3.5">20%</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.4.3.6">28%</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.5.4.1">TidalDecode+L5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.5.4.2">2%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.5.4.3">10%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.5.4.4">22%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.5.4.5">26%</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.5.4.6">36%</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.6.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.6.5.1">TidalDecode+L6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.6.5.2">18%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.6.5.3">26%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.6.5.4">26%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.6.5.5">32%</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.6.5.6">46%</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.7.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.7.6.1">TidalDecode+L7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.7.6.2">6%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.7.6.3">10%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.7.6.4">16%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.7.6.5">18%</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.7.6.6">28%</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.8.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.8.7.1">TidalDecode+L8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.8.7.2">20%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.8.7.3">20%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.8.7.4">46%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.8.7.5">60%</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.8.7.6">84%</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.9.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.9.8.1">TidalDecode+L9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.9.8.2">6%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.9.8.3">12%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.9.8.4">32%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.9.8.5">58%</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.9.8.6">66%</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.10.9">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.10.9.1">TidalDecode+L10</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.10.9.2">44%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.10.9.3">58%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.10.9.4">50%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.10.9.5">60%</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.10.9.6">64%</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.11.10">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.11.10.1">TidalDecode+L11</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.11.10.2">4%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.11.10.3">12%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.11.10.4">16%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.11.10.5">22%</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.11.10.6">28%</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.12.11">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.12.11.1">TidalDecode+L12</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.12.11.2"><span class="ltx_text ltx_font_bold" id="A1.T7.1.12.11.2.1">50%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.12.11.3"><span class="ltx_text ltx_font_bold" id="A1.T7.1.12.11.3.1">84%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.12.11.4"><span class="ltx_text ltx_font_bold" id="A1.T7.1.12.11.4.1">96%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.12.11.5"><span class="ltx_text ltx_font_bold" id="A1.T7.1.12.11.5.1">98%</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.12.11.6">98%</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.13.12">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.13.12.1">TidalDecode+L13</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.13.12.2">42%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.13.12.3">80%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.13.12.4">94%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.13.12.5"><span class="ltx_text ltx_font_bold" id="A1.T7.1.13.12.5.1">98%</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.13.12.6"><span class="ltx_text ltx_font_bold" id="A1.T7.1.13.12.6.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.14.13">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.14.13.1">TidalDecode+L14</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.14.13.2">28%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.14.13.3">44%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.14.13.4">54%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.14.13.5">60%</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.14.13.6">72%</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.15.14">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.15.14.1">TidalDecode+L15</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.15.14.2">2%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.15.14.3">8%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.15.14.4">16%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.15.14.5">22%</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.15.14.6">36%</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.16.15">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.16.15.1">TidalDecode+L16</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.16.15.2">4%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.16.15.3">16%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.16.15.4">12%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.16.15.5">22%</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.16.15.6">34%</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.17.16">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.17.16.1">TidalDecode+L17</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.17.16.2">2%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.17.16.3">6%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.17.16.4">16%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.17.16.5">20%</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.17.16.6">32%</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.18.17">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.18.17.1">TidalDecode+L18</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.18.17.2">2%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.18.17.3">10%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.18.17.4">12%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.18.17.5">18%</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.18.17.6">28%</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.19.18">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.19.18.1">TidalDecode+L19</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.19.18.2">2%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.19.18.3">6%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.19.18.4">10%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.19.18.5">18%</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.19.18.6">32%</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.20.19">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.20.19.1">TidalDecode+L20</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.20.19.2">6%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.20.19.3">10%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.20.19.4">12%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.20.19.5">18%</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.20.19.6">24%</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.21.20">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.21.20.1">TidalDecode+L21</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.21.20.2">6%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.21.20.3">8%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.21.20.4">10%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.21.20.5">16%</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.21.20.6">26%</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.22.21">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.22.21.1">TidalDecode+L22</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.22.21.2">6%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.22.21.3">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.22.21.4">12%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.22.21.5">12%</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.22.21.6">26%</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.23.22">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.23.22.1">TidalDecode+L23</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.23.22.2">4%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.23.22.3">14%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.23.22.4">14%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.23.22.5">18%</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.23.22.6">26%</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.24.23">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.24.23.1">TidalDecode+L24</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.24.23.2">2%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.24.23.3">10%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.24.23.4">16%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.24.23.5">20%</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.24.23.6">28%</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.25.24">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.25.24.1">TidalDecode+L25</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.25.24.2">4%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.25.24.3">8%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.25.24.4">14%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.25.24.5">16%</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.25.24.6">22%</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.26.25">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.26.25.1">TidalDecode+L26</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.26.25.2">2%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.26.25.3">10%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.26.25.4">10%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.26.25.5">22%</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.26.25.6">26%</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.27.26">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.27.26.1">TidalDecode+L27</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.27.26.2">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.27.26.3">10%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.27.26.4">12%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.1.27.26.5">22%</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.27.26.6">26%</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.28.27">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T7.1.28.27.1">Quest</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T7.1.28.27.2">46%</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T7.1.28.27.3">56%</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T7.1.28.27.4">72%</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T7.1.28.27.5">88%</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="A1.T7.1.28.27.6">96%</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A1.T8">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Sensitivity study of re-selection layer (RL) on 10k-context-length Needle-in-the-Haystack test for LLaMA-3.1-8B-Instruct with TidalDecode. The best accuracy for each token budget (K) is in bold. Layer 13 and Layer 14 are the best two re-selection layers for accuracy. </figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T8.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T8.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T8.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.1.1.1.1.1">RL/K</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T8.1.1.1.2">32</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T8.1.1.1.3">64</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T8.1.1.1.4">128</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T8.1.1.1.5">256</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T8.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T8.1.2.1.1">TidalDecode+L2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T8.1.2.1.2">36%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T8.1.2.1.3">38%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T8.1.2.1.4">46%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.1.2.1.5">58%</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.3.2.1">TidalDecode+L3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.3.2.2">8%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.3.2.3">10%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.3.2.4">14%</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.3.2.5">34%</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.4.3.1">TidalDecode+L4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.4.3.2">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.4.3.3">10%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.4.3.4">16%</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.4.3.5">34%</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.5.4.1">TidalDecode+L5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.5.4.2">14%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.5.4.3">30%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.5.4.4">52%</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.5.4.5">52%</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.6.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.6.5.1">TidalDecode+L6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.6.5.2">8%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.6.5.3">12%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.6.5.4">28%</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.6.5.5">40%</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.7.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.7.6.1">TidalDecode+L7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.7.6.2">6%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.7.6.3">10%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.7.6.4">10%</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.7.6.5">18%</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.8.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.8.7.1">TidalDecode+L8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.8.7.2">34%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.8.7.3">44%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.8.7.4">50%</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.8.7.5">66%</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.9.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.9.8.1">TidalDecode+L9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.9.8.2">64%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.9.8.3">78%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.9.8.4">82%</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.9.8.5">90%</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.10.9">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.10.9.1">TidalDecode+L10</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.10.9.2">56%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.10.9.3">74%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.10.9.4">84%</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.10.9.5">94%</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.11.10">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.11.10.1">TidalDecode+L11</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.11.10.2">52%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.11.10.3">76%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.11.10.4">82%</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.11.10.5">86%</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.12.11">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.12.11.1">TidalDecode+L12</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.12.11.2">8%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.12.11.3">10%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.12.11.4">28%</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.12.11.5">40%</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.13.12">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.13.12.1">TidalDecode+L13</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.13.12.2"><span class="ltx_text ltx_font_bold" id="A1.T8.1.13.12.2.1">100%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.13.12.3"><span class="ltx_text ltx_font_bold" id="A1.T8.1.13.12.3.1">100%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.13.12.4"><span class="ltx_text ltx_font_bold" id="A1.T8.1.13.12.4.1">100%</span></td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.13.12.5"><span class="ltx_text ltx_font_bold" id="A1.T8.1.13.12.5.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.14.13">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.14.13.1">TidalDecode+L14</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.14.13.2">98%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.14.13.3"><span class="ltx_text ltx_font_bold" id="A1.T8.1.14.13.3.1">100%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.14.13.4"><span class="ltx_text ltx_font_bold" id="A1.T8.1.14.13.4.1">100%</span></td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.14.13.5"><span class="ltx_text ltx_font_bold" id="A1.T8.1.14.13.5.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.15.14">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.15.14.1">TidalDecode+L15</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.15.14.2">56%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.15.14.3">78%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.15.14.4">88%</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.15.14.5">96%</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.16.15">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.16.15.1">TidalDecode+L16</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.16.15.2">18%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.16.15.3">46%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.16.15.4">54%</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.16.15.5">72%</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.17.16">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.17.16.1">TidalDecode+L17</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.17.16.2">64%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.17.16.3">74%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.17.16.4">86%</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.17.16.5">98%</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.18.17">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.18.17.1">TidalDecode+L18</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.18.17.2">64%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.18.17.3">70%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.18.17.4">74%</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.18.17.5">84%</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.19.18">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.19.18.1">TidalDecode+L19</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.19.18.2">58%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.19.18.3">50%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.19.18.4">60%</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.19.18.5">68%</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.20.19">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.20.19.1">TidalDecode+L20</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.20.19.2">68%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.20.19.3">60%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.20.19.4">62%</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.20.19.5">76%</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.21.20">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.21.20.1">TidalDecode+L21</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.21.20.2">40%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.21.20.3">48%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.21.20.4">48%</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.21.20.5">62%</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.22.21">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.22.21.1">TidalDecode+L22</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.22.21.2">28%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.22.21.3">38%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.22.21.4">46%</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.22.21.5">56%</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.23.22">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.23.22.1">TidalDecode+L23</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.23.22.2">40%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.23.22.3">46%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.23.22.4">52%</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.23.22.5">64%</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.24.23">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.24.23.1">TidalDecode+L24</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.24.23.2">30%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.24.23.3">46%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.24.23.4">54%</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.24.23.5">66%</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.25.24">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.25.24.1">TidalDecode+L25</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.25.24.2">40%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.25.24.3">54%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.25.24.4">50%</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.25.24.5">66%</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.26.25">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.26.25.1">TidalDecode+L26</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.26.25.2">34%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.26.25.3">48%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.26.25.4">62%</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.26.25.5">64%</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.27.26">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.27.26.1">TidalDecode+L27</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.27.26.2">38%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.27.26.3">50%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.27.26.4">54%</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.27.26.5">70%</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.28.27">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.28.27.1">TidalDecode+L28</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.28.27.2">30%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.28.27.3">40%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.28.27.4">56%</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.28.27.5">58%</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.29.28">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.29.28.1">TidalDecode+L29</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.29.28.2">32%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.29.28.3">48%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.29.28.4">56%</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.29.28.5">68%</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.30.29">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.30.29.1">TidalDecode+L30</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.30.29.2">36%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.30.29.3">48%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.1.30.29.4">52%</td>
<td class="ltx_td ltx_align_center" id="A1.T8.1.30.29.5">70%</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.31.30">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T8.1.31.30.1">TidalDecode+L31</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T8.1.31.30.2">30%</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T8.1.31.30.3">36%</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T8.1.31.30.4">42%</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T8.1.31.30.5">56%</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A1.T9">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>Sensitivity study of re-selection layer (RL) on 10k-context-length Needle-in-the-Haystack test for LLaMA-3-70B-Instruct-Gradient-1048k; we first run top_k = 512 and filter out those layers that do not achieve full accuracy with TidalDecode. The best accuracy for each token budget (K) is in bold. Layer 14 and Layer 31 are the best two Re-selection layers for accuracy.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_minipage ltx_align_middle" id="A1.T9.1" style="width:216.8pt;">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T9.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T9.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.1.1.1">RL/K</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T9.1.1.1.2">32</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T9.1.1.1.3">64</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T9.1.1.1.4">128</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T9.1.1.1.5">256</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T9.1.1.1.6">512</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T9.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T9.1.2.1.1">L2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T9.1.2.1.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T9.1.2.1.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T9.1.2.1.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T9.1.2.1.5">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.1.2.1.6">6%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.3.2.1">L3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.3.2.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.3.2.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.3.2.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.3.2.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.3.2.6">37%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.4.3.1">L4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.4.3.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.4.3.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.4.3.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.4.3.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.4.3.6">23%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.5.4.1">L5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.5.4.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.5.4.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.5.4.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.5.4.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.5.4.6">63%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.6.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.6.5.1">L6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.6.5.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.6.5.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.6.5.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.6.5.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.6.5.6">70%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.7.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.7.6.1">L7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.7.6.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.7.6.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.7.6.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.7.6.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.7.6.6">90%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.8.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.8.7.1">L8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.8.7.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.8.7.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.8.7.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.8.7.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.8.7.6">70%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.9.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.9.8.1">L9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.9.8.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.9.8.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.9.8.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.9.8.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.9.8.6">30%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.10.9">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.10.9.1">L10</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.10.9.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.10.9.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.10.9.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.10.9.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.10.9.6">83%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.11.10">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.11.10.1">L11</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.11.10.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.11.10.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.11.10.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.11.10.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.11.10.6">70%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.12.11">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.12.11.1">L12</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.12.11.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.12.11.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.12.11.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.12.11.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.12.11.6">63%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.13.12">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.13.12.1">L13</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.13.12.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.13.12.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.13.12.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.13.12.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.13.12.6">50%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.14.13">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.14.13.1">L14</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.14.13.2">87%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.14.13.3">93%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.14.13.4"><span class="ltx_text ltx_font_bold" id="A1.T9.1.14.13.4.1">100%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.14.13.5"><span class="ltx_text ltx_font_bold" id="A1.T9.1.14.13.5.1">100%</span></td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.14.13.6"><span class="ltx_text ltx_font_bold" id="A1.T9.1.14.13.6.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.15.14">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.15.14.1">L15</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.15.14.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.15.14.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.15.14.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.15.14.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.15.14.6">97%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.16.15">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.16.15.1">L16</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.16.15.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.16.15.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.16.15.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.16.15.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.16.15.6">63%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.17.16">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.17.16.1">L17</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.17.16.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.17.16.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.17.16.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.17.16.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.17.16.6">87%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.18.17">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.18.17.1">L18</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.18.17.2">50%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.18.17.3">70%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.18.17.4">83%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.18.17.5">97%</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.18.17.6"><span class="ltx_text ltx_font_bold" id="A1.T9.1.18.17.6.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.19.18">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.19.18.1">L19</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.19.18.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.19.18.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.19.18.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.19.18.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.19.18.6">93%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.20.19">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.20.19.1">L20</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.20.19.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.20.19.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.20.19.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.20.19.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.20.19.6">87%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.21.20">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.21.20.1">L21</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.21.20.2">53%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.21.20.3">80%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.21.20.4">93%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.21.20.5">97%</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.21.20.6"><span class="ltx_text ltx_font_bold" id="A1.T9.1.21.20.6.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.22.21">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.22.21.1">L22</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.22.21.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.22.21.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.22.21.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.22.21.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.22.21.6">97%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.23.22">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.23.22.1">L23</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.23.22.2">53%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.23.22.3">93%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.23.22.4">97%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.23.22.5"><span class="ltx_text ltx_font_bold" id="A1.T9.1.23.22.5.1">100%</span></td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.23.22.6"><span class="ltx_text ltx_font_bold" id="A1.T9.1.23.22.6.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.24.23">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.24.23.1">L24</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.24.23.2">33%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.24.23.3">60%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.24.23.4">77%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.24.23.5">93%</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.24.23.6"><span class="ltx_text ltx_font_bold" id="A1.T9.1.24.23.6.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.25.24">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.25.24.1">L25</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.25.24.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.25.24.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.25.24.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.25.24.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.25.24.6">80%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.26.25">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.26.25.1">L26</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.26.25.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.26.25.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.26.25.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.26.25.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.26.25.6">87%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.27.26">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.27.26.1">L27</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.27.26.2">50%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.27.26.3">87%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.27.26.4">93%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.27.26.5">93%</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.27.26.6"><span class="ltx_text ltx_font_bold" id="A1.T9.1.27.26.6.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.28.27">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.28.27.1">L28</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.28.27.2">80%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.28.27.3">83%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.28.27.4">93%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.28.27.5">87%</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.28.27.6"><span class="ltx_text ltx_font_bold" id="A1.T9.1.28.27.6.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.29.28">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.29.28.1">L29</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.29.28.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.29.28.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.29.28.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.29.28.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.29.28.6">97%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.30.29">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.30.29.1">L30</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.30.29.2">33%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.30.29.3">67%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.30.29.4">80%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.30.29.5">90%</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.30.29.6"><span class="ltx_text ltx_font_bold" id="A1.T9.1.30.29.6.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.31.30">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.31.30.1">L31</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.31.30.2"><span class="ltx_text ltx_font_bold" id="A1.T9.1.31.30.2.1">90%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.31.30.3"><span class="ltx_text ltx_font_bold" id="A1.T9.1.31.30.3.1">97%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.31.30.4"><span class="ltx_text ltx_font_bold" id="A1.T9.1.31.30.4.1">100%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.31.30.5"><span class="ltx_text ltx_font_bold" id="A1.T9.1.31.30.5.1">100%</span></td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.31.30.6"><span class="ltx_text ltx_font_bold" id="A1.T9.1.31.30.6.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.32.31">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T9.1.32.31.1">L32</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T9.1.32.31.2">27%</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T9.1.32.31.3">73%</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T9.1.32.31.4">80%</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T9.1.32.31.5">97%</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T9.1.32.31.6"><span class="ltx_text ltx_font_bold" id="A1.T9.1.32.31.6.1">100%</span></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_minipage ltx_align_middle" id="A1.T9.2" style="width:216.8pt;">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T9.2.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T9.2.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T9.2.1.1.1.1">RL/K</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T9.2.1.1.2">32</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T9.2.1.1.3">64</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T9.2.1.1.4">128</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T9.2.1.1.5">256</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T9.2.1.1.6">512</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T9.2.2.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T9.2.2.1.1">L33</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T9.2.2.1.2">50%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T9.2.2.1.3">87%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T9.2.2.1.4">90%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T9.2.2.1.5">93%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.2.2.1.6"><span class="ltx_text ltx_font_bold" id="A1.T9.2.2.1.6.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.3.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.3.2.1">L34</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.3.2.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.3.2.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.3.2.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.3.2.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.3.2.6">97%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.4.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.4.3.1">L35</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.4.3.2">70%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.4.3.3">83%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.4.3.4"><span class="ltx_text ltx_font_bold" id="A1.T9.2.4.3.4.1">100%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.4.3.5"><span class="ltx_text ltx_font_bold" id="A1.T9.2.4.3.5.1">100%</span></td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.4.3.6"><span class="ltx_text ltx_font_bold" id="A1.T9.2.4.3.6.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.5.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.5.4.1">L36</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.5.4.2">50%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.5.4.3">83%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.5.4.4">97%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.5.4.5">97%</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.5.4.6"><span class="ltx_text ltx_font_bold" id="A1.T9.2.5.4.6.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.6.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.6.5.1">L37</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.6.5.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.6.5.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.6.5.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.6.5.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.6.5.6">90%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.7.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.7.6.1">L38</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.7.6.2">37%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.7.6.3">83%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.7.6.4">83%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.7.6.5">80%</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.7.6.6"><span class="ltx_text ltx_font_bold" id="A1.T9.2.7.6.6.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.8.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.8.7.1">L39</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.8.7.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.8.7.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.8.7.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.8.7.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.8.7.6">87%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.9.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.9.8.1">L40</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.9.8.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.9.8.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.9.8.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.9.8.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.9.8.6">50%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.10.9">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.10.9.1">L41</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.10.9.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.10.9.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.10.9.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.10.9.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.10.9.6">97%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.11.10">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.11.10.1">L42</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.11.10.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.11.10.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.11.10.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.11.10.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.11.10.6">53%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.12.11">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.12.11.1">L43</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.12.11.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.12.11.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.12.11.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.12.11.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.12.11.6">67%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.13.12">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.13.12.1">L44</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.13.12.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.13.12.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.13.12.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.13.12.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.13.12.6">83%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.14.13">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.14.13.1">L45</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.14.13.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.14.13.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.14.13.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.14.13.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.14.13.6">70%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.15.14">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.15.14.1">L46</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.15.14.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.15.14.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.15.14.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.15.14.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.15.14.6">63%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.16.15">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.16.15.1">L47</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.16.15.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.16.15.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.16.15.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.16.15.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.16.15.6">77%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.17.16">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.17.16.1">L48</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.17.16.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.17.16.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.17.16.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.17.16.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.17.16.6">97%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.18.17">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.18.17.1">L49</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.18.17.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.18.17.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.18.17.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.18.17.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.18.17.6">77%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.19.18">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.19.18.1">L50</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.19.18.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.19.18.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.19.18.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.19.18.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.19.18.6">70%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.20.19">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.20.19.1">L51</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.20.19.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.20.19.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.20.19.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.20.19.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.20.19.6">93%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.21.20">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.21.20.1">L52</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.21.20.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.21.20.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.21.20.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.21.20.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.21.20.6">77%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.22.21">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.22.21.1">L53</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.22.21.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.22.21.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.22.21.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.22.21.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.22.21.6">70%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.23.22">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.23.22.1">L54</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.23.22.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.23.22.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.23.22.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.23.22.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.23.22.6">60%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.24.23">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.24.23.1">L55</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.24.23.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.24.23.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.24.23.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.24.23.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.24.23.6">53%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.25.24">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.25.24.1">L56</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.25.24.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.25.24.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.25.24.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.25.24.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.25.24.6">87%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.26.25">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.26.25.1">L57</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.26.25.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.26.25.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.26.25.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.26.25.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.26.25.6">57%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.27.26">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.27.26.1">L58</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.27.26.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.27.26.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.27.26.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.27.26.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.27.26.6">50%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.28.27">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.28.27.1">L59</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.28.27.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.28.27.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.28.27.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.28.27.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.28.27.6">50%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.29.28">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.29.28.1">L60</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.29.28.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.29.28.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.29.28.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.29.28.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.29.28.6">57%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.30.29">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.30.29.1">L61</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.30.29.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.30.29.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.30.29.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.30.29.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.30.29.6">30%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.31.30">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.31.30.1">L62</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.31.30.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.31.30.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.31.30.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.2.31.30.5">-</td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.31.30.6">43%</td>
</tr>
<tr class="ltx_tr" id="A1.T9.2.32.31">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T9.2.32.31.1">L63</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T9.2.32.31.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T9.2.32.31.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T9.2.32.31.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T9.2.32.31.5">-</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T9.2.32.31.6">43%</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A1.T10">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span>Sensitivity study of re-selection layer (RL) on 10k-context-length Needle-in-the-Haystack test for Llama-3-8B-Instruct-Gradient-1048k with TidalDecode. The best accuracy for each token budget (K) is in bold. Layer 9, Layer 13, and Layer 14 are the best three re-selection layers for accuracy.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T10.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T10.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T10.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T10.1.1.1.1.1">RL/K</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T10.1.1.1.2">16</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T10.1.1.1.3">32</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T10.1.1.1.4">64</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T10.1.1.1.5">128</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T10.1.1.1.6">256</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T10.1.1.1.7">512</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T10.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T10.1.2.1.1">TidalDecode+L2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T10.1.2.1.2">78%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T10.1.2.1.3">84%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T10.1.2.1.4">76%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T10.1.2.1.5">94%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T10.1.2.1.6">88%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.1.2.1.7">98%</td>
</tr>
<tr class="ltx_tr" id="A1.T10.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.3.2.1">TidalDecode+L3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.3.2.2">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.3.2.3">6%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.3.2.4">10%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.3.2.5">16%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.3.2.6">28%</td>
<td class="ltx_td ltx_align_center" id="A1.T10.1.3.2.7">64%</td>
</tr>
<tr class="ltx_tr" id="A1.T10.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.4.3.1">TidalDecode+L4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.4.3.2">2%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.4.3.3">10%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.4.3.4">16%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.4.3.5">28%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.4.3.6">68%</td>
<td class="ltx_td ltx_align_center" id="A1.T10.1.4.3.7">84%</td>
</tr>
<tr class="ltx_tr" id="A1.T10.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.5.4.1">TidalDecode+L5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.5.4.2">10%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.5.4.3">12%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.5.4.4">32%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.5.4.5">52%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.5.4.6">72%</td>
<td class="ltx_td ltx_align_center" id="A1.T10.1.5.4.7">80%</td>
</tr>
<tr class="ltx_tr" id="A1.T10.1.6.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.6.5.1">TidalDecode+L6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.6.5.2">4%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.6.5.3">6%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.6.5.4">10%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.6.5.5">14%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.6.5.6">16%</td>
<td class="ltx_td ltx_align_center" id="A1.T10.1.6.5.7">24%</td>
</tr>
<tr class="ltx_tr" id="A1.T10.1.7.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.7.6.1">TidalDecode+L7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.7.6.2">2%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.7.6.3">10%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.7.6.4">10%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.7.6.5">10%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.7.6.6">14%</td>
<td class="ltx_td ltx_align_center" id="A1.T10.1.7.6.7">28%</td>
</tr>
<tr class="ltx_tr" id="A1.T10.1.8.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.8.7.1">TidalDecode+L8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.8.7.2">26%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.8.7.3">64%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.8.7.4">80%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.8.7.5">90%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.8.7.6">92%</td>
<td class="ltx_td ltx_align_center" id="A1.T10.1.8.7.7">96%</td>
</tr>
<tr class="ltx_tr" id="A1.T10.1.9.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.9.8.1">TidalDecode+L9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.9.8.2">52%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.9.8.3">90%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.9.8.4">96%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.9.8.5"><span class="ltx_text ltx_font_bold" id="A1.T10.1.9.8.5.1">100%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.9.8.6">98%</td>
<td class="ltx_td ltx_align_center" id="A1.T10.1.9.8.7"><span class="ltx_text ltx_font_bold" id="A1.T10.1.9.8.7.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="A1.T10.1.10.9">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.10.9.1">TidalDecode+L10</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.10.9.2">72%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.10.9.3">76%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.10.9.4">86%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.10.9.5">94%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.10.9.6">96%</td>
<td class="ltx_td ltx_align_center" id="A1.T10.1.10.9.7"><span class="ltx_text ltx_font_bold" id="A1.T10.1.10.9.7.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="A1.T10.1.11.10">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.11.10.1">TidalDecode+L11</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.11.10.2">56%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.11.10.3">74%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.11.10.4">94%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.11.10.5"><span class="ltx_text ltx_font_bold" id="A1.T10.1.11.10.5.1">100%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.11.10.6">98%</td>
<td class="ltx_td ltx_align_center" id="A1.T10.1.11.10.7">98%</td>
</tr>
<tr class="ltx_tr" id="A1.T10.1.12.11">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.12.11.1">TidalDecode+L12</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.12.11.2">8%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.12.11.3">14%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.12.11.4">22%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.12.11.5">44%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.12.11.6">66%</td>
<td class="ltx_td ltx_align_center" id="A1.T10.1.12.11.7">94%</td>
</tr>
<tr class="ltx_tr" id="A1.T10.1.13.12">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.13.12.1">TidalDecode+L13</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.13.12.2"><span class="ltx_text ltx_font_bold" id="A1.T10.1.13.12.2.1">92%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.13.12.3"><span class="ltx_text ltx_font_bold" id="A1.T10.1.13.12.3.1">92%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.13.12.4"><span class="ltx_text ltx_font_bold" id="A1.T10.1.13.12.4.1">96%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.13.12.5"><span class="ltx_text ltx_font_bold" id="A1.T10.1.13.12.5.1">100%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.13.12.6"><span class="ltx_text ltx_font_bold" id="A1.T10.1.13.12.6.1">100%</span></td>
<td class="ltx_td ltx_align_center" id="A1.T10.1.13.12.7"><span class="ltx_text ltx_font_bold" id="A1.T10.1.13.12.7.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="A1.T10.1.14.13">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.14.13.1">TidalDecode+L14</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.14.13.2">74%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.14.13.3">68%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.14.13.4">88%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.14.13.5">98%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.14.13.6"><span class="ltx_text ltx_font_bold" id="A1.T10.1.14.13.6.1">100%</span></td>
<td class="ltx_td ltx_align_center" id="A1.T10.1.14.13.7"><span class="ltx_text ltx_font_bold" id="A1.T10.1.14.13.7.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="A1.T10.1.15.14">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.15.14.1">TidalDecode+L15</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.15.14.2">74%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.15.14.3">94%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.15.14.4">92%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.15.14.5">88%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.15.14.6"><span class="ltx_text ltx_font_bold" id="A1.T10.1.15.14.6.1">100%</span></td>
<td class="ltx_td ltx_align_center" id="A1.T10.1.15.14.7"><span class="ltx_text ltx_font_bold" id="A1.T10.1.15.14.7.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="A1.T10.1.16.15">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.16.15.1">TidalDecode+L16</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.16.15.2">44%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.16.15.3">50%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.16.15.4">72%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.16.15.5">66%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.16.15.6">82%</td>
<td class="ltx_td ltx_align_center" id="A1.T10.1.16.15.7">94%</td>
</tr>
<tr class="ltx_tr" id="A1.T10.1.17.16">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.17.16.1">TidalDecode+L17</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.17.16.2">42%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.17.16.3">60%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.17.16.4">74%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.17.16.5">82%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.17.16.6">96%</td>
<td class="ltx_td ltx_align_center" id="A1.T10.1.17.16.7">96%</td>
</tr>
<tr class="ltx_tr" id="A1.T10.1.18.17">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.18.17.1">TidalDecode+L18</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.18.17.2">60%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.18.17.3">72%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.18.17.4">74%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.18.17.5">74%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.18.17.6">88%</td>
<td class="ltx_td ltx_align_center" id="A1.T10.1.18.17.7">98%</td>
</tr>
<tr class="ltx_tr" id="A1.T10.1.19.18">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.19.18.1">TidalDecode+L19</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.19.18.2">58%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.19.18.3">74%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.19.18.4">82%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.19.18.5">84%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.19.18.6"><span class="ltx_text ltx_font_bold" id="A1.T10.1.19.18.6.1">98%</span></td>
<td class="ltx_td ltx_align_center" id="A1.T10.1.19.18.7">96%</td>
</tr>
<tr class="ltx_tr" id="A1.T10.1.20.19">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.20.19.1">TidalDecode+L20</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.20.19.2">64%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.20.19.3">74%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.20.19.4"><span class="ltx_text ltx_font_bold" id="A1.T10.1.20.19.4.1">96%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.20.19.5">78%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.20.19.6">90%</td>
<td class="ltx_td ltx_align_center" id="A1.T10.1.20.19.7">98%</td>
</tr>
<tr class="ltx_tr" id="A1.T10.1.21.20">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.21.20.1">TidalDecode+L21</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.21.20.2">10%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.21.20.3">38%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.21.20.4">60%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.21.20.5">66%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.21.20.6">90%</td>
<td class="ltx_td ltx_align_center" id="A1.T10.1.21.20.7">94%</td>
</tr>
<tr class="ltx_tr" id="A1.T10.1.22.21">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.22.21.1">TidalDecode+L22</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.22.21.2">60%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.22.21.3">70%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.22.21.4">68%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.22.21.5">72%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.22.21.6">82%</td>
<td class="ltx_td ltx_align_center" id="A1.T10.1.22.21.7">98%</td>
</tr>
<tr class="ltx_tr" id="A1.T10.1.23.22">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.23.22.1">TidalDecode+L23</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.23.22.2">58%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.23.22.3">78%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.23.22.4">70%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.23.22.5">86%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.23.22.6">88%</td>
<td class="ltx_td ltx_align_center" id="A1.T10.1.23.22.7">98%</td>
</tr>
<tr class="ltx_tr" id="A1.T10.1.24.23">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.24.23.1">TidalDecode+L24</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.24.23.2">62%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.24.23.3">58%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.24.23.4">76%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.24.23.5">70%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.24.23.6">78%</td>
<td class="ltx_td ltx_align_center" id="A1.T10.1.24.23.7">92%</td>
</tr>
<tr class="ltx_tr" id="A1.T10.1.25.24">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.25.24.1">TidalDecode+L25</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.25.24.2">66%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.25.24.3">86%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.25.24.4">84%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.25.24.5">82%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.25.24.6">92%</td>
<td class="ltx_td ltx_align_center" id="A1.T10.1.25.24.7"><span class="ltx_text ltx_font_bold" id="A1.T10.1.25.24.7.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="A1.T10.1.26.25">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.26.25.1">TidalDecode+L26</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.26.25.2">54%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.26.25.3">64%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.26.25.4">66%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.26.25.5">80%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.26.25.6">90%</td>
<td class="ltx_td ltx_align_center" id="A1.T10.1.26.25.7">94%</td>
</tr>
<tr class="ltx_tr" id="A1.T10.1.27.26">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.27.26.1">TidalDecode+L27</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.27.26.2">84%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.27.26.3">80%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.27.26.4">94%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.27.26.5">96%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.27.26.6">88%</td>
<td class="ltx_td ltx_align_center" id="A1.T10.1.27.26.7"><span class="ltx_text ltx_font_bold" id="A1.T10.1.27.26.7.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="A1.T10.1.28.27">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.28.27.1">TidalDecode+L28</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.28.27.2">66%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.28.27.3">66%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.28.27.4">76%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.28.27.5">84%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.28.27.6"><span class="ltx_text ltx_font_bold" id="A1.T10.1.28.27.6.1">94%</span></td>
<td class="ltx_td ltx_align_center" id="A1.T10.1.28.27.7">94%</td>
</tr>
<tr class="ltx_tr" id="A1.T10.1.29.28">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.29.28.1">TidalDecode+L29</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.29.28.2">72%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.29.28.3">80%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.29.28.4">88%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.29.28.5">80%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.29.28.6">90%</td>
<td class="ltx_td ltx_align_center" id="A1.T10.1.29.28.7">96%</td>
</tr>
<tr class="ltx_tr" id="A1.T10.1.30.29">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.30.29.1">TidalDecode+L30</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.30.29.2">80%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.30.29.3"><span class="ltx_text ltx_font_bold" id="A1.T10.1.30.29.3.1">90%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.30.29.4">86%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.30.29.5">88%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T10.1.30.29.6">96%</td>
<td class="ltx_td ltx_align_center" id="A1.T10.1.30.29.7"><span class="ltx_text ltx_font_bold" id="A1.T10.1.30.29.7.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="A1.T10.1.31.30">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T10.1.31.30.1">TidalDecode+L31</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T10.1.31.30.2">74%</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T10.1.31.30.3">90%</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T10.1.31.30.4">88%</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T10.1.31.30.5">84%</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T10.1.31.30.6">90%</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T10.1.31.30.7">96%</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A1.T11">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 11: </span>Sensitivity study of re-selection layer (RL) on 3k-context-length Needle-in-the-Haystack test for LongChat-7b-v1.5-32k with TidalDecode. The best accuracy for each token budget (K) is in bold. Layer 7 serves the best re-selection layer for accuracy.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T11.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T11.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T11.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T11.1.1.1.1.1">RL/K</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T11.1.1.1.2">32</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T11.1.1.1.3">64</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T11.1.1.1.4">128</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T11.1.1.1.5">256</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T11.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T11.1.2.1.1">TidalDecode+L2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T11.1.2.1.2">2%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T11.1.2.1.3">2%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T11.1.2.1.4">6%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T11.1.2.1.5">54%</td>
</tr>
<tr class="ltx_tr" id="A1.T11.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.3.2.1">TidalDecode+L3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.3.2.2">10%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.3.2.3">52%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.3.2.4">67%</td>
<td class="ltx_td ltx_align_center" id="A1.T11.1.3.2.5">78%</td>
</tr>
<tr class="ltx_tr" id="A1.T11.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.4.3.1">TidalDecode+L4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.4.3.2">4%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.4.3.3">36%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.4.3.4">65%</td>
<td class="ltx_td ltx_align_center" id="A1.T11.1.4.3.5">76%</td>
</tr>
<tr class="ltx_tr" id="A1.T11.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.5.4.1">TidalDecode+L5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.5.4.2">17%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.5.4.3">87%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.5.4.4">94%</td>
<td class="ltx_td ltx_align_center" id="A1.T11.1.5.4.5">99%</td>
</tr>
<tr class="ltx_tr" id="A1.T11.1.6.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.6.5.1">TidalDecode+L6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.6.5.2">70%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.6.5.3">96%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.6.5.4">99%</td>
<td class="ltx_td ltx_align_center" id="A1.T11.1.6.5.5">99%</td>
</tr>
<tr class="ltx_tr" id="A1.T11.1.7.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.7.6.1">TidalDecode+L7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.7.6.2"><span class="ltx_text ltx_font_bold" id="A1.T11.1.7.6.2.1">80%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.7.6.3"><span class="ltx_text ltx_font_bold" id="A1.T11.1.7.6.3.1">98%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.7.6.4"><span class="ltx_text ltx_font_bold" id="A1.T11.1.7.6.4.1">100%</span></td>
<td class="ltx_td ltx_align_center" id="A1.T11.1.7.6.5"><span class="ltx_text ltx_font_bold" id="A1.T11.1.7.6.5.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="A1.T11.1.8.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.8.7.1">TidalDecode+L8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.8.7.2">58%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.8.7.3">82%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.8.7.4">96%</td>
<td class="ltx_td ltx_align_center" id="A1.T11.1.8.7.5">96%</td>
</tr>
<tr class="ltx_tr" id="A1.T11.1.9.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.9.8.1">TidalDecode+L9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.9.8.2">7%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.9.8.3">31%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.9.8.4">59%</td>
<td class="ltx_td ltx_align_center" id="A1.T11.1.9.8.5">71%</td>
</tr>
<tr class="ltx_tr" id="A1.T11.1.10.9">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.10.9.1">TidalDecode+L10</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.10.9.2">16%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.10.9.3">59%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.10.9.4">71%</td>
<td class="ltx_td ltx_align_center" id="A1.T11.1.10.9.5">78%</td>
</tr>
<tr class="ltx_tr" id="A1.T11.1.11.10">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.11.10.1">TidalDecode+L11</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.11.10.2">34%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.11.10.3">61%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.11.10.4">68%</td>
<td class="ltx_td ltx_align_center" id="A1.T11.1.11.10.5">77%</td>
</tr>
<tr class="ltx_tr" id="A1.T11.1.12.11">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.12.11.1">TidalDecode+L12</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.12.11.2">17%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.12.11.3">32%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.12.11.4">53%</td>
<td class="ltx_td ltx_align_center" id="A1.T11.1.12.11.5">77%</td>
</tr>
<tr class="ltx_tr" id="A1.T11.1.13.12">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.13.12.1">TidalDecode+L13</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.13.12.2">5%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.13.12.3">10%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.13.12.4">28%</td>
<td class="ltx_td ltx_align_center" id="A1.T11.1.13.12.5">48%</td>
</tr>
<tr class="ltx_tr" id="A1.T11.1.14.13">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.14.13.1">TidalDecode+L14</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.14.13.2">24%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.14.13.3">41%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.14.13.4">57%</td>
<td class="ltx_td ltx_align_center" id="A1.T11.1.14.13.5">64%</td>
</tr>
<tr class="ltx_tr" id="A1.T11.1.15.14">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.15.14.1">TidalDecode+L15</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.15.14.2">37%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.15.14.3">47%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.15.14.4">62%</td>
<td class="ltx_td ltx_align_center" id="A1.T11.1.15.14.5">69%</td>
</tr>
<tr class="ltx_tr" id="A1.T11.1.16.15">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.16.15.1">TidalDecode+L16</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.16.15.2">16%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.16.15.3">24%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.16.15.4">28%</td>
<td class="ltx_td ltx_align_center" id="A1.T11.1.16.15.5">46%</td>
</tr>
<tr class="ltx_tr" id="A1.T11.1.17.16">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.17.16.1">TidalDecode+L17</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.17.16.2">4%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.17.16.3">4%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.17.16.4">10%</td>
<td class="ltx_td ltx_align_center" id="A1.T11.1.17.16.5">34%</td>
</tr>
<tr class="ltx_tr" id="A1.T11.1.18.17">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.18.17.1">TidalDecode+L18</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.18.17.2">2%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.18.17.3">3%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.18.17.4">8%</td>
<td class="ltx_td ltx_align_center" id="A1.T11.1.18.17.5">15%</td>
</tr>
<tr class="ltx_tr" id="A1.T11.1.19.18">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.19.18.1">TidalDecode+L19</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.19.18.2">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.19.18.3">1%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.19.18.4">7%</td>
<td class="ltx_td ltx_align_center" id="A1.T11.1.19.18.5">19%</td>
</tr>
<tr class="ltx_tr" id="A1.T11.1.20.19">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.20.19.1">TidalDecode+L20</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.20.19.2">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.20.19.3">3%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.20.19.4">6%</td>
<td class="ltx_td ltx_align_center" id="A1.T11.1.20.19.5">20%</td>
</tr>
<tr class="ltx_tr" id="A1.T11.1.21.20">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.21.20.1">TidalDecode+L21</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.21.20.2">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.21.20.3">2%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.21.20.4">10%</td>
<td class="ltx_td ltx_align_center" id="A1.T11.1.21.20.5">19%</td>
</tr>
<tr class="ltx_tr" id="A1.T11.1.22.21">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.22.21.1">TidalDecode+L22</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.22.21.2">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.22.21.3">4%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.22.21.4">4%</td>
<td class="ltx_td ltx_align_center" id="A1.T11.1.22.21.5">18%</td>
</tr>
<tr class="ltx_tr" id="A1.T11.1.23.22">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.23.22.1">TidalDecode+L23</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.23.22.2">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.23.22.3">2%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.23.22.4">5%</td>
<td class="ltx_td ltx_align_center" id="A1.T11.1.23.22.5">13%</td>
</tr>
<tr class="ltx_tr" id="A1.T11.1.24.23">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.24.23.1">TidalDecode+L24</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.24.23.2">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.24.23.3">2%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.24.23.4">6%</td>
<td class="ltx_td ltx_align_center" id="A1.T11.1.24.23.5">21%</td>
</tr>
<tr class="ltx_tr" id="A1.T11.1.25.24">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.25.24.1">TidalDecode+L25</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.25.24.2">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.25.24.3">2%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.25.24.4">7%</td>
<td class="ltx_td ltx_align_center" id="A1.T11.1.25.24.5">16%</td>
</tr>
<tr class="ltx_tr" id="A1.T11.1.26.25">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.26.25.1">TidalDecode+L26</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.26.25.2">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.26.25.3">1%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.26.25.4">7%</td>
<td class="ltx_td ltx_align_center" id="A1.T11.1.26.25.5">19%</td>
</tr>
<tr class="ltx_tr" id="A1.T11.1.27.26">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.27.26.1">TidalDecode+L27</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.27.26.2">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.27.26.3">2%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.27.26.4">4%</td>
<td class="ltx_td ltx_align_center" id="A1.T11.1.27.26.5">21%</td>
</tr>
<tr class="ltx_tr" id="A1.T11.1.28.27">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.28.27.1">TidalDecode+L28</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.28.27.2">1%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.28.27.3">2%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.28.27.4">10%</td>
<td class="ltx_td ltx_align_center" id="A1.T11.1.28.27.5">17%</td>
</tr>
<tr class="ltx_tr" id="A1.T11.1.29.28">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.29.28.1">TidalDecode+L29</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.29.28.2">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.29.28.3">3%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.29.28.4">7%</td>
<td class="ltx_td ltx_align_center" id="A1.T11.1.29.28.5">16%</td>
</tr>
<tr class="ltx_tr" id="A1.T11.1.30.29">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.30.29.1">TidalDecode+L30</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.30.29.2">1%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.30.29.3">1%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T11.1.30.29.4">9%</td>
<td class="ltx_td ltx_align_center" id="A1.T11.1.30.29.5">15%</td>
</tr>
<tr class="ltx_tr" id="A1.T11.1.31.30">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T11.1.31.30.1">TidalDecode+L31</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T11.1.31.30.2">1%</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T11.1.31.30.3">2%</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T11.1.31.30.4">5%</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T11.1.31.30.5">16%</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A1.T12">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 12: </span>Sensitivity study of re-selection layer (RL) on 3k-context-length Needle-in-the-Haystack test for Yarn-Llama-2-7b-128k with TidalDecode. The best accuracy for each token budget (K) is in bold. Layer 7 serves the best re-selection layer for accuracy.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T12.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T12.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T12.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T12.1.1.1.1.1">RL/K</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T12.1.1.1.2">32</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T12.1.1.1.3">64</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T12.1.1.1.4">128</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T12.1.1.1.5">256</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T12.1.1.1.6">512</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T12.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T12.1.2.1.1">TidalDecode+L2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T12.1.2.1.2">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T12.1.2.1.3">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T12.1.2.1.4">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T12.1.2.1.5">3%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T12.1.2.1.6">25%</td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.3.2.1">TidalDecode+L3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.3.2.2">11%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.3.2.3">26%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.3.2.4">39%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.3.2.5">65%</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.3.2.6">85%</td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.4.3.1">TidalDecode+L4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.4.3.2">5%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.4.3.3">17%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.4.3.4">34%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.4.3.5">60%</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.4.3.6">92%</td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.5.4.1">TidalDecode+L5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.5.4.2">16%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.5.4.3">42%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.5.4.4">65%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.5.4.5">87%</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.5.4.6">96%</td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.6.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.6.5.1">TidalDecode+L6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.6.5.2">73%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.6.5.3">83%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.6.5.4">89%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.6.5.5">95%</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.6.5.6"><span class="ltx_text ltx_font_bold" id="A1.T12.1.6.5.6.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.7.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.7.6.1">TidalDecode+L7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.7.6.2">73%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.7.6.3"><span class="ltx_text ltx_font_bold" id="A1.T12.1.7.6.3.1">95%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.7.6.4"><span class="ltx_text ltx_font_bold" id="A1.T12.1.7.6.4.1">98%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.7.6.5"><span class="ltx_text ltx_font_bold" id="A1.T12.1.7.6.5.1">98%</span></td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.7.6.6"><span class="ltx_text ltx_font_bold" id="A1.T12.1.7.6.6.1">100%</span></td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.8.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.8.7.1">TidalDecode+L8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.8.7.2"><span class="ltx_text ltx_font_bold" id="A1.T12.1.8.7.2.1">87%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.8.7.3">92%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.8.7.4">97%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.8.7.5">94%</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.8.7.6">99%</td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.9.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.9.8.1">TidalDecode+L9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.9.8.2">7%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.9.8.3">21%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.9.8.4">43%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.9.8.5">60%</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.9.8.6">95%</td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.10.9">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.10.9.1">TidalDecode+L10</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.10.9.2">12%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.10.9.3">31%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.10.9.4">58%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.10.9.5">69%</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.10.9.6">93%</td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.11.10">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.11.10.1">TidalDecode+L11</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.11.10.2">20%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.11.10.3">21%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.11.10.4">46%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.11.10.5">68%</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.11.10.6">97%</td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.12.11">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.12.11.1">TidalDecode+L12</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.12.11.2">2%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.12.11.3">15%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.12.11.4">28%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.12.11.5">51%</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.12.11.6">92%</td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.13.12">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.13.12.1">TidalDecode+L13</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.13.12.2">4%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.13.12.3">5%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.13.12.4">20%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.13.12.5">34%</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.13.12.6">88%</td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.14.13">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.14.13.1">TidalDecode+L14</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.14.13.2">16%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.14.13.3">20%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.14.13.4">49%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.14.13.5">53%</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.14.13.6">91%</td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.15.14">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.15.14.1">TidalDecode+L15</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.15.14.2">2%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.15.14.3">25%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.15.14.4">44%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.15.14.5">56%</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.15.14.6">90%</td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.16.15">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.16.15.1">TidalDecode+L16</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.16.15.2">10%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.16.15.3">13%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.16.15.4">21%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.16.15.5">43%</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.16.15.6">86%</td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.17.16">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.17.16.1">TidalDecode+L17</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.17.16.2">3%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.17.16.3">4%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.17.16.4">9%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.17.16.5">16%</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.17.16.6">85%</td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.18.17">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.18.17.1">TidalDecode+L18</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.18.17.2">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.18.17.3">1%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.18.17.4">2%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.18.17.5">15%</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.18.17.6">84%</td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.19.18">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.19.18.1">TidalDecode+L19</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.19.18.2">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.19.18.3">2%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.19.18.4">3%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.19.18.5">7%</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.19.18.6">80%</td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.20.19">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.20.19.1">TidalDecode+L20</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.20.19.2">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.20.19.3">2%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.20.19.4">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.20.19.5">7%</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.20.19.6">79%</td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.21.20">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.21.20.1">TidalDecode+L21</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.21.20.2">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.21.20.3">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.21.20.4">1%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.21.20.5">5%</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.21.20.6">77%</td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.22.21">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.22.21.1">TidalDecode+L22</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.22.21.2">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.22.21.3">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.22.21.4">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.22.21.5">8%</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.22.21.6">76%</td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.23.22">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.23.22.1">TidalDecode+L23</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.23.22.2">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.23.22.3">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.23.22.4">1%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.23.22.5">7%</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.23.22.6">74%</td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.24.23">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.24.23.1">TidalDecode+L24</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.24.23.2">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.24.23.3">2%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.24.23.4">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.24.23.5">9%</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.24.23.6">73%</td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.25.24">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.25.24.1">TidalDecode+L25</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.25.24.2">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.25.24.3">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.25.24.4">2%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.25.24.5">10%</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.25.24.6">71%</td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.26.25">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.26.25.1">TidalDecode+L26</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.26.25.2">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.26.25.3">1%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.26.25.4">1%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.26.25.5">5%</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.26.25.6">70%</td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.27.26">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.27.26.1">TidalDecode+L27</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.27.26.2">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.27.26.3">1%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.27.26.4">3%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.27.26.5">8%</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.27.26.6">68%</td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.28.27">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.28.27.1">TidalDecode+L28</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.28.27.2">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.28.27.3">1%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.28.27.4">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.28.27.5">9%</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.28.27.6">67%</td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.29.28">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.29.28.1">TidalDecode+L29</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.29.28.2">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.29.28.3">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.29.28.4">2%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.29.28.5">8%</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.29.28.6">65%</td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.30.29">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.30.29.1">TidalDecode+L30</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.30.29.2">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.30.29.3">1%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.30.29.4">1%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T12.1.30.29.5">8%</td>
<td class="ltx_td ltx_align_center" id="A1.T12.1.30.29.6">64%</td>
</tr>
<tr class="ltx_tr" id="A1.T12.1.31.30">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T12.1.31.30.1">TidalDecode+L31</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T12.1.31.30.2">0%</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T12.1.31.30.3">0%</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T12.1.31.30.4">2%</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T12.1.31.30.5">10%</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A1.T12.1.31.30.6">62%</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>

</div>


<button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button><div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer></body></html>